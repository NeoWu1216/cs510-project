<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Active Image Segmentation Propagation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suyog</forename><surname>Dutt</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Texas at Austin</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jain</forename><forename type="middle">Kristen</forename><surname>Grauman</surname></persName>
							<email>grauman@cs.utexas.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Texas at Austin</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Active Image Segmentation Propagation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:37+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a semi-automatic method to obtain foreground object masks for a large set of related images. We develop a stagewise active approach to propagation: in each stage, we actively determine the images that appear most valuable for human annotation, then revise the foreground estimates in all unlabeled images accordingly. In order to identify images that, once annotated, will propagate well to other examples, we introduce an active selection procedure that operates on the joint segmentation graph over all images. It prioritizes human intervention for those images that are uncertain and influential in the graph, while also mutually diverse. We apply our method to obtain foreground masks for over 1 million images. Our method yields state-of-the-art accuracy on the ImageNet and MIT Object Discovery datasets, and it focuses human attention more effectively than existing propagation strategies.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Large-scale labeled image datasets have had a transformative impact on computer vision in recent years, most notably for image classification. However, image annotation remains a costly undertaking in terms of both time and money. In particular, gathering high quality spatial annotations-pixel-level foreground masks-is challenging. First of all, the physical mousing actions required are time intensive (e.g., compared to simply labeling which object is present). Furthermore, non-expert annotators exhibit inconsistencies in how precisely they mark object boundaries, which means leveraging the crowd typically requires some finessing and "re-dos".</p><p>As a result, datasets with spatial annotations lag seriously behind their category-labeled counterparts. For example, while ImageNet is comprised of an impressive 14M labeled images, there are orders of magnitude fewer spatial annotations-only 1M images (7% of the dataset) offer bounding box annotations, and only 4K images (0.03%) have foreground segmentation masks <ref type="bibr" target="#b11">[12]</ref>. While the new Microsoft COCO dataset <ref type="bibr" target="#b26">[27]</ref> has spatial annotations for 2.5M object instances, these were obtained only by investing staggering amounts of time and money; at over 22 person hours per 1,000 segmentations <ref type="bibr" target="#b26">[27]</ref>, that's more than $400,000 if paying minimum wage.</p><p>The difficulty in generating foreground spatial annotations for image collections is problematic given their high potential utility. For example, they are useful to build training sets for region-based object detectors, or improve image retrieval by focusing on the region of interest. Aside from serving as training data, there are also applications that directly use a well-segmented image database, such as datadriven image synthesis or retargeting.</p><p>Aware of this need, researchers have developed an array of weakly supervised segmentation algorithms <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b11">12]</ref>. The main idea is to take a pool of images known to contain the same object category, and exploit the repeated patterns to jointly segment out the foreground per image. On the one hand, this paradigm is attractive for its low manual effort, especially since such weakly labeled images are readily available on the Web via keyword search. On the other hand, the resulting segmentations are imperfect. No matter the method, the foreground masks hit a ceiling of accuracy since the segmentation task is underconstrained even with weak supervision.</p><p>We propose an intermediate solution.</p><p>Rather than rely solely on human-provided segmentations (accurate but too expensive) or automatic segmentations (inexpensive but too inaccurate), we develop a semi-automatic segmentation propagation approach. The idea is to actively request human annotations for select images that, once labeled with their foreground, are most expected to help co-segment the remaining unlabeled images. The propagation engine proceeds in stages, each time (1) using the recently annotated images to revise foreground estimates in all unlabeled images, and (2) using those results to determine the next best batch of images to present to human annotators. In this way, we neither restrict ourselves to the saturation point of the fully automatic methods, nor do we get large volumes of data labeled by humans (see <ref type="figure" target="#fig_0">Figure 1</ref>).</p><p>To achieve this goal, we develop an active selection approach tailored to foreground propagation. It operates on a graph constructed over all images in the collection. Our active selection process favors choosing images that are uncertain-poorly explained by any images labeled so far, as well as influential-similar to many unlabeled images, making their foreground mask transferrable-and mutually diverse-so as to avoid redundant human effort. A critical part of our method design is its stagewise propagation, which permits both human-annotated and automatically annotated images to influence the system's view of what most needs human attention next.</p><p>Our framework differs in important ways from existing work on both active learning and segmentation propagation. Active learning methods for recognition aim to train a model that will make accurate category label predictions on unseen test images (e.g., <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b42">43]</ref>). In contrast, our goal is to get all available images spatially annotated by semiautomatic propagation (i.e., ours is a transductive setting). Whereas active learning iteratively refines a single classifier, our method iteratively refines the foreground estimates for a collection of images. There is very limited prior work on segmentation propagation, and existing methods are either passive <ref type="bibr" target="#b11">[12]</ref> or only select annotations to initialize the algorithm <ref type="bibr" target="#b35">[36]</ref>. A key insight of our technical approach is to repeatedly analyze the current segmentations (both humanand algorithm-provided) to actively decide on subsequent annotations.</p><p>Our main contributions are (1) a scalable approach for semi-automatic segmentation propagation in image collections, (2) a stage-wise active selection algorithm to determine the images which appear most valuable for human annotation and (3) a large-scale empirical demonstration that actively allocating human effort can lead to substantial savings in annotation costs for the segmentation problem.</p><p>We evaluate our approach on ImageNet <ref type="bibr" target="#b36">[37]</ref> and the MIT Object Discovery <ref type="bibr" target="#b34">[35]</ref> dataset. Applying our method to more than 1 million images, we show that intelligently focusing human effort leads to significantly better foreground extraction. As a secondary result, we show that in its fully automatic form, our model produces state-of-the-art foreground segmentation accuracy on these widely used datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Fully supervised and unsupervised methods Current segmentation methods can be organized according to the human supervision they assume. One extreme consists of strongly supervised semantic segmentation methods (e.g., <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b46">47]</ref>), which train object models from manually segmented multi-label images. Such methods demand substantial labeled data for training, which could be more efficiently acquired with the help of our approach. The other extreme consists of fully unsupervised methods that use unlabeled images to discover object categories (e.g., <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b25">26]</ref>). Whereas ambiguity about the object(s) of interest poses a significant challenge for those methods, we work in the "weakly labeled" setting.</p><p>Weakly supervised foreground segmentation Our work is more related to weakly supervised methods, which aim to segment the foreground object(s) while exploiting the fact that all input images contain instances of the same object category <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b8">9]</ref>. <ref type="bibr" target="#b0">1</ref> Depending on the method, the output segmentation might be pixel-level masks <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b34">35]</ref> or bounding boxes <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b41">42]</ref>. Recent advances include ways to accommodate noisily labeled inputs <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b41">42]</ref>, multi-class data <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b19">20]</ref>, and object proposal regions <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b0">1]</ref>. While typically the entire weakly labeled set is treated as a whole, some methods aim to limit the influence of co-segmentation to closely related images <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b15">16]</ref>.</p><p>Our basic co-segmentation engine builds on this rich body of work, with refinements that (as we will see in results) improve the state-of-the-art when applied without any manual foreground labels. In particular, our idea for selecting and fusing multiple region proposals per image offers important advantages. More importantly, active segmentation propagation is new; the existing weakly supervised methods above use no human intervention.</p><p>Segmentation propagation Most closely related to our work are methods for segmentation propagation, which use labeled seeds to propagate foreground masks to other images in the weakly labeled set <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b11">12]</ref>. Our method has two key novel aspects. First, we actively select which images should next receive foreground labels from human annotators. In contrast, existing methods are either opportunistic (and hence passive) about the labeled seeds, using only existing labeled data <ref type="bibr" target="#b11">[12]</ref>, or else select them in a one-shot manner without reacting to the impact of previously annotated examples <ref type="bibr" target="#b35">[36]</ref>. Second, our stagewise procedure constantly re-evaluates the impact of new labels, revising the current foreground estimates on all images. In contrast, <ref type="bibr" target="#b11">[12]</ref> assumes that propagation will proceed best among the closest semantically related classes in an external object hierarchy (ImageNet), and <ref type="bibr" target="#b35">[36]</ref> assumes that propagation will proceed best among each image's GIST neighbors. Empirically, our approach compares favorably to both existing propagation methods (cf. Section 4).</p><p>Interactive segmentation Another way to make image segmentation semi-automatic is to let a human guide the segmentation of an individual image. This is the concept behind the popular GrabCut <ref type="bibr" target="#b33">[34]</ref> method: a user's bounding box coarsely localizes the foreground, and the system completes the pixel-level mask. Recent work considers how to guide a user to regions where a "scribble" would be most valuable <ref type="bibr" target="#b5">[6]</ref>, or predict which type of input (bounding box, contour) is best suited for an image <ref type="bibr" target="#b14">[15]</ref>. Our method also aims to intelligently engage annotators, but our objective is to segment an entire batch of images based on minimal manual foreground masks. Whereas existing work <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b14">15]</ref> considers uncertainty only within individual images, our method reasons about the image collection as a whole.</p><p>Active learning with images Active learning has been explored for object recognition and image classification <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b3">4]</ref>. The goal is to focus human labels on those images that will most reduce the uncertainty of the classifier, such that it can generalize well to novel images. Selection strategies include reducing the classifier's expected error <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b3">4]</ref> or maximizing the diversity among the selected images <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b10">11]</ref>. As discussed above, all such methods are closely coupled to their classifier of interest, and they aim to find good images to label by category. This is the case even for those that operate on image regions <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b42">43]</ref>. In contrast, our task is to select images from which segmentation will propagate well, and we aim to find good images to annotate with foreground masks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach</head><p>Let I = {I 1 , I 2 , . . . , I N } be a collection of weakly supervised images, all of which contain instances of the same object category. Our goal is to jointly segment these images, yielding a foreground object mask M = {M 1 , M 2 , . . . , M N } for each one.</p><p>We first describe the regions and descriptors we use to construct the image graph (Sec. 3.1). Then we define our joint segmentation procedure to simultaneously solve for all foreground masks, given foreground annotations on only a subset of the images (Sec. 3.2). Finally, we introduce our active procedure for identifying the set of images that should be annotated next (Sec. 3.3). <ref type="figure" target="#fig_1">Figure 2</ref> visually illustrates all the steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Region proposals and descriptors</head><p>We define our segmentation graph over region proposals. Region proposals are "object-like" segments that are prioritized among all bottom-up regions as those being most likely to agree with true object boundaries <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b4">5]</ref>. We assume that at least some of them capture the foreground object well-and possibly more than one per image. Thus, the goal of our joint segmentation procedure will be to identify the subset of region proposals that are good, and fuse them to obtain the final segmentation (see Sec. 3.2 for details). Apart from being more efficient than traditional pixel-based graphs (e.g., <ref type="bibr" target="#b34">[35]</ref>), we show that a region-based representation lets us define strong pairwise consistency potentials based on regions matched across images.</p><p>Existing region proposal methods typically produce ∼ 500 regions per image, a large sample that may include redundant candidates and background objects. To refine the set of proposals, we develop the following filtering steps. First we generate the generic object proposals and compute a saliency map using <ref type="bibr" target="#b17">[18]</ref>. Next we obtain two ranked lists of these proposals using saliency and objectness scores <ref type="bibr" target="#b7">[8]</ref>, respectively. We retain the union of the top 30% from each list. Then, we cluster the reduced set into r clusters. To capture shape and spatial alignment, respectively, we use the regions' HOG similarity and spatial overlap (IoU metric), and cluster with k-medoids. The r cluster centers (typically r=10) form the final set of proposals for each image. We found that this careful filtering was much more accurate than constraining the number of region proposals using the objectness scores directly. For example, on the MIT dataset our filtering step results in a mean average best score (MABO) of 72.2 with only 10 proposals. In contrast, simply retaining the top 10 proposals using scores from <ref type="bibr" target="#b7">[8]</ref> results in a MABO of 64.95. The clustering step selects diverse proposals, leading to higher recall with fewer proposals.</p><p>Let R = {R ij } denote the set of all region proposals in all N images, where R ij denotes the j-th region for image I i . Our joint segmentation approach, to be defined next, relies on both image-and region-level features. For each image I i , we extract a global appearance descriptor denoted I c i . For each region R ij , we extract two features: a saliency rating R s ij , and a region appearance descriptor R c ij . 2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Semi-automatic joint foreground segmentation</head><p>We define a Markov Random Field (MRF) joint segmentation graph G = (R, E) based on the filtered region proposals across all images in the collection. Each region R ij ∈ R forms a node and the edges E connect pairs of regions. During segmentation, the edges will encourage consistent labels for similar regions, while the nodes will encourage (1) Joint segmentation propagation: Given a set of images {I 1 , I 2 , I 3 , I 4 } with I 2 already segmented by a human, the goal is to generate foreground segmentations for the remaining images. We first generate a set of filtered region proposals for each image. Next, a joint segmentation graph over these region proposals (edges = region similarity) is defined. An energy function defined over this graph is minimized to obtain a set of good proposals for each image, which are then fused to obtain the final segmentation. (2) Active human annotation: Our active selection method works over a joint graph defined over all images in the collection (darker edges = high similarity). These pairwise similarities allow us to identify influential images (most useful for others) and also help in enforcing diversity in selection (to avoid redundancy). We also account for uncertainty (not depicted here) by predicting the quality of the current segmentation. Example selections by our method are shown in pink. Best viewed in color.</p><p>foreground labels for salient regions that are consistent with well-segmented exemplars. We keep a sparse set of edges E by only connecting regions whose similarity exceeds a threshold τ . No edges connect regions in the same image.</p><p>Let Y = {Y ij } be a set of binary region labels, where:</p><formula xml:id="formula_0">Y ij = 1 if proposal R ij is a good segmentation for I i 0 otherwise.</formula><p>(1) Let S ⊆ I denote the current subset of images labeled with foreground masks by human annotators. (We explain in Sec. 3.3 how the composition of this set is iteratively and actively defined.) Once an image I s has been labeled, meaning it first appears in S, we adjust the graph accordingly. First, we replace all nodes R sj by the single mask region given by the human annotator, denotedR s , and we clamp its label Y s = 1. Then, we modify the edge set E appropriately, such that in image s, only the maskR s has edges to similar regions in unlabeled images. <ref type="bibr" target="#b2">3</ref> These updates inject the human-labeled regions into the segmentation pipeline, allowing us to propagate the valuable information through the pairwise terms (defined below).</p><p>There are several ways to use the human-labeled masks to guide the joint segmentation. One could use them to train a foreground appearance model (e.g., as in iCoseg <ref type="bibr" target="#b5">[6]</ref>). However, this is most effective only in the stricter cosegmentation setting where the same exact foreground object instance repeats across images. An alternative could be to directly transfer the segmentation from labeled images to unlabeled images, e.g., using dense matching <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b46">47]</ref>. However, due to variations in scale and shape of foreground objects, global alignment is difficult in many cases.</p><p>Instead, our approach relies on strong matches discov-ered between foreground regions in human-labeled images and region proposals in unlabeled images. The intuition is that a good region proposal (i.e., one close to the actual foreground object segment) will strongly match a humanlabeled ground truth region. On the contrary, a bad proposal will have weaker matches. We define the following energy function E(Y) for jointly segmenting the image collection I:</p><formula xml:id="formula_1">E(Y) = Rij − log Φ(Y ij ) + Rij ,R ′ ij ∈E Ψ(Y ij , Y ′ ij ). (2)</formula><p>The unary term is defined as</p><formula xml:id="formula_2">Φ(Y ij ) = Y ij if i ∈ S α s Φ s (Y ij ) + α m Φ m (Y ij ) if i ∈ I\S.</formula><p>This unary prefers to label as foreground those regions that are (1) salient and/or (2) form a good match with some previously labeled foreground mask. The variables α s and α m weight the influence of the saliency and matching terms, respectively. The saliency term is defined using the saliency region feature (R s ij ) as:</p><formula xml:id="formula_3">Φ s (Y ij ) = Y ij R s ij + (1 − Y ij )(1 − R s ij ),<label>(3)</label></formula><p>so that we favor assigning Y ij = 1 if R ij is very salient.</p><p>The match component of the unary term encodes that a region proposal with a good ground truth region match is likely foreground. In particular, we identify matches for a region by considering its "local neighborhood" of images in the graph. For each unlabeled image I i , we retrieve its p nearest neighbors from the labeled set S using the imagelevel features I c i . Denote that set N (I i , S). Then, for each region proposal R ij , we find the best matching ground truth foreground region among these p neighbors, and use this matching score in the unary term:</p><formula xml:id="formula_4">Φ m (Y ij ) = Y ij R m ij + (1 − Y ij )(1 − R m ij ), where<label>(4)</label></formula><formula xml:id="formula_5">R m ij = max p∈N (Ii,S) sim(R c ij ,R c p ),<label>(5)</label></formula><p>and sim is the cosine similarity, andR p denotes the p-th ground truth region. The pairwise term in Eq (2) encourages similar-looking regions to take the same label:</p><formula xml:id="formula_6">Ψ(Y ij , Y ′ ij ) = δ(Y ij = Y ′ ij ) sim(R c ij , R ′ c ij ).<label>(6)</label></formula><p>This term enforces consistency in our joint selection of good region proposals, since we incur a penalty proportional to region similarity if the two regions receive different labels. The minimum energy solution Y * = arg min Y E(Y) yields a set of good region proposals for each image in the collection. Note that we do not constrain only one proposal to be selected per image. We purposely allow selecting multiple good regions per image, for two reasons. First, an image can naturally have multiple good region proposals (e.g. covering different object parts). As we will see next, our fusion step can take these multiple partial proposals to obtain a single accurate segmentation. Second, it allows us to efficiently and exactly minimize our energy function using graph-cuts <ref type="bibr" target="#b6">[7]</ref>. We found this works much better in practice than approximate inference techniques. A complete round of propagation for N = 1, 400 images takes just 1 minute on a single CPU (excluding feature extraction). In contrast, the state-of-the-art propagation method of <ref type="bibr" target="#b35">[36]</ref> would take 225 hours to propagate labels (excluding both feature extraction and SIFT-Flow).</p><p>To obtain the final segmentation mask M i , we fuse the chosen good region proposals Y * i . We use the selected regions as a rough prior for the object's spatial extent, and then use that to build an image-specific foreground appearance model. Specifically, for each chosen proposal in I i we retrieve the p nearest human-labeled masks. We transfer those masks into I i (we use simple resizing and transfer, similar to <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b21">22]</ref>), average the transferred masks of all proposals, and mean threshold the result to obtain a spatial prior. We then build a GMM over RGB color values for all pixels in the spatial prior. Finally, the combined appearance and spatial prior are used to define an image-specific MRF, which is minimized using graph cuts to obtain M i .</p><p>In summary, our semi-supervised segmentation propagation algorithm is designed to be accurate (through careful filtering of regions and use of sparse actively chosen human annotations) and efficient (by avoiding expensive dense matching steps <ref type="bibr" target="#b34">[35]</ref> and by using an efficient graph cuts energy minimization framework instead of costly approximate inference techniques as in <ref type="bibr" target="#b9">[10]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Active selection for propagation</head><p>We now describe our stagewise algorithm to actively select images for annotation. The active selection procedure takes as input the image collection I, an annotation budget k specifying the number of images to get labeled per stage, and the number of total annotation stages T . In each stage t, we solicit annotations for the actively chosen batch S t , augment S with that newly labeled data (S ← S ∪ S t ), and propagate the segmentation as described above. The output after T rounds is the resulting propagated masks M on all images. Note that throughout the stages, each unlabeled mask is continually refined, and its intermediate results affect subsequent stages' active selections.</p><p>Our active selection algorithm accounts for three criteria-influence, diversity, and uncertainty. The former two criteria account for relationships between images that are relevant to propagation, while the latter accounts for the inherent difficulty of individual images.</p><p>An image influential for propagation is similar to many other images in the collection. Intuitively, labeling such a "hub" image can directly improve the mask quality of the related images, particularly given our match-based unaries and localized image neighborhoods (Eq (5) and Eq <ref type="formula" target="#formula_6">(6)</ref>). We measure the influence of a candidate batch S t as:</p><formula xml:id="formula_7">INFLUENCE(S t ) = 1 |S ′ t | Ii∈St Ij ∈S ′ t sim(I c i , I c j ),<label>(7)</label></formula><p>where S ′ t denotes all unlabeled images not in the candidate batch S t and sim is the cosine similarity.</p><p>A batch of images that are diverse ensures broad coverage over the entire collection. Selecting images which are influential but also very similar would not lead to a large information gain. Hence, we also add a penalty for selecting mutually similar images:</p><formula xml:id="formula_8">DIVERSITY(S t ) = − 1 |S t | Ii∈St Ij ∈St sim(I c i , I c j ). (8)</formula><p>An image that is uncertain-inherently difficult to segment automatically-is also a good candidate for human supervision. We quantify the uncertainty of a batch as:</p><formula xml:id="formula_9">UNCERTAINTY(S t ) = 1 |S t | Ii∈St D(M i ),<label>(9)</label></formula><p>where D(·) is a learned predictor of image difficulty. This prediction function is trained to infer when an image is badly segmented. Taking inspiration from prior work <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b14">15]</ref>, we devise a set of descriptors suggestive of segmentation quality, and train a regression function using images for which we know each region's overlap with the true foreground. Given a region, the predictor returns its expected normalized overlap with the ground truth. Specifically, we train a random forest regressor using 1,385 images from the MSRC <ref type="bibr" target="#b28">[29]</ref>, iCoseg <ref type="bibr" target="#b5">[6]</ref>, and IIS <ref type="bibr" target="#b12">[13]</ref> datasets. The regression target is the overlap score with ground truth. To generate training samples, we sample CPMC <ref type="bibr" target="#b7">[8]</ref> region proposals whose overlap falls in the top and bottom 5% of all proposals. We use the following features as indicators of segmentation quality: (1) boundary alignment between the input region and superpixel boundaries, (2) the number of connected components, which reflects segment coherence, (3) color separability of the region from the background based on χ 2 distance on RGB histograms, and (4) region compactness, as measured by the ratios of the region's area to its tight bounding box and its convex hull. See Supp. for details.</p><p>We would like to identify the set maximizing all three criteria simultaneously. This is a combinatorial problem over all subsets S t ⊆ I and impractical to solve optimally. We instead employ a greedy approach to account for all factors. First, we extract the K &gt; k most uncertain unlabeled images, as judged using the predictor D(M i ) applied to the current mask estimated at the end of the previous stage. From among that pool, we select the subset S t , accounting for both influence and diversity. Starting with an empty set, we iteratively add an image at a time until we reach the budget k. The selected image is the one giving the maximal marginal increase for INFLUENCE(S t ) + DIVERSITY(S t ). See Algorithm 1 for complete pseudocode.</p><p>Our greedy algorithm is inspired by the maximization procedure typically used for monotone submodular functions, which offers theoretical gurantees <ref type="bibr" target="#b22">[23]</ref>. Due to the diversity penalty, our objective is non-monotonic, hence known approximation guarantees do not apply; nonetheless, it works well in practice. It is also fast: for a pool of 1,400 unlabeled images, our active selection requires just seconds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results</head><p>Datasets: We evaluate on two datasets:</p><p>• ImageNet: We conduct a large-scale evaluation of our approach using ImageNet <ref type="bibr" target="#b36">[37]</ref> (∼1M images, 3,624 classes). We follow the setup of <ref type="bibr" target="#b41">[42]</ref>, and consider all images with bounding box annotations available. <ref type="bibr" target="#b3">4</ref> • MIT Object Discovery: This challenging dataset consists of Airplanes, Cars and Horses <ref type="bibr" target="#b34">[35]</ref>. Its intra-class appearance variation is much greater than that of older co-segmentation datasets (MSRC <ref type="bibr" target="#b28">[29]</ref> or iCoseg <ref type="bibr" target="#b5">[6]</ref>).</p><p>Baselines: Apart from an ablated version of our method (i.e., w/o uncertainty), we compare with these baselines:</p><p>• Passive: This is a simple passive baseline where at every stage, we randomly pick k images from the unlabeled set to be labeled by humans.</p><p>• PageRank Selection <ref type="bibr" target="#b35">[36]</ref>: This is the only active propagation method in the literature, making it critical for comparison. It uses PageRank importance ranking and clustering to pick k good images at each stage.</p><p>• Semantic Propagation <ref type="bibr" target="#b11">[12]</ref>: An existing propagation method that promotes propagation between semantically related classes. It seeds the propagation with labeled images from existing datasets.</p><p>• State-of-the art weakly supervised methods: We compare the special case of our method (only weak supervision) with several existing approaches <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b41">42]</ref>. Other weakly supervised methods <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32]</ref> for semantic segmentation consider multi-label data, and so are not directly comparable.</p><p>Evaluation metrics: We use: (1) Jaccard Score: Standard intersection-over-union (IoU) metric between predicted and ground truth segmentation masks (for MIT) and between bounding boxes (for ImageNet), and (2) CorLoc Score: Percentage of images correctly localized according to PAS-CAL criterion (i.e IoU &gt; 0.5) used in <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b9">10]</ref>. For MIT we use the segmentation masks (Seg-CorLoc) and for Ima-geNet we use bounding boxes (BBox-CorLoc) since it lacks ground truth masks. Implementation details: We generate region proposals for MIT using CPMC <ref type="bibr" target="#b7">[8]</ref> and for ImageNet using MCG <ref type="bibr" target="#b4">[5]</ref> (due to efficiency). For global appearance I c i , we extract 4096-dim Convolutional Neural Network (CNN) features <ref type="bibr" target="#b23">[24]</ref> using Caffe <ref type="bibr" target="#b16">[17]</ref>. For saliency R s ij , we average the region's pixel-level saliency values from <ref type="bibr" target="#b17">[18]</ref>. For region appearance R c ij , we extract a CNN feature for the region's tight bounding box. We set: τ = 0.7, p = 5, α s = α m = 0.5, # rounds T = 20, k = ( # images/T ), K = 4 * k. All parameters were set after manual inspection of few images, then fixed for all experiments. In all experiments human annotation is simulated using ground truth data. Our run-time Passive PageRank <ref type="bibr" target="#b35">[36]</ref> Ours w/o uncertainty Ours  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Active segmentation propagation</head><p>First we present results for active selection. In this setting we iteratively request annotators to provide true segmentations for a subset of images. We then use these labeled images to improve the joint segmentation of other unlabeled images in the collection. <ref type="figure" target="#fig_4">Figure 5</ref> shows qualitative examples of annotation choices made by our active selection algorithm. We find the impact of all the components quite visible in the choices. Several influential and diverse images which provide good coverage over the collection are chosen, along with some relatively difficult and unusual ones. <ref type="figure">Figures 3 and 4</ref> show the quantitative results. On the extreme left, we have the performance of the purely weakly supervised setting (no human input) and on the extreme right, annotators provide ground-truth segmentations for all images in the collection. In between we see the trade-off between actively allocating human effort versus other baselines. Since this is a transductive setting where the goal is to generate segmentations for all images, we plot average results over all the images in the collection (whether human or computer segmented). This scoring protocol has an additional advantage of averaging over the same number of images after each round of annotation, making trends on the x-axis easy to interpret.</p><p>For all metrics and datasets, the proposed approach outperforms all baselines. While all methods naturally improve with more labeled data, the slope of our improvement curve is substantially sharper using minimal human effortsometimes dramatically so (e.g., Jetliner on ImageNet or Airplane on MIT). It is important to note that all methods are using identical CNN features and the same propagation algorithm, hence our gains exactly show the impact of making wiser annotation choices.</p><p>Surprisingly, we find that the Passive baseline outperforms the active PageRank method employed in <ref type="bibr" target="#b35">[36]</ref>. We believe this is because PageRank emphasizes the influence property more, and, despite its clustering component, fails to select sufficiently diverse examples 5 (in <ref type="bibr" target="#b35">[36]</ref> no comparison with a passive baseline is shown). On the other hand, our method takes into account influence, diversity, and uncertainty to choose good candidates for annotation. This leads to better annotation choices and in turn better propagation. We also see that omitting uncertainty from our approach decreases accuracy, showing the value of this segmentation-specific active selection component.</p><p>While all methods fare better on the "easier" task of localization (vs estimating pixel-perfect masks), our gains are actually substantially higher for localization (as measured by Seg-CorLoc and BBox-CorLoc). In addition, for both datasets, our gains are much higher for larger collections (&gt; 100 images). Larger collections exhibit both greater redun-dancy as well as several modes within the data. Our method successfully exploits these patterns while making annotation choices. For example, for MIT "Airplanes", we correctly localize 90% of the images with only 30% of the data labeled by annotators. In contrast, the Passive and active PageRank baselines require significantly more annotations (55% and 70%, respectively) to achieve the same accuracy. <ref type="figure">Figure 3</ref> also shows an interesting failure case for the ImageNet "Animal" class. Upon inspection, we found that it contains images from several different animal types with very little structural similarity; in this case, our active annotation method did not fare any better than the baselines.</p><p>We stress that, to our knowledge, <ref type="bibr" target="#b35">[36]</ref> represents the only prior attempt to incorporate active selection with segmentation propagation. Before any inference, that method seeds a dense-flow graph with images chosen with a PageRank sampling. Our stage-wise method takes a very different strategy, iteratively self-inspecting its own estimates and redirecting human attention accordingly. As seen in <ref type="figure">Figure  3</ref> &amp; 4, our approach significantly outperforms the one-shot PageRank approach <ref type="bibr" target="#b35">[36]</ref> in all experiments, and our propagation method is orders of magnitude faster (cf. Sec 3.2).</p><p>We also compare with the other state of the art segmentation propagation approach from Guillaumin et al. <ref type="bibr" target="#b11">[12]</ref>. For this, we consider all images which are common between our experimental setup and that of <ref type="bibr" target="#b11">[12]</ref>. This gives us a total of 99,020 images across 352 ImageNet classes. From the data provided by the authors, we found that groundtruth bounding boxes for 67,029 of those images were used to seed the propagation in <ref type="bibr" target="#b11">[12]</ref>. For the same amount of labeled data our active segmentation propagation approach achieves a Jaccard score of 65% as opposed to 62.63% by <ref type="bibr" target="#b11">[12]</ref>. More importantly, reducing the supervision budget for our method, we achieve the same accuracy as this (passive) state of the art propagation method <ref type="bibr" target="#b11">[12]</ref> when using 26% less human-annotated data. This large savings in human effort shows the clear value of actively determining where human guidance is most needed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Weakly supervised foreground segmentation</head><p>Next we test our method in a purely weakly supervised setting against several existing methods. In this special case, weak supervision (i.e., all images have an object from the same category) is the only information available. No additional human annotation is requested. This corresponds to setting S = ∅, α s = 1 and α m = 0. <ref type="table">Table 1</ref> compares our approach to several existing methods <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21]</ref> on the MIT (subset from <ref type="bibr" target="#b34">[35]</ref> and full) dataset. Our approach outperforms all existing methods in 4 out of 6 cases and has consistently good accuracy in all cases. This is really encouraging because our joint segmentation model is simpler and more efficient than existing methods (e.g <ref type="bibr" target="#b34">[35]</ref> uses dense matching, <ref type="bibr" target="#b8">[9]</ref> uses neg-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>MIT dataset (subset) MIT dataset (full) Airplane Car Horse Airplane Car Horse # Images 82 89 93 470 1208 810 Joulin et al. <ref type="bibr" target="#b18">[19]</ref> 15.36 37.15 30.16 n/a n/a n/a Joulin et al. <ref type="bibr" target="#b19">[20]</ref> 11.72 35.15 29.53 n/a n/a n/a Kim et al. <ref type="bibr" target="#b20">[21]</ref> 7.9 0.04 6.43 n/a n/a n/a Rubinstein et al. <ref type="bibr" target="#b34">[35]</ref> 55  <ref type="table">Table 1</ref>: Comparison with state-of-the-art methods on MIT dataset for weakly supervised joint foreground segmentation (Metric: Jaccard score).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ImageNet dataset # Classes</head><p># Images 3,624 939,516</p><p>Methods BBox-CorLoc Top obj. box <ref type="bibr" target="#b2">[3]</ref> 37.42 Tang et al. <ref type="bibr" target="#b41">[42]</ref> 53.20 Ours 57.64 ative training data to train detectors). The key strengths of our propagation design lie in carefully selecting region proposals that have good coverage over the objects and are not redundant (without this we see a 8% drop in performance on average, see Supp. for details), combined with the region-based matching potentials. Jointly selecting good region proposals then helps in discovering similar pattern configurations over the entire collection. The method of <ref type="bibr" target="#b8">[9]</ref> possibly benefits from stronger discriminative exemplarappearance models for the Horse class in MIT (full). <ref type="table" target="#tab_1">Table 2</ref> shows results on ImageNet. The "Top obj" baseline is the result of taking the top Objectness window <ref type="bibr" target="#b2">[3]</ref>, as reported in <ref type="bibr" target="#b41">[42]</ref>. Our method outperforms the state of the art <ref type="bibr" target="#b41">[42]</ref> by a considerable margin, which again highlights the strengths of our joint segmentation graph. With nearly 1 million images, a performance gain of 4.44% means that we correctly localize 41,715 more images than <ref type="bibr" target="#b41">[42]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MIT dataset</head><p>ImageNet dataset  <ref type="figure" target="#fig_5">Figure 6</ref> shows qualitative results. Our method is able to segment objects well in spite of large intra-class variations. Because of the joint segmentation graph, our method can successfully segment some challenging instances where the object is not easily separable from the background but matches well with similar regions in easier images.</p><p>Conclusions We proposed a scalable approach to actively solicit foreground annotations useful to propagate segmentations in large image collections. Our results demonstrate its effectiveness: we improve the state of the art in multiple datasets for both weakly supervised segmentation and active propagation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Our active image segmentation propagation method alternates between: (1) Actively choosing images which once annotated by humans will likely be most useful in propagating segmentations to other images and (2) Given human annotations on actively chosen images (marked in pink), propagating them (dark arrows) to generate segmentations for other unlabeled images. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Approach:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>I l = I l ∪ St; I u = I u \ St; 15:end for 16: end procedure</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>Active propagation for varying amounts of human annotation on a subset of the 3,624 ImageNet total synsets we tested (more in Supp.). Since only bounding box ground truth is available, we show bounding-box localization (BBox-CorLoc) accuracy (see Supp. for bounding-box Jaccard plots). Last plot (Animal) shows a failure case. Best viewed in color. Active propagation results for varying amounts of human annotation for MIT Object Discovery dataset. We show both segmentation overlap (Jaccard) and segmentation localization (Seg-CorLoc) accuracy for each of the three classes. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Example active annotation choices for the 3 image collections (Airplane, Car, Horse) in the MIT dataset during the first stage with k = 10. The algorithm selects influential and diverse images (e.g., prototypical shapes) with some relatively difficult/unusual ones (best viewed in color).is dominated by the cost of computing pairwise similarities between region proposals, O((N r) 2 ) for N images and r region proposals per image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Qualitative results for weakly supervised joint segmentation. The segmentation result is shown with a green overlay over the image. See Supp. for more results (incl. failure cases). Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc>Comparison with state-of-the-art methods on ImageNet for weakly supervised joint foreground segmentation (Metric: Avg. BBox-CorLoc).</figDesc><table></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">This class of techniques can also be described as co-segmentation or joint segmentation or object discovery or co-localization methods; in all cases, a set of related images is used to discover the common foreground.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">One could choose from a variety of features; we employ off-the-shelf CNN-based descriptors and saliency metrics (see Sec. 4 for details).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">For simplicity of notation, below we continue to use R ij for all regions unless strictly required; it should be understood that ∀I i ∈ S there is only one proposal, instead of r proposals.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Since ImageNet lacks segmentation ground truth for all images, (1) we evaluate our masks against the bounding boxes, using a tight bounding box around the predicted segmentation and (2) when our method requests a human-drawn segmentation, it gets the region proposal with maximum overlap with the ground-truth bounding box.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">Restricting our proposed method to use "influence" alone also performs worse than passive and comparable to<ref type="bibr" target="#b35">[36]</ref>.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments: This research is supported in part by ONR YIP N00014-12-1-0754.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Semantic object selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Price</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Classcut for unsupervised class segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Alexe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">What is an Object</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Alexe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Hierarchical subquery evaluation for active learning on a graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multiscale combinatorial grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">iCoseg: Interactive Co-segmentation with Intelligent Scribble Guidance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kowdle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">An experimental comparison of min-cut/max-flow algorithms for energy minimization in vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Boykov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kolmogorov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2004-09" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="1124" to="1137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">CPMC: Automatic object segmentation using constrained parametric min-cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="1312" to="1328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Enriching Visual Knowledge Bases via Object Discovery and Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Weakly supervised localization and learning with generic knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Alexe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="275" to="293" />
			<date type="published" when="2012-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A convex optimization framework for active learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Elhamifar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sastry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">ImageNet autoannotation with segmentation propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Guillaumin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Küttel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="328" to="348" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Zisserman. Geodesic star convexity for interactive image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Gulshan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Semi-supervised SVM Batch Mode Active Learning with Applications to Ima ge Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lyu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Information Systems</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Predicting sufficent annotation strength for interactive foreground segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Which image pairs will cosegment well? predicting partners for cosegmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<title level="m">Caffe: Convolutional architecture for fast feature embedding</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Saliency detection via absorbing markov chain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Discriminative clustering for image co-segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multi-class cosegmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Distributed cosegmentation via submodular optimization on anisotropic diffusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">Fei</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Shape sharing for object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Submodular function maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Golovin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Tractability: Practical Approaches to Hard Problems</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Figure-ground segmentation by transferring window masks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kuettel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Collect-Cut: Segmentation with top-down cues discovered in multi-object images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Sift flow: dense correspondence across different scenes and its applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Spatial support for objects via multiple segmentations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Malisiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Weakly-and semi-supervised learning of a deep convolutional network for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Fully convolutional multi-class multiple instance learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">From image-level to pixellevel labeling with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">O</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning a classification model for segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Grabcutinteractive foreground extraction using iterated graph cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kolmogorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Unsupervised joint object discovery and segmentation in internet images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rubinstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Annotation propagation in large image databases via dense image correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rubinstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Using Multiple Segmentations to Discover Objects and their Extent in Image Collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Unsupervised co-segmentation through region matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Serrat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Paragios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Rubio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Textonboost: Joint Appearance, Shape and Context Modeling for Multi-Class Object Recognition and Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Beyond Active Noun Tagging: Modeling Contextual Interactions for Multi-Cl ass Active Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Siddiquie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Co-localization in real-world images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Active learning for semantic segmentation with expected change</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vezhnevets</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Buhmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Object cosegmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vicente</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kolmogorov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">What&apos;s it going to cost you?: Predicting effort vs. informativeness for multilabel image annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Far-sighted active learning on a budget for image and video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Supervised label transfer for semantic segmentation of street scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Quan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
