<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Comparative Deep Learning of Hybrid Representations for Image Recommendations</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyi</forename><surname>Lei</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Geo-Spatial Information Processing and Application System</orgName>
								<orgName type="institution" key="instit1">CAS Key Laboratory of Technology</orgName>
								<orgName type="institution" key="instit2">University of Science and Technology of China</orgName>
								<address>
									<postCode>230027</postCode>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Geo-Spatial Information Processing and Application System</orgName>
								<orgName type="institution" key="instit1">CAS Key Laboratory of Technology</orgName>
								<orgName type="institution" key="instit2">University of Science and Technology of China</orgName>
								<address>
									<postCode>230027</postCode>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiping</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Geo-Spatial Information Processing and Application System</orgName>
								<orgName type="institution" key="instit1">CAS Key Laboratory of Technology</orgName>
								<orgName type="institution" key="instit2">University of Science and Technology of China</orgName>
								<address>
									<postCode>230027</postCode>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng-Jun</forename><surname>Zha</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Geo-Spatial Information Processing and Application System</orgName>
								<orgName type="institution" key="instit1">CAS Key Laboratory of Technology</orgName>
								<orgName type="institution" key="instit2">University of Science and Technology of China</orgName>
								<address>
									<postCode>230027</postCode>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houqiang</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Geo-Spatial Information Processing and Application System</orgName>
								<orgName type="institution" key="instit1">CAS Key Laboratory of Technology</orgName>
								<orgName type="institution" key="instit2">University of Science and Technology of China</orgName>
								<address>
									<postCode>230027</postCode>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Comparative Deep Learning of Hybrid Representations for Image Recommendations</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:38+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In many image-related tasks, learning expressive and discriminative representations of images is essential, and deep learning has been studied for automating the learning of such representations. Some user-centric tasks, such as image recommendations, call for effective representations of not only images but also preferences and intents of users over images. Such representations are termed hybrid and addressed via a deep learning approach in this paper. We design a dual-net deep network, in which the two subnetworks map input images and preferences of users into a same latent semantic space, and then the distances between images and users in the latent space are calculated to make decisions. We further propose a comparative deep learning (CDL) method to train the deep network, using a pair of images compared against one user to learn the pattern of their relative distances. The CDL embraces much more training data than naive deep learning, and thus achieves superior performance than the latter, with no cost of increasing network complexity. Experimental results with real-world data sets for image recommendations have shown the proposed dual-net network and CDL greatly outperform other stateof-the-art image recommendation solutions.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>With the increasing abundance of images, finding out images that satisfy user needs from a huge collection is more and more required, which emphasizes the importance of image search and image recommendations working as filters for users. Such tasks are not trivial, however, due to the gap in understanding the semantics of images as well as the gap in understanding the intents or preferences of users over images. Compared to their counterparts for structured data, such as search of text and recommendations of book or movie, image search and recommendations raise more challenges since images lack an immediately effective representation.</p><p>How to represent images both expressively and discriminatively is of essential importance in many image-related tasks including detection, registration, recognition, classification, and retrieval. This problem had been extensively studied, and many kinds of hand-crafted features had been designed and adopted in different tasks <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b27">28]</ref>. Most of previous work focuses on low-level visual features of images, but for image search and recommendations, it is often not clear how to represent the intents or preferences of users within the framework of low-level features.</p><p>One feasible solution that has been studied is to utilize the users' information as constraints to refine the image representations, making them consistent with both semantic labels and user provided hints [19, <ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b32">33]</ref>. For example, <ref type="bibr">Liu et al. [19]</ref> proposes to learn an image distance metric by combining the images' visual similarity and their "social similarity," defined from users' interests in images that are mined from user data in online social networks. Nonetheless, visual content of images and users' intents/preferences on images are of two different modalities, simply combining them may not turn out efficient enough.</p><p>Recently, deep network models have attracted much attention of researchers in the image processing field. One significant advantage of deep networks is the automated learning of image representations, which are demonstrated to be more effective than hand-crafted features, especially in semantic level image understanding <ref type="bibr" target="#b12">[13]</ref>. Moreover, deep networks have achieved great success in processing other forms of data such as speech and text <ref type="bibr" target="#b21">[22]</ref>. Promisingly, multimodal data, such as images and users' intents/preferences, may be efficiently handled by a single integrated deep network.</p><p>In this paper, we study a dual-net deep network model for the purpose of making recommendations of images to users. The network consists of two sub-networks, which map an image and the preferences of a user into a same latent semantic space, respectively. Therefore, the network achieves representations of both images and users, termed hybrid representations hereafter, and these hybrid representations are directly comparable to make decisions of recommendations.</p><p>Moreover, we propose a comparative deep learning (CDL) method to train the designed deep network. Instead of a naive learning, e.g. learning a distance between a user and an image, the CDL uses two images compared against one user, and learns the relative distances among them. Our key idea is depicted in <ref type="figure">Fig. 1</ref>, where for a query user, her historical data used for learning consist of "positive" images, e.g. her favorites, and "negative" images, e.g. her dislikes; the objective of CDL is that the distance between the user and a positive image shall be less than the distance between the user and a negative image. Thus, training data for CDL are triplets of (user, positive image, negative image) and these data are fed into a triple-net deep network consisting of three sub-networks, one of which is for user, and the other two are for positive and negative images and are actually identical, as shown in <ref type="figure">Fig. 2</ref>. Note that after training, we need only two sub-networks for user and image, respectively.</p><p>The designed dual-net network and CDL method have been verified on an image recommendation task with realworld data sets. Experimental results display that the proposed CDL achieves superior performance than naive learning, and our proposed solution outperforms other state-ofthe-art image recommendation methods significantly.</p><p>The remainder of this paper is organized as follows. Related work is discussed in Section 2. Then our proposed CDL-based image recommendation solution is described, the objective of CDL is formulated in Section 3, followed by detailed description of the deep network model in Section 4, and details of making image recommendations in Section 5. Experimental results are reported in Section 6, and concluding remarks in Section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>We give brief overview of related work at two aspects: learning of image representations and personalized image recommendations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Learning of Image Representations</head><p>In view of the limitation of hand-crafted image features such as those designed in <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b27">28]</ref>, more and more research focuses on designing effective deep learning models to extract image representations automatically <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b30">31]</ref>. Karpathy et al. <ref type="bibr" target="#b30">[31]</ref> proposes a supervised hashing method with deep learning architecture, followed by a stage of simultaneous learning of hash function and image represen-  <ref type="figure">Figure 1</ref>. This figure depicts the key idea of our proposed comparative deep learning (CDL). One user's preferences can be described by her frequently used tags as well as her friends' preferences and her joined groups' preferences. These preferences, together with images, are mapped into a same latent semantic space. In that space, the distance between the user and a "positive" image (e.g. favorite image) shall be less than the distance between the user and a "negative" image (e.g. disliked image), which is taken as the objective of CDL.</p><p>tations. Furthermore, it is noticed that middle-layer outputs in deep learning models can be seamlessly utilized as image representations, though the deep network is not trained for that <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b33">34]</ref>. For example, Krizhevsky et al. <ref type="bibr" target="#b12">[13]</ref> proposes a deep learning architecture to perform image classification, and the outputs of the 7th full-connection layer are also verified to be kind of robust image representations. The abovementioned work mainly focuses on low-level visual features of images. But recently, along with the development of user-centric applications such as image recommendations, it is worthwhile to learn not only visual information but also intents or preferences of users for image representations. A paucity of work has made attempts at this aspect <ref type="bibr" target="#b6">[7,</ref><ref type="bibr">19,</ref><ref type="bibr" target="#b22">23]</ref>. Pan et al. <ref type="bibr" target="#b22">[23]</ref> proposes an embedding method to study the cross-view (i.e. text to image views) search problem with analyses of user click log. <ref type="bibr">Liu et al. [19]</ref> consider jointly the users' social relationship and images' visual similarity to learn a new image distance metric. But such work relies heavily on carefully designed hand-crafted features. Liu et al. <ref type="bibr" target="#b17">[18]</ref> employ deep learning architecture to capture user intent and image visual information, where user intent is described by only similarity between a pair of users. But in practice, there is multimodal information for drawing upon user intents, such as tags, browsing history and social groups. Moreover, the deep architecture in <ref type="bibr" target="#b17">[18]</ref> considers only one image at each training round. To the contrary, recent studies <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b29">30]</ref> indicate that deep ranking models perform much better by forming training data as triplets. To summary, how to design an effective deep learning architecture to capture both visual information and the intents or preferences of users over images is still a challenging open problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Personalized Image Recommendations</head><p>Personalized recommendations for structured data such as book, movie, and music have been studied for a long while <ref type="bibr" target="#b0">[1]</ref>. Typical technologies include content-based filtering, collaborative filtering, and hybrid of both <ref type="bibr" target="#b23">[24]</ref>. However, it is difficult to directly adopt these technologies for image recommendations, possibly due to several difficulties: images are highly unstructured and lack an immediate representations, user-image interaction data are often too sparse, users rarely provide ratings on images but rather give implicit feedback. Nevertheless, mature technologies in recommender systems are still inspiring, for example, matrix factorization <ref type="bibr" target="#b11">[12]</ref> can be perceived as to learn latent representations of users and items in a same semantic space.</p><p>With the development of social networks, recent research starts to leverage social data to improve the performance of recommendations <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b9">10]</ref>. Most of existing work on image recommendations also follows this line <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b15">16]</ref>. For example, Jing et al. <ref type="bibr" target="#b10">[11]</ref> propose a novel probabilistic matrix factorization framework that combines the ratings of local community users for recommending Flickr photos. Cui et al. <ref type="bibr" target="#b2">[3]</ref> propose a regularized dual-factor regression method based on matrix factorization to capture the social attributes for recommendations. These methods ignore the visual information of images, instead, they focus solely on modeling users by discovering user profiles and behavior patterns. The representations of images and users are still isolated due to semantic gap and the sparsity of user-image interactions.</p><p>Only a few recent work is concentrated on joint modeling of users and images for making recommendations <ref type="bibr" target="#b17">[18,</ref><ref type="bibr">19,</ref><ref type="bibr" target="#b24">25]</ref>. Sang et al. <ref type="bibr" target="#b24">[25]</ref> propose a topic sensitive model that concerns user preferences and user uploaded images to study users' influences in social networks. <ref type="bibr">Liu et al. [19]</ref> propose to recommend images by voting strategy according to learnt social embedded image representations. Till now, the existing methods often perform separate processing of user information and image and then simply combining them. A fully integrated solution is to be investigated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Problem Formulation of Comparative Learning</head><p>We address the hybrid representations, i.e. simultaneous representations of both users and images in a same latent space, via a deep learning approach. For this learning, how to prepare training data is not obvious. Given the fact that users rarely provide ratings on images due to the abundance of online images, we shall be able to utilize users' implicit feedback on images. Such feedback, however, is still sparse and severely unbalanced, usually negative feedback is almost none <ref type="bibr" target="#b5">[6]</ref>. A naive learning, e.g. learning a distance between a user and an image, will probably fail due to the training data.</p><p>Motivated by previous efforts on deep ranking models <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b29">30]</ref>, we propose a comparative learning method to tackle the imperfect training data. Several symbols are defined as follows. Let an image be I and a user be U , we have defined functions π(I) and φ(U ) that map I and U to a same latent space, respectively. Another function D is used to measure the distance between any two vectors in the learnt latent space. Note that instead of learning a distance between user and image, we propose to learn comparatively the relative distances between a user and two images. That is, the learning algorithm is given a sequence of triplets,</p><formula xml:id="formula_0">{T t = (U t , I +</formula><p>To fulfill this learning, we may perceive Eq. (2) as a binary classification problem (the former distance is less or more than the latter), and thus can reuse the 0-1 loss function, or its better alternatives such as hinge loss function. However, in order to make the distance measure more discriminative (in Eq. (2) the difference between the two distances should be as large as possible), we may also adopt cross entropy as loss function. Specifically, let</p><formula xml:id="formula_1">o Ut ij = D(π(U t ), φ(i)) − D(π(U t ), φ(j)), and P Ut ij = e o U t ij 1 + e o U t ij ,<label>(3)</label></formula><p>we further definē</p><formula xml:id="formula_2">P Ut ij = 0, (i = I + t , j = I − t ) 1, (i = I − t , j = I + t ) ,<label>(4)</label></formula><p>then our learning objective is defined by cross entropy as,</p><formula xml:id="formula_3">min π,φ,D L({T t }) = t −P Ut ij log(P Ut ij ) − (1 −P Ut ij ) log(1 − P Ut ij ).<label>(5)</label></formula><p>In this paper, we are interested in learning representations of users and images and thus we may assume the distance function D to be quite simple, for example the Euclidean. Then, the comparative learning leads to solutions  <ref type="figure">Figure 2</ref>. This figure depicts the deep network used for comparative deep learning (CDL). There are three sub-networks that all output 1024-dim vectors as representations of images and users, respectively. The top and bottom sub-networks processing images are identical. The middle sub-network is processing users. Following these sub-networks are two distance calculating nets. The difference between distances is fed into the final cross-entropy loss function for comparison with label. The numbers shown above each arrow give the size of the corresponding output. The numbers shown above each box indicate the size of kernel and size of stride for the corresponding layer.</p><p>to the mapping functions π(·) and φ(·) that generate representations seamlessly. Traditionally, such learning problems were solved by hand-crafted shallow models, but our case raises more difficulties, since it is required to learn two mapping functions at the same time and the two functions are dealing with quite different modalities but shall embed into a same space. We turn to deep learning to solve this problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Comparative Deep Learning (CDL)</head><p>As illustrated in <ref type="figure">Fig. 2</ref>, we design a deep network to perform the proposed comparative deep learning (CDL). This network architecture takes triplets as inputs, i.e.</p><formula xml:id="formula_4">(U t , I + t , I − t )</formula><p>with a query user U t having relatively shorter distance from image I + than from image I − . There are three sub-networks in the CDL architecture. The top and bottom sub-networks are two convolutional neural networks (CNNs) with identical configuration and shared parameters, they are designed to capture image visual information. The middle sub-network is a full-connection neural network that is designed for user's information.</p><p>The two kinds of sub-networks in our architecture correspond to mapping functions for image I : π(I) ∈ R d and for user U : φ(U ) ∈ R d , respectively, where R d is the target latent space. The outputs of these sub-networks are indeed hybrid representations of images and users (FC1 3, FC2 4 and FC3 3 in <ref type="figure">Fig. 2</ref>). To guarantee that the learnt functions π(·) and φ(·) can embed multimodal information into the same latent space, we add two distance calculating nets that outputs two distances (FC top and FC bottom in <ref type="figure">Fig. 2</ref>), and the difference between distances, i.e. o Ut ij in Eq. <ref type="formula" target="#formula_1">(3)</ref>, is fed into the final cross entropy loss function to be verified by the label. In the rest of this section, we will describe each part of the architecture in more details.</p><p>In the top/bottom sub-network, there are 5 convolutional layers, 3 max-pooling layers and 3 full-connection layers. These configurations including the sizes of convolution kernels in the convolution layers and the numbers of neurons in the full-connection layers are remarked in <ref type="figure">Fig. 2</ref>. The architecture and settings of this sub-network are inspired by AlexNet <ref type="bibr" target="#b12">[13]</ref>, which achieves great success in modeling image visual information. Input to this sub-network are the pixel data of RGB channels of an image, and output of this sub-network is a 1024-dim vector <ref type="figure">(FC1 3 and FC3 3)</ref>.</p><p>The middle sub-network is designed for capturing user's information. Users' preferences/intents can be described in various forms and different kinds of data. However, normally neural networks accept only numerical vector inputs. We adopt a traditional full-connection network to map an input user vector to the representation, and leave the process of converting practical data into user vectors to be defined in Section 5. This sub-network also outputs 1024-dim vectors (FC2 4) to be comparable with the image representations.</p><p>Afterwards, the deep network performs distance calculation. As the focus of this paper is on effective hybrid representations, we assume the distance function shall be quite simple, yet we still design a sub-network for calculating distance. It is completed by first calculating the element-wise difference vector (DIFF1 and DIFF2 shown in <ref type="figure">Fig. 2)</ref>, then calculating the element-wise square (Square1 and Square2 in <ref type="figure">Fig. 2)</ref>, and finally using a full-connection layer to derive the distance. A special note is that we adopt the idea of dropout (at rate 0.5) to bring in some randomization factors to select partial dimensions of the learnt representations. The full-connection layer acts as weighting factors on the different dimensions of squared difference vector, and thus the distance calculating sub-network is equivalent to weighted l 2 -norm distance function. Many complicated distance calculating networks can be adopted herein, but we leave them for future exploration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CDL for Image Recommendations</head><p>Since our proposed CDL learns hybrid representations, it is well suitable for user-centric image processing tasks. In this paper, we take personalized image recommendation task as an example to discuss on the utility of CDL. We will restrict our discussions to recommending new images to a user based on her browsing history and will not dive into details of practice. There are two key issues to be solved before applying the CDL. First, how to preprocess user data to generate user vectors as inputs to the deep network. Second, how to prepare triplets as training data.</p><p>There are several intuitive methods to generate user vectors. A straightforward method is bag-of-words, for example, using a vector whose dimension is equal to the amount of possible tags, and entries of this vector correspond to the interest levels of this user in these tags. Such interest levels can be estimated from the user's browsing history and tagging history, and so on. This method faces two challenges. First, tags may be too many and accordingly the vector may be too sparse. Second, the method cannot deal with synonyms of tags. In this paper, we use the well-known word2vector <ref type="bibr" target="#b21">[22]</ref> as a remedy for these problems. Tags are converted to vectors 1 and then vectors are clustered by kmeans into 1024 semantic clusters. Then, tags are replaced by clusters and the bag-of-words method works on these clusters. <ref type="figure">Fig. 3</ref> shows the distribution of clusters, where we observe the clusters have variant frequencies and bear topical polymerism to some degree. <ref type="bibr" target="#b0">1</ref> Actually we use Google trained vectors downloaded from https://code.google.com/p/word2vec/.  <ref type="table">1  25  49  73  97  121  145  169  193  217  241  265  289  313  337  361  385  409  433  457  481  505  529  553  577  601  625  649  673  697  721  745  769  793  817  841  865  889  913  937  961  985  1009</ref> The amount of users clusters <ref type="figure">Figure 3</ref>. This figure shows the distribution of clusters. The x-axis displays 1024 clusters and the y-axis is the number of users having interests in this cluster. A user can be described by bag of words where words are indeed clusters.</p><p>Since the input is a set of triplets in our proposed CDL, it is desirable to generate a set of pairwise images (a positive image and a negative image) for each user. Positive images for users are often handy since users' behavior data such as "add to favorites" and "like" give such information explicitly. However, negative images are not obvious <ref type="bibr" target="#b5">[6]</ref>. An image is not "liked" by a user dose not necessarily indicate the user is not interested in the image, but rather the user never saw it. We utilize social data to help solve this problem. In general, a user has friendship with another usually indicates that both users have similar interests, and users of the same social group have similar interests also. For a specific user, we define the set of potentially liked images as her friends' favorite images and the images "liked" by users in her joined groups. We then assign the images to be negative, which have no tag of the user's interests and are not belonging to the set of potentially liked images. Due to abundance of negative images assigned in this manner, random sampling can be performed to generate a subset of triplets for training purpose.</p><p>Last but not the least question is how to make recommendations for users. This is performed in the following steps. First, a set of candidate images are selected, where each candidate shall have at least one tag of the user's interests. Second, the representations of these candidate images as well as of the user are calculated; these representations can be calculated and stored in advance, or can be calculated in parallel to accelerate. Third, distances are calculated among the images and the user. Finally, K nearest neighboring images having minimum distances are chosen as recommendations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experiments</head><p>In this section, we report the conducted experiments to evaluate the efficacy of the proposed CDL of hybrid representations. We first introduce our experimental settings. Then, we evaluate the performance of our learnt hybrid rep- The amount of users (in logs)</p><p>The amount of user favorite images The amount of users (in logs)</p><p>The amount of user tags resentations in personalized image recommendation task. Finally, we give some insights of our proposed approach.  <ref type="figure" target="#fig_4">Fig. 4</ref>. Both of them are typical long-tail distributions. Users having modest favorite images and tags usually have most valuable and robust information <ref type="bibr" target="#b25">[26]</ref>. Too few favorites indicate inactivity of user, and too many favorites indicate quite diverse interests of user. Thus, we filter out users that have less than 40 or more than 200 favorite images from test <ref type="bibr">[19]</ref>. Furthermore, according to statistics shown in <ref type="figure">Fig. 3</ref>, we further filter out users that have interests in less than 80 or more than 280 clusters from training data, so as to improve the accuracy of training, but keep them for test. Finally, we have 8, 616 users for training and 15, 023 users for test.</p><p>For each user, 20 images are randomly selected from her favorite images and "concealed" for test. Training data are then generated by randomly sampling the rest favorite images as well as assigned negative images (c.f. Section 5). 20 triplets are sampled for each user for training. Finally, there are 72, 161 distinct images in training data. After training, the concealed favorite images are retrieved and mixed with other 80 images (for each user) for test.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.2">Compared Approaches</head><p>As discussed in Section 3, our proposed CDL allows the choice of different loss functions and we have used cross en-tropy for better performance. We also tested the use of hinge loss in replacement of cross entropy. Moreover, we compare our method with several state-of-the-art approaches.</p><p>Borda Count with SIDL <ref type="bibr" target="#b1">[2,</ref><ref type="bibr">19]</ref>. Social embedding Image Distance Learning (SIDL) is a novel image distance learning method that embeds the similarity of collective social and behavioral information into visual space. After learning the social embedding image distance metric, it can be adopted together with Borda Count method <ref type="bibr" target="#b1">[2]</ref> to perform personalized image recommendations, as detailed in <ref type="bibr">[19]</ref>.</p><p>Borda Count with BoW, ImageNet <ref type="bibr" target="#b12">[13]</ref>, LMNN <ref type="bibr" target="#b28">[29]</ref>, Social+LMNN <ref type="bibr">[19]</ref>. Bag of Words (BoW) feature is a traditional hand-crafted visual representation, and ImageNet feature stands for deep learning based representation, both can be used to measure image similarity with e.g. Euclidean distance. Large Margin Nearest Neighbor (LMNN) is a metric learning method to reduce the margins of the nearest neighbors. Liu et al. proposes to embed social similarity into LMNN, termed Social+LMNN. We then use BoW, ImageNet, LMNN, and Social+LMNN with Borda Count method to evaluate the performance of personalized image recommendations.</p><p>TwoNets. In this paper, we propose the CDL instead of naive deep learning to learn hybrid representations. To demonstrate the effectiveness of CDL, we also perform the naive learning experiment called TwoNets. Specifically, TwoNets is similar to the CDL but it has only two sub-networks, which process user and image, respectively; the output representations of the two sub-networks are directly compared to calculate a distance, and the distance is re-scaled by a logistic sigmoid function and then compared with the ground-truth by cross entropy loss function; note that in TwoNets, training data consist of doublets of (user, image) and the ground-truth is 0 or 1 indicating negative or positive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.3">Implementation</head><p>We implement the CDL and TwoNets methods based on the open source deep learning software Caffe <ref type="bibr" target="#b8">[9]</ref>. In our experiments, all images are resized to 256 × 256. The structure and parameters of sub-networks are illustrated in <ref type="figure">Fig. 2</ref>, and all probabilities of dropout are set to 0.5 <ref type="bibr" target="#b12">[13]</ref>. The learning rate starts from 0.001 for all layers and the momentum is 0.9. The mini-batch size of images is 128. The weight decay parameter is 0.0005. Training was done on a single GeForce Tesla K20c GPU with 5GB graphical memory, and it took about 4 days to finish training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Overall Performance</head><p>In our personalized image recommendation task, the target is to recommend 20 images out of 100 candidates for each user. To make a fair comparison, we implement every comparative method to return top K recommended images where K is adjustable. Precision@K and Recall@K are used to evaluate the performance of each method, which are shown in Figs. 5 and 6, respectively. It can be seen that our approach performs the best in both precision and recall for all K values, which demonstrates the effectiveness of our proposed CDL of hybrid representations. Note that using cross entropy as loss function has obvious advantage compared to using hinge loss function in our image recommendation task (c.f. Section 3).</p><p>The approaches based solely on hand-crafted visual representations, i.e. BoW and LMNN, perform poorly in making recommendations. The Precision result of BoW is near to random guess (random guess for recommending 20 out of 100 achieves precision 0.2). ImageNet Features lead to much better results, almost the third best after our CDL methods and SIDL, which shows the advantage of deep learning based representations.</p><p>If we add social factors to constrain LMNN (i.e. So-cial+LMNN), the performance will be improved a lot, due to the utilization of extra information besides visual features. The SIDL performs better than Social+LMNN, indicating the importance of carefully designed features to capture visual information and embedding functions to integrate multimodal information. Compared to SIDL, our approach leads significant gains of average 42.58% and 46.50% for precision and recall, respectively. It owes to the superiority of deep network models over traditional handcrafted models especially in capturing visual information.</p><p>It should be noted that TwoNets, also adopting deep network model, has very poor performance. It is slightly better than BoW but the latter is near to random guess. Thus, deep network models do not guarantee great success especially when the task is complicated (learning hybrid representations) and the training data are imperfect (unreliable negative samples). The proposed CDL outperforms TwoNets significantly and consistently, which further demonstrates the effectiveness of the proposed comparative learning method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Case Study and Insights</head><p>In this section, we present a case for comprehensive study to give some insights of our proposed approach. For the selected user whose word cloud of frequent tags can be found in <ref type="figure" target="#fig_7">Fig. 7</ref> (Middle), we prepare a set of images for training, illustrated in <ref type="figure" target="#fig_7">Fig. 7 (Left)</ref>. It can be observed that positive images match the user's preferences as described by the word cloud, e.g. portrait, woman and mood. Obviously, there are large differences between positive images and negative images, which verifies the effectiveness of our designed process for assigning negative images for training (c.f. Section 5).</p><p>Our approach's recommendation results for this user are shown in <ref type="figure" target="#fig_7">Fig. 7 (Right)</ref>. Precision@20 is as high as 70% in this case. Given a closeup view, most of correct recommendations made by our approach are portraits with darker tone and gloomy atmosphere, in the similar topics and styles of the user's word cloud and the positive images in training data. Interestingly, the 1st image in the 2nd row in <ref type="figure" target="#fig_7">Fig. 7</ref> (Right) is not belonging to the styles mentioned above. Such images are not easy to be recommended if using purely tags. But we may compare this image with the 2nd image in the 2nd row in <ref type="figure" target="#fig_7">Fig. 7 (Left)</ref>, and observe their similarity in the sense of color, bokeh, and theme. This is where image representations help.</p><p>Several images in the recommendation results are not "correct" according to ground-truth, but we cannot say firmly that the user dislikes these images since we do not know whether the user has ever seen them. Especially, the 2nd image in the 2nd row and the last image in <ref type="figure" target="#fig_7">Fig. 7 (Right)</ref> are probably what user may like. Both images match the word cloud and the user's positive images in the training data. However, the 1st image in the 4th row is probably a mistake of recommendation. This photo has darker background and a human-like object (which is actually a skele-   <ref type="figure">Figure 8</ref>. Exemplar input and output of the user sub-network in our designed dual-net deep network. Left: pre-processed user vector (input). Right: learnt user representation (output).  <ref type="table">1  41  81  121  161  201  241  281  321  361  401  441  481  521  561  601  641  681  721  761  801  841  881  921  961  1001</ref> Features -  <ref type="figure" target="#fig_9">Figure 9</ref>. Exemplar learnt representations of positive images (top row) and negative images (bottom row). Note the similarity between positive images and dissimilarity between positive and negative images, especially in the circled areas.</p><p>ton), which interprets its being selected, but per view of the user's positive images, skeleton may not be his/her favorite. In such cases, making finer discrimination of similar objects may help improve the recommendation accuracy. <ref type="figure">Fig. 8</ref> illustrates the input and output of the user subnetwork learnt by CDL. <ref type="figure">Fig. 8 (Left)</ref> is the input vector, indeed a bag-of-words vector spanned over clusters of word2vector results, such vector is very sparse, dominated by several interests. <ref type="figure">Fig. 8</ref> (Right) is the learnt user representation, not sparse any more. It shall be noted that such dense vectors are due to the following distance calculation (weighted l 2 -norm distance). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusions</head><p>In this paper, we explore learning of hybrid representations to capture both visual information and intents or preferences of users over images, and utilizing such representations for user-centric tasks such as personalized image recommendations. A dual-net deep network model is proposed to learn representations in a latent semantic space. We also propose a comparative deep learning method to train the designed deep network, in which triplets of users and positive/negative images are taken as inputs and the relative distances are the objective of learning. The empirical evaluations on personalized image recommendation task show that our proposed approach achieves much better performance than naive deep learning as well as several state-of-the-art image recommendation solutions. The proposed comparative deep learning can be applied to many other user-centric applications, such as image search and image editing. We will further explore along these directions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>This work was supported by the National Program on Key Basic Research Projects (973 Program) under Grant 2015CB351800, by the Natural Science Foundation of China (NSFC) under Grants 61303149, 61331017, 61390512, and 61472392, and by the Fundamental Research Funds for the Central Universities under Grants WK2100060011 and WK3490000001.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Distributions of the amounts of user favorite images and user tags. Note the logarithm scale of y-axis.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Precision@K for different K values of compared image recommendation methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 .</head><label>6</label><figDesc>Recall@K for different K values of compared image recommendation methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 .</head><label>7</label><figDesc>(Best view in color.) Case study of making recommendations to a selected user. Left: some samples of training images for this user, 10 positive and 10 negative, separated by the red line; unlike positive images that are indeed favorite images of this user, negative images are "assigned" by the process discussed in Section 5. Middle: the word cloud of this user's frequent tags retrieved from her tagging history and browsing history. Right: recommendation results sorted in relevance (ascending order of distance calculated by hybrid representations), where correct results are highlighted by red borders.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 9</head><label>9</label><figDesc>illustrates several examples of learnt image representations, where the top row shows positive images and the bottom row shows negative images. It is interesting to find that the representations of two positive images are quite similar, while they are very different from the representations of two negative images. Some obvious similarity and dissimilarity are highlighted by circles in the figure. However, such information is not easily perceived from the images themselves. Therefore, simultaneous learning of hybrid representations is indeed quite different from only learning the representations of images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>Tags: Flowers, Animals, Cat Friend List: Alice, Bob Group List: Animal Lover</figDesc><table>Positive Images 
Negative Images 

Latent Space 

User Preferences 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head></head><label></label><figDesc>The details of crawling can be found in[19]. On average, there are 23.5 tags and 5.8 favorite images for each user. Due to the sparsity of user-image interactions, this dataset is not quite suitable for traditional recommendation algorithms, especially collaborative filtering. Therefore, we do not compare our method with them.The distributions of the amounts of user favorite images and user tags are shown in</figDesc><table>6.1. Experimental Settings 

6.1.1 Datasets 

In this paper, we use the same dataset as reported in [19]. 
The images and users' information in this dataset are 
crawled from Flickr through its API. There are 101, 496 im-
ages, 54, 173 users, 6, 439 groups and 35, 844 tags in this 
dataset. </table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">t , I − t ), t = 1, 2, ..., T },(1)where T is the total amount of triplets, U t , I + t , I − t indicate the triple input elements, i.e. query user U t prefers image I + t than image I − t . Then, the learning is to find such mapping functions π(·) and φ(·) and such a distance function D(·, ·), to satisfy D(π(U t ), φ(I + t )) &lt; D(π(U t ), φ(I − t )), ∀t.(2)</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Toward the next generation of recommender systems: A survey of the state-of-the-art and possible extensions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Adomavicius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tuzhilin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TKDE</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="734" to="749" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Models for metasearch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Aslam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Montague</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="276" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">What videos are similar with you?: Learning a common attributed representation for video recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="597" to="606" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="886" to="893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Justclick: Personalized image recommendation via exploratory search from large-scale flickr images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Keim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TCSVT</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1051" to="8215" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Collaborative filtering for implicit feedback datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Volinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Clickage: towards bridging semantic and intent gaps via mining click logs of search engines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-S</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="243" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Trustwalker: a random walk model for combining trust-based and item-based recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jamali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="397" to="406" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Caffe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<title level="m">Convolutional architecture for fast feature embedding</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Scalable recommendation with social contextual information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TKDE</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2789" to="2802" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Recommendation on flickr by combining community user ratings and item importance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICME</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Matrix factorization techniques for recommender systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Volinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="30" to="37" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Simultaneous feature learning and hash coding with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Understanding image representations by measuring their equivariance and equivalence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lenc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="991" to="999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Personalized image recommendation for web search engine users</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICME</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning deep representations for ground-to-aerial geolocalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5007" to="5015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning socially embeded visual representation from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Social embedding image distance learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Object recognition from local scale-invariant features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1150" to="1157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Understanding deep image representations by inverting them</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mahendran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5188" to="5196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Clickthrough-based cross-view learning for image search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">W</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SI-GIR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="717" to="726" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A framework for collaborative, content-based and demographic filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Pazzani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence Review</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="393" to="408" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Right buddy makes the difference: An early exploration of social relation analysis in multimedia applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="19" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Flickr tag recommendation based on collective knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sigurbjörnsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Van Zwol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="327" to="336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning fine-grained image similarity with deep ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Locality-constrained linear coding for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3360" to="3367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Distance metric learning for large margin nearest neighbor classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="207" to="244" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Online multimodal deep similarity learning with application to image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Hoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Miao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Supervised hashing for image retrieval via image representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2156" to="2162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A discriminative cnn video representation for event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1798" to="1807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Latent feature learning in social media network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="253" to="262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="818" to="833" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
