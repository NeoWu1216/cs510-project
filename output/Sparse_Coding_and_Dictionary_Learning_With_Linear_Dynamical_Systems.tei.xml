<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Sparse Coding and Dictionary Learning with Linear Dynamical Systems *</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science and Technology</orgName>
								<orgName type="department" key="dep2">Tsinghua National Lab. for Information Science and Technology (TNList)</orgName>
								<orgName type="laboratory">State Key Lab. of Intelligent Technology and Systems</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuchun</forename><surname>Sun</surname></persName>
							<email>fcsun@mail</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science and Technology</orgName>
								<orgName type="department" key="dep2">Tsinghua National Lab. for Information Science and Technology (TNList)</orgName>
								<orgName type="laboratory">State Key Lab. of Intelligent Technology and Systems</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lele</forename><surname>Cao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science and Technology</orgName>
								<orgName type="department" key="dep2">Tsinghua National Lab. for Information Science and Technology (TNList)</orgName>
								<orgName type="laboratory">State Key Lab. of Intelligent Technology and Systems</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deli</forename><surname>Zhao</surname></persName>
							<email>2zhaodeli@gmail.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaping</forename><surname>Liu</surname></persName>
							<email>hpliu@mail.tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science and Technology</orgName>
								<orgName type="department" key="dep2">Tsinghua National Lab. for Information Science and Technology (TNList)</orgName>
								<orgName type="laboratory">State Key Lab. of Intelligent Technology and Systems</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrtash</forename><surname>Harandi</surname></persName>
							<email>3mehrtash.harandi@nicta.com.au</email>
							<affiliation key="aff1">
								<orgName type="institution">Australian National University &amp; NICTA</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Sparse Coding and Dictionary Learning with Linear Dynamical Systems *</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:46+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Linear Dynamical Systems (LDSs) are the fundamental tools for encoding spatio-temporal data in various disciplines. To enhance the performance of LDSs, in this paper, we address the challenging issue of performing sparse coding on the space of LDSs, where both data and dictionary atoms are LDSs. Rather than approximate the extended observability with a finite-order matrix, we represent the space of LDSs by an infinite Grassmannian consisting of the orthonormalized extended observability subspaces. Via a homeomorphic mapping, such Grassmannian is embedded into the space of symmetric matrices, where a tractable objective function can be derived for sparse coding. Then, we propose an efficient method to learn the system parameters of the dictionary atoms explicitly, by imposing the symmetric constraint to the transition matrices of the data and dictionary systems. Moreover, we combine the state covariance into the algorithm formulation, thus further promoting the performance of the models with symmetric transition matrices. Comparative experimental evaluations reveal the superior performance of proposed methods on various tasks including video classification and tactile recognition.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Object recognition based on spatio-temporal data is an active research area across several domains such as machine learning <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b17">18]</ref>, computer vision <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17]</ref> and robotics <ref type="bibr" target="#b27">[28]</ref>. The coupling of the spatial texture and the temporal dynamics makes spatio-temporal data analysis more challenging than static data. A popular method of representing spatio-temporal data is to model them by Linear Dynamical Systems (LDSs) <ref type="bibr" target="#b8">[9]</ref>. To allow the comparison between dynamical processes, a distance metric or kernel function needs to be defined first. Once the distance or kernel has been defined, classifiers such as Nearest Neighbors (NNs) and Support Vector Machines (SVMs) can be used to recognize spatio-temporal sequences. For this purpose, various kinds of distances or kernels have been proposed, such as Martin Distance <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b5">6]</ref>, Kullback-Leibler divergence <ref type="bibr" target="#b4">[5]</ref>, and Binet-Cauchy kernel <ref type="bibr" target="#b39">[40]</ref>. Several recent studies have been carried out to integrate learning techniques into LDSs; for instance, Vidal et al. <ref type="bibr" target="#b38">[39]</ref> proposed a LDS-based boosting method for time series modeling; and Ravichandran et al. <ref type="bibr" target="#b32">[33]</ref> designed bag-of-systems for video analysis.</p><p>Despite the wide applications of LDSs, little attention has been paid to combining sparse coding with LDS modeling to deliver robust techniques. In the past decade, sparse coding has been successfully adopted in various tasks such as image restoration <ref type="bibr" target="#b29">[30]</ref>, face recognition <ref type="bibr" target="#b42">[43]</ref>, and texture classification <ref type="bibr" target="#b30">[31]</ref> to name a few. For sparse coding, natural signals such as images are represented as a combination of a few atoms in a dictionary that is usually over-complete. Using sparsity as a prior leads to state-of-the-art results in many fields <ref type="bibr" target="#b42">[43]</ref>. In this paper, we generalize sparse coding from Euclidean space to the space of LDSs. Specifically, we attempt to reconstruct a given LDS by using a superposition of LDS atoms, where the coefficients of the superposition are enforced to be sparse. Both the codes and the dictionary atoms are learned to minimize the coding objective function. Sparse coding with the LDS dictionary can then be seamlessly used for categorizing spatio-temporal data.</p><p>However, the space of LDSs, which is non-Euclidean, has a complicated manifold structure <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b32">33]</ref>. Carrying out sparse coding and dictionary learning on this kind of space is challenging. Recent studies such as <ref type="bibr" target="#b36">[37]</ref> proposed to embed LDSs into a finite-dimensional Grassmann manifold. With this embedding, sparse coding and dictionary learning with LDSs can then be performed on the finite Grassmannian <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b20">21]</ref>. The first cornerstone of these models <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b20">21]</ref> is to represent each LDS with its finite observability sub-space by taking a fixed-order approximation of the extended observability matrix. Nevertheless, as we will discuss in this paper, this may result in several drawbacks. Firstly, such finite approximation is computationally expensive if the observability order is large; but it is insufficient to model the changes along the rows of the extended observability otherwise. Secondly, if we want to learn the dictionary atoms with the finite method, we can only learn the embedding points of the finite observability but not the parameters of the dictionary LDSs (e.g. the measurement matrix and the transition matrix). It is believed that these parameters are important for further analysis of the learned dictionary. Moreover, various methods have been developed for defining the distance metric <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b33">34]</ref> and performing classification tasks <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b32">33]</ref> on the space of infinite LDSs, indicating that deriving sparse coding and dictionary learning with infinite LDSs could be theoretically interesting.</p><p>Hence, in this paper, we attempt to make the following contributions. (1) We perform sparse coding and dictionary learning with the original form of LDSs that is represented by the extended observability subspace. As a more general framework of <ref type="bibr" target="#b20">[21]</ref>, learning the codes and dictionary atoms on infinite Grassmannian maintains the full changes along the sequences. More importantly, in our models, the calculations related to the infinite observability subspaces can be efficiently derived by the representation of the system parameters, which enables us to learn the system parameters of the dictionary explicitly and reduce the computational cost significantly compared to the finite method. (2)To overcome the limitation caused by the the symmetry constraint to the state transition matrix in dictionary learning, we additionally consider the state covariance as a complementary feature of the symmetric transition matrix to describe the state process, thus further promoting the modeling performance. (3) We employ proposed models to categorize spatio-temporal sequences on diversified benchmark datasets including videos and tactile series. Compared to state-of-the-art methods, our models achieve considerable improvements in discrimination accuracy on most tasks.</p><p>The rest of the paper is organized as follows. Section 2 reviews the LDS preliminaries. Sparse coding is derived in Section 3 and dictionary learning is developed in Section 4. Then, Section 5 combines the state covariance into the algorithm framework and Section 6 analyzes the computational complexities of proposed models. Finally, Section 7 conducts the experiments; and Section 8 concludes this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Briefs of Fundamental Concepts</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Linear dynamical systems</head><p>LDSs represent time series by assuming them to be the output of the following model:</p><p>x t+1 = Ax t + Bv t ,</p><formula xml:id="formula_0">y t = Cx t + w t + y,<label>(1)</label></formula><p>where X = [x 1 , · · · , x T ] ∈ R n×T is a sequence of ndimensional hidden state vectors, and Y = [y 1 , · · · , y T ] ∈ R m×T is a sequence of m-dimensional observed variables. The model is parameterized by Θ = {A, B, C, R, y}, where A ∈ R n×n is the transition matrix; C ∈ R m×n is the measurement matrix; B ∈ R n×nv (n v ≤ n) is the noise transformation matrix; v t ∼ N (0, I nv×nv ) and w t ∼ N (0, R) denote the process and measurement noise components, respectively; y ∈ R m represents the mean of Y. Given the observed sequence, several methods <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b35">36]</ref> have been proposed to learn the optimal system parameters, while the method in <ref type="bibr" target="#b8">[9]</ref> is widely used.</p><p>Since C describes the spatial appearance and A represents the dynamics, the tuple (A, C) can be adopted as the feature descriptor for an LDS . Unfortunately, (A,C) does not lie in a vector space as it needs to satisfy several constraints <ref type="bibr" target="#b36">[37]</ref>. The transition matrix A needs to be stable with eigenvectors inside the unit circle. The columns of C are constrained to be orthonormal. Furthermore, any Riemannian metric for the space of LDS needs to be invariant to the changes of the state space basis. All these constraints make it hard to determine the Riemannian geometry of the LDS space <ref type="bibr" target="#b32">[33]</ref>. To circumvent the difficulties associated to utilizing the tuple (A, C), a family of approaches apply the extended observability subspace to represent an LDS <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b36">37]</ref>, which is the topic of the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">The extended observability subspaces</head><p>Starting from the initial state x 1 , the expected observation sequence is obtained as E[y 1 , y 2 , y 3 , · · · ] = [C T , (CA) T , (CA 2 ) T , · · · ] T x 1 , meaning that it lies in the column space of the extended observability matrix given by O = [C T , (CA) T , (CA 2 ) T , · · · ] T ∈ R ∞×n . Since the column space of O , i.e. the extended observability subspace, is invariant to the choice of the basis of the state space, it can be applied as the descriptor of an LDS. Therefore, the distance between two LDSs is considered as the distance between the respective extended observability subspaces, which can be derived by computing the subspace angles <ref type="bibr" target="#b6">[7]</ref>. The subspace angles between two extended observability matrices O 1 and O 2 , associated with parameters (A 1 ,C 1 ) and (A 2 ,C 2 ) respectively, can be calculated by solving the following Lyapunov equation</p><formula xml:id="formula_1">A T i O ij A j − O ij = −C T i C j ,<label>(2)</label></formula><formula xml:id="formula_2">where O ij = O T i O j = ∞ t=0 (A t i ) T C T i C j A t j , i, j ∈ {1, 2}. The squared cosine of the subspace angle α k is equal to the k-th principal eigenvalue of O −1 11 O 12 O −1 22 O 21 .</formula><p>The LDS distance (such as geodesic distance <ref type="bibr" target="#b43">[44]</ref> and Martin distance <ref type="bibr" target="#b31">[32]</ref>) can then be defined with the subspace angles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Sparse coding on finite Grassmannian</head><p>As proposed by <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b20">21]</ref>, one can approximate the extended observability by taking the L-order observability matrix, i.e. O(n, L) = [C T , (CA) T , · · · , (CA L−1 ) T ] T . In this way, an LDS can be alternately identified as an ndimensional subspace of R Lm . Sparse coding with LDSs is then performed on finite Grassmannian. Because it is hard to define tractable arithmetical calculations and distance metric on Grassmannian, Harandi et al. <ref type="bibr" target="#b20">[21]</ref> homeomorphically embeds the Grassmannian into the space of symmetric matrices, thus leading to the coding objective:</p><formula xml:id="formula_3">min Z N i=1 X i X T i − K j=1 Z j,i D j D T j 2 F +λ [Z] i 1 . (3)</formula><p>Here, X i ∈ R Lm×n and D j ∈ R Lm×n are the L-order orthonormalized observability matrices of the i-th data LDS and the j-th dictionary atom, respectively, while the coefficient matrix is Z ∈ R K×N and [Z] i denotes the i-th column of Z. The learning task aims to represent each data in the set</p><formula xml:id="formula_4">{X i } N i=1 of size N as a sparse linear combination of the dic- tionary atoms {D j } K j=1 of size K, where Z j,i is the repre- sentation coefficient of X i with respect to D j . The ℓ 1 -norm regularization is employed to the coefficients {[Z] i } N i=1</formula><p>for sparsity assurance; and λ is the sparsity penalty factor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Sparse Coding with Infinite LDSs</head><p>Approximating the observability with a finite matrix results in an unavoidable issue about how to choose the value of the order L: if L is small, it is insufficient to model the asymptotical behavior of the extended observability; increasing the value of L could make the finite observability contain rich information but also increase the computational complexity. In this section, we perform sparse coding directly on the space of extended observability subspaces, i.e. infinite Grassmannian. To this end, the space formulation, the distance metric and arithmetical calculations on infinite Grassmannian should be discussed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Formulation of the infinite Grassmannian</head><p>The group of the extended observability matrices together with the stability and orthonormality constraints for A and C respectively, can be written as</p><formula xml:id="formula_5">O(n, ∞) = {O | O = [C T , (CA) T , (CA 2 ) T , · · · ] T , C T C = I n , | µ(A) |&lt; 1},</formula><p>where I n is a n × n identity matrix, and µ(A) denotes an arbitrary eigenvalue of A. Prior to further derivation, we need to perform orthonormalization on O(n, ∞) by virtue of the Cholesky decomposition. </p><formula xml:id="formula_6">for any V 1 , V 2 ∈ V(n, ∞), V 1 ∼ V 2 if and only if Span(V 1 ) = Span(V 2 ),</formula><p>where Span(V) denotes the subspace spanned by columns of V. The infinite Grassmannian that is embedded in the infinite-dimensional vector space, i.e. G(n, ∞), has already been defined in <ref type="bibr" target="#b43">[44]</ref>. The definition of S(n, ∞) indicates that S(n, ∞) is actually a special G(n, ∞) with an extra intrinsic structure due to the stability and orthonormality constraints to A and C, respectively. We represent LDSs with points in S(n, ∞).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Constructing the coding objective</head><p>Inspired by the method proposed in <ref type="bibr" target="#b20">[21]</ref>, we attempt to embed S(n, ∞) into the space of symmetric matrices via mapping Π : S(n, ∞) → Sym(∞), Π(V) = VV T . The metric on Sym(∞) is naturally induced by the Frobenius norm:</p><formula xml:id="formula_7">W 2 F = Tr(W T W), W ∈ Sym(∞).</formula><p>However, it will encounter the difficulty that the Frobenius norm of a point on Sym(∞) is usually infinite due to the infinite dimensionality. Fortunately, the Frobenius norm of the point in the embedding Π(S(n, ∞)) is guaranteed to be finite, which can be derived by Corollary 1. More generally, the Frobenius norm of the linear combination of the points in Π(S(n, ∞)) is finite, as proven in the following theorem.</p><formula xml:id="formula_8">Theorem 1. Suppose V 1 , V 2 , · · · , V M ∈ S(n, ∞), and y 1 , y 2 , · · · , y M ∈ R, we have M i=1 y i Π(V i ) 2 F = M i,j=1 y i y j V T i V j 2 F , where V T i V j = L −1 i O T i O j L −T j . O T i O j is computed with the Lyapunov equation defined in Equation (2), L i and L j are Cholesky decomposition matrices for O T i O i and O T j O j , respectively. 1</formula><p>Based on Theorem (1), we have two corollaries:</p><formula xml:id="formula_9">Corollary 1. For any V 1 , V 2 ∈ S(n, ∞), we have Π(V 1 ) − Π(V 2 ) 2 F = 2 n− V T 1 V 2 2 F . Furthermore, Π(V 1 ) − Π(V 2 ) 2 F = 2 n k=1 sin 2 α k , where {α k } n k=1</formula><p>are subspace angles between V 1 and V 2 . Corollary 2. The embedding map Π(V) is diffeomorphism (a one-to-one, continuous, and differentiable mapping with a continuous and differentiable inverse), meaning that S(n, ∞) is topologically isomorphic to the embedding Π(S(n, ∞)), i.e. S(n, ∞) ∼ = Π(S(n, ∞)).</p><p>Hence, the sparse coding objective function on infinite Grassmannian is formulated as min</p><formula xml:id="formula_10">Z L(Z, D), where L(Z, D) = N i=1 dist 2 (V i , D) + λ [Z] i 1 ,<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Dictionary learning with infinite LDSs</head><p>Input: X Extract the data system parameters</p><formula xml:id="formula_11">{(Θ i , C i )} N i=1</formula><p>with Algorithm 2 (proposed in the supplementary material); Assign the values of the dictionary system parameters {(Θ r ,C r )} K r=1 by random; for t = 1 to M axN umIters do Learn the sparse codes Z by solving Equation <ref type="formula">(5)</ref>; for r = 1 to K do for k = 1 to n do Compute S(r, k) as defined in Equation <ref type="formula" target="#formula_16">(8)</ref>; Update [C r ] k according to Theorem 3; Updateθ r,k according to Equation <ref type="formula" target="#formula_0">(10)</ref>; end for end for end for Output:</p><formula xml:id="formula_12">{(Θ r ,C r )} K r=1 and, dist(V i , D) = V i V T i − K j=1 Z j,i D j D T j F ; V i and D j are points in S(n, ∞).</formula><p>According to Theorem (1), we can ignore the terms that are irrelevant to Z and rewrite L(Z, D) as</p><formula xml:id="formula_13">N i=1 [Z] T i K(D)[Z] i − 2[Z] T i k(V i , D) + λ [Z] i 1 , (5) where K(D) i,j = D T i D j 2 F and [k(V i , D)] j = V T i D j 2</formula><p>F . This problem is convex as K(D) is positive semi-definite. It can be solved efficiently by using methods like homotopy-LARS algorithm <ref type="bibr" target="#b7">[8]</ref>. We are aware that Equation (5) is similar to the kernel sparse coding for static images which is recently proposed in <ref type="bibr" target="#b18">[19]</ref>. However, our goal is to obtain sparse coding of LDSs using LDSs as dictionary atoms. Moreover, the dedicated algorithm for dictionary learning should be devised, which will be discussed in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Dictionary Learning with Infinite LDSs</head><p>The dictionary learning problem is finding the good dictionary that has a small reconstruction error over all observations while preserving the sparsity penalty. Based on Equation (4), dictionary learning on LDSs can be defined as min Z,D L(Z, D). A common approach for solving this problem is to update Z and D alternately. When the dictionary D is fixed, optimizing the codes Z is exactly the sparse coding problem raised in Equation <ref type="bibr" target="#b4">(5)</ref>. In reverse, to update dictionary atoms with the codes fixed, we break the minimization problem into K sub-minimization problems by updating each atom independently. As we have denoted in Section 3.1, each dictionary atom or data sequence is associated with a parameter tuple consisting of a transition matrix and a measurement matrix. The tuples of the atom D r and the data V i are (Ā r ,C r ) and (A i , C i ), respectively. By substituting the tuples into L(Z, D) and ignoring the terms that are irrelevant to dictionary atoms, dictionary learning can be seen as solving min</p><formula xml:id="formula_14">D K r=1 2Γ(r), where Γ(r) = N i=1 K j=1 j =r Z r,i Z j,i L −1 r ∞ t=0 (Ā t r ) TCT rCjĀ t jL −T j 2 F − N i=1 Z r,i L −1 r ∞ t=0 (Ā t r ) TCT r C i A t i L −T i 2 F .<label>(6)</label></formula><p>Here,L j and L i are the Cholesky decomposition matrices for orthonormalizing the extended observability matrices associated with the dictionary atom D j and the data V i , respectively. By imposing the stability constraint toĀ r and the orthonormality constraint toC r , the sub-problem can be written as min Ar,Cr Γ(r), s.t.C T rCr = I n ; |µ(Ā r )| &lt; 1.</p><p>There are mainly two challenges in solving this subproblem: (1) The infinite summations involved in Equation <ref type="formula" target="#formula_14">(6)</ref> make the transition matrix and the measurement matrix coupled together, hence impeding separate update ofĀ r andC r . (2) For any orthonormal square matrix P ∈ R n×n , the tuple (P −1Ā r P,C r P) derives the same objective Γ(r) as (Ā r ,C r ), implying that (Ā r ,C r ) does not lie in a Euclidean space. The traditional optimization methods adopted in Euclidean space such as gradient decent method and Newton method may be inapplicable to this problem.</p><p>Fortunately, this minimization sub-problem can be efficiently addressed if assuming the transition matrices of the dictionary and the data to be symmetric. As presented in the supplement material, ifĀ r is symmetric, (Ā r ,C r ) can be equivalently transformed to (Θ r ,Ĉ r ), where the diagonal matrixΘ r consists of the eigenvalues ofĀ r ;Ĉ r =C r P −1 r and P r is an orthonormal square matrix. For consistency, we denoteĈ r asC r by ignoring the difference between them in the following context. We can derive: Theorem 2. If the transition matrices of dictionary atoms and the data systems are all symmetric, then Equation <ref type="formula" target="#formula_15">(7)</ref> is equivalent to</p><formula xml:id="formula_16">min Cr,θr n k=1 [C r ] T k S(r, k)[C r ] k s.t.C T rCr = I n ; − 1 &lt;θ r,k &lt; 1, 1 ≤ k ≤ n.<label>(8)</label></formula><formula xml:id="formula_17">Here, S(r, k) = N i=1 K j=1,j =r Z r,i Z j,iCj E(r, j, k)C T j − N i=1 Z r,i C i F(r, i, k)C T i ;</formula><p>Both E(r, j, k) and F(r, i, k) are diagonal matrices:</p><formula xml:id="formula_18">E(r, j, k) = diag([ (1−θ 2 r,k )(1−θ 2 j,1 ) (1−θ r,kθj,1 ) 2 , · · · , (1−θ 2 r,k )(1−θ 2 j,n ) (1−θ r,kθj,n ) 2 ]); F(r, i, k) = diag([ (1−θ 2 r,k )(1−θ 2 i,1 )</formula><p>(1−θ r,k θi,1) 2 , · · · ,</p><formula xml:id="formula_19">(1−θ 2 r,k )(1−θ 2 i,n )</formula><p>(1−θ r,k θi,n) 2 ]); where [θ j,1 , · · · ,θ j,n ] and [θ i,1 , · · · , θ i,n ] denote the eigenvalues of the matrixĀ j and A i , respectively.</p><p>We further break the optimization in Equation <ref type="formula" target="#formula_16">(8)</ref> into n sub-minimization problems. Precisely speaking, we find the optimal pair ([C r ] k ,θ r,k ) by fixing other pairs {([C r ] o ,θ r,o )} n o=1,o =k , thereby leading to the following sub-minimization problem, min</p><formula xml:id="formula_20">[Cr] k ,θ r,k [C r ] T k S(r, k)[C r ] k s.t. [C r ] T k [C r ] k = 1, [C r ] T k [C r ] o = 0, 1 ≤ o ≤ n, o = k, −1 &lt;θ r,k &lt; 1.<label>(9)</label></formula><p>We are able to obtain the solution of [C r ] k for Equation (9) by the following theorem.</p><p>Theorem 3. We denote [C r ] −k ∈ R m×(n−1) as the sub-matrix ofC r by removing the column</p><formula xml:id="formula_21">[C r ] k , i.e. [C r ] −k = [[C r ] 1 , · · · , [C r ] k−1 , [C r ] k+1 , · · · , [C r ] n ], and define W = [w 1 , · · · , w m−n+1 ] ∈ R m×(m−n+1) as the orthonormal basis of the orthonormal complemen- t of [C r ] −k . If u ∈ R (m−n+1)×1</formula><p>is the eigenvector of W T S(r, k)W corresponding to the smallest eigenvalue, then Wu is the optimal solution of [C r ] k for Equation <ref type="bibr" target="#b8">(9)</ref>.</p><p>We apply gradient-based method to updateθ r,k . Since the value ofθ r,k is constrained within (−1, 1), an auxiliary variable ρ r,k is used to replaceθ r,k by settinḡ</p><formula xml:id="formula_22">θ r,k = 2Sig(ρ r,k ) − 1,<label>(10)</label></formula><p>where Sig(·) is a sigmoid function. The gradient of the objective function in Equation <ref type="formula" target="#formula_20">(9)</ref> with respect to ρ r,k is given by ∂Φ(r,k) ∂ρ r,k = 2 ∂Φ(r,k) ∂θ r,k ∂Sig(ρ r,k ) ∂ρ r,k , where Φ(r, k) = [C r ] T k S(r, k)[C r ] k . In our dictionary learning algorithm, we use LDS with Symmetric Transition matrix (LDSST) to model the spatiotemporal data. Given the observed sequences, learning the transition matrix in LDSST is different from that in LD-S. The details are presented in the supplementary material. For reader's convenience, we provide the algorithmic procedures for dictionary learning in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Models Considering the State Covariance</head><p>We have derived sparse coding and dictionary learning by parameterizing each LDS with the tuple (A, C). As shown in Equation <ref type="formula" target="#formula_0">(1)</ref>, the matrix B determines the covariance of the state process. Applying B as an additional descriptor is able to re-discover the dynamical patterns contained in the covariance component when A can not model the dynamics well. In our dictionary learning algorithm, we constrain A to be symmetric, which could somewhat limit the modeling ability of LDSs . Combining the matrix B into the model formulation helps to overcome this limitation.</p><p>The covariance matrix of the whole sequence derived by <ref type="bibr" target="#b4">[5]</ref> is hard to be combined in our models. In this paper, we consider the one-step covariance. Equation (1) demonstrates that the conditional probability of frame y t+1 given x t is expressed as p(y t+1 | x t ) = N (y t+1 ; CAx t +ȳ , CBB T C T + R), with the one-step covariance of CBB T C T +R. We neglect the measurement covariance R as we only focus on the covariance of the state dynamic. As presented in the supplementary material, B = U ′ S ′ 1/2 . For more stable performance, we normalize B by eliminating the scale effect and only reserving the directions term. Then the final one-step covariance we obtain is CU ′ U ′ T C T . Since the covariance locates in the space of symmetric matrices, the distance metric can be induced by Frobenius norm.</p><p>Adding the covariance terms to the sparse coding objective in Equation <ref type="formula" target="#formula_10">(4)</ref> with a linear combination, we obtain</p><formula xml:id="formula_23">L(Z, D) = βL mean + (1 − β)L cov + λ [Z] i 1 , (11) where L mean = N i=1 V i V T i − K j=1 Z j,i D j D T j 2 F ; L cov = N i=1 Ω i − K j=1 Z j,iΩj 2 F</formula><p>; Ω i andΩ j denote the one-step covariances of the i-th data and the j-th dictionary, respectively; β determines the weights of the trade-off between L mean and L cov . Equation <ref type="bibr" target="#b10">(11)</ref> can be reduced to the form similar to Equation 5 for learning the codes.</p><p>The dictionary learning problem is reformulated as solving min D K r=1 Γ(r), where Γ(r) = βΓ mean (r) + (1 − β)Γ cov (r).</p><p>(12) Here, Γ mean (r) has been defined in Equation <ref type="formula" target="#formula_14">(6)</ref>;</p><formula xml:id="formula_24">Γ cov (r) = N i=1 K j=1,j =r Z r,i Z j,i Tr(Ω rΩj ) − N i=1 Z r,i Tr(Ω r Ω i ). SinceΩ r =C rŪ ′ rŪ ′ T rC T</formula><p>r , Γ mean (r) and Γ cov (r) are relevant due to the common factorC r . For simplicity and practicability, we get rid of this relevance by reassigning the covariance as Ω r =H rH T r , whereH r ∈ R m×nv is orthonormal. For data LDS, H i = C i U ′ i ; while for dictionary atoms,H r is independent ofC r . In this way, we updateC r andĀ r by minimizing Γ mean (r) and updateH r by minimizing Γ cov (r), separately. With derivations similar to Theorem 3, the optimizedH r is given as the eigenvectors of the matrix S H corresponding to the n v smallest eigenvalues, where</p><formula xml:id="formula_25">S H = N i=1 K j=1,j =r Z r,i Z j,iHjH T j − N i=1 Z r,i H i H T i .</formula><p>It is easy to develop the algorithm for learning the covariance-involved dictionary. We only need to revise Algorithm 1 by computing the sparse codes with Equation <ref type="bibr" target="#b10">(11)</ref> instead and adding the update ofH r for each atom.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Computational Complexity</head><p>For sparse coding (Equation <ref type="formula" target="#formula_0">(11)</ref>), the key is the kernelmatrix computation. For each kernel, we need to perform Cholesky decomposition, solve the Lyapunov Equation and calculate the matrix multiplication, which scale O(n 3 ), O(n 3 ) and O(mn 2 ), respectively. Recalling that n ≪ m, all these computations scale O(mn 2 ). The number of kernels between dictionary atoms and that between dictionary and data are K 2 and N K, respectively. Thus, the total complexity of sparse coding is O((N K + K 2 )mn 2 ).</p><p>For each subproblem of dictionary learning (Equation (12)), we primarily need to calculate the matrix S(r, k) and find the smallest eigenvector of W T S(r, k)W for minimizing Γ mean (r); calculate the matrix S H and find its n v smallest eigenvectors for minimizing Γ cov (r). Computing S(r, k) and S H scales O(K(N + nm 2 ) + γnm 2 ), where γ denotes the number of non-zero members in the r-th row of Z. We apply the Grassmannian-based Conjugate Gradient Method <ref type="bibr" target="#b11">[12]</ref> to find the smallest eigenvector of W T S(r, k)W, which has a computational cost of O(m 2 ). This operation needs to be repeated for n times until we have all the columns ofC r updated. Thus solving the eigenvector problem costs O(nm 2 ) in total. Similarly, finding the the n v smallest eigenvectors of S H scales O(n v m 2 ). To sum up, the computation complexity of updating one dictionary atom adds up to O(K(N + nm 2 ) + γnm 2 ).</p><p>As shown in Equation <ref type="formula">(3)</ref>, the finite-approximation method <ref type="bibr" target="#b20">[21]</ref> employs the L-order observability O(n, L) ∈ R Lm×n as the representations of the data and dictionary LDSs. With the analysis similar to our models, the computation complexities of the finite method are found to be O(L(N K + K 2 )mn 2 ) for sparse coding and O(K(N + nL 2 m 2 ) + γnL 2 m 2 ) for updating one dictionary atom, respectively. Compared to our infinite models, the finite method scales poorly specially when L is large; we will further demonstrate this in the experimental section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Experiments</head><p>We evaluate our proposed models on two groups of experiments in this section. For the first group, we compare the performance of our sparse coding algorithms with stateof-the-art methods on several benchmark datasets. For the second group, we evaluate the effectiveness of the dictionary learning method. For sake of consistency, we hereafter denote sparse coding on LDSs with arbitrary transition matrices (Section 3) as LDS-SC, sparse coding on LDSs with symmetric transition matrices (Section 4) as LDSST-SC, the LDSST-SC model combining the state covariance (Section 5) as covLDSST-SC, the dictionary learning algorithm (Section 4) as LDSST-DL, LDSST-DL considering the the state covariance (Section 5) as covLDSST-DL. For the compared models, the basic LDS model <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b5">6]</ref> where the Martin distance is applied is denoted as LDS-Martin; sparse coding and dictionary learning on finite Grassmannian <ref type="bibr" target="#b20">[21]</ref> are denoted as gLDS-SC and gLDS-DL, respectively. All experiments are carried out with Matlab 8.1.0.604 (R2013a) on Intel Core i7, 2.90-GHz CPU with 8-GB RAM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.">Benchmark datasets</head><p>A variety of datasets are applied in our experiments, including the hand gesture dataset Cambridge <ref type="bibr" target="#b24">[25]</ref>, the traffic scene analysis dataset UCSD <ref type="bibr" target="#b4">[5]</ref>, the face emotion recognition dataset CK+ <ref type="bibr" target="#b26">[27]</ref>, the dynamic texture recognition dataset DynTex++ <ref type="bibr" target="#b19">[20]</ref>, and three tactile recognition datasets SD <ref type="bibr" target="#b27">[28]</ref>, SPR <ref type="bibr" target="#b27">[28]</ref> and BDH <ref type="bibr" target="#b41">[42]</ref>. For Cambridge and UCSD, the image sequences are treated as the input. The images in Cambridge are resized to 20 × 20 pixels as suggested by <ref type="bibr" target="#b24">[25]</ref>. For DynTex++, we utilize the histogram of LBP from Three Orthogonal Planes (LBP-TOP) <ref type="bibr" target="#b44">[45]</ref> by splitting each video into sub-videos of length 8, with a 6frame overlap. For CK+, the input are the extracted 68landmark of face images. For SD, SPR, and BDH, the tactile series obtained from the array sensors on the robot hands are used to recognize the objects that the robot hands are grasping. Thus, the input are the force values recorded in the sensor arrays along the time axis. We apply the suggested divisions of the training set and testing set by previous works on all datasets except CK+. Specifically, on Cambridge, the first 80 videos of each class are used for testing while the remaining 20 for training <ref type="bibr" target="#b21">[22]</ref>. On UCSD, four random divisions have been performed by the authors in <ref type="bibr" target="#b4">[5]</ref>. In each division, 75% of the sequences are utilized for training and the rest 25% for testing. On DynTex++, the training and testing data are generated with a random fiftyfifty division of the dataset over 20 trials <ref type="bibr" target="#b19">[20]</ref>. The three tactile datasets, i.e. SD, SPR, and BDH, are split randomly into the training and testing sets with a ratio of 9 : 1 over 10 trials <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b41">42]</ref>. For CK+, the authors in <ref type="bibr" target="#b14">[15]</ref> employed the leave-one-out cross-validation scheme. Here, we perform a more challenging division by applying half of the dataset for training while the remaining for testing. For reader's convenience, we illustrate some samples in <ref type="figure" target="#fig_1">Figure 1</ref>. The details of the datasets are presented in the supplement material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.">Sparse coding</head><p>In this section, the training samples are considered to be the dictionary atoms without dictionary learning; and the reconstruction error approach presented in the the supplementary material is adopted for classification.</p><p>Comparison with the state-of-the-arts. We compare the proposed sparse coding methods, i.e. LDS-SC, LDSST-SC and covLDSST-SC, with models that achieved competitive results on Cambridge, UCSD, CK+, SD, SPR, and B-DH. We also implement the LDS-Martin model as a referenced baseline, where the Nearest-Neighbor (NN) method is utilized as the classifier. For proposed models and LDS-Martin, we vary the value of n and report the best results. Additionally for covLDSST-SC, the parameter n v is fixed to be 4 and the weight β is selected from {0.8, 0.6, 0.2}. <ref type="table">Table  1</ref> reports the classification results. We first note that the best <ref type="table">Table 1</ref>. Averaged classification accuracies of the proposed sparse coding methods compared with the state-of-the-arts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets</head><p>References LDS-Martin</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proposed models</head><p>Our best LDS-SC LDSST-SC covLDSST-SC (n v = 4) β = 0.8 β = 0.6 β = 0.2 Cambridge 90.7 <ref type="bibr" target="#b21">[22]</ref>, 83.05 <ref type="bibr" target="#b28">[29]</ref> 88     results of proposed models outperform all compared models on all datasets except UCSD. On UCSD, the method proposed in <ref type="bibr" target="#b4">[5]</ref> achieves the best performance due to its highlycomplicated distance; LDS-SC obtains a comparable accuracy while its complexity to calculate the distance is much simpler. LDSST-SC is found to be worse than LDS-SC as a whole, because the symmetric constraint to transition matrices could limit the modeling ability. With an appropriate β, covLDSST-SC can promote the performance of LDSST-SC significantly, and even outperform LDS-SC in some cases, thus verifying the effectiveness of the state covariance on enhancing the modeling ability of LDSST.</p><p>Varying n. To evaluate the sensitivity of the hidden dimensionality n to the eventual performance, we vary the value of n and report the classification results of LDS-Martin, LDS-ST, LDSST-SC, and covLDSST-SC on Cam-bridge, UCSD, CK+, and SD. <ref type="figure" target="#fig_2">Figure 2</ref> demonstrates that, the LDS-Martin model performs consistently on Cambridge and UCSD but much worse on CK+ and SD. Our models perform consistently on all datasets after n grows beyond a certain value. The model covLDSST-SC can constantly improve the performance of LDSST-SC, which once again validates the importance of the state covariance to the performance of covLDSST-SC.</p><p>Comparison with the finite method. As clarified in Section 3, the model LDS-SC is an infinite generalization of the finite-approximation method gLDS-SC. Thus, we are interested in the asymptotical behavior of gLDS-SC when the observability order L increases. For this purpose, we carry out experiments on Cambridge, UCSD and SPR. As expected, the classification accuracy of gLDS-SC finally converges to that of LDS-SC when L increases, which is  <ref type="figure" target="#fig_3">Figure 3</ref>. In Section 6, we have shown that the computational complexity of gLDS-SC (O(L(N K + K 2 )mn 2 )) is L times of LDS-SC (O((N K + K 2 )mn 2 )). Larger L will cause more computational cost of gLDS-SC. On the dataset UCSD, for example, gLDS-SC needs more training time than LDS-SC when L &gt; 3, as demonstrated in <ref type="figure" target="#fig_3">Figure 3</ref>. Since LDS-SC additionally requires Cholesky decomposition and Lyapunov equation derivation, it performs more slowly than gLDS-SC when L &lt; 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.">Dictionary learning</head><p>As demonstrated by the experimental results in the last section, taking the state covariance term into the algorithm formulation can further improve the performance. Thus, in this section, we implement covLDSST-DL instead of LDSST-DL to perform comparison with other methods. The dictionary atoms are initialized randomly. The codes of training and testing systems with respect to the learned dictionary are fed to a linear SVM <ref type="bibr" target="#b13">[14]</ref> for classification.</p><p>Learning effectiveness analysis. To verify the effectiveness of covLDSST-DL, we also test the baseline model, i.e covLDSST-Rand, in which the dictionary atoms are chosen from the training set randomly and no dictionary learning is involved. Besides, we implement the finite-approximation method gLDS-DL with L = 2, 3. For fair comparison, we use the same classifier (linear SVM) and the same value of n (n = 10), for covLDSST-DL, covLDSST-Rand and gLDS-DL. Experiments are carried out on Cambridge and Dyn-Tex++. On Cambridge, we apply the first half sequences of each class for learning the dictionary while the rest are for testing. On DynTex++, We evaluate the performance of the compared models on a 9-classes subset. In particular, we select the videos of the first 9 classes from the original dataset, thus constructing a smaller dataset with 900 videos in total. Half of the videos are used for learning and the others for testing. <ref type="figure" target="#fig_5">Figure 4</ref> shows that covLDSST-DL consistently outperforms covLDSST-Rand under the varying number of the dictionary atoms. Compared to gLDS-DLs, covLDSST-DL achieves higher accuracies when the dictionary size K is small (e.g. K &lt; 16), and obtains equivalent performance when K is large. As discussed in Section 5, the computational complexity of gLDS-DL is higher than covLDSST-DL. We also display the training time of gLDS-DLs and covLDSST-DL in <ref type="figure" target="#fig_5">Figure 4</ref>. Obviously, gLDS-DLs become much computationally expensive as K increases. Our covLDSST-DL performs scalably even with a large K. In addition to the 9-classes subset, we also evaluate covLDSST-DL on original DynTex++. The model covLDSST-DL reaches a recognition rate of 92.0% when K = 516, which is comparable to that of the Grassmanniankernel-based dictionary learning method <ref type="bibr" target="#b22">[23]</ref>, i.e. 92.8%.</p><p>Dictionary visualization. The model covLDSST-DL is capable of learning the dictionary measurement matrix C and the transition matrix A, explicitly and separately. Thus, we can visualize the learned pairs (A, C) to demonstrate what patterns they have discovered. For simplicity, we perform covLDSST-DL on the 4-class subset of Cambridge, i.e. Flat\Leftward, Flat\Rightward, Spread\Leftward, and Spread\Rightward. Dictionary atoms are initialized randomly by choosing 8 videos from one single class: Flat\Leftward. <ref type="figure" target="#fig_6">Figure 5</ref> (a) visualizes both the initial and the learned pairs. Clearly, more spatial patterns such as the spread-hand shape and the hand-rightward state, have been discovered by the learned C. There are also slight changes in A after the learning. The transition matrices of different atoms have a small difference, indicating that the dynamic within each dictionary is similar to each other, presumably because the movement speed of the hand and the sampling frequency of the camera keep almost consistent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Conclusion</head><p>In this paper, we address the challenging issue about performing sparse coding and dictionary learning on the true space of LDSs that is formulated as an infinite Grassmannian. Compared to the finite-approximation methods, the proposed models are not only theoretically beneficial but also computationally efficient. In addition, we combine the state covariance into the model formulation, thus further improving the performance significantly. The effectiveness of our models is verified by various experiments on different tasks including hand gesture recognition, dynamical scene classification, face emotion recognition, dynamic texture categorization and tactile recognition.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>For any O ∈ O(n, ∞), we derive the Cholesky decomposition L = Chol(O T O), i.e. LL T = O T O, where L is a lower triangular matrix. According to Equation (2), O T O is positive definite as A is stable. Thus, the Cholesky decomposition of O T O always exists, and L is guaranteed to be inventible. The columns of the matrix V = OL −T are orthonormal and span the same subspace as the columns of O. We denote the orthonormalization of O(n, ∞) as V(n, ∞) = {V | V = OL −T , L = Chol(O T O), O ∈ O(n, ∞)}. The quotient space of V(n, ∞) is defined as S(n, ∞) based on the equivalence relation ∼ which is given by:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>Samples of the benchmark datasets: (a) Cambridge; (b) UCSD; (c) CK+; (d) DynTex++; (e) SD and SPR; (f) BDH.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Averaged classification accuracies of varying state dimensionality n on Cambridge, UCSD, CK+, and SD. (β, nv) = (0.2, 4).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Comparisons between LDS-SC and gLDSs. The first three figures display the averaged classification accuracies on Cambridge, UCSD and SPR. The fourth figure demonstrates the training time of the compared models on UCSD. n = 10.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>Comparisons between covLDSST-DLs and gLDS-DLs by varying number of dictionary atoms K on Cambridge and DynTex++. The left two figures show the averaged accuracies, while the right two ones display the training time. (n, β, nv) = (10, 0.2, 4).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 .</head><label>5</label><figDesc>The visualization of the initial and the learned dictionaries on Cambridge. (n, β, nv) = (10, 0.2, 4). (a) Samples of the 4 sub-categories in Cambridge. (b) Visualization of C: rows corresponds to atoms and columns to the state dimensions. (c) Plots of A: different plots display the values of the transition eigenvalues of different atoms. illustrated in</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The proofs of all theorems are given in the supplementary material.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Group action induced distances for averaging and clustering linear dynamical systems with applications to the analysis of dynamic scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Afsari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chaudhry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ravichandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2208" to="2215" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Using dynamic time warping to find patterns in time series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Berndt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clifford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD workshop</title>
		<meeting><address><addrLine>Seattle, WA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="359" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Pattern recognition and machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>springer</publisher>
			<biblScope unit="volume">4</biblScope>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Hierarchical matching pursuit for image classification: Architecture and fast algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2115" to="2123" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Probabilistic kernels for the classification of auto-regressive visual processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Classifying video with kernel dynamic textures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Subspace angles between ARMA models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>De Cock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B. De</forename><surname>Moor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Systems &amp; Control Letters</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="265" to="270" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Fast solution of ℓ1-norm minimization problems when the solution may be sparse</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Donoho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tsaig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="4789" to="4812" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Dynamic textures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Doretto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chiuso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="109" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Design of a flexible tactile sensor for classification of rigid and deformable objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Drimus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kootstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bilberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kragic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Robotics and Autonomous Systems</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="15" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Pattern classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">O</forename><surname>Duda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">E</forename><surname>Hart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Stork</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The geometry of algorithms with orthogonality constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Edelman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">A</forename><surname>Arias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">T</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM journal on Matrix Analysis and Applications</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="353" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Facial expression recognition based on anatomy. Computer Vision and Image Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Eskil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">S</forename><surname>Benli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Liblinear: A library for large linear classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R.-E</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research (JMLR)</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1871" to="1874" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A spatial-temporal framework based on histogram of gradients and optical flow for facial expression recognition in video sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tjahjadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Hauptmann. Exploring semantic inter-class relationships (sir) for zeroshot action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twenty-Ninth AAAI Conference on Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Devnet: A deep event network for multimedia event detection and evidence recounting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2568" to="2577" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Recognizing an action using its name: A knowledge-based approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="17" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Kernel sparse representation for image classification and face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">W</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">.-H</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-T</forename><surname>Chia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Maximum margin distance learning for dynamic texture recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="223" to="236" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Extrinsic methods for coding and dictionary learning on Grassmann manifolds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Harandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lovell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sanderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Riemannian coding and dictionary learning: Kernels to the rescue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Harandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015-06" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Dictionary learning and sparse coding on Grassmann manifolds: An extrinsic solution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Harandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sanderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lovell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3120" to="3127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">On a nonlinear generalization of sparse coding and dictionary learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Vemuri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1480" to="1488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Canonical correlation analysis of video volume tensors for action categorization and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1415" to="1428" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Hierarchical orthogonal matching pursuit for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Pattern Recognition (ACPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="278" to="282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The extended cohn-kanade dataset (CK+): A complete dataset for action unit and emotion-specified expression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Saragih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ambadar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="94" to="101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">St-hmp: Unsupervised spatio-temporal feature learning for tactile data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Madry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kragic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Semi-supervised spectral clustering for image set classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Owens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="121" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Sparse representation for color image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="53" to="69" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Supervised dictionary learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">R</forename><surname>Bach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems (NIPS)</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1033" to="1040" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A metric for ARMA processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Martin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1164" to="1170" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Categorizing dynamic textures using a bag of dynamical systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ravichandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chaudhry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="342" to="353" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Dynamic texture recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Saisan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Doretto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Compressive acquisition of linear dynamical systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">K</forename><surname>Turaga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">G</forename><surname>Baraniuk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Imaging Sciences</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2109" to="2133" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">An approach to time series smoothing and forecasting using the EM algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">H</forename><surname>Shumway</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Stoffer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of time series analysis</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="253" to="264" />
			<date type="published" when="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Statistical computations on Grassmann and Stiefel manifolds for image and video-based recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Turaga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Veeraraghavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="2273" to="2286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">N4SID: Subspace algorithms for the identification of combined deterministicstochastic systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Van Overschee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B. De</forename><surname>Moor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Automatica</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="75" to="93" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Dynamicboost: Boosting time series generated by dynamical systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Binet-Cauchy kernels on dynamical systems and its application to the analysis of dynamic scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="95" to="119" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Shift-invariant dynamic texture recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Woolfe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fitzgibbon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="549" to="562" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Tactile sequence classification using joint kernel sparse coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Robust sparse coding for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="625" to="632" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-H</forename><surname>Lim</surname></persName>
		</author>
		<title level="m">Distance between subspaces of different dimensions</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Dynamic texture recognition using local binary patterns with an application to facial expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pietikainen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="915" to="928" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
