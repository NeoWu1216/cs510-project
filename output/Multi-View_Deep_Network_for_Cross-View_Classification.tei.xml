<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-view Deep Network for Cross-view Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meina</forename><surname>Kan</surname></persName>
							<email>kanmeina@ict.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS)</orgName>
								<orgName type="department" key="dep2">Institute of Computing Technology</orgName>
								<orgName type="institution">CAS</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">CAS Center for Excellence in Brain Science and Intelligence Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiguang</forename><surname>Shan</surname></persName>
							<email>sgshan@ict.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS)</orgName>
								<orgName type="department" key="dep2">Institute of Computing Technology</orgName>
								<orgName type="institution">CAS</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">CAS Center for Excellence in Brain Science and Intelligence Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
							<email>xlchen@ict.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS)</orgName>
								<orgName type="department" key="dep2">Institute of Computing Technology</orgName>
								<orgName type="institution">CAS</orgName>
								<address>
									<postCode>100190</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Multi-view Deep Network for Cross-view Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:43+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Cross-view recognition that intends to classify samples between different views is an important problem in computer vision. The large discrepancy between different even heterogenous views make this problem quite challenging. To eliminate the complex (maybe even highly nonlinear) view discrepancy for favorable cross-view recognition, we propose a multi-view deep network (MvDN), which seeks for a non-linear discriminant and view-invariant representation shared between multiple views. Specifically, our proposed MvDN network consists of two sub-networks, view-specific sub-network attempting to remove view-specific variations and the following common sub-network attempting to obtain common representation shared by all views. As the objective of MvDN network, the Fisher loss, i.e. the Rayleigh quotient objective, is calculated from the samples of all views so as to guide the learning of the whole network. As a result, the representation from the topmost layers of the MvDN network is robust to view discrepancy, and also discriminative. The experiments of face recognition across pose and face recognition across feature type on three datasets with 13 and 2 views respectively demonstrate the superiority of the proposed method, especially compared to the typical linear ones.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The images of an object can be captured by different cameras, different sensors, or from different view angles, which brings about a great challenge of matching images from these different views to recognize them. This kind of problem is generally called as cross-view recognition or heterogenous recognition. Usually, the appearance of samples from different views are quite different from each other, and the large view discrepancy makes it quite challenging to directly compare them based on the image appearance. Substantial efforts have been dedicated to eliminate the view discrepancy or extract view-invariant feature presentations. A straightforward way to deal with the view discrepancy is to project all views into a common space, like the Canonical Correlation Analysis (CCA) <ref type="bibr" target="#b9">[10]</ref> <ref type="bibr" target="#b1">[2]</ref>. CCA attempted to learn two transforms, one for each view, to respectively project the samples from the two views into a common subspace, by maximizing the cross correlation between two views. CCA is only applicable for two-view scenario. To deal with multi-view cases, the pairwise strategy is usually exploited or a more efficient and robust solution is to learn a unified common space shared by all views rather than only two views. For this purpose, the Multiview CCA (MCCA) <ref type="bibr" target="#b24">[25]</ref> <ref type="bibr" target="#b26">[27]</ref> was proposed to obtain one common space for v views. In MCCA, v view-specific transforms, one for each view, are obtained by maximizing the total correlations between any two views. Another commonly used method was proposed in <ref type="bibr" target="#b27">[28]</ref> <ref type="bibr" target="#b4">[5]</ref>, which employed Partial Least Squares (PLS) regression to regress the samples from one view to another. Specifically for photo vs. sketch face recognition, a coupled information-theoretic projection tree <ref type="bibr" target="#b35">[36]</ref> was proposed to reduce the modality gap by maximizing the mutual information between photos and sketches in the quantized feature spaces. In <ref type="bibr" target="#b33">[34]</ref>, a pair of semi-coupled dictionaries were proposed to characterize both views with a mapping function modeling the intrinsic relationship between the two views, and this work was further extended by using a unified model for coupled dictionary and feature space learning in <ref type="bibr" target="#b10">[11]</ref>. Besides, some methods employed either view as the common space, e.g., a pseudo-sketch of photo was synthesized for photo-sketch recognition <ref type="bibr" target="#b31">[32]</ref> <ref type="bibr" target="#b19">[20]</ref>.</p><p>Although the view discrepancy can be minimized by the above methods, the discriminant information, e.g., class label, is not explicitly taken into account, which is unfavorable for recognition or classification. Therefore, quite a few methods has examined how to incorporate the discriminant information as well as the view discrepancy.</p><p>In <ref type="bibr" target="#b20">[21]</ref>[31] <ref type="bibr" target="#b14">[15]</ref>, CCA was extended to Correlation Discriminant Analysis and Discriminative Canonical Correlation Analysis by maximizing the within-class correlation and minimizing between-class correlation across two-view for a discriminant common space. In <ref type="bibr" target="#b6">[7]</ref>[8], Multiview Fisher Discriminant Analysis was proposed to employ the label information for binary classification. In <ref type="bibr" target="#b18">[19]</ref>, Common Discriminant Feature Extraction (CDFE) was proposed to minimize the intra-class scatter and meanwhile maximize the inter-class separability, resulting in very encouraging performance. In <ref type="bibr" target="#b5">[6]</ref>, a large margin approach was proposed to discover a predictive latent subspace representation shared by two views based on an undirected latent space Markov network. In <ref type="bibr" target="#b16">[17]</ref>, Coupled Spectral Regression (CSR) learnt a projection from the observation to the common low-dimensional embedding of the class label through least squares regression. Similarly, in <ref type="bibr" target="#b32">[33]</ref>, two coupled linear regression models were used to project data from different modalities into a common subspace that is directly defined by the class label. In <ref type="bibr" target="#b15">[16]</ref>, a local featurebased discriminant analysis method was proposed to match a forensic sketch and a mug shot photo. Besides, some other methods proposed to apply discriminant classifier in the common space achieved from some unsupervised method, like <ref type="bibr" target="#b17">[18]</ref>. Recently, a generalized multi-view analysis (G-MA) framework was proposed in <ref type="bibr" target="#b28">[29]</ref>, in which the supervised information of each view was coupled with the correlation between views, leading to a discriminant common subspace. Furthermore, a multi-view discriminant analysis (MvDA) approach <ref type="bibr" target="#b12">[13]</ref> was proposed by considering intra-view discriminancy, inter-view discriminancy, and view correlations all together in an unified framework. Benefitted from the supervised information, these discriminant methods usually outperform those unsupervised ones.</p><p>Most of these methods are linear ones, and may become insufficient for challenging scenarios. At first thought, easily they can be extended to non-linear models with kernel trick, such as Kernel Canonical Correlation Analysis (KC-CA) <ref type="bibr" target="#b1">[2]</ref> <ref type="bibr" target="#b21">[22]</ref>. However, it is trival to design a favorable kernel and also it is inefficient to deal with the out-of-sample problem. So recently, several works proposed to employ the more flexible deep neural network to handle the non-linear discrepancy between views, and achieved promising results.</p><p>In <ref type="bibr" target="#b23">[24]</ref>, a Multimodal Deep Auto-encoder was proposed, which took the two views as input and outputted two views too, so as to learn shared representation of both views. In <ref type="bibr" target="#b29">[30]</ref>, a Multimodal Deep Botlzman Machine (DBM) was proposed to jointly model the distribution over two views. In <ref type="bibr" target="#b2">[3]</ref>, Deep Canonical Correlation Analysis (DC-CA) was proposed to learn complex nonlinear transformations for each of the two views so that the resulting representations are highly linearly correlated. As evaluated, DCCA performs better than Kernel CCA [2] <ref type="bibr" target="#b21">[22]</ref>. Specifically for the face recognition across view, e.g. pose and illumination, <ref type="bibr" target="#b36">[37]</ref> proposed a convolutional-like deep neural network to learn the face identity-preserving features, in which all views are reconstructed to a common view, i.e. the canonical view. Furthermore, <ref type="bibr" target="#b37">[38]</ref> proposed a Multi-view Perceptron (MVP) to untangle the identity and view features, and in the meanwhile infer a full spectrum of multipose images given a single 2D face image.</p><p>All these methods <ref type="bibr" target="#b23">[24]</ref>[30] <ref type="bibr" target="#b2">[3]</ref>[37] <ref type="bibr" target="#b37">[38]</ref> employ the deep neural network to model the view distribution and achieved quite promising performance benefited from the favorable ability of non-linearly modeling. However, they are all unsupervised methods, so generally need a successive supervised feature extraction or classifier inducing a better performance for classification or recognition.</p><p>Over all speaking, much of the research has well examined how to deal with the view-discrepancy for recognition or classification across view. However, they are linear which cannot well hand the challenging non-linear scenarios, unsupervised deep approaches which are incapable of recognition, or kernelized supervised methods which may get suck with the out-of-sample problem. To cope with all these challenges, we propose an explicitly non-linear and supervised method, named as Multi-view Deep Network.</p><p>Our proposed Multi-view Deep Network (MvDN) considers the view-discrepancy and discriminancy simultaneously through a deep architecture, resulting in a discriminant and view-invariant representation shared between multiple views. Specifically, our proposed MvDN consists of two sub-networks, the view-specific sub-network and the common sub-network shared by all views. As the loss function, the Rayleigh quotient objective of samples from all views is employed to ensure the discriminancy of the whole network. As a result, the feature representation from the topmost layers of MvDN is robust to view variations, and also discriminative.</p><p>In the following, Section 2 presents the formulation of Multi-view Deep Network followed by the optimization and some discussions, and Section 3 evaluates the MvDN on two databases, followed by a conclusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Multi-view Deep Network (MvDN)</head><p>In this section, we firstly introduce the overview of the proposed Multi-view Deep Network (MvDN) method, and then present the formulation of MvDN, followed by the optimization and some discussions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Overview</head><p>The problem this work mainly attempts to deal with is recognition or classification across view. To deal with this problem, the proposed MvDN seeks for a discriminant and view-invariant representation shared between multiple views. Specifically, MvDN consists of two types of sub-networks, the view-specific sub-networks and the common sub-network, as shown in <ref type="figure" target="#fig_0">Figure 1</ref>. The view-specific sub-networks f i | v i=1 are expected to reduce the discrepancy between that view and the commonality of all views. The commonality of all views is enforced by the following common sub-network g c with the Rayleigh quotient objective.</p><p>The common sub-network is shared by all views in a multiplex way, which means that the common sub-network g c is connected to each view-specific sub-network f i independently. As a result, the common sub-network can extract a view-invariant representation for any single input view. The Fisher loss is calculated with the samples of all views, which ensures the discriminancy and view-invariancy of the representation from the common sub-network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Formulation</head><p>For clear description in the following, we first define some notations. In the whole text, upper-case and lowercase characters represent the matrices and vectors respectively. Given v views, the j-th sample of i-th view is denoted as x i j ∈ R pi×1 , and all the n i samples of i-th view are denoted as</p><formula xml:id="formula_0">X i = [x i 1 , x i 2 , · · · , x i ni ] ∈ R pi×ni . The cor- responding class label is denoted as L i = [l i 1 , l i 2 , · · · , l i ni ], where l i j ∈ {1, 2, 3</formula><p>, · · · , c} representing the class label of sample x i j . Different views contains samples from the same classes but in different views.</p><p>For any sample x i j from i th view, its representation from MvDN y i j is generated by passing through the i th view-specific sub-network and following the common subnetwork successively, formulated as below:</p><formula xml:id="formula_1">y i j = g c f i x i j .<label>(1)</label></formula><p>In Eq. (1), f i , i.e. the sub-network specific to the i th view, is responsible for eliminating the particular information of i th view, and g c , i.e. the common sub-network shared by all views, further extracts the discriminant representation shared by all views. With the view-invariant feature representation y i j , the samples of different views can be effectively compared for recognition or classification. Generally, the two sub-networks is designed with non-linear activation function so as to characterize the challenging discrepancy between views, e.g. sigmoid, tanh, ReLU, etc. Besides, each of the two sub-networks can contain one or more layers resulting in a deep architecture of MvDN.</p><p>To ensure that the representation y i j from MvDN is discriminant and robust to view discrepancy, the Rayleigh quotient of samples from all views is employed as the objective function as follows:</p><formula xml:id="formula_2">[g * c , f * 1 , · · · , f * v ] = arg min gc,f1,··· ,fv T r S y W S y B ,<label>(2)</label></formula><p>where T r(·) denotes the trace of a matrix, S y W denotes the within-class scatter of samples from all v view, and S y B denotes the between-class scatter of samples from all v views. The within-class scatter and between-class scatter in Eq.</p><p>(2) are both calculated with samples from all views, meaning that not only the view-discrepancy but also the intraview and inter-view discriminancy are considered, inducing a discriminant and view-invariant representation shared between all views. More detailed illustration will be given in the following.</p><p>The within-class scatter S y W is calculated as below:</p><formula xml:id="formula_3">S y W = c k=1 v i=1 n ki j=1 y i jk − µ k y i jk − µ k T ,<label>(3)</label></formula><p>where y i jk representing the j th sample of i th view of k th class, and n ki representing the number of samples in i th view of k th class. µ k is the mean of k th class, calculated as µ k = 1</p><formula xml:id="formula_4">n ki v i=1 n ki j=1 y i jk . Eq. (3)</formula><p>can be equivalently reformulated as below with a scale of γ:</p><formula xml:id="formula_5">S y W = γ c k=1 v i,i ′ =1 n ki j,j ′ =1 y i jk − y i ′ j ′ k y i jk − y i ′ j ′ k T ,<label>(4)</label></formula><p>As seen, the within-class scatter S y W calculated from sample of all view not only ensures the closeness of samples from the same class and same view (i.e. when i = i ′ ), but also ensures the closeness of sample from the same class but different views (i.e. when i = i ′ ). In other words, minimizing the within-class scatter S y W ensures the closeness of samples from the same class regardless of the view.</p><p>Similarly, the between-class scater S y B is calculated as below:</p><formula xml:id="formula_6">S y B = c k=1 n k (µ k − µ) (µ k − µ) T ,<label>(5)</label></formula><p>where n k = v i=1 n ki is the number of samples from all views in k th class, µ k is the mean of the k th class defined as in Eq. (3), and µ is the mean of all samples from all views, calculated as µ = 1 n c k=1 v i=1 n ki j=1 y i jk with n = c k=1 n k . In Eq. (5), the between-class scatter is computed with samples from all views, and as so it can maximize the distance of different classes regardless of view.</p><p>As in Eq. <ref type="formula" target="#formula_3">(3)</ref>, the within-class scatter enforces the samples of different view but from the same class close to each other so as to obtain a view-invariant representation shared between all views. Furthermore, as in Eq. (3) and Eq. <ref type="formula" target="#formula_6">(5)</ref>, the within-class scatter and between-class scatter consider both the intra-view discriminancy and inter-view discriminancy can achieve a discriminant representation. Therefore, maximizing the between-class scatter and minimizing the within-class scatter calculated from samples of all view simultaneously as in Eq. <ref type="formula" target="#formula_2">(2)</ref> can result in a discriminant and view-invariant representation shared between all views.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Optimization</head><p>Following the most existing works of deep learning, we employ the gradient descent to optimize the multi-view deep network in Eq. (2). In the following, we will give the details about how to conduct the gradient descent for the Fisher loss, the common sub-network, and the view-specific sub-networks respectively.</p><p>Overall speaking, the whole MvDN is optimized by the gradient descent following the chain rule, i.e. firstly compute the loss of objective, and then prorogate the loss to each layer so as to compute the gradient of each layer, and finally employ gradient descent to update the whole network.</p><p>Step 1: Feed forward and calculate the loss. For each of the v views, the samples of X i are fed forward to the MvDN as in Eq. (1), and the output of the MvDN is denoted</p><formula xml:id="formula_7">as Y i , with Y i = [y i 1 , y i 2 , · · · , y i ni ].</formula><p>Then based on the samples of all views Y = [Y 1 , Y 2 , · · · , Y v ], the loss of the whole network is calculated as in Eq. (2), denoted as</p><formula xml:id="formula_8">J = T r S y W S y B .</formula><p>Step 2: Gradient of the loss layer. As the loss J is directly calculated with the output of the MvDN, there is no parameter involved, so we firstly need to calculate the gradient of J with respect to Y.</p><p>According to <ref type="bibr" target="#b13">[14]</ref>, the gradient of Fisher loss can be calculated as follows:</p><formula xml:id="formula_9">∂T r X T A1X X T A2X ∂X = − 2A 2 X X T A 2 X −1 X T A 1 X X T A 2 X −1 + 2A 1 X X T A 2 X −1<label>(6)</label></formula><p>According to <ref type="bibr" target="#b34">[35]</ref>, the within-class scatter matrix S y W , the total scatter matrix S y T , and the between-class scatter matrix S y B can be reformulated as below:</p><formula xml:id="formula_10">S y W = Y I − c k=1 1 n k e k e kT Y T = YA W Y T , (7) S y T = Y I − 1 n ee T Y T = YA T Y T ,<label>(8)</label></formula><formula xml:id="formula_11">S y B = S y T − S y W = YA B Y T<label>(9)</label></formula><p>with A B = A T − A W . Here, I ∈ R n×n is an identity matrix, e is an n-dimensional vector with all elements as 1, and e k is an n-dimensional vector with e k (i) = 1 if the i th sample of Y belongs to k th class. With Eq. (10), Eq. <ref type="formula">(7)</ref> and Eq. <ref type="formula" target="#formula_11">(9)</ref>, the gradient of J w.r.t. Y can be calculated as follows:</p><formula xml:id="formula_12">∂J ∂Y =− 2A B Y T YA B Y T −1 YA W Y T YA B Y T −1 + 2A W Y T YA B Y T −1<label>(10)</label></formula><p>Step 3: Gradient of common sub-network g c . Denote the output of the view-specific sub-network, i.e. the input of the common sub-network, as</p><formula xml:id="formula_13">z i j = f i (x i j ). The Z i = z i 1 , z i 2 , · · · , z i ni and Z = [Z 1 , Z 2 , · · · ,</formula><p>Z v ] denote the output of the view-specific sub-network for samples of i th view and all views respectively.</p><p>With the gradient of the loss J as in Eq. (10), nextly according to the chain rule we can compute ∂Y ∂gc (the gradient of Y w.r.t. the common sub-network g c ) to update the g c afterwards. Besides, we also need compute ∂Y ∂Z (the gradient of Y w.r.t. Z) to propagate the loss to its next layers, i.e. the view-specific sub-network.</p><p>The common sub-network can include one or more layers, and each layer can be non-linear by using a non-linear activation function, e.g. the sigmoid, tanh, or relu. The gradient of ∂Y ∂gc and ∂Y ∂Z w.r.t. different activation functions can be easily computed according to the 'UFLDL Tutorial' website <ref type="bibr" target="#b22">[23]</ref>.</p><p>Step 4: Gradient of view-specific sub-network f i . ∂Y ∂Z computes the gradient of Y w.r.t. to all views sam-</p><formula xml:id="formula_14">ples of Z 1 , Z 2 , · · · , Z v , i.e. ∂Y ∂Z = ∂Y ∂Z1 , ∂Y ∂Z2 , · · · , ∂Y ∂Zv . As Z i | v i=1</formula><p>are computed from different view-specific subnetworks. Therefore, the loss can be propagated independently, and the gradient of each view-specific sub-network f i can be calculated independently too. Similarly, the gradient of ∂Zi ∂fi w.r.t. different activation functions can be easily computed according to the 'UFLDL Tutorial' website <ref type="bibr" target="#b22">[23]</ref>.</p><p>Step 5: MvDN update via gradient descent. Let us denote the parameter of the whole MvDN as θ = [g c , f 1 , f 2 , · · · , f v ]. At the iteration t + 1, the network parameters are updated through the Limited-memory BFGS (L-BFGS) optimization algorithm, with the gradient calculate as follows:</p><formula xml:id="formula_15">△θ t = △θ t gc , △θ t f1 , △θ t f2 , · · · , △θ t fv △θ t gc = ∂J ∂Y · ∂Y ∂g c △θ t fi = ∂J ∂Y · ∂Y ∂Z i · ∂Z i ∂f i , i = 1, 2, · · · , v.<label>(11)</label></formula><p>Here, J, Y, g c , f i , and Z i are all calculated from the network in the t th iteration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Discussions</head><p>Differences from CCA, MCCA and PLS. CCA <ref type="bibr" target="#b9">[10]</ref>, MCCA <ref type="bibr" target="#b26">[27]</ref>, and PLS <ref type="bibr" target="#b27">[28]</ref> are all unsupervised and linear approaches. Among them, CCA and PLS can only deal with two-view problem, so usually the pair-wise strategy is employed for multiple views. MCCA can naturally deal with multi-view problem. In contrast, our MvDN is not only non-linear and supervised, but also can be applicable for multi-view problem.</p><p>Differences from MvDA, GMA and CDFE. Compared with CCA-like methods, MvDA <ref type="bibr" target="#b12">[13]</ref>, GMA <ref type="bibr" target="#b28">[29]</ref> and CDFE <ref type="bibr" target="#b18">[19]</ref> are supervised. Generally, they can achieve better performance benefited from the considering of discriminancy. However, they are all linear methods, which may be hard to well characterize challenging view discrepancy. In contrast, our MvDN is non-linear and discriminative.</p><p>Differences from Kernel CCA <ref type="bibr" target="#b1">[2]</ref>. There are general theories on CCA, so if having some idea of the underlying distribution, we can easily select proper kernel functions to transform it to a new distribution that is suitable for CCA forming an effective KCCA. In most real-world problems, however, we barely have any idea of the underlying distribution, and it will be difficult to design a favorable kernel function. In contrast, our MvDN equipped with deep neural work can automatically be optimized to form a proper distribution for CCA or Fisher objective. Besides, kernel methods suffer from scalability issue, while deep learning methods including our MvDN naturally scale to large-scale problems benefited from the explicit non-linear mapping. Although deep neural network does not have sophisticated theories yet, there are lots of works about how to design an effective network, e.g. those from Geoffrey Hinton, Yann LeCun, Yoshua Bengio, etc. Moreover, our MvDN is discriminative, while KCCA is not.</p><p>Differences from DCCA, FIP and MVP. To model challenging view variations, DCCA <ref type="bibr" target="#b2">[3]</ref>, FIP <ref type="bibr" target="#b36">[37]</ref> and MVP <ref type="bibr" target="#b37">[38]</ref> propose to employ the deep neural network to capture the highly non-linear discrepancy between views, and have achieved promising performance. However, they are unsupervised, and with a risk of discarding the discriminancy. Take DCCA for example, the deep network may overfit a given dataset with sacrificing the discriminancy. On the contrary, our MvDN is supervised, and it simultaneously considers discriminancy as well as the view discrepancy, which can ensure a representation that is robust to view discrepancy and also discriminant. Besides, FIP and MVP are designed especially for face recognition across pose, which is inapplicable for the general cross-view problem when the dimension of each view is different. On the contrary, DC-CA and our MvDN is applicable even the dimension of each view is different.</p><p>Differences from a vanilla CNN. A vanilla CNN taking all samples from multiple views as one view may be applicable for cross-view problems with homogeneous view representations (e.g. cross-pose), but will fail in general heterogenous cross-view problems where each view is of different dimensionality. Contrastly, our MvDN is general for more than 2 views, even for heterogenous views.</p><p>Overall speaking, the existing methods for cross-view recognition problem are unsupervised linear approaches, supervised linear approaches, unsupervised deep non-linear approaches, or implicitly kernel-based supervised nonlinear approaches. Differently, our multi-view deep network is a supervised and explicitly deep non-linear approach, which can efficiently characterize more challenging view discrepancy for a better discriminant and viewinvariant representation shared between multiple views.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experiments</head><p>In this section, we evaluate the proposed MvDN and a few existing methods on two cross-view face recognition tasks, i.e., face recognition across pose on MultiPIE <ref type="bibr" target="#b8">[9]</ref>, and face recognition across feature type on FRGC <ref type="bibr" target="#b25">[26]</ref> and LFW <ref type="bibr" target="#b11">[12]</ref> datasets. The existing methods of CCA <ref type="bibr" target="#b9">[10]</ref>, KC-CA <ref type="bibr" target="#b1">[2]</ref>, MCCA <ref type="bibr" target="#b26">[27]</ref>, PLS <ref type="bibr" target="#b27">[28]</ref>, MvDA <ref type="bibr" target="#b12">[13]</ref>, GMA <ref type="bibr" target="#b28">[29]</ref>, FIP <ref type="bibr" target="#b36">[37]</ref> and MVP <ref type="bibr" target="#b37">[38]</ref> are evaluated. For the experiments on MultiPIE and FRGC, the face images are cropped into 64x80 with manually labeled eye locations, and for experiments on LFW, the face images are cropped into 80x120. The intensity is used as the feature in all experiments unless otherwise stated. To reduce the feature dimension, Principal Component Analysis (PCA) is applied for each view, and the reduced dimension is set as 200, 300 and 400 on MultiPIE, FRGC and LFW respectively to preserve more than 95% energy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Face recognition across pose</head><p>Face recognition across view angle endeavors to recognize the probe images from one view angle by comparing them with the gallery images that are from another view angle. Face recognition across pose is evaluated on Multi-PIE dataset by taking each pose as one view. The MultiPIE dataset <ref type="bibr" target="#b8">[9]</ref> contains images of 337 subjects under various poses, illuminations and expressions.  For CCA, MCCA, PLS and MvDA, the main parameter is the dimension of the projected representation, and we tune the dimension with step of 50 to report a best result. For GMA, following the suggestions in <ref type="bibr" target="#b28">[29]</ref> we set µ = 1, γ = trace ratio, tune the λ in [0.1 500], and tune the dimension of the projected representation also in step of 50 to report the best result. In our MvDN, each of the three view-specific sub-networks consists of one input layer with 200 neurons(i.e. the PCA dimension) and one hidden layer with 300 neurons equipped with ReLU activation function, and the common sub-network consists of one hidden layer with 1200 neurons equipped with ReLU activation function followed by a linear hidden layer with 200 neurons, resulting in a four-layer deep network including the input layer.</p><p>That is, f i is a 200-300 network, and g c is 300-1200-200 network, where the input layer of g c is also the output of f i with 300 neurons. The neurons of the last layer should be smaller than the number of classes to ensure the S y W and S y B non-singular.</p><p>The evaluation results are shown in <ref type="table" target="#tab_0">Table 1</ref>. As seen, the MCCA and PLS performs the worst, e.g. the rank-1 recognition rate is only about 26%∼40% for recognition between 90 • and 0 • , which is mainly due to no consideration of supervised information. So, a straightforward idea is to apply a supervised method after them. In this work we apply the Linear Discriminant Analysis (LDA) <ref type="bibr" target="#b3">[4]</ref> after the MCCA and PLS, denoted as MCCA+LDA and PLS+LDA respectively. As expected it perform better than both MCCA and PLS. However, as MCCA/PLS and LDA are learnt separately, some discriminancy may be lost in MCCA/PLS which cannot be recalled in the following LDA. Furthermore, the MvDA and GMA perform better than MCCA+LDA, which are benefited from the simultaneous consideration of the view discrepancy and discriminancy. MvDA and GMA are linear, so we tried to make them non-linear for a better performance via the kernel trick, however we hardly got a promising result. Moreover, our MvDN performs the best, with significant improvement even up to 13% for large pose, e.g. 90 • . This is because that the pose varies nonlinearly, especially for large degree, leading to much more challenging view discrepancy, so the existing linear methods, e.g. MvDA and GMA, are unable to model so large discrepancy, while our MvDN is flexible to model high nonlinearity effectively, benefited from the advantages of the deep network. Another observation is that the improvement of MvDN compared to the existing methods becomes larger as the view discrepancy expands, further demonstrating the good robustness of MvDN to view discrepancy. We also compare the proposed MvDN with the deep unsupervised methods FIP and MVP in <ref type="table" target="#tab_1">Table 3</ref>. The methods including CCA , KCCA and our MvDN are evaluated following the same protocol as <ref type="bibr" target="#b36">[37]</ref> <ref type="bibr" target="#b37">[38]</ref>, i.e. the first 200 subjects are used for training and the rest 137 are used for testing, but with much less sample per subject. The results of FIP and MVP are directly from <ref type="bibr" target="#b36">[37]</ref> and <ref type="bibr" target="#b37">[38]</ref> for reference and thus are marked with superscript of * in <ref type="table" target="#tab_1">Table 3</ref>.</p><p>Besides, we evaluate the performance of MvDN w.r.t. the number of neurons in the common-subnetwork. For the MvDN on this dataset, the second from the last is actually the first layer in the common sub-network g c , and this layer captures the variations from all views serving as the bases of the representation in last layer of g c . So, the final performance is essentially influenced by this layer which heavily depends on the number of neurons. Therefore, we mainly investigate the number of neurons in the second layer from last. The results are shown in <ref type="table">Table.</ref> 2, and the averaged rank-1 recognition rate is also shown in <ref type="figure" target="#fig_2">Figure 2</ref>. As seen, MvDN can achieve a better performance even if with only 400 neurons in the second layer from last, and can achieve a further improvement with more neurons, e.g. 1200, 1400. However, it begins to degenerate when the number of neurons is too large to overfit. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Face recognition across feature type</head><p>In some scenarios, different types of feature are favorable for different views. For example, in scenario of images vs. video, intensity and covariance of intensity are preferred for representing the images and videos respectively, or different lighting pre-processing are preferred for different images. In these scenarios, classification is conducted across feature type. To simulate face recognition across feature type, we firstly conduct experiment on the Face Recognition Grand Challenge (FRGC) <ref type="bibr" target="#b25">[26]</ref> with two views, i.e. intensity feature (64x80=5120 dim) as one view and Local Binary Pattern (LBP) <ref type="bibr" target="#b0">[1]</ref> feature (8850 dim) as another view.</p><p>Face Recognition Grand Challenge (FRGC) <ref type="bibr" target="#b25">[26]</ref> is a large-scale face recognition evaluation system. It presents six challenging experiments along with data corpus of 50,000 recordings, taken under both controlled and uncontrolled conditions. We follow the protocol of the challenging Experiment 4 to evaluate our approach. In standard Experiment 4, training set consists of 12,776 images of 222 subjects, the target set consists of 16,028 controlled images, and the query set consists of 8,014 uncontrolled images. In our experiments, we randomly select 50% of the training images for training, and 25% of the target and query images for testing, i.e. 6388 training images, 4007 target images, and 2004 query images.</p><p>For training images, both intensity and LBP feature are available, each type of feature as one view. For target images, only intensity feature is provided, and for query images, only LBP feature is provided. So, the task is to do face verification between the target and query images with different type of features respectively. The performance is measured in terms of ROC curve.</p><p>Similarly as that in the experiment on MultiPIE, CCA, CCA+LDA, PLS, MvDA and GMA is tuned to report the best result. In our MvDN, each of two view-specific subnetworks f i is a 300-300 network, and the common subnetwork g c is a 300-1000-200 network.</p><p>All methods are evaluated in <ref type="figure" target="#fig_3">Figure 3</ref>. The same conclusion can be obtained as that on the face recognition across pose. The unsupervised CCA and PLS perform the worst followed by CCA+LDA. Furthermore, MvDA and GMA perform much better. Finally, the proposed MvDN performs the best, with a significant improvement, benefited from the deep non-linear and discriminant architecture.</p><p>Following the protocol on FRGC, we further compare the linear and deep methods on a more challenging dataset LFW <ref type="bibr" target="#b11">[12]</ref>. The LFW is a large data set consisting of 13,233 uncontrolled images from 5,749 individuals. On this dataset, only those subjects with more than 1 image are employed. Among these subjects sorted according to the number of images of each subject, the first 1000 subjects are used for testing, one image of each subject as target and the rest as query. The rest 680 subjects are used for training. In total, there are 1000 images from 1000 subjects in the Target set, 1221 images from the same 1000 subjects in the Query set, 6943 images from the rest 680 subjects in the Training set. We conduct experiment on the LFW with two views, i.e. intensity feature (120x80=9600 dim) as one view and LBP feature (8850 dim) as another view. For training images, both intensity and LBP feature are available, each type of feature as one view. For target images, only intensity feature is provided, and for query images, only LBP feature is provided. Same as that on FRGC, the task is to do face verification between the target and query images with different type of features respectively. The performance is measured in terms of ROC curve.</p><p>The evaluation results are as shown in <ref type="figure" target="#fig_4">Figure 4</ref>. In <ref type="figure" target="#fig_4">Figure 4</ref>, the linear methods for evaluation include CCA, GMA and MvDA, denoted in dashed lines. For the deep methods we attempt to evaluate the Deep CCA, Deep GAM and our MvDN which share similar objective as the linear methods. However, we cannot get a reasonable performance for Deep CCA even with the authors' released code, so we instead evaluate the Kernel CCA. Besides, as GMA has no deep version, we extend it as Deep GMA following our MvDN scheme, i.e., replace the objective in our MvDN with the G-MA objective but with the same deep architecture. As can be seen, both MvDN and Deep GMA perform much better than the linear MvDA and GMA, and our MvDN outperforms Deep GMA, demonstrating the effectiveness of our deep multi-view scheme.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusions and Future Works</head><p>In this work, we propose a multi-view deep network, which attempts to learn a discriminant and view-invariant representation shared between multiple views. The MvDN consist of two sub-networks, the view-specific sub-network that endeavors to eliminate the discrepancy between each view and the commonality, and the common sub-network with the Fisher loss further aims for a discriminant and view-invariant representation. As evaluated, the proposed MvDN achieves quite promising performance, with significant improvement. In future, we will explore how to extend this framework for feature fusion.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>An overview of Multi-view Deep Network (MvD-N). MvDN consists of two sub-networks, the view-specific subnetwork fi| v i=1 and the common sub-network gc, along with a discriminant Fisher objective.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Specifically, a subset including images from all subjects at 13 poses (-90 • , -75 • , -60 • , -45 • , -30 • , -15 • , 0 • , 15 • , 30 • , 45 • , 60 • , 75 • , 90 • ) under no flush illumination from 4 collecting sessions is selected as the evaluation dataset. This evaluation dataset is divided into 13 subsets according to view angle. For each view angle, 708 randomly selected images of the first 229 subjects are used for training, and 213 randomly selected images of the remaining 108 subjects are used for testing. For training data, all 13 poses are divided into only 3 views, i.e. [-90 • , -45 • ], [-30 • , 30 • ], and [45 • , 90 • ], to simulate a challenging scenario.In the process of testing, the view of 0 • is used as the gallery, and the rest 12 views are used as the probe. All methods are evaluated in terms of rank-1 recognition rate.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Averaged recognition rate of MvDN with different neurons for the 1 st in the common-subnetwork on MultiPIE dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Face verification across feature type on FRGC dataset in terms of ROC.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Face verification across feature type on LFW dataset in terms of ROC.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><head>Table 1 .</head><label>1</label><figDesc>Evaluation of face recogntion across view angle on the MultiPIE dataset.</figDesc><table>Methods 
-90 • 
-75 • 
-60 • 
-45 • 
-30 • 
-15 • 
15 • 
30 • 
45 • 
60 • 
75 • 
90 • 
Average 

PLS 
0.319 0.775 0.892 0.934 0.883 0.981 0.981 0.934 0.906 0.873 0.723 0.268 
0.789 

MCCA 
0.409 0.742 0.822 0.723 0.685 0.920 0.906 0.798 0.747 0.779 0.714 0.376 
0.718 

PLS+LDA 
0.380 0.798 0.869 0.944 0.920 0.995 0.986 0.967 0.883 0.850 0.709 0.319 
0.802 

MCCA+LDA 
0.488 0.662 0.817 0.887 
1.00 
1.00 
1.00 
0.995 0.831 0.803 0.676 0.568 
0.811 

MvDA 
0.568 0.723 0.845 0.920 0.967 
1.00 
1.00 
0.991 0.897 0.864 0.714 0.559 
0.837 

GMA 
0.526 0.732 0.845 0.901 
1.00 
1.00 
1.00 
1.00 
0.906 0.859 0.718 0.573 
0.838 

MvDN (Ours) 0.704 0.822 0.883 0.911 0.991 
1.00 
1.00 
0.991 0.930 0.911 0.798 0.709 
0.887 

Table 2. Performance of MvDN with different neurons for the second layer from last in the common-subnetwork on the MultiPIE dataset. 
Neurons 
-90 • 
-75 • 
-60 • 
-45 • 
-30 • 
-15 • 
15 • 
30 • 
45 • 
60 • 
75 • 
90 • 
Average 

400 
0.695 
0.812 
0.887 
0.897 
0.972 
0.995 
1.000 
0.972 
0.892 
0.878 
0.779 
0.676 
0.871 

600 
0.695 
0.793 
0.850 
0.897 
0.981 
1.000 
1.000 
0.991 
0.887 
0.887 
0.803 
0.695 
0.873 

800 
0.723 
0.822 
0.873 
0.897 
0.986 
0.995 
1.000 
0.981 
0.878 
0.864 
0.808 
0.690 
0.876 

1000 
0.747 
0.808 
0.873 
0.901 
0.977 
0.995 
1.000 
0.977 
0.897 
0.869 
0.812 
0.737 
0.883 

1200 
0.704 
0.822 
0.883 
0.911 
0.991 
1.00 
1.00 
0.991 
0.930 
0.911 
0.798 
0.709 
0.887 

1400 
0.751 
0.836 
0.878 
0.897 
0.981 
1.000 
1.000 
0.986 
0.916 
0.897 
0.831 
0.737 
0.892 

1600 
0.723 
0.840 
0.859 
0.892 
0.977 
0.991 
1.000 
0.977 
0.892 
0.878 
0.798 
0.732 
0.880 

1800 
0.676 
0.822 
0.864 
0.916 
0.991 
1.000 
1.000 
0.986 
0.906 
0.869 
0.798 
0.695 
0.877 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 3 .</head><label>3</label><figDesc>Performance of face recognition across view angle on MultiPIE with 7 poses.</figDesc><table>Methods 
-45 • -30 • -15 • 
15 • 
30 • 
45 • 

CCA[11] 
0.732 0.959 1.00 0.999 0.961 0.688 

KCCA(RBF)[3] 0.801 0.977 0.999 1.00 0.979 0.717 

FIP+LDA[36]  *  0.934 0.964 1.00 0.985 0.956 0.898 

MVP+LDA[37]  *  0.934 1.00 1.00 1.00 0.993 0.956 

MvDN(Ours) 
0.991 0.995 1.00 1.00 0.991 0.976 
</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Acknowledgements</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Face recognition with local binary patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ahonen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hadid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pietikäinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="469" to="481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A kernel method for canonical correlation analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Akaho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Meeting of Psychometric Society (IMP-S)</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep canonical correlation analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bilmes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Livescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Eigenfaces vs. fisherfaces: Recognition using class specific linear projection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">N</forename><surname>Belhumeur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Hespanha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Kriegman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="711" to="720" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Regularized latent least square regression for cross pose face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence (IJCAI)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1247" to="1253" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Predictive subspace learning for multi-view data: A large margin approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multiview fisher discriminant analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Diethe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>Hardoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems Workshop(NIPSW)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Constructing nonlinear discriminants from multiple data views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Diethe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>Hardoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Machine learning and knowledge discovery in databases</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">The cmu multi-pose, illumination, and expression (multipie) face database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
		<idno>TR-07-08</idno>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
		<respStmt>
			<orgName>Carnegie Mellon University Robotics Institute</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Relations between two sets of variates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hotelling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="321" to="377" />
			<date type="published" when="1936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Coupled dictionary and feature space learning with applications to cross-domain image synthesis and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><forename type="middle">F</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Labeled faces in the wild: A database for studying face recognition in unconstrained environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Multi-view discriminant analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="808" to="821" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Introduction to statistical pattern recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Keinosuke</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990" />
			<publisher>Academic Press</publisher>
			<pubPlace>Boston</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning discriminative canonical correlations for object recognition with image sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="251" to="262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Matching forensic sketches to mug shot photos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">F</forename><surname>Klare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis And Machine Intelligence (TPAMI)</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="39" to="646" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Coupled spectral regression for matching heterogeneous faces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1123" to="1128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Locally aligned feature transforms across views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Inter-modality face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="13" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A nonlinear approach for face sketch synthesis and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1005" to="1010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Discriminant analysis in correlation similarity measure space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Takikawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kawade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="577" to="584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Nonlinear feature extraction using generalized canonical correlation analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Melzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Neural Networks (I-CANN)</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="353" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Unsupervised feature learning and deep learning tutorial</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Foo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Suen</surname></persName>
		</author>
		<ptr target="http://ufldl.stanford.edu/wiki/index.php/UFLDLTutorial" />
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Multimodal deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Multiset canonical correlations analysis and multispectral, truly multitemporal remote sensing data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Nielsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing (TIP)</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="293" to="305" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Overview of the face recognition grand challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Scruggs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">W</forename><surname>Bowyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Worek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="947" to="954" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Multi-view canonical correlation analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rupnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Slovenian KDD Conference on Data Mining and Data Warehouses (SiKDD)</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Bypassing synthesis: Pls for face recognition with pose, low-resolution and sketch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="593" to="600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Generalized multiview analysis: A discriminative latent space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">D</forename><surname>Iii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Multimodal learning with deep boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2222" to="2230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A novel method of combined feature extraction for recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Data Mining (ICDM)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1043" to="1048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Face sketch recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology (TCSVT)</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="50" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning coupled feature spaces for cross-modal matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Semi-coupled dictionary learning with applications to image super-resolution and photo-sketch image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Graph embedding and extensions: a general framework for dimensionality reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="40" to="51" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Coupled informationtheoretic encoding for face photo-sketch recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="513" to="520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deep learning identity-preserving face space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="113" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Multi-view perceptron: a deep model for learning face identity and view representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="217" to="225" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
