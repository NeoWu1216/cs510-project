<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Closed-Form Training of Mahalanobis Distance for Supervised Clustering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><forename type="middle">T</forename><surname>Law</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">UMR 7606</orgName>
								<orgName type="institution" key="instit1">Sorbonne Universités</orgName>
								<orgName type="institution" key="instit2">UPMC Univ Paris 06</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<address>
									<addrLine>4 place Jussieu</addrLine>
									<postCode>LIP6, 75005</postCode>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaoliang</forename><surname>Yu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Machine Learning Department</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<addrLine>5000 Forbes Ave</addrLine>
									<postCode>15213</postCode>
									<settlement>Pittsburgh</settlement>
									<region>Pennsylvania</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">UMR 7606</orgName>
								<orgName type="institution" key="instit1">Sorbonne Universités</orgName>
								<orgName type="institution" key="instit2">UPMC Univ Paris 06</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<address>
									<addrLine>4 place Jussieu</addrLine>
									<postCode>LIP6, 75005</postCode>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Machine Learning Department</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<addrLine>5000 Forbes Ave</addrLine>
									<postCode>15213</postCode>
									<settlement>Pittsburgh</settlement>
									<region>Pennsylvania</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Closed-Form Training of Mahalanobis Distance for Supervised Clustering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:38+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Clustering is the task of grouping a set of objects so that objects in the same cluster are more similar to each other than to those in other clusters. The crucial step in most clustering algorithms is to find an appropriate similarity metric, which is both challenging and problem-dependent. Supervised clustering approaches, which can exploit labeled clustered training data that share a common metric with the test set, have thus been proposed. Unfortunately, current metric learning approaches for supervised clustering do not scale to large or even medium-sized datasets. In this paper, we propose a new structured Mahalanobis Distance Metric Learning method for supervised clustering. We formulate our problem as an instance of large margin structured prediction and prove that it can be solved very efficiently in closed-form. The complexity of our method is (in most cases) linear in the size of the training dataset. We further reveal a striking similarity between our approach and multivariate linear regression. Experiments on both synthetic and real datasets confirm several orders of magnitude speedup while still achieving state-of-the-art performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Clustering, that is, grouping a set of objects so that "similar" objects are in the same cluster while "dissimilar" objects are in different clusters is an important task in computer vision, image processing, and machine learning. A challenging key step in most clustering algorithms is to find a similarity measure, or equivalently a distance metric, so that "similar" and "dissimilar" objects can be easily identified. In some applications, experts with domain knowledge can help determine an appropriate distance metric. However, in high dimensional problems with many noisy irrelevant features, it becomes increasingly difficult even for an expert to determine an effective metric, and standard metrics such as the Euclidean distance can lead to very poor results. See <ref type="bibr" target="#b19">[20,</ref><ref type="bibr">Chapter 7]</ref> for an overview on clustering.</p><p>Approaches <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b35">36]</ref> that learn an appropri-Learned Metric <ref type="figure">Figure 1</ref>. Our approach learns a metric in a supervised way so that elements in the same category (here with the same color/shape) are organized into the same cluster.</p><p>ate distance metric from data to produce a desirable clustering result have thus been proposed. These approaches can be roughly divided into two groups: semi-supervised and supervised. The semi-supervised approaches <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b35">36]</ref> take into account the side information provided by the user, usually in the form of a few pairs of items that are expected to be in the same or different clusters. The supervised approaches <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b16">17]</ref> assume the availability of a labeled training sample: {(X i , Y i ) : i = 1, . . . , m}, where X i = {x i,1 , . . . , x i,ni } is a set of objects to be clustered and Y i is the desired clustering of X i . One then learns a distance metric so that one's favorite clustering algorithm, exploiting the learned distance metric, will produce a clustering close to Y i on each X i . Supervised clustering can be seen as a "limit" of semi-supervised clustering when all pairwise relations are given. Supervised clustering and semi-supervised clustering have proven successful in many applications such as foreground/background image segmentation <ref type="bibr" target="#b16">[17]</ref>, video segmentation <ref type="bibr" target="#b31">[32]</ref>, face recognition <ref type="bibr" target="#b2">[3]</ref>, natural language processing <ref type="bibr" target="#b10">[11]</ref>, audio alignment <ref type="bibr" target="#b11">[12]</ref>, detection of important regions in webpages <ref type="bibr" target="#b17">[18]</ref>, etc. <ref type="figure">Fig. 1</ref> illustrates an example where each category is represented by a color/shape, and a metric is learned so that elements in the same category are grouped into the same cluster. Supervised clustering tries to group similar objects together (i.e. in the same cluster) and separate dissimilar objects. In this paper, supervised clustering can be seen as a classification problem where every training object has to be closer to the centroid of its category than to the centroids of other categories.</p><p>We follow the large-margin supervised clustering framework of <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b16">17]</ref>. For simplicity, we assume that the number of clusters is given. This is for example the case in foreground/background image segmentation where the number of clusters is two: one for foreground and one for background. We also assume that, when we are provided with many labeled sets of observations X i (i.e. m ≥ 2), we can link similar objects from the different sets into common clusters. This is for example the case in image cosegmentation <ref type="bibr" target="#b13">[14]</ref> where multiple images are assumed to contain instances of the same object categories, and in dynamic texture segmentation <ref type="bibr" target="#b24">[25]</ref> where multiple videos contain a limited set of common textures (e.g. fire, grass, sea, pond, river) that are manually labeled.</p><p>The large-margin formulations in <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b16">17]</ref>, while excellent in modeling power, cannot scale to large datasets: they require iterative numerical gradient algorithms that converge only at a sublinear rate. In this paper we use a different large-margin formulation than in <ref type="bibr" target="#b16">[17]</ref> that allows us to derive a closed-form solution when we can link the objects between the different sets of observations X i (or simply when m = 1). Our closed-form solution minimizes an upper bound of the empirical risk of our problem, and the complexity to compute this closed-form solution is (in most cases) linear in the size of the training dataset. Consequently, we are able to learn to cluster millions of examples in seconds on a single machine. Our method is easy to implement and scales up to several orders of magnitude larger than previous approaches while obtaining comparable, if not better, clustering and segmentation performance on synthetic and real-world datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Preliminaries</head><p>In this section we provide some technical background for the rest of the paper, and set up the notations throughout.</p><p>Supervised clustering: Let us first formally define the supervised clustering problem. Suppose that we are given a labeled training dataset {(X i , Y i ) : i = 1, . . . , m}, where X i = {x i,1 , . . . , x i,ni } is the i-th group of objects (e.g. X i is the i-th image, and ∀j, x i,j is a patch in X i ) and Y i is the desired clustering of X i (e.g. the foreground/background partition). For simplicity we assume x i,j ∈ R d for all i, j. We will fix a clustering algorithm A such as kmeans that uses a distance metric d to evaluate similarity/dissimilarity. Then the goal of supervised clustering is to learn a common distance function d such that for all i, we have A(X i ; d) ≈ Y i . Some restrictions on the distance function d are needed. In this work we consider the Mahalanobis distance that is parameterized by a symmetric positive semidefinite (PSD) matrix M ∈ R d×d :</p><formula xml:id="formula_0">d M (x, z) = � (x − z) � M (x − z).<label>(1)</label></formula><p>Thus, the goal is to learn the PSD matrix M so that we can produce the desired clusterings Y i using the clustering algorithm A and distance metric d M . Introducing the factorization M = LL � , we see that learning d M is equivalent to learning a linear transformation using L � . The clustering algorithm A: We will need to fix a clustering algorithm. For simplicity we consider the kmeans algorithm <ref type="bibr" target="#b21">[22]</ref>. In details, let X = [x 1 , . . . , x n ] � ∈ R n×d be n objects in R d , and we want to partition them into k clusters. The popular kmeans algorithm aims to find:</p><p>• An assignment matrix Y ∈ {0, 1} n×k with Y ic = 1 if x i belongs to the c-th cluster, and 0 otherwise. Since each object belongs to one and only one cluster, we have Y 1 = 1 where 1 is the vector of all ones with appropriate dimension (or equivalently for all i, � c Y ic = 1 ). In this paper, we also add the constraint rank(Y ) = k (or equivalently for all c, � i Y ic ≥ 1) to avoid empty clusters.</p><formula xml:id="formula_1">• A set of centroids Z = [z 1 , . . . , z k ] � ∈ R k×d .</formula><p>kmeans does so by minimizing the energy function:</p><formula xml:id="formula_2">min Y ∈{0,1} n×k ,Y 1=1,rank(Y )=k Z∈R k×d n � i=1 k � c=1 Y ic · d 2 (x i , z c ),<label>(2)</label></formula><p>where d is a distance metric. Note that given the cen-</p><formula xml:id="formula_3">troid set Z, Y ic = 1 iff for all p we have d(x i , z c ) ≤ d(x i , z p ) (ignoring ties), while given the assignment ma- trix Y , for all Bregman divergence [2] function d, z c = � i Y ic x i / � i Y ic , i.e.</formula><p>the mean vector of the c-th cluster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mahalanobis metric learning:</head><p>Specializing the kmeans formulation (2) to the Mahalanobis distance (1) and defining �X� 2 M = tr(XM X � ), we can write the problem in the matrix form:</p><formula xml:id="formula_4">min Y ∈{0,1} n×k ,Y 1=1,rank(Y )=k Z∈R k×d �X − Y Z� 2 M ,<label>(3)</label></formula><p>which is equivalent to minimizing �(X − Y Z)L� 2 where M = LL � and � · � is the usual Frobenius norm. The centroids Z can be found in closed-form (see e.g. [37, Example 2]): Z = Y † XLL † , where A † is the Moore-Penrose pseudoinverse of the matrix A. Thus, we can eliminate Z in <ref type="formula" target="#formula_4">(3)</ref>:</p><formula xml:id="formula_5">min C∈P �X −ĈX� 2 M = �XM X � , I −Ĉ�,<label>(4)</label></formula><p>where I is the identity matrix, �A, B� = tr(A � B) is the Frobenius inner product, and we define the set</p><formula xml:id="formula_6">P = P n k := {Y Y † : Y ∈{0, 1} n×k ,Y 1 = 1,rank(Y ) = k}. (5)</formula><p>It can be shown that each partition/clustering matrixĈ = Y Y † ∈ P n k is of the following form:</p><formula xml:id="formula_7">C ij = � 1</formula><p>qc , if both i-th and j-th data are in cluster c 0, otherwise ,</p><p>where q c = � i Y ic is the number of instances in cluster c. Note that each matrix in P is (symmetric) PSD and idempotent (i.e. ,Ĉ 2 =Ĉ). Additional restrictions on the assignment matrix Y can be incorporated. For instance, Lajugie et al. <ref type="bibr" target="#b16">[17]</ref> considered the so-called hard prior case where clusters appear consecutively, i.e. if x i and x j are in the kth cluster then x p is also in the k-th cluster for all i ≤ p ≤ j.</p><p>Relaxations: Even optimizing a linear function such as the one in (4) over the nonconvex set P can be hard, thus we consider relaxations. In particular, we exploit the set of rank-k orthogonal projection matrices, which includes P:</p><formula xml:id="formula_9">L = L n k := {Ĉ :Ĉ ∈ S n + ,Ĉ 2 =Ĉ, tr(Ĉ) = k}<label>(7)</label></formula><p>where S n + is the set of n × n symmetric PSD matrices. It is well-known that (see e.g. <ref type="bibr" target="#b28">[29]</ref>) P = L ∩ R n×n + . We then consider the following relaxation of kmeans (in Eq. (4)):</p><formula xml:id="formula_10">min C∈L �XM X � , I −Ĉ� ≡ max C∈L �Ĉ, XM X � �.<label>(8)</label></formula><p>An optimal solution of Eq. (8) is the orthogonal projector onto the k leading eigenvectors of XM X � <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b27">28]</ref>. It also optimizes minĈ ∈L �X − XĈ� 2 M for a given and fixed M . However, this relaxed solution does not return a hard assignment since it is generally not in P. One heuristic to obtain a hard assignment is to perform, as in <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b25">26]</ref>, kmeans over the k leading eigenvectors of XM X � .</p><p>Structured loss: We have shown in <ref type="bibr" target="#b7">(8)</ref> how to partition a dataset once a Mahalanobis distance matrix M has already been learned. We now present how Lajugie et al. <ref type="bibr" target="#b16">[17]</ref> formulated their problem to learn a matrix M ∈ S d + that produces a desirable clustering. For this purpose, they assume that we are given m labeled datasets</p><formula xml:id="formula_11">(X i = [x i,1 , . . . , x i,ni ], C i ) m i=1</formula><p>, where x i,j ∈ R d and the ground truth partition matrix C i ∈ P ni k , that all share a common metric. Naturally, we want to satisfy the maximum number of the following constraints:</p><formula xml:id="formula_12">∀Ĉ i ∈ L ni k \ {C i }, �C i −Ĉ i , X i M X � i � &gt; 0,<label>(9)</label></formula><p>i.e. we want the groundtruth partition C i ∈ P ni k to be the unique solution of the (relaxed) kmeans problem (see <ref type="formula" target="#formula_10">(8)</ref>). Lajugie et al. <ref type="bibr" target="#b16">[17]</ref> thus formulated the supervised clustering problem as an instance of convex structured output prediction:</p><formula xml:id="formula_13">min M �0 � λR(M )+ m � i=1 max Ci∈L n i k (∆(Ĉ i , C i )+� i (M ;Ĉ i )) �<label>(10)</label></formula><p>where the linear penalty loss is</p><formula xml:id="formula_14">� i (M ;Ĉ i ) = �Ĉ i − C i , X i M X � i �<label>(11)</label></formula><p>and ∆(Ĉ i , C i ) = �Ĉ i − C i � 2 measures the difference between the partitionsĈ i and C i . Here λ ≥ 0 is a regularization parameter and the regularizer R(M ) is used to induce desirable properties on the matrix M or on the numerical algorithm (see e.g. <ref type="bibr">[4, page 26]</ref>). Lajugie et al. <ref type="bibr" target="#b16">[17]</ref> use the squared Frobenius norm (i.e. R(M ) = �M � 2 ) to enjoy the same convergence guarantees as PEGASOS <ref type="bibr" target="#b29">[30]</ref>. Optimization: The structured SVM formulation (10) of <ref type="bibr" target="#b16">[17]</ref> can be optimized using the projected subgradient method. Each iteration requires computing the 2k largest eigenvalues (in absolute values) of an n i × n i matrix where n i is the number of observations in each X i . Since the subgradient method converges only at a sublinear rate, the overall training complexity of <ref type="bibr" target="#b16">[17]</ref> can be quite high and hence not likely to scale to large datasets.</p><p>In the following we propose to directly minimize the (nonconvex) worst-case empirical risk (see def in supp material) of the problem in <ref type="bibr" target="#b16">[17]</ref>, and obtain an efficient closedform solution when m = 1 (i.e. , when there is one dataset).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Model and Optimization</head><p>In this section, we present our distance metric learning approach for the supervised clustering problem. We call our method Metric Learning for Cluster Analysis (MLCA). A special case with two clusters is treated in a simplified way in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">A general formulation</head><p>Let us define our prediction rule, i.e. the solution of (8), under the metric matrix M and the relaxed set L in <ref type="formula" target="#formula_9">(7)</ref>:</p><formula xml:id="formula_15">f M,L (X) := argmax C∈L �Ĉ, XM X � �.<label>(12)</label></formula><p>We first remark that our prediction rule is invariant to the scale of the metric matrix M , i.e. for all ε &gt; 0, M and εM predict the same set of clustering matrices: f M,L (X) = f εM,L (X). To avoid the degenerate case M = 0, we can then fix the scale of M by optimizing over all PSD matrices with unit trace. Furthermore, our prediction rule f M,L (X) is not a singleton iff the k-th largest eigenvalue of XM X � equals the (k + 1)-th largest eigenvalue of XM X � , provided that k &lt; n <ref type="bibr" target="#b27">[28]</ref>. To deal with this non-uniqueness, we directly optimize the following (nonconvex) worst-case empirical risk (see definition in supplementary material):</p><formula xml:id="formula_16">min M �0,tr(M )=1 m � i=1 max Ci∈L n i k (∆(Ĉ i , C i ) + ι i (M ;Ĉ i )),<label>(13)</label></formula><p>where ι i is an indicator function defined as:</p><formula xml:id="formula_17">ι i (M ;Ĉ i ) = � 0, ifĈ i ∈ f M,L n i k (X i ) −∞, otherwise .<label>(14)</label></formula><p>Compared with the structural loss formulation (11), we have replaced the linear penalty �Ĉ i − C i , XM X � � with a more severe penalty ι i (M ;Ĉ i ). As in <ref type="bibr" target="#b16">[17]</ref>, we choose the dis-</p><formula xml:id="formula_18">crepancy measure ∆(Ĉ i , C i ) = �Ĉ i − C i � 2 .</formula><p>Interestingly, we can verify that the convex surrogate loss (10) of Lajugie et al. <ref type="bibr" target="#b16">[17]</ref> is an upper bound of (13) (see supplementary material).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">A closed-form solution</head><p>Unlike the convex surrogate in structural SVM (see e.g. (10)), (13) is essentially a bi-level optimization problem, since evaluating the indicator function ι i (M ;Ĉ i ) requires solving an inner problem which also depends on the unknown matrix M . As a consequence, solving <ref type="bibr" target="#b12">(13)</ref> in the general case as we formulated above is difficult. However, in the special case where m = 1, a closed-form solution is readily available. This is our main technical result.</p><p>In the following theorem we denote A (r) as a best rank-r approximation of A, i.e. setting all but the r largest singular values of A to 0. The approximation A (r) is not unique if the r-th and (r + 1)-th largest singular values of A are nonzero and equal. The proportional notation M ∝ A means that there exists a positive real number ε &gt; 0 such that εM = A.</p><p>Theorem 3.1. Let m = 1 in (13) (hence we drop the subscript i) and assume C ∈ S n + ⊇ P n k . Let P X = XX † be the orthogonal projector onto the column space of X and r = min{k, rank(P X CP X )}. Then, M ∝ X † (P X CP X ) (r) (X † ) � is optimal for problem (13), provided that P X CP X � = 0.</p><formula xml:id="formula_19">Theorem 3.2. In Theorem 3.1, if rank(C) ≤ k, then M ∝ X † C(X † ) � is optimal for problem (13), provided that X † C(X † ) � � = 0.</formula><p>The proofs can be found in the supplementary material. In our experiments, these conditions always hold (thanks to our choice of C ∈ P), so that we can simply choose</p><formula xml:id="formula_20">M ∝ X † C(X † ) � .</formula><p>For the degenerate case P X CP X = 0, any feasible M such that rank(XM X � ) = rank(X) is optimal (e.g. M can be the scaled identity matrix). If we denote C = Y Y † ∈ P and use the fact that rank(P X CP X ) = rank(Y � X), then P X CP X = 0 iff Y � X = 0, i.e. the centroids of the desired clusters of the training data are all 0 in the original in-</p><formula xml:id="formula_21">put space (i.e. ∀c ∈ {1, · · · , k}, z c = � i Y ic x i / � i Y ic = 0)</formula><p>, which is unlikely to occur in real world datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">The training algorithm for arbitrary classes</head><p>When we are given many datasets X i (i.e. m &gt; 1), we reduce the general formulation <ref type="bibr" target="#b12">(13)</ref> to the special case m = 1 to use Theorem 3.2 by exploiting a key property of supervised clustering: all observation matrices X i are similar to the test dataset (that we want to partition), and share the same metric. As already mentioned, we also make the assumption that when many labeled sets of observations X i are provided, we can link similar objects from the different sets into common clusters (e.g. in cosegmentation). In other words, we concatenate all the ground truth assignment matrices into a single assignment matrix, from which we then create the training partition matrix.</p><p>In details, we denote</p><formula xml:id="formula_22">X = [X � 1 , . . . , X � m ] � ∈ R n×d the concatenation of all training observation matrices X i ∈ R ni×d , where n = � m i=1 n i is the total number of train- ing data. Similarly, we denote Y = [Y � 1 , . . . , Y � m ] � ∈ {0,</formula><p>1} n×k the appropriate concatenation of all training assignment matrices, where, as before, k ≥ 1 is the number of clusters. For instance, when we partition objects in X i into k categories, objects from the j-th category are assigned 1 in the j-th column of Y i . The ground truth partition matrix for the whole dataset X is formulated as</p><formula xml:id="formula_23">C = Y Y † = Y (Y � Y ) −1 Y � ∈ P n</formula><p>k with rank(C) = k. Now we can apply Theorem 3.2 to conclude that an optimal metric matrix M for problem (13) is given as</p><formula xml:id="formula_24">M ∝ X † C(X † ) � ,<label>(15)</label></formula><p>We recall that the scale of M does not affect the final clustering result f M,L (X) (see Section 3.1), so we can choose M = X † C(X † ) � . The complexity of the above training algorithm is dominated by the computation of X † ∈ R d×n which is O(nd min{d, n}) (see supp. material). For large problems, we can use random subsampling to compute X † even in sublinear time (at the expense of obtaining only an approximate solution). Moreover, sparsity or low-rankness of X can be exploited to largely reduce the complexity. We remark that our method requires full supervision (i.e. knowing the similarity relations for all possible training pairs), which is the price to pay for a comprehensible and efficient closed-form solution.</p><p>Regression problem: We now reveal a connection between our closed-form solution for the Mahalanobis matrix M in <ref type="bibr" target="#b14">(15)</ref> and multivariate linear regression [5, Chapter 3.1.5]. If we treat Y ∈ {0, 1} n×k as the encoded "labels" for k categories and denote J = Y (Y � Y ) −1/2 ∈ R n×k , then the multivariate linear regression problem</p><formula xml:id="formula_25">min W ∈R d×k �XW − J� 2<label>(16)</label></formula><p>has closed-form solution W = X † J ∈ R d×k . We can also think of W from the viewpoint of linear dimensionality reduction: from the original (usually high-dimensional) space R d to the (usually lower-dimensional) space R k . It is now clear that our choice of M in (15) is precisely W W � :</p><formula xml:id="formula_26">W W � = X † JJ � (X † ) � = X † C(X † ) � = M.<label>(17)</label></formula><p>See supplementary material for more details.</p><p>Differences with <ref type="bibr" target="#b16">[17]</ref>: We point out three major differences with <ref type="bibr" target="#b16">[17]</ref>: 1). <ref type="bibr" target="#b16">[17]</ref> used a regularizer to induce good convergence properties. Since we are able to derive a closed-form solution, we do not need such a regularizer for computational purposes. Moreover, our learned matrix M is low-rank (i.e. rank(M ) ≤ rank(C) ≤ k), completely eliminating the need of low-rank regularizers <ref type="bibr" target="#b18">[19]</ref>. 2). We directly optimize the (nonconvex) worst-case empirical risk while <ref type="bibr" target="#b16">[17]</ref> optimized a convex upper bound (see supplementary material). 3). <ref type="bibr" target="#b16">[17]</ref> developed an iterative solver that does not necessarily reach the optimum within a reasonable timeframe while we are able to derive an efficient closed-form solution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Partitioning a test dataset</head><p>To partition a test matrix X t ∈ R nt×d , we solve</p><formula xml:id="formula_27">f M,L n t k (X t ) = argmax C∈L n t k �Ĉ, X t M X � t �,<label>(18)</label></formula><p>with the Mahalanobis matrix M that we learned from the training data (see Section 3.3). The solution is (the orthogonal projector onto) the k leading eigenvectors of the matrix</p><formula xml:id="formula_28">X t M X � t = (X t W )(X t W ) � , where W = X † J and J = Y (Y � Y ) −1/2 .</formula><p>Equivalently, since (X t W ) ∈ R nt×k contains k columns, the k leading eigenvectors of X t M X � t are the k leading left-singular vectors of X t W and can be computed with the (economy size) SVD of X t W = X t X † J.</p><p>Note that the calculation of J = Y (Y � Y ) −1/2 ∈ R n×k can be done efficiently from a labeled assignment matrix Y ∈ {0, 1} n×k with Y 1 = 1 and rank(Y ) = k. By noting y c ∈ {0, 1} n the c-th column of Y , the c-th column of J can be written as j c = y c /�y c � = y c / � y � c 1. The resulting partition matrix C t ∈ f M,L n t k (X t ) is in the relaxed set L nt k (c.f . <ref type="formula" target="#formula_9">(7)</ref>) but may not be in the set P nt k (c.f . (5)). One heuristic to obtain a hard assignment <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b25">26]</ref> is to run kmeans over the k leading left-singular vectors of X t W . Overall, our testing time is similar to the one in <ref type="bibr" target="#b16">[17]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">The Special Two Classes Case</head><p>In this section we specialize our algorithm to the case where there are only two classes, e.g. the foreground and background segmentation problem. We change the partition matrix set L to obtain a more efficient and direct algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Slightly different partition matrices</head><p>When there are only k = 2 classes we can as before concatenate the data into X ∈ R n×d that contains n successive d-dimensional observations x i ∈ R d , i = 1, . . . , n. For each observation x i in X, a score s i ∈ {−1, +1} is assigned where s i &lt; 0 iff x i belongs to a cluster called negative and s i &gt; 0 iff x i belongs to a cluster called positive. Collectively we denote the score vector Y = [s 1 , . . . , s n ] � ∈ {−1, +1} n . For such a matrix X, we associate the ground truth clustering matrix C = Y Y † = Y Y � /�Y � 2 , where clearly C ij &gt; 0 iff x i and x j belong to the same cluster and C ij &lt; 0 otherwise. This representation of the assignment matrix is more succinct than setting k = 2 in Section 3 (which would require Y ∈ R n×2 instead of Y ∈ R n ). The slight price to pay here is a constraint on the kmeans problem (3), i.e. we are now solving the following problem:</p><formula xml:id="formula_29">min Y =[s1,...,sn] � ∈{−1,+1} n ,Z∈R d n � i=1 �x i − s i Z� 2 M . (19)</formula><p>That is, if Z ∈ R d is the centroid of the positive cluster, then −Z is the centroid of the negative cluster, whereas in (3) (for k = 2), there is no constraint on the cluster centroids. We then consider the set of predicted partition matrices:</p><formula xml:id="formula_30">C = C n := {uu � : u ∈ R n , �u� = 1} = L n 1 .<label>(20)</label></formula><p>The interesting observation here is that the new set C, derived here for k = 2, coincides with L n 1 , which originally is a relaxation for k = 1. With this partition set C, we can consider a slight variation of the general formulation <ref type="formula" target="#formula_0">(13)</ref>:</p><formula xml:id="formula_31">min M �0,tr(M )=1 max C∈C � ∆(Ĉ, C) + ι(M ;Ĉ) � ,<label>(21)</label></formula><p>where the indicator function ι is defined similarly as in <ref type="formula" target="#formula_0">(14)</ref> (with L replaced by C). A closed-form solution for problem <ref type="bibr" target="#b20">(21)</ref> follows immediately from Theorem 3.2:</p><p>Theorem 4.1. An optimal solution of the problem <ref type="formula" target="#formula_0">(21)</ref> is M ∝ mm � where m = X † u ∈ R d , provided that C = uu � (e.g. u = Y /�Y �) and X † u � = 0.</p><p>Note that the ground truth partition matrix C is rank-one, hence can always be decomposed as uu � for a unique u (up to sign). In our case, u ∈ R n can be written u = Y /�Y �.</p><p>The condition X † u � = 0 simply excludes the degenerate case, which rarely happens in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Partitioning a test dataset</head><p>We now discuss how to compute efficiently and exactly the partition of a test set of observations X t ∈ R nt×d once we have learned the optimal distance matrix M = mm � , for the special case of k = 2 classes. Like in Section 3.4, partitioning X t is done by first solving f M,C n t (X t ) which corresponds to finding a rankone orthogonal projector onto the leading eigenvector of X t M X � t . Since M = mm � is rank-one, X t m ∈ R nt is an (unnormalized) leading eigenvector of X t M X � t = (X t m)(X t m) � . In order to partition X t , it is thus sufficient to study the signs of X t m: elements with same sign are in the same cluster. Unlike Section 3.4, there is no need to run kmeans as a post-processing step, therefore we obtain a more efficient procedure for the special case of k = 2 classes. The complexity of our training algorithm is again dominated by the computation of X † , which costs O(nd min{n, d}) in a naive implementation.   <ref type="table">Table 3</ref>. Performance on test dataset when the metric is learned on a training dataset with categories with different sizes (without noise).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>We evaluate our method in both clustering and segmentation tasks on synthetic and real-world datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Synthetic dataset</head><p>Clusters with the same size: We evaluate the clustering performance and efficiency of our method on a synthetic dataset inspired by <ref type="bibr" target="#b35">[36]</ref> and illustrated in <ref type="figure" target="#fig_0">Fig 2.</ref> Our training set illustrated in <ref type="figure" target="#fig_0">Fig 2 (a)</ref> is composed of 3-dimensional examples (i.e. d = 3) divided into 3 categories (i.e. desired clusters), with 1, 000 examples per category. Each of these categories is composed of 2 subclusters. The difficulty of the task is that subclusters of the same category are closer to subclusters from different categories than to each other w.r.t. the Euclidean distance. Moreover, some noisy examples are in the subclusters of other categories in the initial space. We generate our test dataset with the same properties (and the same number of examples n = 3, 000) as the training set. We show the kmeans clustering obtained with k = 3 on the test dataset with the Euclidean distance in <ref type="figure" target="#fig_0">Fig 2 (b)</ref> and with our learned metric in <ref type="figure" target="#fig_0">Fig 2 (c)</ref>: all examples with the same color are predicted to be in the same cluster. One can see that the learned metric allows to predict a clustering close to the desired one for the test dataset. The space induced by our metric is illustrated in <ref type="figure" target="#fig_0">Fig 2 (d)</ref>, which shows how similar examples are grouped together.</p><p>We next compare the clustering performance and com-putational efficiency of our method with standard metric learning approaches that are either intended for clustering <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b35">36]</ref>, or known to be efficient <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b34">35]</ref>. The baselines are cross validated on a validation set with similar properties (e.g. to determine the necessary number of gradient descent iterations for <ref type="bibr" target="#b23">[24]</ref>). We exploit all n(n − 1)/2 possible observation pairs to generate the sets of similar (S) and dissimilar (D) pairs in ITML <ref type="bibr" target="#b8">[9]</ref>, KISSME <ref type="bibr" target="#b15">[16]</ref>, XQDA <ref type="bibr" target="#b20">[21]</ref>, PCCA <ref type="bibr" target="#b23">[24]</ref> and <ref type="bibr" target="#b35">[36]</ref>. We do not report the results of XQDA <ref type="bibr" target="#b20">[21]</ref> in <ref type="table">Tables 1 to 3</ref> because it is an extension of KISSME and obtains exactly the same results and training times as KISSME in our toy experiments. We do not evaluate approaches optimized for k-NN classification such as <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b32">33]</ref> other than LMNN because they are not optimized to perform clustering and would perform like LMNN. Given the ground truth clustering matrix C ∈ P n 3 of the test set and the clustering matrixĈ ∈ P n 3 predicted by kmeans with a metric, <ref type="table">Table 1</ref> reports for each method the training time and the loss ∆(Ĉ, C) = �Ĉ − C� 2 , which is a standard clustering error metric <ref type="bibr" target="#b16">[17]</ref> and is the loss that our proposed method tries to minimize during the training phase. While our method and most baselines return comparable clustering performance on this simple dataset 1 , our method is orders of magnitude faster than iterative metric learning approaches such as <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b35">36]</ref>. KISSME is   the only baseline whose training time is less than 1 second in this experiment, but our method is still 100 times faster. KISSME exploits only pairwise constraints through the inverse covariance matrix of pairs of similar (Σ −1 S ) and dissimilar (Σ −1 D ) examples, and it returns a closed-form solution, which is the projection of</p><formula xml:id="formula_32">� Σ −1 S − Σ −1 D �</formula><p>onto the PSD cone. Its complexity is dominated by the computation of Σ S (and Σ D ), which is O(|S|d 2 ) where |S| is the number of similar pairs, and by its inversion followed by the projection of</p><formula xml:id="formula_33">� Σ −1 S − Σ −1 D � onto the PSD cone, which is O(d 3 )</formula><p>. The complexity of KISSME <ref type="bibr" target="#b15">[16]</ref> is then O((|S| + |D|)d 2 + d 3 ) while the complexity of our method is O(nd 2 ) as explained previously. Since |S| + |D| is quadratic in the number of observations n, we have n � |S| + |D|, showing that our method has better complexity than KISSME.</p><p>Clusters with different sizes: We now exploit a training set similar to the previous one except that categories contain a different number of examples: 1000, 2000 and 4000 examples, respectively. The learned metric should be robust to the size of clusters and avoid being biased by the largest category <ref type="bibr" target="#b16">[17]</ref>. The loss scores obtained on a test set with similar properties and 2,000 examples per category are reported in <ref type="table">Table 2</ref>. KISSME obtains worse results than our method, which suggests that it is less robust than our method to the size of categories. PCCA is the only non-clustering baseline that is comparable to our method but it is a lot slower since it requires about 300 gradient descent iterations to learn a metric that produces a desired clustering. <ref type="table">Table 3</ref> reports test scores when the noise is removed from the previous dataset. Only our method, Lajugie et al. <ref type="bibr" target="#b16">[17]</ref> and PCCA produce a desirable clustering. Nonetheless, our method is orders of magnitude faster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Image segmentation</head><p>We evaluate our method in the image segmentation task on the Horses <ref type="bibr" target="#b5">[6]</ref> and Oxford Flowers-17 <ref type="bibr" target="#b26">[27]</ref> datasets composed of 326 and 848 images, respectively. Each image is provided with a ground truth foreground/background segmentation. In this task, an observation is the combined SIFT [23] + color representation (Lab, RGB and intensity of light) of a patch/pixel, its dimensionality is d = 135. To make the method of Lajugie et al. <ref type="bibr" target="#b16">[17]</ref> tractable, we reduce the size of images to a maximum height of 100 pixels (there are then about 10 4 patches per image, and our matrix X has about 2 · 10 6 rows). In the Flowers dataset, the ground truth segmentation of some pixels is uncertain (see <ref type="figure" target="#fig_1">Fig 3)</ref>, these pixels are then ignored from test evaluation and from the training of most methods except <ref type="bibr" target="#b16">[17]</ref> which exploits spatial information. Our method concatenates the representations of the patches of all the training images into a single matrix X, and the assignment matrix Y is created so that background and foreground patches are grouped into 2 clusters.</p><p>We run 5 random splits of 200 training and 30 validation images, we use the rest for test. For evaluation purposes, we use ∆(Ĉ, C) = �Ĉ − C� 2 which evaluates the clustering performance, and Rand loss <ref type="bibr" target="#b12">[13]</ref> </p><formula xml:id="formula_34">(RandLoss(Ĉ, C) = 1 n(n−1) �YĈY � C − Y C Y � C � 2 where YĈ ∈ {0</formula><p>, 1} n×2 is the segmentation predicted forĈ ∈ P n 2 ), a standard evaluation metric in image segmentation <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b33">34]</ref>. <ref type="bibr" target="#b1">2</ref> We call our method presented in Section 4 univariate MLCA and our method presented in Section 3 when k = 2 (i.e. the assignment matrix Y has two columns) multivari-  ate MLCA. In <ref type="table" target="#tab_2">Table 4</ref>, we compare our methods to a popular image segmentation framework <ref type="bibr" target="#b30">[31]</ref> and to metric learning approaches known to be efficient <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b23">24]</ref> or optimized for clustering <ref type="bibr" target="#b16">[17]</ref>. Overall, multivariate MLCA is better than the baselines w.r.t. all the evaluation metrics and is much faster to train. This is expected for the ∆ loss since it is the evaluation metric that it optimizes during training. Its univariate counterpart obtains better rand loss but worse ∆ loss results, which means that it obtains better image segmentation performance but worse clustering performance (the ∆ loss takes into account the size of each cluster whereas the rand loss does not, see <ref type="bibr" target="#b16">[17,</ref><ref type="bibr">Section 3.2]</ref>). <ref type="bibr" target="#b2">3</ref> We note that we outperform the method of <ref type="bibr" target="#b16">[17]</ref> which is also optimized for clustering. This illustrates the fact that it may be better to consider the training set as a whole instead of as independent subsets when it is possible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Dynamic texture segmentation</head><p>We evaluate our method in the segmentation task of dynamic textures on the SynthDB <ref type="bibr" target="#b6">[7]</ref> dataset. The goal is to segment video textures from 12 texture categories such as sea, river, pond, grass, trees, fire etc. The SynthDB dataset is a challenging dataset that contains grayscale videos, and each video contains distinct video textures. A major difficulty is that adjacent textures may be hard to distinguish because their static grayscale representations may be very similar. Since their dynamic appearances are usually more different, motion descriptors are commonly exploited.</p><p>We use the part of the dataset containing synthetic collages of 3 videos as in <ref type="bibr" target="#b31">[32]</ref>. The current state-of-the-art method for segmenting these videos was proposed in <ref type="bibr" target="#b31">[32]</ref>, using filter-based motion features within a hierarchical segmentation framework, and using a custom metric learned to compare those motion features. We use the features kindly provided by Teney et al. <ref type="bibr" target="#b31">[32]</ref>: each feature of dimensionality d = 154 represents a segment made of the histograms of pixel color and responses to motion filters within the segment. As in <ref type="bibr" target="#b31">[32]</ref>, we exploit both ground truth segmentation masks and semantic annotations of the 12 texture categories to learn a metric. The ground truth segmentation is used as an oracle to simulate the hierarchical segmentation of training videos, and the features within the segments during the process are retained as training data. The texture categories are used to provide additional training similarity constraints between segments from different training videos, but known to belong or not to the same categories. From these similarity constraints, we filter out segments that are very different from segments from the same category, and we generate from the remaining segments a ground truth partition of k = 12 clusters with same size (otherwise some clusters are 60 times larger than others). We then keep about 1,000 samples per cluster. We apply our metric learning method to the generated desired partition and integrate the learned metric in the framework of <ref type="bibr" target="#b31">[32]</ref>.</p><p>We evaluate our learned metric on the segmentation of the test sequences as in <ref type="bibr" target="#b31">[32]</ref>, and obtain essentially identical performance. <ref type="table" target="#tab_4">Table 5</ref> reports the training time and rand index of our learned metric averaged on 100 test videos (note that RandIndex(Ĉ, C) = 1 -RandLoss(Ĉ, C)). The results reported in <ref type="bibr" target="#b31">[32]</ref> exploit a specific strategy taking into account the scale of segments when learning their Mahalanobis metric, which we do not. Our method is orders of magnitude faster and returns a distance matrix with smaller rank (rank(M ) = 12). We report the scores obtained with other metric learning baselines. They all obtain worse segmentation results. Particularly, KISSME and XQDA obtain very poor performances, even when balancing the number of similar/dissimilar constraints. This may be because their covariance matrices Σ S and Σ D are ill-conditioned and their inversion is problematic, and/or their approach is less suitable when the number of clusters is large.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Discussion</head><p>We have presented an efficient method for learning a metric to perform supervised clustering. Our approach is simple and its complexity is linear in the number of observations. Three key factors contribute to the efficiency of our method. First, using the assumption that all examples are drawn i.i.d. and can be combined into a single dataset, our method focuses on the dataset as a whole instead of many subsets. Second, we exploit relaxations of our problem to directly optimize the (nonconvex) worst-case empirical risk in a convenient way. Finally, by exploiting algebraic properties on eigenvalues and eigenvectors, we are able to find a closed-form solution of our relaxed problem, which also happens to bear a pleasant similarity to multivariate linear regression. Experiments on both synthetic and real datasets confirm the efficiency of our method.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>(a) Original training dataset, one color for each desired cluster, (b) Clustering obtained by kmeans with the Euclidean distance (all the examples with same color are predicted to be in the same cluster) on the test dataset in the original input space, (c) Clustering obtained by kmeans with our learned metric, (d) Projection of the test set in the space induced by our metric.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>left to right: original image, ground truth segmentation (for the Flowers dataset: foreground in red, background in green and uncertain segmentation in black), segmentation predicted with univariate MLCA, segmentation predicted with multivariate MLCA.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 4 .</head><label>4</label><figDesc>Test segmentation error results (average and standard error) on Horses and Oxford Flowers-17 datasets. Lower is better.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 5 .</head><label>5</label><figDesc>Rand index averaged over the 100 test sequences of the SynthDB dataset in the segmentation task of dynamic textures.</figDesc><table></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The test error of our method and most baselines is 0 when there are no noisy examples in the initial space.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">In the case of univariate MLCA, we convert our rank-1 assignment matrix into a rank-2 matrix to compute the ∆ loss and rand loss.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Nevertheless, univariate MLCA is faster than multivariate MLCA at test time since it does not require post-processing by kmeans: the segmentation of a test image with multivariate MLCA takes about 0.3 seconds whereas univariate MLCA segments an image in less than 10 −3 seconds.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments: This work was done while Marc Law was visiting Carnegie Mellon University thanks to a French General Directorate for Armament (Direction Générale de l'Armement) fellowship. We thank Damien Teney for providing his features and testing our learned metric and other baselines on the SynthDB dataset. We also thank Huishuai Zhang, Mrinmaya Sachan and the anonymous reviewers for their helpful comments. This work was partially supported by NIH R01GM114311.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning spectral clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="305" to="312" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Clustering with bregman divergences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Merugu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ghosh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1705" to="1749" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning a Mahalanobis metric from equivalence constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bar-Hillel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hertz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shental</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weinshall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="937" to="965" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Convex Optimization Algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Bertsekas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Athena Scientific</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Pattern recognition and machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning to segment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Borenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ullman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="315" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Modeling, clustering, and segmenting video with mixtures of dynamic textures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="909" to="926" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Normalized cut segmentation code. copyright 2004 university of pennsylvania</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
		<respStmt>
			<orgName>Computer and Information Science Department</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Information-theoretic metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">V</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="209" to="216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">On a theorem of weyl concerning eigenvalues of linear transformations i</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the National Academy of Sciences of the United States of America</title>
		<meeting>the National Academy of Sciences of the United States of America</meeting>
		<imprint>
			<date type="published" when="1949" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page">652</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Supervised clustering with support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Finley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="217" to="224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Metric learning for temporal sequence alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Garreau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lajugie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Arlot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Comparing partitions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hubert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arabie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of classification</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="193" to="218" />
			<date type="published" when="1985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multi-class cosegmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="542" to="549" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Non-linear metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kedem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tyree</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">R</forename><surname>Lanckriet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2573" to="2581" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Large scale metric learning from equivalence constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kostinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hirzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2288" to="2295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Large-margin metric learning for constrained partitioning problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lajugie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Arlot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="297" to="305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Quadruplet-wise image similarity learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Fantope regularization in metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1051" to="1058" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Mining of massive datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rajaraman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Ullman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Person re-identification by local maximal occurrence representation and metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2197" to="2206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Least square quantization in PCM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Lloyd</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1957" />
		</imprint>
		<respStmt>
			<orgName>Bell Telephone Laboratories Paper</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Distinctive image features from scaleinvariant keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Pcca: A new approach for distance learning from sparse pairwise constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mignon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jurie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Clustering dynamic textures with the hierarchical em algorithm for modeling video. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mumtaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Coviello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">R</forename><surname>Lanckriet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1606" to="1621" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">On spectral clustering: Analysis and an algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="849" to="856" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A visual vocabulary for flower classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-E</forename><surname>Nilsback</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1447" to="1454" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Optimality conditions and duality theory for minimizing sums of the largest eigenvalues of symmetric matrices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Overton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Womersley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical Programming</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="321" to="357" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Approximating k-means-type clustering via semidefinite programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM journal on optimization</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="186" to="205" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Pegasos: Primal estimated sub-gradient solver for svm. Mathematical programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shalev-Shwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srebro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cotter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="page" from="3" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Normalized cuts and image segmentation. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="888" to="905" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning Similarity Metrics for Dynamic Scene Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Discriminative metric learning by neighborhood gerrymandering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Trivedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shakhnarovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3392" to="3400" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Toward objective evaluation of image segmentation algorithms. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Unnikrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pantofaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="929" to="944" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Distance metric learning for large margin nearest neighbor classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="207" to="244" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Distance metric learning with application to clustering with side-information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="505" to="512" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Rank/norm regularization with closed-form solutions: Application to subspace clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Schuurmans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Uncertainty in Artificial Intelligence (UAI)</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
