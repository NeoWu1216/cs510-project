<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Action Recognition in Video Using Sparse Coding and Relative Features</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Analí</forename><surname>Alfaro</surname></persName>
							<email>ajalfaro@uc.cl</email>
							<affiliation key="aff0">
								<orgName type="institution">Universidad Catolica de Chile Santiago</orgName>
								<address>
									<country key="CL">Chile</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Domingo</forename><surname>Mery</surname></persName>
							<email>dmery@ing.puc.cl</email>
							<affiliation key="aff1">
								<orgName type="institution">Universidad Catolica de Chile Santiago</orgName>
								<address>
									<country key="CL">Chile</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alvaro</forename><surname>Soto</surname></persName>
							<email>asoto@ing.uc.cl</email>
							<affiliation key="aff2">
								<orgName type="institution">Universidad Catolica de Chile Santiago</orgName>
								<address>
									<country key="CL">Chile</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Action Recognition in Video Using Sparse Coding and Relative Features</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:36+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work presents an approach to category-based action recognition in video using sparse coding techniques. The proposed approach includes two main contributions: i) A new method to handle intra-class variations by decomposing each video into a reduced set of representative atomic action acts or key-sequences, and ii) A new video descriptor, ITRA: Inter-Temporal Relational Act Descriptor, that exploits the power of comparative reasoning to capture relative similarity relations among key-sequences. In terms of the method to obtain key-sequences, we introduce a loss function that, for each video, leads to the identification of a sparse set of representative key-frames capturing both, relevant particularities arising in the input video, as well as relevant generalities arising in the complete class collection. In terms of the method to obtain the ITRA descriptor, we introduce a novel scheme to quantify relative intra and inter-class similarities among local temporal patterns arising in the videos. The resulting ITRA descriptor demonstrates to be highly effective to discriminate among action categories. As a result, the proposed approach reaches remarkable action recognition performance on several popular benchmark datasets, outperforming alternative state-ofthe-art techniques by a large margin.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>This work presents a new method for action recognition in video that incorporates two novel ideas: (1) A new method to select relevant key-frames from each video, and (2) A new method to extract an informative video descriptor. In terms of our technique for key-frame selection, previous works have also built their action recognition schemes on top of key-frames <ref type="bibr" target="#b44">[45]</ref>, Snippets <ref type="bibr" target="#b32">[33]</ref>, Exemplars <ref type="bibr" target="#b39">[40]</ref>, Actoms <ref type="bibr" target="#b13">[14]</ref>, or other informative subset of short video sub-sequences <ref type="bibr" target="#b26">[27]</ref> <ref type="bibr" target="#b29">[30]</ref>. As a relevant advantage, by representing a video using a compressed set of distinctive subsequences, it is possible to eliminate irrelevant or noisy temporal patterns and to reduce computation, while still retaining enough information to recognize a target action <ref type="bibr" target="#b3">[4]</ref>. Fur-thermore, it is possible to obtain a normalized video representation that avoids distracting sources of intra-class variation, such as different velocities in the execution of an action.</p><p>Previous works have mainly defined a set of key-frames using manual labelling <ref type="bibr" target="#b13">[14]</ref>, clustering techniques <ref type="bibr" target="#b44">[45]</ref>, or discriminative approaches <ref type="bibr" target="#b29">[30]</ref>. In the case of clustering techniques, the usual loss functions produce a set of keyframes that captures temporal action patterns occurring frequently in the target classes. As a relevant drawback, training instances presenting less common patterns are usually poorly represented <ref type="bibr" target="#b46">[47]</ref> and, as a consequence, the diversity of intra-class patterns is not fully captured. In the case of discriminative approaches, identification of relevant keyframes is usually connected to classification stages, focusing learning on mining patterns that capture relevant interclass differences. As a relevant drawback, the mining of key-frames again does not focus directly on effectively capturing the diversity of intra-class patterns that usually arise in complex action videos.</p><p>In contrast to previous work, our method to select keyframes explicitly focuses on an effective mining of relevant intra-class variations. As a novel guiding strategy, our technique selects, from each training video, a set of key-frames that balances two main objectives: (i) They are informative about the target video, and (ii) They are informative about the complete set of videos in an action class. In other words, we simultaneously favour the selection of relevant particularities arising in the input video, as well as meaningful generalities arising in an entire class collection. To achieve this, we establish a loss function that selects from each video a sparse set of key-frames that minimizes the reconstruction error of the input video and the complete set of videos in the corresponding action class.</p><p>In terms of our technique to obtain an informative video descriptor, most current video descriptors are based on quantifying the absolute presence or absence of a set of visual features. Bag-of-Words schemes are a good example of this strategy <ref type="bibr" target="#b18">[19]</ref>. As a relevant alternative, recent works have shown that the relative strength <ref type="bibr" target="#b41">[42]</ref>, or sim-ilarity among visual features <ref type="bibr" target="#b28">[29]</ref>, can be a powerful cue to perform visual recognition. As an example, the work in <ref type="bibr" target="#b41">[42]</ref> demonstrates a notable increase in object recognition performance by using the relative ordering, or rank, among feature dimensions. Similarly, the work in <ref type="bibr" target="#b20">[21]</ref> achieves excellent results using a feature coding strategy based on similarities among pairs of attributes (similes).</p><p>In contrast to previous work, our method to obtain a video descriptor is based on quantifying relative intra and inter-class similarities among local temporal patterns or key-sequences. As a building block, we use our proposed technique to identify key-frames that are augmented with neighbouring frames to form local key-sequences encoding local action acts. These key-sequences, in conjunction with sparse coding techniques, are then used to learn temporal class-dependent dictionaries of local acts. As a key idea, cross-projections of acts into dictionaries coming from different temporal positions or action classes allow us to quantify relative local similarities among action categories. As we demonstrate, these similarities prove to be highly discriminative to perform action recognition in video.</p><p>In summary, our method initially represents an action in a video as a sparse set of meaningful local acts or keysequences. Afterwards, we use these key-sequences to quantify relative local intra and inter-class similarities by projecting the key-sequences to a bank of dictionaries encoding patterns from different temporal positions or actions classes. These similarities form our core video descriptor that is then fed to a suitable classifier to access action recognition in video. Consequently, this work makes the following three main contributions:</p><p>• A new method to identify a set of relevant key-frames in a video that manages intra-class variations by preserving essential temporal intra-class patterns. • A new method to obtain a video descriptor that quantifies relative local temporal similarities among local action acts. • Empirical evidence indicating that the combination of the two previous contributions provides a substantial increase in action recognition performance with respect to alternative state-of-the-art techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>There is a large list of works related to category-based action recognition in video, we refer the reader to <ref type="bibr" target="#b0">[1]</ref> for a suitable review. Here, we focus our review on methods that also decompose the input video into key-sequences, propose related video descriptors, or use sparse coding. Key-sequences: Several previous works have tackled the problem of action recognition in video by representing each video by a reduced set of meaningful temporal parts. Weiland and Boyer <ref type="bibr" target="#b39">[40]</ref> propose an action recognition approach based on key-frames that they refer to as Exemplars.</p><p>Schindler and Van Gool <ref type="bibr" target="#b32">[33]</ref> add motion cues by studying the amount of frames, or Snippets, needed to recognize periodic human actions. Gaidon et al. <ref type="bibr" target="#b13">[14]</ref> present an action recognition approach that is built on top of atomic action units, or Actoms. As a relevant disadvantage, at training time, these previous methods require a manual selection or labelling of a set of key-frames or key-sequences.</p><p>Discriminative approaches to identify key-frames have also been used. Zhao and Elgammal <ref type="bibr" target="#b44">[45]</ref> use an entropybased score to select as key-frames the most discriminative frames from each video. Liu et al. <ref type="bibr" target="#b25">[26]</ref> propose a method to select key-frames using the Adaboost classifier to identify highly discriminative frames for each target class. Extending DPMs <ref type="bibr" target="#b11">[12]</ref> to action recognition, Niebles et al. <ref type="bibr" target="#b26">[27]</ref> represent a video using global information and short temporal motion segments. Raptis and Sigal <ref type="bibr" target="#b30">[31]</ref> use a video frame representation based on max-pooled Poselet <ref type="bibr" target="#b6">[7]</ref> activations, in conjunction with a latent SVM approach to select relevant key-frames and learn action classifiers. In contrast to these previous approaches, we do not assume that all videos in an action class share a common set of key-sequences. In our case, we adaptively identify in each video key-sequences that consider reconstruction error and similarities to other local temporal patterns present in the class collection. Video descriptors: Extensive research has been oriented to propose suitable spatio-temporal low-level features <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b36">37]</ref>. In our case, we build our descriptor on top of key-sequences that are characterized by low-level spatio-temporal features. In this sense, the proposed descriptor is more closely related to mid-level representations, such as the ones described in <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b38">39]</ref>. In contrast to our approach, current mid-level representations do not encode local temporal similarities among key-sequences.</p><p>In terms of encoding similarities among training instances, Kumar et al. <ref type="bibr" target="#b20">[21]</ref> propose a method that exploits facial similarities with respect to a specific list of reference people. Yagnik et al. <ref type="bibr" target="#b41">[42]</ref> presents a locality sensitive hashing approach that provides a feature representation based on relative rank ordering. Similarly, Parikh and Grauman <ref type="bibr" target="#b28">[29]</ref> use a max-margin approach to learn a function that encodes relative rank ordering. Wang et al. <ref type="bibr" target="#b35">[36]</ref> present a method that uses information about object-class similarities to train a classifier that responds more strongly to examples of similar categories than to examples of dissimilar categories. These previous works share with our approach the idea of explicitly encoding the relative strength of visual properties to achieve visual recognition. However, they are not based on sparse coding, or they do not exploit relative temporal relations among visual patterns. Sparse Coding: A functional approach to action recognition is to create dictionaries based on low-level representations. Several methods can be used to produce a suitable dictionary, BoW <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24]</ref>, Fisher vectors <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b4">5]</ref>, random forest <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b43">44]</ref>, and sparse coding techniques <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b7">8]</ref>. Tran et al. <ref type="bibr" target="#b34">[35]</ref> use motion information from human body parts and sparse coding techniques to classify human actions in video. For each body part, they build a dictionary that integrates information from all classes. Similarly to our approach, the atoms in each dictionary are given by the training samples themselves. As a main drawback, at training time, this method requires manual annotation of human body parts. Guha and Ward <ref type="bibr" target="#b15">[16]</ref> explore several schemes to construct an overcomplete dictionary from a set of spatio-temporal descriptors extracted from training videos, however, this method does not use key-sequences or relative features in its operation. Castrodad et al. <ref type="bibr" target="#b7">[8]</ref> propose a hierarchical two-level sparse coding approach for action recognition. In contrast to our approach, this work uses a global representation that discards local temporal information. Furthermore, it does not exploit key-frames or intra-class relations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Our Method</head><p>Our proposed method has three main parts: i) Video Decomposition, ii) Video Description, and iii) Video Classification. We explain next the main details behind each of these parts. <ref type="figure" target="#fig_0">Fig. 1</ref> summarizes the main steps to decompose an input video into a set of K key-sequences. We explain next the details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Video Decomposition</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Selection of Key-Frames</head><p>We address the selection of key-frames from an action video as a reconstruction problem using sparse coding techniques <ref type="bibr" target="#b9">[10]</ref>. Let V = {v i } p i=1 be a set of p training videos of a given action class, where video v i contains n i frames</p><formula xml:id="formula_0">f j i , j ∈ [1 . . . n i ].</formula><p>We encode each frame f j i using a pyramid of histograms of oriented gradients or PHOGdescriptor <ref type="bibr" target="#b5">[6]</ref>. Then, video v i is represented by a matrix Z i ∈ R m×ni , where column j contains the m-dimensional PHOG-descriptor of frame f j i . Our sparse coding representation considers two main design goals. First, similarly to <ref type="bibr" target="#b10">[11]</ref>, the atoms of the resulting representation must correspond to frames from the input video. Second, as mentioned before, the resulting atoms must simultaneously provide a suitable representation of the input video and the complete class. To achieve this, for each input video we solve the following optimization:</p><formula xml:id="formula_1">min Wi,W (−i) Z i − Z i W i 2 F + α Z (−i) − Z i W (−i) 2 F (1) s.t. W i 1,2 ≤ λ, W (−i) 1,2 ≤ λ,</formula><p>where W i ∈ R ni×ni corresponds to the matrix of coefficients that minimize the constrained reconstruction of the n i  </p><formula xml:id="formula_2">frame descriptors in Z i . Z (−i) = [. . . , Z i−1 , Z i+1 , . . . ] ∈ R m×(n−ni)</formula><p>corresponds to the matrix of PHOG descriptors for all the n frames in a target class, excluding the n i frames from video v i .</p><formula xml:id="formula_3">W (−i) = [. . . , W i−1 , W i+1 , . . . ] ∈ R ni×(n−ni) corresponds to the sparse representation of Z (−i) using the frame descriptors in Z i . The mixed ℓ 1 /ℓ 2 norm is defined as A 1,2 N i=1 a i 2 ,</formula><p>where A is a sparse matrix and a i denotes the i-th row of A. Then, the mixed norm expresses the sum of the ℓ 2 norms of the rows of A. Parameter λ &gt; 0 controls the level of sparsity in the reconstruction, and parameter α &gt; 0 balances the penalty between errors in the reconstruction of video v i and errors in the reconstruction of the remaining videos in the class collection. Following <ref type="bibr" target="#b10">[11]</ref>, we solve the constrained optimization in Eq. 1 using the Alternating Direction Method of Multipliers (ADMM) technique <ref type="bibr" target="#b12">[13]</ref>. </p><formula xml:id="formula_4">W i = [W1, · · · , Wp].</formula><p>This decomposition highlights that each row j in a submatrix W l contains information about the contribution delivered by frame j in video vi to reconstruct the frames in video v l . Then, row j in matrix W i contains all the reconstruction coefficients associated to frame j in video vi.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Selection of Key-Sequences</head><formula xml:id="formula_5">Matrix W i = [W i |W (−i) ]</formula><p>provides information about the contribution of each frame in v i to summarize each of the videos in the entire class collection. <ref type="figure" target="#fig_2">Fig. 3</ref> shows a diagram of matrix W i that highlights this property. Specifically, each row j in W i provides information about the contribution provided by frame j in video v i , f j i , to reconstruct the p videos in the class collection. Using this property and the notation in <ref type="figure" target="#fig_2">Fig. 3</ref>, we define the following score to quantify the contribution of frame f j i to the reconstruction process:</p><formula xml:id="formula_6">R(f j i ) = p l=1 n l s=1 w l j,s .<label>(2)</label></formula><p>R(f j i ) corresponds to the sum of the elements in the j-th row of matrix W i . We use this score to rank the frames in video v i according to their contribution to the reconstruction process. In particular, a frame with a high ranking score provides a high contribution to the reconstruction of the videos in the class collection. Therefore, high scoring frames represent good candidates to be selected as keyframes for video v i .</p><p>Let L i be the set of frames f j i from v i that satisfy R(f j i ) &gt; θ, where θ is a given threshold. We obtain a set of key-frames from v i by selecting K frames from the candidates in L i . Several criterion can be used to select these K frames. In particular, to guarantee that the selected key-frames provide a good temporal coverage of the input video, we use the following scheme. First, we select K time instants uniformly distributed with respect to the length of the video. Then, for each of these time instants, we select as a key-frame the closest neighbouring frame in L i . <ref type="figure" target="#fig_1">Fig.  2</ref> shows instances of key-frames selected by this approach using K = 3.</p><p>To include motion cues, we add neighbouring frames to each key-frame in order to form brief video acts that we refer to as key-sequences. Specifically, for a key-frame f j i in video v i , its corresponding key-sequence is given by the set</p><formula xml:id="formula_7">s j i = {f l i } l=j+t l=j−t ,</formula><p>i.e., 2t + 1 consecutive frames centered at the corresponding key-frame (t ∈ N). Consequently, each input video v i is decomposed into a set s i = {s 1 i , . . . , s K i }, corresponding to K temporally ordered key-sequences. <ref type="figure" target="#fig_4">Fig. 4</ref> summarizes the main steps to build our video descriptor. We explain next the details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Video Description</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Relative Local Temporal Features</head><p>At the core of our method to obtain relative features is the use of sparse coding to learn a set of dictionaries that encode local temporal patterns present in the action classes. Specifically, in the case of C action classes and K temporal key-sequences, we use training data to learn a total of</p><formula xml:id="formula_8">C × K dictionaries, where dictionary D ci kj , c i ∈ [1 . . . C], k j ∈ [1 . . . K]</formula><p>, encodes relevant local temporal patterns occurring in class c i at time instance k j .</p><p>As a key observation, by projecting a given key-sequence to a concatenated version of a subset of the dictionaries, it is possible to quantify the relative similarity between the key-sequence and the individual dictionaries. This can be achieved by quantifying the total contribution of the atoms in each individual dictionary to the reconstruction of the projected key-sequence. As an example, consider the case of a concatenated dictionary that encodes local patterns learnt from sport actions. In this case, key-sequences from an action class such as running should use in their reconstruction a significant amount of dictionary atoms coming from similar action classes, such as jogging and walking. As a consequence, by quantifying the cross-talk among reconstruction contributions coming from different dictionaries, one can obtain a feature vector that encodes relative local similarities between the projected key-sequence and the temporal patterns encoded in each dictionary. Next, we exploit this property to apply two concatenation strategies that allow us to obtain a video descriptor capturing inter and intra-class similarity relations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Inter-class Relative Act Descriptor</head><p>Our method to obtain inter-class relative local temporal features is composed of three main steps. In the first step we obtain a low-level feature representation for each keysequence. Specifically, we randomly sample a set of spatiotemporal cuboids (300 in our experiments) from each keysequence. These cuboids are encoded using the spatiotemporal HOG3D descriptor <ref type="bibr" target="#b19">[20]</ref>. Section 4 provides further implementation details.</p><p>In the second step we use the resulting HOG3D descriptors and sparse coding to build a set of local temporal dictionaries for each class. Temporal locality is given by organizing the key-sequences according to their K temporal positions in the training videos. Let Y c j be the set of HOG3D descriptors extracted from all key-sequences occurring at the j-th temporal position in the training videos from class c, where j ∈ [1, . . . , K], c ∈ [1, . . . , C]. We find a classbased temporal dictionary D c j for position j using the K-SVD algorithm <ref type="bibr" target="#b1">[2]</ref> to solve:</p><formula xml:id="formula_9">min D c j ,X c j Y c j − D c j X c j 2 F s.t. x i 0 ≤ λ 1 ,<label>(3)</label></formula><p>where Y c j ∈ R m×ns , m is the dimensionality of the descriptors and n s is the total number of cuboids sampled from videos of class c and temporal position j, D c j ∈ R m×na , X c j ∈ R na×ns , n a is the number of atoms in each dictionary D c j , and the sparsity restriction on each column x i ∈ X c j indicates that its total number of nonzero entries must not exceed λ 1 .</p><p>Finally, in the third step we use the previous set of dictionaries to obtain a local temporal similarity descriptor for each key-sequence. To achieve this, for each temporal position j, we concatenate the C class-based dictionaries obtained in the previous step. This provides a set of K temporal dictionaries, where each dictionary contains information about local patterns occurring in all target classes at a given temporal position j. These K representations allow us to quantify local temporal similarities among the target classes. Specifically, let D j = [D 1 j D 2 j . . . D C j ] be the concatenated temporal dictionary corresponding to temporal position j. To obtain a descriptor for key-sequence s j i from video v i , we first project s j i onto dictionary D j imposing a sparsity constraint. We achieve this by using the Orthogonal Matching Pursuit (OMP) technique to solve:</p><formula xml:id="formula_10">min x j i s j i − D j x j i 2 F s.t. x j i 0 ≤ λ 2 ,<label>(4)</label></formula><p>where vector</p><formula xml:id="formula_11">x j i = {x j i [D 1 j ], . . . , x j i [D C j ]</formula><p>} is the resulting set of coefficients, and a component vector x j i [D c j ] ∈ R na corresponds to the coefficients associated to the projection of s j i onto the atoms in subdictionary D c j . We quantify the similarity of s j i to the atoms corresponding to each class by using a sum-pooling operator that evaluates the contribution provided by the words in each subdictionary D c j to the reconstruction of s j i . We define this sum-pooling operator as:</p><formula xml:id="formula_12">φ c j (s j i ) = na l=1 x j i [D c j ](l).<label>(5)</label></formula><p>By applying the previous method to the set of K keysequences s j i in a video v i , we obtain a video descriptor Φ i = [φ 1 , . . . , φ K ] ∈ R C×K , where each component vector φ j is given by φ j = [φ 1 j , . . . , φ C j ]. In this way, Φ i contains information about relative inter-class similarities among key-sequences or acts. Therefore, we refer to this descriptor as Inter-class Relative Act Descriptor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Intra-class Relative Act Descriptor</head><p>The procedure in Section 3.2.2 provides a descriptor that encodes relative local temporal similarities across the target classes. In this section, we use a similar procedure to obtain local temporal similarities at an intra-class level. Specifically, we quantify the similarity of a key-sequence occurring at temporal position j with respect to the patterns occurring at the remaining K − 1 temporal positions in a target class. To do this, we follow the procedure described in Section 3.2.2 but, this time we project a keysequence s j i onto the concatenated dictionary D c</p><formula xml:id="formula_13">(−j) = [. . . , D c j−1 D c j+1 .</formula><p>. . ], i.e., the concatenation of the k − 1 key-sequence dictionaries for class c, excepting the dictionary corresponding to temporal position j. We again use the OMP technique to perform this projection, i.e., to solve:</p><formula xml:id="formula_14">min x j i s j i − D c (−j) x j i 2 F s.t. x j i 0 ≤ λ 3 .<label>(6)</label></formula><p>Similarly to the Inter-class Relative Act Descriptor, we obtain a video descriptor, Ψ i = [ψ 1 , . . . , ψ K ] ∈ R K×(K−1) , by applying the projection to all key-sequences in a video v i and then using the corresponding sum-pooling operations to quantify the reconstruction contribution of each subdictionary D c l . In this way, Ψ i contains information about relative intra-class similarities among key-sequences or local acts, therefore, we refer to this descriptor as Intra-class Relative Act Descriptor. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4">Inter Temporal Relational Act</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Video Classification</head><p>ITRA can be used by any off-the-shelf supervised classification scheme. Here, we use a sparse coding approach. Training: During the training phase, we first use the method described in Section 3.1 to decompose each training video into a set of key-sequences. Then, we use the method described in Section 3.2 to obtain the ITRA descriptor for each training video. Afterwards, these descriptors, along with sparse coding techniques, are used to build a dictionary for each target class. Specifically, let Y c be a matrix containing in its columns the ITRA descriptors corresponding to the training videos from action class c ∈ [1, . . . , C]. For each action class, we use the K-SVD algorithm <ref type="bibr" target="#b1">[2]</ref> to obtain a class-based dictionary B c by solving:</p><formula xml:id="formula_15">min B c ,X c Y c − B c X c 2 F s.t. x i 0 ≤ λ 4 , ∀i,<label>(7)</label></formula><p>where B c ∈ R |ITRA|×na , |ITRA| represents the dimensionality of the ITRA descriptor, and n a is the selected number of atoms to build the dictionary. X c corresponds to the matrix of coefficients and vectors x i to its columns. As a final step, we concatenate the C class-based dictionaries to obtain the joint dictionary B = [B 1 |B 2 | · · · |B C ] that forms the core representation to classify new action videos.</p><p>Inference: To classify a new input video, similarly to the training phase, we first use the methods in Sections 3.1 and 3.2 to obtain its ITRA descriptor. As a relevant difference from the training phase, in this case we do not know the class label of the input video, therefore, we need to obtain its key-sequence decomposition with respect to each target class. This task leads to C ITRA descriptors to represent each input video. Consequently, the classification of an input video consists of projecting these C descriptors onto the joint dictionary B and then using a majority vote scheme to assign the video to the class that contributes the most to the reconstruction of the descriptors. Specifically, let v q be a test video, and Ω c (v q ) its ITRA descriptor with respect to class c. We obtain a sparse representation for each of the C ITRA descriptors using the OMP technique to solve:</p><formula xml:id="formula_16">min αc Ω c (v q ) − Bα c 2 2 s.t. α c 0 ≤ λ 5 .<label>(8)</label></formula><p>The previous process provides C sets of coefficients α c . We use each of these sets to obtain a partial classification of the input video. We achieve this by applying, to each set, a sum-pooling operator similar to the one presented in Eq. <ref type="bibr" target="#b4">(5)</ref>, and classifying the input video according to the class that presents the greatest contribution to the reconstruction of the corresponding set α c . Finally, using these C partial classifications, we use majority vote to assign the input video to the most voted class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments and Results</head><p>We validate our method by using three popular benchmark datasets for action recognition: KTH <ref type="bibr" target="#b33">[34]</ref>, Olympic <ref type="bibr" target="#b26">[27]</ref>, and HOHA <ref type="bibr" target="#b22">[23]</ref>. In all the experiments, we select values for the main parameters using the following criteria. Estimation of Key-Sequences: We use training data from the Olympic dataset to tune the number of acts to represent an action. Experimentally, we find that 3 acts are enough to achieve high recognition performance. Hence, in all our experiments, we select K = 3 key-sequences to represent each training or test video.</p><p>In terms of the time span of each key-sequence, we take a fixed group of 7 frames (t = 3) to form each key-sequence. For each sequence we randomly extract 300 cuboids, described using HOG3D (300 dimensions). To filter out uninformative cuboids, we set a threshold to the magnitude of the HOG3D descriptor. We calibrate this threshold to eliminate the 5% least informative cuboids from each dataset. Afterwards, the remaining descriptors are normalized. Table 1 shows the value of the resulting thresholds for each dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head><p>Train Estimation of ITRA descriptor: Parameters for the extraction of ITRA descriptors are related to the construction of the dictionaries described in Section 3.2. Let µ be the redundancy 1 and let δ be the dimensionality of a descriptor. Following the empirical results in <ref type="bibr" target="#b15">[16]</ref>, we fix the number of atoms in each local dictionary to be n a = µ × δ. Therefore, the number of atoms for the concatenated dictionaries are: P = µ×δ ×C for the extraction of Inter-class Relative Act Descriptors, Φ, and P = µ × δ × (K − 1) for the extraction of Intra-class Relative Act Descriptors, Ψ. In our experiments, we use µ = 2 and δ = 300. As a result, the dimension of the ITRA descriptors for KTH, Olympic, and HOHA datasets are 24, 54 and 30, respectively. Also, following <ref type="bibr" target="#b15">[16]</ref>, the sparsity parameters λ 1 , λ 2 , and λ 3 , are set to be 10% of the number of atoms. Classifier: Parameters P, µ, λ 4 , and λ 5 are configured using the same scheme described above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Action Recognition Performance</head><p>KTH Dataset: This set contains 2391 video sequences displaying six types of human actions. In our experiments we use the original setup <ref type="bibr" target="#b33">[34]</ref> to divide the data into training and test sets. <ref type="table">Table 2</ref> shows the recognition performance reached by our method. <ref type="table">Table 2</ref> also includes the performance of alternative action recognition schemes proposed in the literature, including approaches that also use sparse coding techniques <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b7">8]</ref>. Our method obtains a recognition performance of 97.5%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Acc. Laptev et al. <ref type="bibr">[</ref>  <ref type="table">Table 2</ref>. Recognition rates of our and alternative methods on KTH dataset. In all cases, the same testing protocol is used.</p><p>Olympic Dataset: This dataset contains 16 actions corresponding to 783 videos of athletes practicing different sports <ref type="bibr" target="#b26">[27]</ref>. <ref type="figure" target="#fig_5">Fig. 5</ref> shows sample frames displaying the action classes. In our experiments, we use the original setup <ref type="bibr" target="#b26">[27]</ref> to divide the data into training and test sets. <ref type="table">Table 3</ref> shows the recognition performance reached by our method and several alternative state-of-the-art techniques. Our approach achieves a recognition rate of 96.3%. This is a remarkable increase in performance with respect to previous state-of-the-art approaches. <ref type="figure" target="#fig_5">Fig. 5</ref> shows the confusion matrix reported by our method. We note that many actions from this dataset have a perfect recognition rate. Therefore, our approach effectively captures relevant acts and their temporal relationships.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Acc. Niebles et al. <ref type="bibr">[</ref>  <ref type="table">Table 3</ref>. Recognition rates of our and alternative methods on Olympic dataset. In all cases, the same testing protocol is used.</p><p>Hollywood Dataset: This dataset contains video clips extracted from 32 movies and displaying 8 action classes. <ref type="figure" target="#fig_6">Fig.   6</ref> shows sample frames displaying the action classes. We use only the videos with manual annotations (clean training file) and we limit the dataset to videos with a single label. This is the same testing protocol used by the alternative techniques considered here. <ref type="table">Table 4</ref> shows the recognition performance of our method and several alternative state-ofthe-art techniques. Our approach achieves a recognition rate of 71.9%. Again, this is a remarkable increase in performance with respect to previous state-of-the-art approaches. <ref type="figure" target="#fig_6">Fig. 6</ref> shows the confusion matrix reported by our method.</p><p>Actions such as answer phone, handshake, and hug person obtain high recognition rates. In contrast, the actions get out car, kiss, and sit up present a lower recognition performance. According to the confusion matrix in <ref type="figure" target="#fig_6">Fig. 6</ref>, these actions present a high confusion rate with respect to the action answer phone. This can be explained by the presence of a common pattern among these actions in this dataset, which is given by a slow incorporation of the main actor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Acc. Laptev et al. <ref type="bibr">[</ref>  <ref type="table">Table 4</ref>. Recognition rates of our and alternative methods on HOHA dataset. In all cases, the same testing protocol is used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation of Method to Extract Key-Sequences</head><p>In this section, we evaluate the relevance of the proposed method to obtain key-frames by replacing this step of our approach by alternative strategies. Besides this modification, we maintain the remaining steps of our approach and use the same parameter values reported in Section 4.1. In particular, we implement two baselines, <ref type="table">Table 5</ref> shows our results: Baseline 1 (B1), Uniform Selection: We split the video into K equal-sized temporal segments and select the central frame of each segment as a key-frame. Baseline 2 (B2), K-Means: We generate all possible video sequences containing 2t + 1 frames by applying a temporal sliding window. We then apply the K-Means clustering algorithm to each class to obtain K cluster centers per class. For each video, we select as key-frames the most similar descriptor to each cluster center.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Evaluation of ITRA descriptor</head><p>We evaluate the effectiveness of our ITRA descriptor by replacing this part of our approach by alternative schemes to obtain a video descriptor. These alternative schemes are    <ref type="table">Table 5</ref>. Performances of our method and alternative strategies to extract key-sequences. also based on sparse coding techniques but they do not exploit relative local or temporal information. Specifically, we consider two baselines, <ref type="table">Table 6</ref> shows our results:</p><p>Baseline 1 (B1), Ignoring relative local temporal information: All key-sequences from all temporal positions are combined to build a single class-shared joint dictionary that do not preserve temporal order among the key-sequences. This baseline can be considered as a BoW type of representation that does not encode relative temporal relations among key-sequences.</p><p>Baseline 2 (B2), Ignoring intra-class relations: this baseline only considers the term in ITRA descriptor associated to the Inter-Class Relative Act Descriptor Φ i , discarding intra-class relations provided by the Intra-Class Relative Act Descriptor Ψ i .  <ref type="table">Table 6</ref>. Performances of our method and alternative strategies to construct the video descriptor using sparse coding techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>We present a novel method for category-based action recognition in video. As a main result, our experiments show that the proposed method reaches remarkable action recognition performance on 3 popular benchmark datasets. Furthermore, the reduced dimensionality of the ITRA descriptor provides a fast classification scheme. Actually, using a reduced dimensionality, between 24 and 54 dimensions for the datasets considered here, it provides a representation that demonstrates to be highly discriminative. As future work, the ITRA descriptor opens the possibility to explore several strategies to concatenate the basic dictionaries to access different relative similarity relationships.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Overview of the proposed method to extract keysequences from an input video.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Key-frames selected by the proposed method (rows) for videos of the action category Discus throwing in the Olympic dataset using K=3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Matrix W i . Columns of W i can be decomposed according to the p videos in an action class:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Descriptor: ITRA We obtain a final feature vector descriptor for a video v i by concatenating the Inter and Intra-class Relative Act Descriptors. We refer to this new descriptor as Inter Temporal Relational Act Descriptor or ITRA, where ITRA(v i ) = {Φ i ∪ Ψ i } ∈ R K×(C+(K−1)) .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Overview of the method to obtain the ITRA descriptor. See Section 3.2 for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Olympic dataset. Left: sample from each action class. Right: confusion matrix for our method on Olympic dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 .</head><label>6</label><figDesc>HOHA. Left: sample from each action class. Right: confusion matrix for our method on HOHA dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head></head><label></label><figDesc>2% 37.2% 71.9% Olympic 46.3% 63.4% 96.3%</figDesc><table>Dataset 
Method 
B1 
B2 
Ours 
HOHA 
34.</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head></head><label></label><figDesc>2% 51.3% 71.9% Olympic 72.4% 87.3% 96.3%</figDesc><table>Dataset 
Method 
B1 
B2 
Ours 
HOHA 
42.</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Redundancy indicates the folds of basis vectors that need to be identified with respect to the dimensionality of the descriptor.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was partially funded by FONDECYT grant 1151018, CONICYT, Chile.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Human activity analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ryoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="43" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">An algorithm for designing overcomplete dictionaries for sparse representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aharon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bruckstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>K-Svd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="4311" to="4322" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Human action recognition from inter-temporal dictionaries of key-sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alfaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mery</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Soto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pacific-Rim Symposium on Image and Video Technology</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Action synopsis: Pose selection and illustration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Assa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Caspi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Trajectory-based fisher kernel representation for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Atmosukarto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Representing shape with a spatial pyramid kernel</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bosch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Muñoz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Image and Video Retrieval</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="401" to="408" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Detecting people using mutually consistent poselet activations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conf. on Computer Vision</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Sparse modeling of human actions from motion imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Castrodad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Behavior recognition via sparse spatio-temporal features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Rabaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cottrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VS-PETS</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Optimally sparse representation in general (non orthogonal) dictionaries via l1 minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Donoho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the National Academy of Sciences</title>
		<meeting>of the National Academy of Sciences</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">See all by looking at a few: Sparse modeling for finding representative objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Elhamifar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained part based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1627" to="1645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A dual algorithm for the solution of nonlinear variational problems via finite-element approximations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gabay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mercier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers &amp; Mathematics with Applications</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="17" to="40" />
			<date type="published" when="1976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Actom sequence models for efficient action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gaidon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Activity representation with motion hierarchies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gaidon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="219" to="238" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning sparse representations for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Guha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ward</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Action recognition in video by sparse representation on covariance manifolds of silhouette tunnels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ishwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Konrad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Pattern Recognition</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Trajectorybased modeling of human actions with motion reference points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ngo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conf. on Computer Vision</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Creating efficient codebooks for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jurie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. on Computer Vision</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A spatio-temporal descriptor based on 3D-gradients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kläser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marszalek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Attribute and simile classifiers for face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Belhumeur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. on Computer Vision</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Space-time interest points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lindeberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. on Computer Vision</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning realistic human actions from movies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marszalek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rozenfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Recognizing human actions using multiple features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Recognizing human actions by attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kuipers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Boosted key-frame selection and correlated pyramidal feature representation for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rockett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1810" to="1818" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Modeling temporal structure of decomposable motion segments for activity classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conf. on Computer Vision</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Action and event recognition with fisher vectors on a compact feature set</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Oneata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. on Computer Vision</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Relative attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. on Computer Vision</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Discovering discriminative action parts from mid-level video representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Raptis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Poselet key-framing: A model for human activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Raptis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Action Bank: A high-level representation of activity in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sadanand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Corso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Action Snippets: How many frames does human action recognition require</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Recognizing human actions: A local SVM approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schüldt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Pattern Recognition</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Modeling motion of body parts for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kakadiaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Improved object categorization and detection using comparative object similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Forsyth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2442" to="2453" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Dense trajectories and motion boundary descriptors for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kläser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="60" to="79" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Evaluation of local spatio-temporal features for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ullah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kläser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Hidden part models for human action recognition: Probabilistic versus max margin</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1310" to="1323" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Action recognition using exemplar-based embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weinland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Boyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Action recognition in videos acquired by a moving camera using motion decomposition of lagragian particle trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Oreifej</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. on Computer Vision</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">The power of comparative reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yagnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Strelow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. on Computer Vision</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A Hough transform-based voting framework for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Real-time action recognition by spatiotemporal semantic and structural forest</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Information theoretic key frame selection for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learning spatial and temporal extents of human actions for action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on multimedia</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="512" to="525" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Capturing longtail distributions of object subcategories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
