<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Group MAD Competition − A New Methodology to Compare Objective Image Quality Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kede</forename><surname>Ma</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">The Hong Kong Polytechnic University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingbo</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Electronic Science and Technology of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengfang</forename><surname>Duanmu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwei</forename><surname>Yong</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Xi&apos;an Jiaotong University</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">The Hong Kong Polytechnic University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongliang</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Electronic Science and Technology of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">The Hong Kong Polytechnic University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Group MAD Competition − A New Methodology to Compare Objective Image Quality Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:41+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Objective image quality assessment (IQA) models aim to automatically predict human visual perception of image quality and are of fundamental importance in the field of image processing and computer vision. With an increasing number of IQA models proposed, how to fairly compare their performance becomes a major challenge due to the enormous size of image space and the limited resource for subjective testing. The standard approach in literature is to compute several correlation metrics between subjective mean opinion scores (MOSs) and objective model predictions on several well-known subject-rated databases that contain distorted images generated from a few dozens of source images, which however provide an extremely limited representation of real-world images. Moreover, most IQA models developed on these databases often involve machine learning and/or manual parameter tuning steps to boost their performance, and thus their generalization capabilities are questionable. Here we propose a novel methodology to compare IQA models. We first build a database that contains 4,744 source natural images, together with 94,880 distorted images created from them. We then propose a new mechanism, namely group MAximum Differentiation (gMAD) competition, which automatically selects subsets of image pairs from the database that provide the strongest test to let the IQA models compete with each other. Subjective testing on the selected subsets reveals the relative performance of the IQA models and provides useful insights on potential ways to improve them. We report the gMAD competition results between 16 well-known IQA models, but the framework is extendable, allowing future IQA models to be added into the competition.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Digital images undergo many transformations in their lifetime during acquisition, processing, compression, storage, transmission and reproduction. Any transformation may introduce distortions that result in degradations in visual quality <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b29">30]</ref>. Being able to assess image quality is of fundamental importance in many image processing and computer vision applications. Since the human visual system (HVS) is the ultimate receiver in most applications, subjective evaluation is the most reliable way of quantifying image quality but is time-consuming, cumbersome and expensive. In recent years, there has been a rapidly growing interest in developing objective image quality assessment (IQA) models that can automate the process <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b30">31]</ref>. Depending on the availability of a distortion-free reference image, objective IQA models may be categorized into fullreference (FR), reduced-reference (RR) and no-reference (NR) approaches, where the reference image is fully, partially, and completely not accessible.</p><p>With a significant number of IQA models proposed recently, how to fairly compare their performance becomes a challenge. The standard approach in the literature is to first build databases of images with various content and distortions, and then collect subjective evaluation scores for all images. Widely recognized image databases with subjective ratings include LIVE <ref type="bibr" target="#b26">[27]</ref>, TID2008 <ref type="bibr" target="#b22">[23]</ref>, TID2013 <ref type="bibr" target="#b21">[22]</ref>, CSIQ <ref type="bibr" target="#b11">[12]</ref>, IVC <ref type="bibr" target="#b12">[13]</ref>, Toyama-MICT <ref type="bibr" target="#b8">[9]</ref> and VCL@FER <ref type="bibr" target="#b40">[41]</ref>. Crowdsourcing techniques <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b14">15]</ref> were also adopted to construct subjective databases of real world Internet images <ref type="bibr" target="#b3">[4]</ref>. Given these databases, correlations between subjective mean opinion scores (MOSs) and objective model predictions can then be computed. Higher correlations suggest better model performance.</p><p>A major problem with this conventional evaluation methodology is the conflict between the enormous size of the image space and the limited scale of affordable subjective testing. Subjective testing is expensive and timeconsuming. As a result, a typical "large-scale" subjective test allows for a maximum of several hundreds or a few thousands of test images to be rated. Given the combination of source images, distortion types and distortion levels, realistically only a few dozens of source images (if not fewer) can be included, which is the case in all well-known databases. Moreover, many source test images are repeated in the current databases, and the distortion types being used are also similar. By contrast, digital images live in an extremely high dimensional space, where the dimension equals the number of pixels, which is typically in the order of hundreds of thousands or millions. Therefore, a few thousands of samples that can be evaluated in a typical subjective test are deemed to be extremely sparsely distributed in the space. Furthermore, it is difficult to justify how a few dozens of source images can provide a sufficient representation of the variations of real-world image content. It is also worth noting that most state-of-the-art IQA models were developed after the above-mentioned image databases became publicly available. These models often involve machine learning or manual parameter tuning steps to boost their performance on these databases. In particular, recent IQA models based on sophisticated machine learning approaches employ a very large number of image features together with large-scale learning networks to improve quality prediction performance. It is thus questionable if the reported highly competitive performance of recent IQA models can be generalized to the real-world, where images have much richer content and undergo a much broader variation of quality degradations.</p><p>We believe that to provide a fair comparison of IQA models and to test their generalization capability, a much larger test database (e.g., thousands of source images and tens of thousands distorted images) must be used. Apparently, the main difficulty here is how to make use of such a database to compare IQA models under the constraint of very limited resource for subjective testing, knowing that rating all test images by human subjects is impossible.</p><p>In this paper, we propose a substantially different methodology to address the problem. We first build a database that contains 4,744 source natural images, together with 94,880 distorted images created from them. Assuming a group of IQA models are available for testing, we propose a novel mechanism, namely group MAximum Differentiation (gMAD) competition, that automatically selects subsets of image pairs from the database that provide the strongest test to let the IQA models compete with each other. The key idea behind gMAD is to minimize the number of required subjective tests in order to most efficiently falsify a "defender" model by selecting and testing on image pairs that maximally differentiate the defender model using multiple "attacker" models. In other words, instead of trying to prove a model using a set of pre-defined images from subject-rated databases, we attempt to disprove the model in the most efficient way using a small set of deliberately selected, model-dependent image pairs. The process is applied to every IQA model in the group as the "defender" model. Subjective testing on the selected subsets of test image pairs reveals the relative strengths and weaknesses of the IQA models and also provides useful insights on potential ways to improve them. This work is inspired by the idea behind the MAD competition approach <ref type="bibr" target="#b32">[33]</ref>, but unlike g-MAD, the MAD method includes only two models in the competition, relies on gradient computations of the models, and is not structured to explore a database of images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Well-known subject-rated image databases include LIVE <ref type="bibr" target="#b26">[27]</ref>, TID2008 <ref type="bibr" target="#b22">[23]</ref>, CSIQ <ref type="bibr" target="#b11">[12]</ref>, IVC <ref type="bibr" target="#b12">[13]</ref>, Toyama-MICT <ref type="bibr" target="#b8">[9]</ref>, VCL@FER <ref type="bibr" target="#b40">[41]</ref> and TID2013 <ref type="bibr" target="#b21">[22]</ref>. They have been extensively employed in the training and testing processes in the development and benchmarking of a majority of state-of-the-art IQA models. The specific subjective testing methodologies vary, but eventually each image in the databases is labeled with an MOS, which represents the average subjective opinion about the quality of the image and is often referred to as the "ground truth" quality score of the image. The most common distortion types shared by these databases are JPEG compression, JPEG2000 compression, white Gaussian noise contamination and Gaussian blur. The typical size of these databases is in the order of hundreds or a few thousands images. Among them, the TID2013 database <ref type="bibr" target="#b21">[22]</ref> is the largest and contains 25 source and 3000 distorted images in total. By contrast, the current database we created in this work contains 190 times more source images and 30 times more distorted images, respectively.</p><p>Depending on how the test images are presented to human subjects and how human subjects are instructed to rate the images, subjective testing may be carried out in three ways: 1) single-stimulus method, where one test image is shown at any time instance and the subjects directly give quality scores to the image; 2) paired comparison method (also known as two-alternative forced choice, or 2AFC approach), where a pair of images are shown to the subjects, who are instructed to choose a preferred image from the two; and 3) multiple-stimulus method, where multiple images are shown simultaneously and the subjects rank all images based on their quality or give quality scores to all images. Assume that there are n test images in total. O(n) evaluations are needed in single-stimulus and multiple-stimulus methods, while O(n 2 ) evaluations are needed in a full paired comparison experiment. Although paired comparison method is often preferred to collect reliable subjective evaluations, exhaustive paired comparison requires a very large number of evaluations, which are often impractical when the total number of test images is large. A number of approaches have been proposed to improve the efficiency. Four types of balanced sub-set designs were developed in the 1950's <ref type="bibr" target="#b0">[1]</ref>, among which the square design method became popular and was later further improved <ref type="bibr" target="#b13">[14]</ref>. Another method was to randomly select a small subset of pairs for each subject <ref type="bibr" target="#b2">[3]</ref>, and it was shown that O(n log n) distinct pairs are needed for large random graphs to guarantee graph connectivity and thus to achieve any global ranking using HodgeRank <ref type="bibr" target="#b9">[10]</ref>. In the construction of the TID2013 <ref type="bibr" target="#b21">[22]</ref> database, a Swiss competition principle was adopted to decrease the evaluations to O(n log n). Recently, an active sampling strategy for subjective quality assessment was proposed <ref type="bibr" target="#b38">[39]</ref> with a complexity of O(n). Different from all the above strategies to improve testing efficiency, gMAD requires a fixed number of paired comparisons and thus does not scale with the number of images in the database. This feature allows it to exploit large-scale databases with low and manageable cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Image Database Construction</head><p>We construct a new image database, which currently contains 4,744 high quality source natural images with a great amount of image content. An important consideration in selecting the images is that they need to be representative of the images we see in real-world applications. Therefore, we resort to the Internet, and more specifically, we elaborately select 196 keywords to search for images via Google Images <ref type="bibr" target="#b4">[5]</ref>. The keywords can be broadly classified into 7 categories: human, animal, plant, landscape, cityscape, still-life and transportation. As a result, we initially obtain more than 200, 000 images. Many of these images contain significant distortions or inappropriate content, and thus a sophisticated manual process is applied to refine the selection. In particular, we first delete those images that have obvious distortions, including heavy compression artifacts, strong motion blur or out of focus blur, low contrast, underexposure or overexposure, substantial sensor noise, visible watermarks, artificial image borders, and other distortions due to improper operations during acquisition. Next, images of too small and too large sizes, cartoon and computer generated content, and inappropriate content are excluded. After this step, about 7, 000 images are left in the database. To make sure that the images have pristine or nearly pristine quality, we further carefully investigate each of the remaining images multiple times by zooming in and delete those images with visible compression distortions. Eventually, we end up with 4744 high quality natural images in our database. Sample images are shown in <ref type="figure" target="#fig_0">Fig. 1</ref>.</p><p>Four distortion types, namely JPEG and JPEG2000 compression, white Gaussian noise and Gaussian blur, each with five distortion levels are used to generate 94, 880 distorted images. The four distortion types are common in existing IQA databases <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b22">23]</ref> and many IQA models are claimed to be able to properly handle these distortions <ref type="bibr" target="#b18">[ 19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b34">35]</ref>. The distortion generation process follows the method in <ref type="bibr" target="#b26">[27]</ref>, and the parameters that control the distortion levels for each type are optimized in order to uniformly cover the subjective quality scale. Once determined, the parameters are kept unchanged for all images.</p><p>Overall, our new image database contains a total of 99, 624 images which is the largest image database so far in the IQA research community. It will be made publicly available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">gMAD Competition</head><p>The underlying principle in traditional approaches of IQA model evaluation is to prove a model. This requires the model to be validated using a sufficient number of test samples in the applicable space of the model. Applying such a principle in IQA model evaluation is a major challenge because the applicable space (i.e., the space of all possible images) is enormous (millions of dimensions), but the total number of test samples (subject-rated images) that can be obtained in a realistic subjective experiment is only in the order of thousands (if not fewer). It is extremely difficult to justify that these test samples are sufficient to represent the population of real-world images.</p><p>The most fundamental idea behind the MAD <ref type="bibr" target="#b32">[33]</ref> and the current gMAD competition approaches is to give up the traditional principle. Instead of trying to prove a model, here we attempt to disprove a model, and a model that is more difficult to be disproved is regarded as a relatively better model. This new principle gives us an opportunity to largely reduce the required number of test samples because ideally even one "counter-example" is sufficient to disprove a model. Another important ingredient in the gMAD approach is to use an efficient and automatic way to find potential "counter-examples". When attempting to disprove a model (denoted as the "defender"), instead of trying to hand design or manually search for the best counter-examples, gMAD makes use of a group of other models (denoted as the "attackers") to search for the counter-examples in the database that are optimal with regard to the attacker models such that if the attack is successful, the defender model is simply disproved. If instead, the defender survives from such an attack, it is a strong indicator that it is likely to be a robust and reliable model. gMAD runs this game using all available models with all possible combinations of defender-attacker roles of the models before performing overall statistics that help summarize the relative performance of the competing models.</p><p>The details of the gMAD competition procedure are as follows: We are given a database D that contains N images with different distortion types and levels. Also given are a group of M objective IQA models.  • Step 3. Choose the first quality level k =1from a total of K quality levels, where k ∈{1, 2, ··· ,K};</p><p>• Step 4. At the i-th row in S, find all images at quality level k (based on the defender model i). This results in a subset of images D ik , where all images have the same or similar quality scores according to the defender model i;</p><p>•</p><p>Step 5. Choose one model j from the attacker models (j = i).</p><p>• Step 6. Within D ik , find a pair of images I l ijk and I u ijk that correspond to the minimal and maximal quality scores on the j-th row of matrix S, respectively. This image pair is referred to as the MAD counterexample suggested by model j to attack model i at quality level k;</p><p>• Step 7. Carry out a subjective quality discriminative test on I l ijk and I u ijk (details given in Section 5.1);</p><p>• Step 8. Choose another attacker model j and repeat Steps 6-7 until all attacker models are exhausted;</p><p>• Step 9. Choose the next quality level by setting k = k +1and repeat Steps 4-8 until k = K (all quality levels are exhausted);</p><p>• Step 10. Choose the next defender model by setting i = i +1and repeat Steps 3-9 until i = M (all IQA models are exhausted);</p><p>•</p><p>Step 11. Carry out statistical analysis on the subjective quality discriminative test results (details given in Section 5.2).</p><p>Several useful features of the gMAD competition method are worth mentioning here. First, the process does not depend on the specific image database being explored.</p><p>The same approach can be applied to any collection of images of any content and distortion types. Second, the number of image pairs selected by gMAD for subjective testing is M (M − 1)K, which is independent of the size N of the image database D. As a result, applying gMAD competition to a larger database has no impact on the cost of subjective testing. Third, each selected pair of images are associated with two IQA models, which hold highly different opinions on their perceived image quality; one believes the pair have the same quality while the other suggests that they have very different quality. If the pair are easily differentiated by human subjects, they constitute strong evidence against the defender model. On the other hand, if the pair indeed have similar perceived quality, they provide strong evidence to support the defender model against the attacker model. Fourth, it is easy and cost-effective to add new IQA models into the competition. No change is necessary on all the selected pairs and their corresponding subjective testing. The only additional work is to select a total of 2MK new image pairs for subjective testing, half of which are for the case that the new model acts as a defender and the other half as an attacker. A MATLAB program will be made publicly available to facilitate the future usage of the gMAD competition approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">IQA Models Comparison</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Subjective Testing</head><p>The construction of the test image database has been described in detail in Section 3. A total of sixteen IQA models are selected in the gMAD competition process to cover a wide variety of IQA methodologies with an emphasis on NR models. These include FR models 1) PSNR, 2) S-SIM <ref type="bibr" target="#b31">[32]</ref>, 3) MS-SSIM <ref type="bibr" target="#b33">[34]</ref>, 4) FSIM <ref type="bibr" target="#b42">[43]</ref> and NR models 5) BIQI <ref type="bibr" target="#b18">[19]</ref>, 6) BLINDS II <ref type="bibr" target="#b24">[25]</ref>, 7) BRISQUE <ref type="bibr" target="#b16">[17]</ref>, 8) CORNIA <ref type="bibr" target="#b39">[40]</ref>, 9) DIIVINE <ref type="bibr" target="#b19">[20]</ref>, 10) IL-NIQE <ref type="bibr" target="#b41">[42]</ref>, 11) LPSI <ref type="bibr" target="#b35">[36]</ref>, 12) M3 <ref type="bibr" target="#b36">[37]</ref>, 13) NFERM <ref type="bibr" target="#b6">[7]</ref>, 14) NIQE <ref type="bibr" target="#b17">[18]</ref>, 15) QAC <ref type="bibr" target="#b37">[38]</ref> and 16) TCLT <ref type="bibr" target="#b34">[35]</ref>. The implementations of all algorithms are obtained from the original authors. For IQA models that involve training, we use all images in the LIVE database <ref type="bibr" target="#b26">[27]</ref> to train the models. To compensate the nonlinearity of model predictions on the human perception of image quality and to make the comparison more consistent, we adopt a logistic nonlinear function as suggested in <ref type="bibr" target="#b28">[29]</ref> to map the prediction scores of each model to the MOS scale of the LIVE database <ref type="bibr" target="#b26">[27]</ref>. As a result, the score range of all algorithms spans between [0, 100], where a higher value indicates a better perceptual quality.</p><p>For each defender model, we define six quality levels evenly spaced on the quality scale, so that the selected subsets of images have a good coverage from low to high quality levels. The quality range within each subset of images is set to be within 1 standard deviation (std) 1 of MOSs in the LIVE database <ref type="bibr" target="#b26">[27]</ref>. Thus the images within the same subsets have approximately the same or similar quality by the defender model. The attacker models then choose pairs of images from each of the 6 subsets, as described in Section 4. After the gMAD image pair selection process, a total of 16 × (16 − 1) × 6 = 1440 image pairs are chosen for the subsequent subjective testing.</p><p>A subjective quality discrimination test is conducted in an office environment with normal indoor illumination levels and without reflecting ceiling walls and floor. The display is a Truecolor LCD monitor at a resolution of 2560 × 1600 pixels and is calibrated in accordance with the recommendations of ITU-R BT.500 <ref type="bibr" target="#b28">[29]</ref>. A customized MATLAB interface is adopted to render a pair of images simultaneously at their original pixel resolutions but in random spatial order. A scale-and-slider applet is used for assigning a quality score, as shown in <ref type="figure" target="#fig_2">Fig. 2</ref>. For each pair of images, the subject assigns a score between -100 and 100 to indicate his/her preference to either the left image [-100, -20] (labeled as "left is better") or the right image <ref type="bibr" target="#b19">[20,</ref><ref type="bibr">100]</ref> (labeled as "right is better"). In case the subject is uncertain about his/her decision, he/she can also assign a score between [-20, 20] (labeled as "uncertain"), where a score 0 indicates completely neutral. This approach is different from a typical paired comparison method where the subjects can only make a binary decision on his/her preference even when he/she is uncertain about the answer. The benefit of the current approach is to better capture the subjects' confidence when expressing his/her preferences. During the experiment, the subjects are allowed to move their positions to get closer or further away from the screen for better observation. We divide the experiment into 4 sessions, each of which is limited to a maximum of 30 minutes to minimize the influence of fatigue effect. Furthermore, in order to inspect if subjects are using consistent scoring strategies throughout the experiment, we repeat 10% of the total number of image pairs (144 pairs) during the test.</p><p>A total of 31 naïve subjects, including 16 males and 15 females, participate in the subjective experiment. The subjects do not have any experience in the area of IQA and all have a normal or correct-to-normal visual acuity. Each subject is first introduced about the goal of the experiment and is then given an introduction on the experimental procedure and the user interface. They are also shown pairs of sample images (independent of the test images) in a training session so as to become familiar with the test process and image distortions. All subjects participate in all sessions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Analysis</head><p>After the raw subjective data are collected, we employ the outlier detection and subject rejection algorithm suggested in <ref type="bibr" target="#b28">[29]</ref>. Specifically, the raw score for an image is considered to be an outlier if it is outside 2 stds about the mean score of that image for Gaussian case or outside √ 20 stds for non-Gaussian case. A subject is removed if more than 5% of his/her evaluations are outliers. Moreover, a consistency check is conducted for each subject by making use of the image pairs that have been repeated. We define the consistency measure as the average of stds of scores given by one subject to the repeated pairs. A subject is rejected if his/her consistency measure is more than 2 stds of consistency measures for all subjects. As a result, one subject is rejected due to inconsistent judgements. Among all scores given by the remaining valid subjects, about 1.4% of the total subjective evaluations are identified as outliers and are subsequently removed.</p><p>Since every pair of images in the gMAD competition are associated with two IQA models, we first compare these models in pairs and then aggregate the pairwise comparisons into a global ranking using mature rank aggregation tools such as maximum likelihood for multiple options <ref type="bibr" target="#b27">[28]</ref>, hodgeRank <ref type="bibr" target="#b9">[10]</ref> and ranking by eigenvectors <ref type="bibr" target="#b15">[16]</ref>. We define an aggressiveness matrix A and a resistance matrix R, within which an entry a ij represents the aggressiveness of the i-th model as an attacker against the j-th model as a defender, and an entry r ij represents the resistance of the i-th model as a defender against the j-th model as an attacker, respectively. The aggressiveness measure indicates how strong an attacker in disproving a defender and is evaluated by</p><formula xml:id="formula_0">a ij = K k=1 p jk s ijk K k=1 p jk ,<label>(1)</label></formula><p>where s ijk is the subjective score averaged over all valid subjects on the image pair selected from the k-th subset. p jk is the number of samples in the k-th subset. The value of a ij ranges between [−100, 100] with a larger value indicating stronger aggressiveness of the i-th model. In general, a ij is expected to be positive for a competitive model, but it may also be negative which means that the order of the test image pair selected by the i-th model is the opposite of the average subjective judgements. A negative a ij is a strong indication of a failure of the i-th model. On the other hand, the resistance measure indicates how resistent of a defender to be defeated (disproved) by an attacker. It is evaluated by</p><formula xml:id="formula_1">r ij = K k=1 p ik (100 −|s jik |) K k=1 p ik .<label>(2)</label></formula><p>r ij ranges between [0, 100] with a higher value indicating better resistance of the i-th model as a defender against the  j-th model as an attacker. The matrices A and R are computed by comparing all pairs of IQA models and the results are shown in <ref type="figure" target="#fig_4">Fig. 3</ref>, where the higher value of an entry (warmer color), the stronger the aggressiveness and resistance of the corresponding row model against the column model.</p><p>We aggregate the pairwise comparison results into a global ranking via a maximum likelihood method for multiple options <ref type="bibr" target="#b27">[28]</ref>. The results are shown in <ref type="figure" target="#fig_6">Fig. 4</ref>. Using other ranking aggregation algorithms such as hodgeRank <ref type="bibr" target="#b9">[10]</ref> and ranking by eigenvectors <ref type="bibr" target="#b15">[16]</ref> gives very similar results. From the figure, we have several useful observations. First, an IQA model that has a stronger aggressiveness generally also has a stronger resistance. The Kendall's rank-order  correlation coefficient (KRCC) between them is 0.87. Second, in general, FR-IQA algorithms are more competitive than NR-IQA methods. This is not surprising because FR algorithms make use of more information. Third, the best performance overall is obtained by MS-SSIM <ref type="bibr" target="#b33">[34]</ref>, which is a multi-scale version of SSIM <ref type="bibr" target="#b31">[32]</ref> and significantly improves upon it. This suggests that multi-scale approaches are important in improving the performance of IQA models. Fourth, CORNIA <ref type="bibr" target="#b39">[40]</ref>, NIQE <ref type="bibr" target="#b17">[18]</ref> and its feature enriched version ILNIQE <ref type="bibr" target="#b41">[42]</ref> perform the best among all NR-IQA algorithms. It is worth mentioning that these methods are based on perception-and distortion-relevant natural scene statistics (NSS) features either hand-crafted or learned from data. This reveals the power of the NSS features, which help map images into a perceptually meaningful space for comparison. Fifth, a model that is worth noting is LPSI <ref type="bibr" target="#b35">[36]</ref>, which essentially reduces the feature space to one dimension and without using MOS for training, but it outperforms sophisticated machine learning-based approaches such as BRISQUE <ref type="bibr" target="#b16">[17]</ref> and DIIVINE <ref type="bibr" target="#b19">[20]</ref> which use many features for training. Sixth, machine learning based IQA models, though performed very well in existing publicly available databases, generally do not perform well in the current g-MAD competition. This may be because the training samples are not sufficient to represent the population of realworld natural images and thus the risk of over fitting is high. Furthermore, we perform a rational test to evaluate the robustness of IQA models when rating images with the same content and the same distortion type but different distortion levels. The underlying assumption is that the quality of an image degrades monotonically with the increase of the distortion level for all distortion types, and a good IQA model should rank the images in the same order. An example is given in <ref type="figure">Fig. 5</ref>, where the quality scores given by a good IQA model is supposed to decrease monotonically with the increase of the level of JPEG2000 compression. We use KRCC to check the consistency of the rankings between the distortion levels and the predicted scores of a giv-en model. An overall consistency measure is defined as</p><formula xml:id="formula_2">C = 1 HT H i=1 T j=1 KRCC(l, q ij ),<label>(3)</label></formula><p>where H = 4744 and T = 4 are the numbers of source images and distortion types in the database, respectively. l =[1, 2, 3, 4, 5, 6] represents the 6 distortion levels and q ij is a 6 × 1 vector that contains the corresponding quality scores given by a model to 6 images, which have the same (i-th) source image and the same (j-th) distortion type but different distortion levels. <ref type="figure" target="#fig_7">Fig. 6</ref> shows the overall consistency results of 16 IQA models, from which we have several observations. First, it is not surprising that FR models generally perform better than NR approaches because they are fidelity measures on how far away a distorted image departs from the source image, and such fidelity typically decreases monotonically with increasing distortion levels. Second, the NR model CORNIA <ref type="bibr" target="#b39">[40]</ref>, NIQE <ref type="bibr" target="#b17">[18]</ref> and its feature enriched extension ILNIQE <ref type="bibr" target="#b41">[42]</ref> outperform all other NR-IQA models, which coincides with the results of the gMAD competition shown in <ref type="figure" target="#fig_6">Fig. 4</ref>. Third, training based models generally have a lower overall consistency value and a larger error bar, suggesting lower robustness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion and Future Work</head><p>In this paper, we attempted to address the IQA model comparison problem to overcome the conflict between the enormous size of image space and the limited resource for subjective testing. Our major contributions are threefold. First, we built a database of 4, 744 high quality source natural images and 94, 880 distorted images, which is the largest in the literature of IQA research. Second, we proposed a substantially different methodology named gMAD competition to evaluate the relative performance of multiple IQA models. Different from conventional methods that attempt to prove a model, gMAD focuses on disproving a model in the most efficient way using automatically selected <ref type="figure">Figure 5</ref>. Illustration of the rational test on the "Hip-hop Girl" image under JPEG2000 compression. Obviously, the image quality degrades with the distortion level from left to right and from top to bottom (l =[ 1 , 2, 3, 4, 5, 6]). A good IQA model (ILNIQE <ref type="bibr" target="#b41">[42]</ref> for example) ranks the images in exactly the same order. By contrast, a less competitive model may give a different order, e.g., the QAC model <ref type="bibr" target="#b37">[38]</ref> ranks the image as q =[4, 3, 1, 5, 2, 6]. and model-dependent image pairs. The number of selected image pairs does not scale with the number of images, allowing it to exploit image databases of any size without increasing its complexity. Third, applying gMAD to the new database, we performed a systematic comparison of 16 well-known IQA models and made a number of useful observations.</p><p>The current work can be extended in many ways. First, the current database can continuously grow to include more image contents and more distortion types, and future IQA models can be added into the gMAD competition. We will make the database as well as the gMAD competition protocol and source code available online to facilitate future broader usage by researchers in the IQA community. Second, many useful observations have been made through the gMAD competition process. They can be used to facilitate further improvement of existing IQA models or future de- velopment of new IQA models. Third, the application scope of the fundamental idea behind gMAD is far beyond IQA model comparison. It is indeed a general methodology that can be used to compare any group of computational models used to predict certain continuous-scale quantities that need to be validated by expensive testing such as human subjective evaluation. To give a few examples, these may include comparisons of image/video interestingness predictors in the field of cognitive vision <ref type="bibr" target="#b7">[8]</ref>, the relative attributes (sportiness, furriness) estimators in the field of sematic image search <ref type="bibr" target="#b10">[11]</ref>, machine translation quality estimators in the field of computational linguistics <ref type="bibr" target="#b5">[6]</ref>, and thermal comfort models in the field of thermal environment of buildings <ref type="bibr" target="#b20">[21]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Sample source images in the new image database.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>• Step 1 .</head><label>1</label><figDesc>Apply all M IQA models to all N images in D. This results in a score matrix S of M rows and N columns, where each entry is the quality score given by one specific IQA model to one specific image;• Step 2. Choose the first model as the defender by setting i =1 . The rest of the M − 1 models are the attackers;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>User interface for subjective testing.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 .</head><label>3</label><figDesc>Pairwise gMAD competition matrices: Each entry indicates the Aggressiveness (a) or the Resistance (b) of the row IQA model against the column IQA model. A − A T and R − R T are drawn here for better visibility.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 .</head><label>4</label><figDesc>Global ranking of IQA models in terms of Resistance and Aggressiveness.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 .</head><label>6</label><figDesc>Overall KRCC consistency of IQA models.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Every image in the LIVE database has a MOS and an std associated with it, computed from all valid subjects. The std used here is in fact an average of stds for all images.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported in part by the NSERC Discovery Grant and Steacie Memorial Fellowship programs of Canada, the National Natural Science Foundation of China (No. 61525102) and the Hong Kong RGC GRF grant (PolyU 5313/13E). Kede Ma was also partially supported by the CSC.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Partially balanced incomplete block designs with two associate classes and two treatments per block</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">H</forename><surname>Clatworthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Research of the National Bureau of Standards</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="177" to="190" />
			<date type="published" when="1955" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Is crowdsourcing for optical flow ground truth generation feasible?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Donath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kondermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision Systems</title>
		<meeting><address><addrLine>Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="193" to="202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Randomised pair comparison: an economic and robust method for audiovisual quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Eichhorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Eg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM international workshop on Network and operating systems support for digital audio and video</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="63" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Massive online crowdsourced study of subjective and objective picture quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ghadiyaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="372" to="387" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Google</surname></persName>
		</author>
		<ptr target="https://images.google.com/.3" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Improving evaluation of machine translation quality estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Graham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">53rd Annual Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1804" to="1813" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Using free energy principle for blind image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The interestingness of images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gygli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Grabner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Riemenschneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Nater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1633" to="1640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Toyama-MICT image quality evaluation database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Horita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shibata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kawayoke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">M</forename><surname>Parvez</surname></persName>
		</author>
		<ptr target="http://mict.eng.u-toyama.ac.jp/mictdb" />
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note>online</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Statistical ranking and combinatorial hodge theory. Mathematical Programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-H</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ye</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="page" from="203" to="244" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Whittlesearch: Image search with relative attribute feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kovashka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2973" to="2980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Most apparent distortion: full-reference image quality assessment and the role of strategy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">C</forename><surname>Larson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Chandler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SPIE Journal of Electronic Imaging</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Subjective quality assessment IRCCyN/IVC database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Callet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Autrusseau</surname></persName>
		</author>
		<ptr target="http://www.irccyn.ec-nantes.fr/ivcdb/.1,2" />
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Boosting paired comparison methodology in measuring visual discomfort of 3DTV: performances of three different designs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Barkowsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">Le</forename><surname>Callet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IS&amp;T/SPIE Electronic Imaging</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Crowdtruth validation: a new paradigm for validating algorithms that rely on image correspondences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Maier-Hein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kondermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Roß</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mersmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Heim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bodenstedt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">G</forename><surname>Kenngott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sanchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Preukschas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer assisted radiology and surgery</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Matrix analysis and applied linear algebra</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Meyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">No-reference image quality assessment in the spatial domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Moorthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Making a &quot;completely blind&quot; image quality analyzer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Soundararajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A two-step framework for constructing blind image quality indices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Moorthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Blind image quality assessment: From natural scene statistics to perceptual quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Moorthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Adaptive thermal comfort and sustainable thermal standards for buildings. Energy and buildings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Nicol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Humphreys</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="563" to="572" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Image database TID2013: Peculiarities, results and perspectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ponomarenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ieremeiev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lukin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Egiazarian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Astola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Vozel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chehdi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Carli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Battisti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Signal Processing: Image Communication</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">TID2008-a database for evaluation of full-reference visual quality assessment metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ponomarenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lukin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zelensky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Egiazarian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Carli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Battisti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances of Modern Radioelectronics</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Picture processing by computer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rosenfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="147" to="176" />
			<date type="published" when="1969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Blind image quality assessment: A natural scene statistics approach in the DCT domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Saad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Charrier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Utilizing image scales towards totally training free blind image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">M J</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1879" to="1892" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A statistical evaluation of recent full reference image quality assessment algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Sabir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3440" to="3451" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">How to analyze paired comparison data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tsukida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Gupta</surname></persName>
		</author>
		<idno>UWEETR-2011-0004</idno>
		<imprint>
			<date type="published" when="2011" />
			<pubPlace>University of Washington</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Final report from the video quality experts group on the validation of objective models of video quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vqeg</surname></persName>
		</author>
		<ptr target="http://www.vqeg.org" />
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Modern image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Synthesis Lectures on Image, Video, and Multimedia Processing</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Mean squared error: love it or leave it? a new look at signal fidelity measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="98" to="117" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Maximum differentiation (MAD) competition: A methodology for comparing computational models of perceptual quantities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Vision</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Multiscale structural similarity for image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Asilomar Conference on Signals</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Blind image quality assessment based on multichannel features fusion and label transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">N</forename><surname>Ngan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note>to appear in</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A highly efficient method for blind image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Blind image quality assessment using joint statistics of gradient magnitude and Laplacian features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning without human scores for blind image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="995" to="1002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Active sampling for subjective image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Doermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="4249" to="4256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning framework for no-reference image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Doermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">VCL@FER image quality assessment database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zarić</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tatalović</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Brajković</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hlevnjak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lončarić</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Dumić</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Grgić</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AUTOMATIKA</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="344" to="354" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A feature-enriched completely blind image quality evaluator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2579" to="2591" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">FSIM: a feature similarity index for image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2378" to="2386" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
