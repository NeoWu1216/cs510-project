<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Harnessing Object and Scene Semantics for Large-Scale Video Understanding</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuxuan</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">Lab of Intel. Info. Processing</orgName>
								<orgName type="institution">Fudan University § Disney Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">Lab of Intel. Info. Processing</orgName>
								<orgName type="institution">Fudan University § Disney Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Gang</forename><surname>Jiang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">Lab of Intel. Info. Processing</orgName>
								<orgName type="institution">Fudan University § Disney Research</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Sigal</surname></persName>
							<email>lsigal@disneyresearch.com</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">Lab of Intel. Info. Processing</orgName>
								<orgName type="institution">Fudan University § Disney Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanghai</forename><surname>Key</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">Lab of Intel. Info. Processing</orgName>
								<orgName type="institution">Fudan University § Disney Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Harnessing Object and Scene Semantics for Large-Scale Video Understanding</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:41+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Large-scale action recognition and video categorization are important problems in computer vision. To address these problems, we propose a novel object-and scene-based semantic fusion network and representation. Our semantic fusion network combines three streams of information using a three-layer neural network: (i) frame-based low-level CNN features, (ii) object features from a state-of-the-art large-scale CNN object-detector trained to recognize 20K classes, and (iii) scene features from a state-of-the-art CNN scene-detector trained to recognize 205 scenes. The trained network achieves improvements in supervised activity and video categorization in two complex large-scale datasets -</head><p>ActivityNet and FCVID, respectively. Further, by examining and back propagating information through the fusion network, semantic relationships (correlations) between video classes and objects/scenes can be discovered. These video class-object/video class-scene relationships can in turn be used as semantic representation for the video classes themselves. We illustrate effectiveness of this semantic representation through experiments on zero-shot action/video classification and clustering.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The ubiquitous availability and use of devices that can capture and share videos on social platforms is astounding; an estimated 1 − 5 hours of videos are being uploaded to YouTube per second by the users. Such growth in visual media requires robust and scalable approaches for video indexing, search and summarization. However, general video understanding in unconstrained and, often, user-generated videos is extremely challenging. Videos vary greatly in terms of both the semantic content (e.g., concert) and appearance of that content (e.g., as observed from audience or backstage). The same or similar content can be recorded from a variety of views (e.g., front-row or obstructed-view seat in the back), under a breadth of viewing conditions (e.g., natural or stage lighting), and can be of nearly arbi- trary length (e.g., an hour long professional recoding, egocentric snippet, or iPhone highlight). Hence appearance variability within a given topic is often greater than variability across topics making recognition difficult.</p><p>In computer vision, video understanding is often addressed in the form of action/activity recognition or localization (this limits the scope to human-centric events and video content); generic video categorization <ref type="bibr" target="#b11">[12]</ref> has been much less thoroughly explored. In both domains the focus, over the years, has been largely on learning video-based representations (e.g. HoG, HoF or MBH <ref type="bibr" target="#b34">[35]</ref>), combined with supervised (or weakly supervised <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b28">29]</ref>) classifiers for recognition/categorization. Recent successes in deep learning, particularly Convolutional Neural Networks (CNNs), opened opportunities for learning discriminative hierarchical frame-based <ref type="bibr" target="#b12">[13]</ref> or spatio-temporal representations <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b33">34]</ref> jointly with the classifiers in an end-toend fashion. Recent CNN approaches have shown remarkable improvements in performance on datasets where large amount of labeled data is available <ref type="bibr" target="#b30">[31]</ref>. However, ability to learn from limited labeled data or scale such approaches up from at most few hundred classes to thousands, if not tens of thousands, of classes, presents significant challenges for the community; the latter, in particular, due to insurmountable efforts to scale up annotation to tens of millions of videos and practical inability to find and label rare events.</p><p>Semantic representations provide one way of bridging the current challenges. Semantic representations, in the form of attributes <ref type="bibr" target="#b14">[15]</ref> or objects <ref type="bibr" target="#b15">[16]</ref>, are becoming increasingly popular in object categorization <ref type="bibr" target="#b41">[42]</ref> and scene understanding <ref type="bibr" target="#b15">[16]</ref>. Such representations typically improve generalization by representing classes that may potentially have only few, or even no, training instances in terms of semantic entities that are much easier to train classifiers for. One challenge is that relationship between semantic entities and classes often needs to be defined by hand, which is costly and non-trivial (e.g., there may not be a general agreement on whether horse is furry 1 ); techniques have been proposed to solve for correlations algorithmically (e.g., sparse-coding <ref type="bibr" target="#b7">[8]</ref>) at the cost of model expressiveness 2 . In video categorization the use of such semantic representations has been much more limited, with few recent works focusing on attribute-based event recognition <ref type="bibr" target="#b16">[17]</ref>, joint actor-action based reasoning <ref type="bibr" target="#b36">[37]</ref>, object-action relationships <ref type="bibr" target="#b9">[10]</ref>, and at a very limited scale object + scene features to improve action classification <ref type="bibr" target="#b8">[9]</ref>. However, these works focus largely on the improved video classification performance, with semantic entities as black-box features, and not finding robust semantic decompositions of actions for other tasks (e.g., zero-shot prediction and clustering).</p><p>To address these issues we introduce a novel Object-Scene semantic Fusion (OSF) network for large-scale video categorization. OSF combines three streams of information using a three-layer fusion neural network: (i) frame-based low-level CNN features, (ii) object features from a stateof-the-art large-scale CNN object-detector with 20K classes and (iii) scene features from a state-of-the-art CNN scenedetector trained to recognize 205 scenes. This framework (see <ref type="figure" target="#fig_0">Figure 1</ref>) has a number of appealing properties. First, it is defined as an end-to-end network and hence joint training of all streams and the fusion layers is possible. Second, complex non-linear relationships between semantic entities (objects and scenes) and the video class labels can be learned and need not be specified by hand. Third, by examining and back propagating information through the fusion layers, semantic relationships between video classes and objects/scenes can be discovered. This Object and Scene semantic Representation (OSR), in the form of video classobject/video class-scene relationships, can be used for a variety of tasks, including zero-shot recognition of novel categories and measuring similarity (clustering). In addition to appealing conceptual properties OSF/OSR improves both supervised and zero-shot classification on two challenging and large-scale datasets for activity (ActivityNet <ref type="bibr" target="#b6">[7]</ref>) and generic video categorization (FCVID <ref type="bibr" target="#b11">[12]</ref>). <ref type="bibr" target="#b0">1</ref> See http://www.1freewallpapers.com/furry-black-white-horse. <ref type="bibr" target="#b1">2</ref> Linear model needs to be assumed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>The fields of action recognition and video classification are too broad to review completely; we focus only on the most relevant literature.</p><p>Traditional action/video classification: There is a variety of works in the field of video classification, with most focusing on developing more discriminative features and better classifiers <ref type="bibr" target="#b10">[11]</ref>. A typical video classification pipeline in recent literatures usually relies on the state-of-the-art dense trajectory features <ref type="bibr" target="#b34">[35]</ref>, which are local descriptors (e.g., HoG, HoF and MBH) computed around densely extracted frame patch trajectories. Bag-of-words and more advanced feature encoding strategies such as Fisher Vector <ref type="bibr" target="#b27">[28]</ref> have been adopted to quantize the local descriptors for classification (normally by an SVM classifier).</p><p>Deep models (CNNs/LSTMs): More recently, driven by the great success of Convolutional Neural Networks (CNN) on image analysis tasks <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32]</ref>, a few works attempted to leverage CNN models to learn feature representations for video classification. For instance, Karparthy et al. extended CNN models into the time domain by stacking frames <ref type="bibr" target="#b12">[13]</ref>.</p><p>To better explore the motion information, Simonyan et al. <ref type="bibr" target="#b30">[31]</ref> recently proposed to train two CNNs on still images and optical flow fields separately to capture appearance and motion information. Final predictions were generated by averaging scores from the two corresponding CNN streams. In order to model the temporal dynamics in videos, there are also a few works utilizing recurrent networks like the longshort term memory (LSTM) for recognition <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b35">36]</ref>. All these works, however, focused on extracting and encoding videos directly, using neural networks, and result in the representations that are neither semantic nor inherently interpretable. None of these works investigate or utilize object and/or scene semantics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CNN model visualization:</head><p>Our work is also partly inspired by the techniques for visualization and understanding of CNN networks. Zeiler et al. <ref type="bibr" target="#b38">[39]</ref> proposed Deconvolutional Network (DeconvNet) to approximately reconstruct the input of each layer from the corresponding output. More advanced recent visualization techniques are discussed in <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b39">40]</ref>; Deep Dream 3 has also been influential. We use visualization-inspired technique for discovering object-video class and scene-video class relationships from the proposed OSF network.</p><p>Semantic (Object/Scene) Context: Complex video semantics like activities and events have been shown to strongly correlate with their involved objects and scenes, which provide strong semantic context prior for video classification <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b17">18]</ref>. For example, in <ref type="figure" target="#fig_0">Figure 1</ref>, symphony per-formance often takes place in a concert hall, whilst skiing commonly happens outdoors. Prest et al. <ref type="bibr" target="#b24">[25]</ref> used a weakly supervised method to model human actions as interactions between humans and objects. Ikizler-Cinbis et al. <ref type="bibr" target="#b8">[9]</ref> proposed an approach for understanding human activities by integrating multiple feature clues from objects, scenes and people. Li et al. <ref type="bibr" target="#b15">[16]</ref> developed a large number of pre-trained generic object detectors named ObjectBank to generate high-level visual representations. ActionBank was proposed in <ref type="bibr" target="#b26">[27]</ref> as a semantic feature for video classification. These semantic representations have largely been explored on smaller datasets and almost exclusively as context for improving supervised classification.</p><p>Perhaps the closest to ours is a more recent work of <ref type="bibr" target="#b9">[10]</ref>, where relations between 15,000 object categories and high-level video categories like complex events are systematically studied. The authors conclude that objects are important for action and event recognition, and objectaction/event relations are generic. However, the relations were discovered using relatively simple generative learning method (sum of averaged object response vectors per action class), and hence the obtained relations tend to be noisy. In contrast, we learn the relations using a more advanced and robust discriminative neural network classifier with a special architecture tailored for the task. Discriminative nature of the learning allows us to focus on relationships that tend to improve classification performance. In addition, we also look at importance of scenes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach</head><p>Our goal is to learn a semantic model for video classification that enables effective supervised classification and, at the same time, allows us to discover semantic representations of our classes that are useful for other (unsupervised) tasks, like zero-shot learning or clustering. To this end, we first introduce an Object-Scene semantic Fusion (OSF) network (Sec. 3.1). OSF consists of a three-layer neural network that fuses information from three CNN streams: (i) a generic image feature stream, designed to capture low-level features of video frames, such as texture and color, (ii) an object stream that captures confidences among 20K object categories it is pre-trained to detect, and (iii) a scene stream, that similarly captures confidences among 205 scene categories it is pre-trained to detect.</p><p>Given the learned OSF model, we analyze it to discover, in Sec. 3.2, Object and Scene semantic Representation (OSR) which captures relationships (correlations) between video class labels and semantic entities (objects and scenes). This procedure is not as trivial as it may sound, as unlike with linear models, discovering such relations in our non-linear fusion architecture requires optimization. Finally, we show how discovered OSR can be utilized for zero-shot video classification (Sec. 3.3). Notation: Suppose we have a large-scale video dataset, D, where each video V i is associated with a class label z i ∈ Z T r from the training label set Z T r :</p><formula xml:id="formula_0">D = {(X i , z i )} i=1,··· ,n T r ,</formula><p>where n T r is the total number of training videos; each video is represented by a set of frames:</p><formula xml:id="formula_1">V i = {f i,1 , · · · , f i,ni },</formula><p>where n i is the total number of frames in video V i.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Object-Scene Semantic Fusion (OSF) Network</head><p>As stated above, OSF network has four components: object stream, scene stream, generic feature stream, and a three-layer neural network that fuses information from the three streams. The overall structure of the network is illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>.</p><p>Object stream (O−stream) extracts the object-related information for video classification. We use a VGG-19 CNN model, proposed in <ref type="bibr" target="#b31">[32]</ref>, which consists of 16 convolutional and 3 fully connected layers. VGG-19 for this stream is pretrained by using all ImageNet 20,574 object classes <ref type="bibr" target="#b1">[2]</ref>. We note that since humans can distinguish 30,000 basic object categories <ref type="bibr" target="#b0">[1]</ref>, our 20,574-class model is a good proxy for generic object cognition (covering roughly 2/3 of human distinguishable objects). We use output of the last fully connected layer (F C8) as the input for the fusion network; in other words, for the j-th frame of video i, f i,j , this stream</p><formula xml:id="formula_2">outputs f i,j → x O i,j ∈ R 20574</formula><p>. Scene stream (S−stream) extracts the scene-related information to help video classification. Here we use VGG-16 CNN model provided by <ref type="bibr" target="#b40">[41]</ref>. VGG-16 consists of 13 convolutional and 3 fully connected layers. The model is pre-trained using Places205 dataset <ref type="bibr" target="#b40">[41]</ref> (205 scene classes and 2.5 million images). We again use the output of the last fully connected layer (F C8) as the input for the fusion network; in other words, for the j-th frame of video i, f i,j , this stream outputs  Generic feature stream (F−stream) extracts more generic visual information that maybe directly relevant for video class prediction (e.g., texture, color) that the other two streams may overlook by suppressing object/scene irrelevant feature information. Once again we use a VGG-19 CNN model pre-trained on all of ImageNet. However, for this stream we take features of the first (not last) fully connected layer as input to the fusion network. In other words, for the j-th frame of video i, f i,j , this stream out-</p><formula xml:id="formula_3">f i,j → x S i,j ∈ R 205 .</formula><formula xml:id="formula_4">puts f i,j → x F i,j ∈ R 4096 .</formula><p>Note that this stream could easily adopt more advanced networks (e.g., motion network) to better account for the temporal structures in videos.</p><p>Fusion network is composed of three layers neural network (two hidden layers and one output layer) designed to fuse O−stream, S−stream and F−stream features defined above. Specifically, video-level feature representation is first generated by averaging the frames of each video for each stream. Since we do not fine-tune the network endto-end, this is done explicitly; but can be equivalently implemented by a pooling operation inserted between each stream and the first layer of the fusion network. For ex-</p><formula xml:id="formula_5">ample, video V i we represent asx O i = ni k=1 x O i,k ,x S i = ni k=1 x S i,k ,x F i = ni k=1 x F i,k . The averaged representationsx O i ,x S i ,x F</formula><p>i are fed into a first hidden layer of the fusion network, consisting of 250, 50, and 250 neurons respectively for each stream (550 neurons total). We use fewer neurons for S−stream because it has fewer dimensions. The output of the first hidden layer is fused by the second (250 neurons) fully connected layer across all streams. Then a softmax classifier layer is added for video classification. Note that we normalize the ground truth labels with L 1 norm when a sample has multiple labels. We denote f (·) as the non-linear function approximated by semantic fusion network and f z (x i ) as the score of video instance V i belong to the class z. The most likely class labelẑ i of V i is hence inferred as:</p><formula xml:id="formula_6">z i = argmax z∈Z T r f z (x i ) .<label>(1)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Object and Scene Semantic Representation</head><p>Once the OSF network is trained, the correlation between objects/scenes and video classes can be mined using the "visualization" of the network <ref type="bibr" target="#b29">[30]</ref>. The goal is to find from object and scene streams a pseudo video repre-  sentation that maximizes the neuron activity of each of the classes. Such object/scene representation identifies the most discriminative objects for the specified video class 4 .</p><p>More formally, let f z (x i ) be the score of the class z computed by the fusion network for video V i . We need to find an L 2 -regularized feature representation, such that the score f z (x i ) is maximized with respect to object or scene:</p><formula xml:id="formula_7">x k z = arg max x k i f z (x i ) − λ x k i 2<label>(2)</label></formula><p>where λ is the regularization parameter and k ∈ {O, S}.</p><p>The locally-optimal representationx k i can be obtained by back-propagation with randomly initializedx i . We set λ = 1e−3, learning rate to 0.8 and also fix the maximum iterations to 1000 to maximize the classification score of each class, aiming to find the representative objects/scenes associated with those classes. This way we can obtain object-video class / scene-video class semantic representation (OSR) matrices:</p><formula xml:id="formula_8">Π k = x k z T z ; k ∈ {O, S} .<label>(3)</label></formula><p>Interpretability of OSR: Given the object scene representation matrix, we try to answer the following question: what are intrinsic semantic properties of a video concept? <ref type="figure" target="#fig_3">Figure 3</ref> presents a depiction of the objects with the highest value for the category horse riding, including Piaffe, Arabian Horse and horseback Rider. It is appealing that these objects are semantically meaningful for horse riding and can help discriminate the class. Interestingly, we also find horse riding is related to bassFiddle, which is largely due to similar visual (e.g., shape and color) appearance.</p><p>To further validate the effectiveness of the learned representation, we illustrate part of the Π O matrix in <ref type="figure" target="#fig_4">Figure 4</ref>, where each entry indicates the response score between the object and the video class. Objects with high scores tend to be semantically meaningful for corresponding classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Zero-shot Learning via OSR Correlations</head><p>One of the perhaps most interesting applications of the discovered OSR correlation matrices Π = Π O , Π S , computed using Eq. <ref type="formula" target="#formula_7">(2)</ref>, is zero-shot learning. Different from the the supervised learning, this task is defined as transferring knowledge from known (source) classes to a disjoint set of unknown (target) classes in order to improve recognition. Specifically, zero-shot learning attempts to do this without having any labeled instances of the unknown classes available. We denote Z T e as the label set of testing instances, under assumption that Z T r ∩ Z T e = ∅.</p><p>One of the key assumptions we make for zero-shot recognition is that object-scene semantic space is a good proxy for measuring semantic distance of video content. In other words, video samples that contain similar objects and scenes are likely to belong to the same video class. In this sense, if we are able to represent a video sample by a vector containing probability (or confidences) of it containing objects and scene we can do classification by a simple nearest neighbor approach; comparing this object-scene vector representation to video class prototypes represented in the same object-scene semantic space. Matrix Π implicitly defines prototypes for all training video classes. For testing zero-shot classes, however, we have no data and hence cannot learn prototypes directly, but can synthesize them using some knowledge of similarity between zero-shot and training video categories.</p><p>Testing-class Prototype: The prototype of testing classes can be defined using the OSR matrix,</p><formula xml:id="formula_9">Πz = |Z T r | z=1 sim (z, z) · Π z<label>(4)</label></formula><p>where z ∈ Z T r andz ∈ Z T e (remember Z T r Z T e = ∅); sim (z, z) is the semantic similarity between the testing classz and the training class z; Π z is the the column of Π corresponding to class z. Similarity function can be defined manually <ref type="bibr" target="#b37">[38]</ref>, using WordNet <ref type="bibr" target="#b25">[26]</ref>, or by semantic word vectors <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b22">23]</ref>. Here we use word2vec <ref type="bibr" target="#b18">[19]</ref> to define the similarity function sim (z, z) between the known (source) and unknown (target) classes.</p><p>Zero-shot Recognition: Given the synthesized testing prototypes and the representation of the test sample g(V ), the class label can be inferred using a simple nearest neighbor lookup:ẑ = argmaxz ∈Z T e cos (g (V ) , Πz)</p><p>cos (·) indicates the cosine similarity, which takes scale into account and works better than a dot product (as we illustrate in <ref type="table">Table 3</ref>). The only missing part we have not discussed is how to obtain representation of the test video g(V ). The simplest approach is to define g(V ) using the O−stream and S−stream directly. This approach would correspond to g(V ) = x O ,x S . However, as we show in the experiments this tends to produce poor performance. One of the reasons is that contextual information among all three streams and the fusion network are not utilized and hence the individual predictions obtained using the object and scene streams tend to be noisier. The alternative which works much better in practice, is to define g(V ) with respect to the training class prototypes, by forming a "pseudoinstance" prototype. This approach is inspired by ConSE <ref type="bibr" target="#b21">[22]</ref> and we describe it in details below.</p><p>Probability Calibration: We first employ Platt Scaling <ref type="bibr" target="#b23">[24]</ref> to calibrate the output of the fusion network f (·) into a probability distribution p (·), defined for each of the training classes. Hence the probability of video V i belonging to a class label z ∈ Z T r is denoted p (z|x i ), such that the sum across all training classes is</p><formula xml:id="formula_11">|Z T r | z=1 p (z|x i ) = 1.</formula><p>Pseudo-Instance Prototype: For a testing instance, we synthesize a "pseudo" prototypes as in ConSE <ref type="bibr" target="#b21">[22]</ref>. We use z t to denote the t th most likely training label for video V according to p (·) function; and p (z t |x) is the probablity of video V belonging to training label z t , which is also the t th largest probability for the posterior of video V over all training classes. Thus given the top T predictions, the pseudo prototype of the testing instance V can be synthesized by using our OSR matrix Π, formally we have</p><formula xml:id="formula_12">g(V ) = 1 ∆ T t=1 p (z t |x) · Π zt<label>(6)</label></formula><p>where ∆ = T t=1 p (z t |x) is a normalization factor; and Π zt indicates the z t -th column of Π.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiment</head><p>We conduct a number of experiments to explore the benefits of our semantic formulation. We start by showing that our object-scene semantic fusion (OSF) network is effective for supervised action and video categorization (Sec. 4.2).</p><p>We then show effectiveness of object and scene semantic representation (OSR) that OSF allows us to discover from data. Specifically, we show that OSR is effective for (i) computing semantic distance among video classes by using it to discover class group structure through clustering (Sec. 4.3) and (ii) that it can be utilized for effective zeroshot classification (Sec. 4.4) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Setup</head><p>Datasets: We adopt two challenging large-scale video benchmark datasets to evaluate our approach. ActivityNet <ref type="bibr" target="#b6">[7]</ref> is a recently released large-scale video dataset for human activity recognition and understanding. ActivityNet consists of 27, 801 video clips annotated into 203 activity classes, totaling 849 hours of video. Compared with existing action recognition benchmarks (e.g., UCF101 <ref type="bibr" target="#b32">[33]</ref> or HMDB51 <ref type="bibr" target="#b13">[14]</ref>), ActivityNet is more challenging, since it contains fine-grained action categories that require subtle details to differentiate among (e.g., drinking beer and drinking coffee). ActivityNet provides both trimmed and untrimmed videos for its classes. Trimmed videos consist of hand annotated segments that contain frames corresponding to given actions; untrimmed videos have much longer videos which contain frames irrelevant to the dominant action or multiple actions. We use the more challenging untrimmed setting for our experiments. ActivityNet consists of training, validation and test splits, however, test split is not made available by the authors. To this end we use validation split as our test set.</p><p>Fudan-Columbia Video Dataset (FCVID) <ref type="bibr" target="#b11">[12]</ref> contains 91,223 web videos annotated manually into 239 categories. Categories cover a wide range of topics (not only activities), such as social events (e.g., tailgate party), procedural events (e.g., making cake), object appearances (e.g., panda) and scenic videos (e.g., beach). We use standard split of 45,611 videos for training and 45,612 videos for testing.</p><p>Evaluation Metrics: To evaluate our OSF network for supervised classification, we adopt the standard training and testing splits and compute average precision for each class as suggested in <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b11">12]</ref>. Mean average precision (mAP) is used to measure the overall performance on both datasets. For zero-shot learning, since there is no off-the-shelf splits defined on these datasets, we split the video datasets into source and target categories. More precisely, we split Ac-tivityNet into 140 source and 63 target classes; FCVID into 160 source and 79 target classes. Mean accuracy (the mean of the diagonal of the confusion matrix) is used to measure the zero-shot learning performance.</p><p>Word2Vec Embedding: To generate semantic word representations, we compute 1,000-dimensional embedding vector by training word2vec <ref type="bibr" target="#b18">[19]</ref> on a large text corpus, including UMBC WebBase (3 billion words) and the latest Wikipedia articles (3 billion words).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">OSF Network for Supervised Recognition</head><p>In this section we focus on exploring the effectiveness of our object-scene fusion network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Baselines:</head><p>We compare with a number of alternative methods to combine multiple features in supervised classification. Among them, early and late fusions are two straightforward ways to integrate multiple features and are two variants of our model. 5. SVM-MKL <ref type="bibr" target="#b19">[20]</ref>, combines the multiple stream features using multiple kernel learning with χ 2 -kernel. We note that most of these baselines are very strong as they are using exactly the same features as our fusion network and complex state-of-the-art non-linear classifiers.</p><p>Results: <ref type="table">Table 1</ref> summarizes the comparisons of our approach and alternative methods. As can be seen from the table, our OSF network achieves 56.8% and 76.5% mAP on ActivityNet and FCVID respectively, outperforming other fusion baselines by clear margins. For early fusion, direct concatenation of features is the most straightforward way of combining representations; this, however, suffers from high dimensionality (&gt; 25k dimensions) which leads to overfitting. Late fusion suffers from the "heterogeneous" classification scores coming from each stream; each stream has varying discriminative capacity and (may) results in incomparable classification scores. In contrast to the alternative fusion methods, our OSF network can implicitly explore the correlations among the streams to derive a fused representation, which is more semantically discriminative for recognition. In addition, compared with the state-of-the-art published results 42.5% <ref type="bibr" target="#b11">[12]</ref> and 73.0% <ref type="bibr" target="#b6">[7]</ref>, our OSF framework achieves 3.5% and 14.3% (percentage points) improvement on FCVID and ActivityNet respectively. Note, results in <ref type="bibr" target="#b11">[12]</ref> and <ref type="bibr" target="#b6">[7]</ref> are obtained by combining multiple state-of-the-art handcrafted visual features (e.g., improved dense trajectories) and deep features. Our network achieves superior performance by jointly modeling semantic representations (objects and scenes) with low-level deep features. Further improvement can be obtained by considering motion features, which we currently omit.  <ref type="bibr" target="#b19">[20]</ref> 56.3 74.9</p><p>Heilbron et al. <ref type="bibr" target="#b6">[7]</ref> 42.5 -Jiang et al. <ref type="bibr" target="#b11">[12]</ref> -73.0 OSF Network 56.8 76.5 <ref type="table">Table 1</ref>. Comparisons with alternative baselines on ActivityNet and FCVID datasets.</p><p>To further evaluate the contribution of each stream in our network, we break down our network with different combinations of the three streams. The results are reported in <ref type="table">Table 2</ref>. We adopt a 3-layer NN classifier for each single stream (similar in structure to our fusion network); and a variant network with two streams. We can see that the performance of F − stream &gt; O − stream &gt; S − stream, which indicates that though the high-level semantic information expressed in objects and scene is important, the generic feature stream (F − stream) still has significant low-level discriminative information which is very useful for classification. In addition, since scene detectors are usually prone to noise, especially in complex long videos with cluttered background, the performance of S − stream is lower. Notice that S − stream achieves significantly better results on FCVID than ActivityNet, since categories in Ac-tivityNet are all actions while FCVID contains more generic video classes, where scene clues are more important. In addition, the three streams are complementary. Combining arbitrary two streams offers better performance than single stream. Our OSF network result is further improved performance over pair-wise combinations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Object and Scene Semantic Representation</head><p>We now investigate the object and scene semantic representation derived from the trained OSF network on FCVID. For each class of interest, we obtain a pseudo video representation that maximizes the neuron activity, identifying the most discriminative objects for the specified video class. Since related classes share certain objects and scenes, we expect their pseudo representations to be similar. To validate the effectiveness of the derived video representation, we compute the cosine similarity between each pair of video classes using the pseudo representations and then obtain the similarity matrix of all the categories. We perform Normalized Cut method to group and order the categories of the similarity matrix for visualization in <ref type="figure" target="#fig_6">Figure 5</ref>.</p><p>As we can see from the figure, the pseudo video representation can indeed discover some group structures of the video classes. We also compare with the groups discovered using word vectors (blue dashed lines). Comparing all rows in the figure, the group structure generated by the object and scene semantic representations can identify more finegrained categories, while similarity computed by word vectors seems to group too many classes together. Since word vectors are trained on large text corpus, they fail to distinguish categories with similar class names that are visually and semantically different (e.g., make juice and make paper plane). In the first two groups of the second row, clearly the scene information played an important role to separate classes baseball, sportsTrack and soccer Professional (outdoor sports) into a separate group from classes of barbell workout, fencing, and pull ups (indoor sports). This result validates the superiority of our object and scene representation. Interestingly, our grouping results are even better than the manually labeled hierarchy provided by the dataset. For example, our method can group the following classes together: rafting, fishing, mountain, since these classes have similar objects appearing (e.g., mountain, raft and water) and highly coherent scene information (e.g., outdoor, sky). Nevertheless, manual defined ontology categorizes them as extreme sports, sceneries and leisure sports respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Zero-shot Learning</head><p>Baselines: We compare the following methods for largescale zero-shot recognition:</p><p>1. DAP-word. Support vector regressors are used to learn to regress from concatenated features from three streams to each dimension of 1000-d word vector representing a class. For zero-shot learning, the predicted word vectors of testing instances are matched against the 1000-d word vector prototypes of unknown classes, obtained using word2vec, with nearest neighbor approach. This is a generalization of DAP <ref type="bibr" target="#b14">[15]</ref>. <ref type="bibr" target="#b21">[22]</ref> uses the same p (·) function to predict the posterior of one testing instance belonging to each known class. Eq.(6) is utilized to synthesize the pseudo-instance prototypes from known classes by replacing the semantic representations (Π zt ) with 1000d word vectors for each class; the testing class prototypes, Eq.(5), are also replaced by 1000-d word vectors.  3. ConSE-pseudo is a variant of ConSE which is more comparable to our method. The main steps of ConSEpseudo are the same as ours, while the difference is that ConSE-pseudo replaces our semantic representation of known and unknown classes with 1000-d word vectors for zero-shot recognition in both Eq.(6) and Eq.(5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">ConSE</head><p>4. Nearest neighbor (NN): uses Eq.(4) to synthesize the prototypes of testing classes andẑ i = argmaxz ∈Z T e cos x O i ,x S i , Πz to infer the class label for V i . This alternative is discussed in Sec. 3.3.</p><p>We compare these methods to our proposed model (Ours) that uses OSR and the two variants discussed in Section 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results:</head><p>The zero-shot learning results are summarized in <ref type="table">Table 3</ref>. Our method is better than all the other baselines on both datasets. We can see that using the semantic representation derived by Eq.(2) can offer better zero-shot recognition performance. This is validated by two observations: (1) Our results improve by 1.4% and 1.3% percentage points (or 13% and 12% respectively) over ConSE, which is a state-of-the-art approach for zero-shot learning. The improvements are largely due to our semantic representation obtained by mining visual video class-object/video class-scene correlations, which therefore is more semantically discriminative than word vectors trained with text corpus. (2) This improvement does not come from the way we generate testing-class prototype by Eq.(5): our results are 3.5% and 2.6% higher than those of ConSE-pseudo; the only difference between our method and ConSE-pseudo is that ConSE-pseudo replaces our semantic representation with semantic word vectors. <ref type="formula" target="#formula_8">(3)</ref> The results of all methods are better than those of NN. This is in part due to the contextual information shared across streams, which can be ActivityNet FCVID Chance 1.6 1.3 DAP-word <ref type="bibr" target="#b14">[15]</ref> 11.3 9.0 ConSE <ref type="bibr" target="#b21">[22]</ref> 10.7 10.6 ConSE-pseudo 8.6 9.3 NN 8.5 8.8</p><p>Ours (Dot Product) 11.8 11.4 Ours (+F-Stream) 11.6 11.8 Ours 12.1 11.9 <ref type="table">Table 3</ref>. Zero-shot Learning Accuracy(%). discovered in the OSF network, is not fully utilized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We present a novel Object-Scene semantic Fusion (OSF) framework for large-scale video understanding, which has a number of appealing properties. Our fusion network combines three streams (i.e., object, scene and generic feature) of information using a three-layer neural network to model object and scene dependencies. This results in supervised video classification improvements in two large-scale benchmark datasets. Further, by examining and back propagating information through the fusion layers, semantic relationships (correlations) between video classes (or activities) and objects/scenes can be identified. These relationships can, in turn, be utilized as semantic representation for the video classes themselves. We empirically evaluate the learned representations in the task of zero-shot learning and clustering, and the results corroborate the effectiveness of the discovered relationships.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Illustration of the proposed Object-Scene semantic Fusion (OSF) network and its application on several tasks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>...... ...... ...... Object-Scene semantic Fusion (OSF) network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Objects with highest responses for the class horse riding; size indicates importance. See text for discussion.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>The visualization of a part from the Π O learned on FCVID, where each entry denotes the response score between each object and video class pair.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>1 .</head><label>1</label><figDesc>Early Fusion-NN, concatenates all three streams into a long vector and then uses it as the input to train a neural network for categorization; 2. Late Fusion-NN, trains a neural network classifier using each of three streams independently and then the outputs from all the networks are averaged to obtain the final prediction scores; 3. Early Fusion-SVM, utilizes the χ 2 -kernel SVM for classification, where kernel matrices are first computed for each stream and then averaged for classification; 4. Late Fusion-SVM, learns a χ 2 -kernel SVM classifier for each stream and then combines prediction results;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 .</head><label>5</label><figDesc>Left: Similarity matrix of categories in FCVID computed with the derived OSR, where the red boxes indicate the automatically generated category groups. Right: Visual examples of the groups indicated on the similarity matrix, with the red arrows indicating the correspondence of the similarity matrix with the class examples. Groups we discovered are separated by black lines; groups discovered by word vectors are circled by blue dashed lines.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head></head><label></label><figDesc>Table 2. Results of variants of our network. "+" denotes two streams used in network fusion.</figDesc><table>ActivityNet FCVID 
F − stream 
47.4 
67.7 
O − stream 
44.8 
55.5 
S − stream 
18.8 
41.3 
F − stream + O − stream 
56.2 
75.6 
F − stream + S − stream 
52.6 
72.3 
O − stream + S − stream 
55.4 
72.8 
OSF Network 
56.8 
76.5 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">http://googleresearch.blogspot.fr/2015/06/ inceptionism-going-deeper-into-neural.html</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Note that the method in Eq.(2) is also applicable to generic feature stream which, however, has no semantic meaning and hence is less useful as confirmed in Sec 4.4(Table 3).</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Recognition by components -a theory of human image understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Biederman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">ImageNet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning multi-modal latent attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Semi-supervised vocabulary-informed learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Activitynet: A large-scale video benchmark for human activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">C</forename><surname>Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>. 1, 4.1, 4.2</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A unified semantic embedding: relating taxonomies and attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Object, scene and actions: Combining multiple features for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ikizler-Cinbis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">What do 15,000 object categories tell us about classifying and localizing actions?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Van Gemert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">High-level event recognition in unconstrained videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJMIR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Exploiting feature and class relationships in video categorization with regularized deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>CoRR, 2015. 1, 4.1, 4.2</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">HMDB: A large video database for human motion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Garrote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>In ICCV, 2011. 4.1</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Attributebased classification for zero-shot visual object categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nickisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harmeling</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>IEEE TPAMI, 2013. 1, 1, 4.4</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Object bank: A high-level image representation for scene classification &amp; semantic feature sparsification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Video event recognition using concept attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Javed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tamrakar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Divakaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sawhney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Workshop on WACV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Actions in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marszalek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>3.3, 4.1</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multimodal feature fusion for robust event detection in web videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Natarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vitaladevuni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tsakalidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Natarajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Beyond short snippets: Deep networks for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Zero-shot learning by convex combination of semantic embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>ICLR, 2014. 3.3, 2, 4.4</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Probabilities for SV machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Platt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Large Margin Classifiers</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Weakly supervised learning of interactions between humans and objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Prest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">What helps where -and why? semantic relatedness for knowledge transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Szarvas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gurevych</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Action bank: A high-level representation of activity in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sadanand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Corso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Image classification with the fisher vector: Theory and practice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Similarity constrained latent support vector machine: An application to weakly supervised action classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shapovalova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cannons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Deep inside convolutional networks: Visualising image classification models and saliency maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>ICLR. 2014. 2, 3.2</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">UCF101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno>CRCV- TR-12-01, 2012. 4.1</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Action recognition with improved trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Modeling spatial-temporal clues in a hybrid deep learning framework for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Can humans fly? action understanding with multiple classes of actors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-H</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Designing category-level attributes for discriminative visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Object detectors emerge in deep scene cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Learning deep features for scene recognition using places database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<idno>NIPS. 2014. 3.1</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Reasoning about object affordances in a knowledge base representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
