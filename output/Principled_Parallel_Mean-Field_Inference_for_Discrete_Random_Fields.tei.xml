<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Principled Parallel Mean-Field Inference for Discrete Random Fields</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Baqué</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">CVLab</orgName>
								<orgName type="institution">EPFL</orgName>
								<address>
									<settlement>Lausanne</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timur</forename><surname>Bagautdinov</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">CVLab</orgName>
								<orgName type="institution">EPFL</orgName>
								<address>
									<settlement>Lausanne</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Fleuret</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">CVLab</orgName>
								<orgName type="institution">EPFL</orgName>
								<address>
									<settlement>Lausanne</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">IDIAP</orgName>
								<address>
									<settlement>Martigny</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">CVLab</orgName>
								<orgName type="institution">EPFL</orgName>
								<address>
									<settlement>Lausanne</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Principled Parallel Mean-Field Inference for Discrete Random Fields</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:44+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Mean-field variational inference is one of the most popular approaches to inference in discrete random fields. Standard mean-field optimization is based on coordinate descent and in many situations can be impractical. Thus, in practice, various parallel techniques are used, which either rely on ad hoc smoothing with heuristically set parameters, or put strong constraints on the type of models.</p><p>In this paper, we propose a novel proximal gradientbased approach to optimizing the variational objective. It is naturally parallelizable and easy to implement.</p><p>We prove its convergence, and demonstrate that, in practice, it yields faster convergence and often finds better optima than more traditional mean-field optimization techniques. Moreover, our method is less sensitive to the choice of parameters.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Many Computer Vision problems, ranging from image segmentation to depth estimation from stereo, can be naturally formulated in terms of Conditional Random Fields (CRFs). Solving these problems then requires either estimating the most probable state of the CRF, or the marginal distributions over the unobserved variables. Since there are many such variables, it is usually impossible to get an exact answer, and one must instead look for an approximation.</p><p>Mean-field variational inference <ref type="bibr" target="#b32">[33]</ref> is one of the most effective ways to do approximate inference and has become increasingly popular in our field <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b23">24]</ref>. It involves introducing a variational distribution that is a product of terms, typically one per hidden variable. These terms are then estimated by minimizing the Kullback-Leibler (KL) divergence between the variational and the true posterior. The standard scheme is to iteratively update each factor of the distribution one-by-one. This is guaranteed to converge <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b20">21]</ref>, but is not very scalable, because all variables have to be updated sequentially. It becomes impractical for * The authors contributed equally. input baseline ours ground truth <ref type="figure">Figure 1</ref>. First two rows: VOC2012 images in which we outperform a baseline by adding simple co-ocurrence terms, which our optimization scheme, unlike earlier ones, can handle. Bottom row: Our scheme also allows us to improve upon a baseline for the purpose of recovering a character from its corrupted version.</p><p>realistically-sized problems when there are substantial interactions between the variables. This can be remedied by replacing the sequential updates by parallel ones, often at the cost of failing to converge. It has nonetheless recently been shown that parallel updates could be done in a provably convergent way for pairwise CRFs, provided that the potentials are concave <ref type="bibr" target="#b23">[24]</ref>. When they are not, an ad hoc heuristic designed to achieve convergence, which essentially smooths steps by averaging between the next and current iterate, has been used over the years. This heuristic is mentioned explicitly in some works <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b13">14]</ref>, or used implicitly in optimization schemes <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b31">32]</ref> by introducing an additional damping parameter.</p><p>However, a formal justification for such smoothing is never provided, which we do in this paper. More specifically, we show that, by damping in the natural parameter space instead of the mean-parameter one, we can reformulate the optimization scheme as a specific form of proximal gradient descent. This yields a theoretically sound and practical way to chose the damping parameters, which guarantees convergence, no matter the shape of the potentials. When they are attractive, we show that our approach is equivalent to that of <ref type="bibr" target="#b23">[24]</ref>. However, even when they are re-pulsive and can cause the earlier methods to oscillate without ever converging, our scheme still delivers convergence. For example, as shown in <ref type="figure">Fig. 1</ref>, this allows us to add cooccurrence terms to the model used by a state-of-the-art semantic segmentation method <ref type="bibr" target="#b7">[8]</ref> and improves its results. Furthermore, we retain the simplicity of the closed-form mean-field update rule, which is one of the key strengths of the mean-field approach.</p><p>In short, our contribution is threefold:</p><p>• We introduce a principled, simple, and efficient approach to performing parallel inference in discrete random fields. We formally prove that it converges and demonstrate that it performs better than state-of-the-art inference methods on realistic Computer Vision tasks such as segmentation and people detection.</p><p>• We show that many of the earlier methods can be interpreted as variants of ours. However, we offer a principled way to set its metaparameters.</p><p>• We demonstrate how parallel mean-field inference in random fields relates to the gradient descent. This allows us to integrate advanced gradient descent techniques, such as momentum and ADAM <ref type="bibr" target="#b19">[20]</ref>, which makes mean-field inference even more powerful.</p><p>To validate our approach, we first evaluate its performance on a set of standardized benchmarks, which include a range of inference problems and have recently been used to assess inference methods <ref type="bibr" target="#b13">[14]</ref>. We then demonstrate that the performance improvements we observed carry over to three realistic Compute Vision problems, namely Characters Inpainting, People Detection and Semantic Segmentation. In each case, we show that modifying the optimization scheme while retaining the objective function of state-ofthe-art models <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b7">8]</ref> yields improved performance and addresses the convergence issues that sometimes arise <ref type="bibr" target="#b31">[32]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background and Related Work</head><p>In this section, we briefly review basic Conditional Random Field (CRF) theory and the use of mean-field inference to solve the resulting optimization problems. We also give a short introduction into proximal gradient descent algorithms, on which our method is based. Note, in this work, we focus on models involving discrete random variables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Conditional Random Fields</head><p>Let X = (X 1 , . . . , X N ) represent hidden variables and I represent observed variables. For example, for semantic segmentation, the X i s are taken to be variables representing semantic classes of N pixels, and I represents the observed image evidence.</p><p>A Conditional Random Field (CRF) models the relation-ship between X and I in terms of the posterior distribution</p><formula xml:id="formula_0">P (X | I) = exp   c⊂{1,...,N } φ c (X c | I) − log Z(I)   ,</formula><p>(1) where φ c (.) are non-negative functions known as potentials and log Z(I) is the log-partition function. It is a constant that we will omit for simplicity since we are mostly concerned by estimating values of X that maximize P (X | I).</p><p>This model is often further simplified by only considering unary and pairwise terms:</p><formula xml:id="formula_1">P (X | I) ∝ exp   i φ i (X i , I i ) + (i,j) φ ij (X i , X j )   .</formula><p>(2)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Mean-Field Inference</head><p>Typically, one wants either to estimate the posterior P (X|I) or to find the vectorX that maximizes P (X|I), which is known as the MAP assignment. Unfortunately, even for the simplified formulation of Eq. 2, both are intractable for realistic sizes of X. As a result, many approaches settle for approximate solutions. These include sampling methods, such as Gibbs sampling <ref type="bibr" target="#b14">[15]</ref>, and deterministic ones such as mean-field variational inference <ref type="bibr" target="#b33">[34]</ref>, belief propagation <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b21">22]</ref>, and others <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b15">16]</ref>. A comprehensive comparison of inference methods in discrete models is provided in <ref type="bibr" target="#b17">[18]</ref>.</p><p>Note that, mean-field methods have been shown to combine the advantages of good convergence guarantees <ref type="bibr" target="#b4">[5]</ref>, flexibility with respect to the potential functions that can be handled <ref type="bibr" target="#b28">[29]</ref>, and potential for parallelization <ref type="bibr" target="#b23">[24]</ref>. As a result, they have become very popular in our field. Furthermore, they have recently been shown to yield state-of-theart performance for several Computer Vision tasks <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b34">35]</ref>.</p><p>Mean-field involves introducing a distribution Q of the factorized form</p><formula xml:id="formula_2">Q(X = (x 1 , . . . , x N ); q) = N i=1 Q i (X i = x i ; q i ) ,<label>(3)</label></formula><p>where Q i ( . ; q i ) is a categorical distribution with mean parameters q i . That is,</p><formula xml:id="formula_3">∀l, Q i (X i = l; q i ) = q i,l ,<label>(4)</label></formula><p>with q in the space M such that ∀i ∈ {1, . . . , N }, l ∈ {1, . . . , L}, 0 ≤ q i,l ≤ 1 and ∀i, l q i,l = 1, where N is often the number of pixels, and L is the number of labels. Q is then used to approximate P (X | I) by minimizing the KL-divergence:</p><formula xml:id="formula_4">KL(Q||P ) = x Q(X = x; q) log Q(X = x; q) P (X = x | I) .<label>(5)</label></formula><p>In some cases, this approximation is the desired final result. In others, one seeks a MAP assignment. To this end, a standard method is to select the assignment that maximizes the approximate posterior Q(X; q), which is equivalent to rounding when the X i s are Bernoulli variables. An alternative approach is to draw samples from Q(X; q). When minimizing the KL-divergence of Eq. 5, Q(X; q) can be reparameterized in terms of its natural parameters defined as follows. For each variable X i and label l, we take the natural parameter θ i,l to be such that</p><formula xml:id="formula_5">Q(X i = l; q i ) = q i,l ∝ exp[−θ i,l ].<label>(6)</label></formula><p>As we will see below, this parameterization often yields simpler notations and implementations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Sweep Mean-Field Inference</head><p>Minimizing the expression of Eq. 5 is equivalent <ref type="bibr" target="#b4">[5]</ref> to minimizing</p><formula xml:id="formula_6">F(q) = −E Q(X;q) [log P (X | I)] E(q) + E Q(X;q) [log Q(X; q)] −H(q) ,<label>(7)</label></formula><p>with respect to q ∈ M. F(.) is sometimes called the variational free energy. Its first term is the expectation of the energy under Q(X; q), and its second term is the negative entropy, which acts as a regularizer.</p><p>One can minimize F(q) by iteratively updating each q i,l in sequence while keeping the others fixed <ref type="bibr" target="#b4">[5]</ref>. Each update involves setting q i,l to q ⋆ i,l ∝ exp E Q(X/Xi;q) log P (X | I) .</p><p>This coordinate descent procedure, which we will call SWEEP, is guaranteed to converge to a local minimum of F <ref type="bibr" target="#b4">[5]</ref>. However, it tends to be very slow for realistic image sizes and impractical for many Computer Vision problems <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b23">24]</ref>. Namely, in the case of dense random fields, it involves re-computing a large number of expectations (one per factor adjacent to the variable) after each sequential update. Filter-based mean-field inference <ref type="bibr" target="#b22">[23]</ref> attempts to reduce the complexity of these updates, but it effectively performs parallel updates, which we will describe below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Parallel Mean-Field Inference</head><p>To obtain reasonable efficiency in practice, Computer Vision practitioners often perform the updates of Eq. 8 in parallel as opposed to sequentially. Not only does it avoid having to reevaluate a large number of factors after each update, it also allows the use of vectorized instructions and GPUs, both of which can have a dramatic impact on the computation speed.</p><p>Unfortunately, these parallel updates invalidate the convergence guarantees and in practice often lead to undesirable oscillations in the objective. Several approaches to remedying this problem have been proposed, which we review below.</p><p>Damping A natural way to improve convergence is to replace the updates of Eq. 8 by a damped version, expressed as</p><formula xml:id="formula_8">q t+1 i,l = (1 − η) · q t i,l + η · q ⋆ i,l ,<label>(9)</label></formula><p>where t denotes the current iteration, q ⋆ i,l is the result of solving the optimization problem of Eq. 8, and η is a heuristically chosen damping parameter. This damping is explicitly mentioned in papers such as <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b13">14]</ref>. In <ref type="bibr" target="#b31">[32]</ref>, convergence issues are mentioned and a damping parameter is provided in the publicly available code. Similarly, in <ref type="bibr" target="#b12">[13]</ref>, the algorithm relies on mean-field optimization with repulsive terms. The need for damping is not explicitly discussed in the paper, but the publicly available code also includes a damping.</p><p>Damping delivers satisfactory results in many cases, but does not formally guarantee convergence. It may fail if the parameter η is not carefully chosen, and sometimes changed at different stages of the optimization. In all the approaches that we are aware of, this is done heuristically. We will refer to this type of methods as ADHOC.</p><p>Concave potentials A principled way to address the convergence issue for the pairwise random fields is offered in <ref type="bibr" target="#b23">[24]</ref>, and we refer to the corresponding algorithm as FULL-PARALLEL. However, authors restrict their potentials φ ij of Eq. 2 to be concave, which in some cases is reasonable, but as we will show in Section 4, many Computer Vision models violate this requirement. By contrast, our approach is similarly principled but without additional constraints. In practice it works for higher-order, or, equivalently, non-pairwise potentials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Proximal Gradient Descent</head><p>Let F be a generic objective function of the form</p><formula xml:id="formula_9">F (x) = f (x) + g(x)</formula><p>, where g is a regularizer, and x t is the value of the optimized variable at iteration t of a minimization procedure on a constraint set X . Proximal gradient descent, also known as composite mirror-descent <ref type="bibr" target="#b10">[11]</ref>, is an iterative method that relies on the update rule</p><formula xml:id="formula_10">x t+1 = argmin x∈X { x, ∇f (x t ) +g(x)+λΨ(x, x t )} ,<label>(10)</label></formula><p>where Ψ is a non-negative proximal function that satisfies Ψ(x, x t ) = 0 if and only if x = x t , and λ &gt; 0 is a scalar parameter. g contains the terms of the objective function that do not need to be approximated to the first order, while still allowing efficient computation of update of Eq. 10. Ψ can be understood as a distance function that accounts for the geometry of X <ref type="bibr" target="#b30">[31]</ref> while also making it possible to compute the update of Eq. 10 efficiently. λ can then be thought of as the inverse of the step size.</p><p>As shown in Section 3.1, our algorithm is a version of proximal gradient descent in which Ψ is based on the KLdivergence and allows automated step-size adaptation as the optimization progresses. Recently, a variational approach that also relies on the KL-divergence as the proximal function has been proposed <ref type="bibr" target="#b18">[19]</ref>. This paper explores the connection between the KL-proximal method and the Stochastic Variational Inference <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b16">17]</ref>. However, the method presented there is not directly applicable to discrete random fields, especially for the Vision problems we consider. Moreover, it does not allow for step size adaptation, which often yields better performance, as we demonstrate in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>As discussed in the previous section, the goal of meanfield inference is to</p><formula xml:id="formula_11">minimize q∈M F(q)<label>(11)</label></formula><p>where F is the variational free energy of Eq. 7. Performing sequential updates of the q i,l is guaranteed to converge, but can be slow. Parallel updates are usually much faster, but the optimization procedure may fail to converge.</p><p>In this section, we introduce our approach to guaranteeing convergence whatever the shape of the pairwise potentials. To this end, we rely on proximal gradient descent as described in Section 3.1 and formulate the proximal function Ψ in terms of the KL-divergence. This is motivated by the fact that it is more adapted to measuring the distance between probability distributions than the usual L2 norm, while being independent of how the distribution is parameterized.</p><p>We will show that this both guarantees convergence and yields a principled way to obtain a closed form damped update equation equivalent to Eq. 9.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Proximal Gradient for Mean-Field Inference</head><p>In our approach to minimizing the variational free energy of Eq. 7, we treat E as the function f of Eq. 10 and the negative entropy −H as the regularizer g. This choice stems from the fact that −H is separable, and therefore, can be minimized in parallel in Eq. 10, without using a first order approximation. Also, −H being the regularizer g means that we do not need to look at its derivatives with respect to the mean-parameters, which are not well behaved when they approach zero. We then define</p><formula xml:id="formula_12">Ψ t (q, q t ) = i l d t i,l q i,l log q i,l q t i,l = D t ⊙ KL(q||q t ) ,<label>(12)</label></formula><p>where KL is the non-negative KL-divergence, which is a natural choice for a distance between distributions. D t is a diagonal matrix with positive diagonal elements d t i,l s, which we introduce to allow for anisotropic scaling of the proximal KL-divergence term. As will be discussed below, different choices of the d t i,l s yield different variants of our algorithms. Note however that, Ψ t is a valid proximal function.</p><p>The update of Eq. 10 then becomes</p><formula xml:id="formula_13">q t+1 = argmin q∈M { q, ∇E(q t ) − H(q) + D t ⊙ KL(q||q t )} .<label>(13)</label></formula><p>This computation can be performed independently for each index i ∈ {1, . . . , N }. Furthermore, we prove in the supplementary material that it can be done in closed form and can be written as</p><formula xml:id="formula_14">q t+1 i,l ∝ exp[ η t i,l · E Q(X/Xi=l;q) log P (X|I) (14) +(1 − η t i,l ) · log q t i,l ] , where η t i,l = 1 1 + d t i,l</formula><p>. Eq 14 can be rewritten as</p><formula xml:id="formula_15">θ t+1 i,l = η t i,l · θ ⋆ i,l + (1 − η t i,l ) · θ t i,l ,<label>(15)</label></formula><p>where θ ⋆ i,l = −E Q(X/Xi;q) log P (X|I) now is a natural parameter, like those of Eq. 6. In other words, we have replaced the heuristic update rule of Eq. 9 in the space of mean parameters by a principled one in the space of natural ones. As we will see, this yields performance and convergence improvements in most cases. As for the stopping criteria, one can define one based on the value of the objective, or, in practice, run inference for a fixed number of iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Fixed Step Size</head><p>The simplest way to instantiate our algorithm is to fix all the d t i,l s of Eq. 12 to the same value d and to write</p><formula xml:id="formula_16">∀t , D t = D = dI ⇒ ∀t, i, l, η t i,l = 1 1 + d ,<label>(16)</label></formula><p>where η t i,l plays the same role as the damping factor of Eq. 9. We now show that this is guaranteed to converge when the proximal term is given enough weight.</p><p>In our mean-field settings, E(q) is a polynomial function of the mean-parameters vector q. Therefore, one can always find some positive real number L such that the gradient of E is L-Lipschitz continuous. In the supplementary material, we prove that this property implies that our proximal gradient descent scheme is guaranteed to converge for any fixed matrix D = dI such that d &gt; L.</p><p>Intuitively, when updating the value of q t to q t+1 , the magnitude of the gradient change controlled and thus the coordinate-wise optimum θ ⋆ i,l = −∇E(q t ) i,l will also be changing smoothly across iterations. As a result, L is the key value to understand oscillations. In practice, our goal is to find its smallest possible value to allow steps as large as possible while guaranteeing convergence.</p><p>In the pairwise case, the Hessian of the objective function is a constant matrix, which we call potential matrix. Therefore, the highest eigenvalue of the potential matrix is a valid Lipschitz constant and efficient methods allow to compute it for moderately sized problems.</p><p>In fact, the convergence result presented in <ref type="bibr" target="#b23">[24]</ref> is strongly related to this. Namely, assuming that the potential matrix is negative semi-definite, is equivalent to assuming that L &lt; 0 in our formulation. This directly corresponds to the concavity assumptions on the potentials in <ref type="bibr" target="#b23">[24]</ref>. Therefore, under the assumptions of <ref type="bibr" target="#b23">[24]</ref>, our algorithm leads to η = 1, corresponding to the fully-parallel update procedure. In that sense, our procedure is a generalization of the one proposed by <ref type="bibr" target="#b23">[24]</ref>.</p><p>In the non-pairwise case, the Hessian is not constant, and the calculation of the Lipschitz constant is not trivial. For each specific problem, bounds should be derived using the particular shape of the CRF at hand.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Adaptive Step Size</head><p>Note that the Hessian of the KL-proximal term is diagonal with</p><formula xml:id="formula_17">∂ 2 D t · KL(q||q t ) ∂q 2 i,l | q=q t = d t i,l q t i,l .<label>(17)</label></formula><p>Therefore, when some of the q i,l s get close to 0, the elements of the Hessian may become very large, especially when using a constant value for the d t i,l as suggested above. When that happens, the local KL-approximation remains a valid upper bound of the objective function, but not a tight enough one, which results in step sizes that are too small for fast convergence. This can be reduced by choosing a matrix D t that compensates for this. A simple way to do this would be to scale the d t i,l proportionally to max(q i,0 , . . . , q i,Li−1 ) to start compensating for diagonal terms. However, this method is still sub-optimal because it ignores the fact that all our variables lie inside the simplex M. A better alternative is to bound from below the proximal term by a quadratic function, but on M rather than on R n .</p><p>In this paper, we only apply this method to the binary case, for which we set</p><formula xml:id="formula_18">d t i,0 = d t i,1 = q t i,0 q t i,1 · d ,<label>(18)</label></formula><p>were d is an additional parameter that should be set close to L. Extending this approach to the multi-label case will be a topic for future work. In Section 3.4, we provide a different alternative to performing adaptive anisotropic updates in all settings.</p><p>Intuitively, when the current parameters are close to the borders of the simplex, the mean parameters are less sensitive to natural parameters, which, therefore, need less damping. We demonstrate in our experiments that it provides a way to choose the step size without tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Momentum</head><p>Our approach can easily be extended to incorporate techniques that are known to speed-up gradient descent and help to avoid local minima, such as the classic momentum method <ref type="bibr" target="#b27">[28]</ref> or the more recent ADAM technique <ref type="bibr" target="#b19">[20]</ref>. The momentum method involves averaging the gradients of the objective f (x) over the iterations in a momentum vector m and use it as the direction for the update instead of simply following the current gradient. To integrate it into our framework, we replace the gradient ∇E in Eq. 13 by its rolling exponentially weighted average m computed as</p><formula xml:id="formula_19">m t+1 = γ 1 m t + (1 − γ 1 )∇E(q t ) ,<label>(19)</label></formula><p>with the exponential decay parameter γ 1 ∈ [0; 1]. This substitution brings the following update rule</p><formula xml:id="formula_20">θ t+1 i,l = η · m t i,l + (1 − η) · θ t i,l .<label>(20)</label></formula><p>We will refer to this approach as OURS-MOMENTUM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">ADAM</head><p>The ADAM method <ref type="bibr" target="#b19">[20]</ref> has become very popular in deep learning. Our framework makes it easy to use for mean-field inference as well by appropriately choosing the matrix D t at each step and combining it with the momentum technique.</p><p>We define the averaged second moment vector v of the natural gradient as v t+1</p><formula xml:id="formula_21">i,l = γ 2 [θ t i,l + ∇E(q t ) i,l ] 2 + (1 − γ 2 )v t i,l ,<label>(21)</label></formula><p>where v is initialized to a strictly positive value and γ 2 ∈ [0; 1] is an exponential memory parameter for v. Then, the D t matrix is defined through each of its diagonal entries as</p><formula xml:id="formula_22">d t i,l = v t+1 i,l d + ǫ − 1 ,<label>(22)</label></formula><p>where ǫ is a fixed parameters and d controls the damping. We will refer to this method as OURS-ADAM.</p><p>Intuitively it is good at exploring parameter space thanks to a form of auto-annealing of the gradient. The natural gradient θ t + ∇E(q t ) is zero at a local minimum of the objective function <ref type="bibr" target="#b16">[17]</ref>. Therefore, close to a minimum, the proximal term D t becomes small, thus allowing more exploration of the space. On the other hand, after a long period of exploration with large natural gradients, more damping will tend to make the algorithm converge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Evaluation</head><p>In this section, we evaluate our method on a variety of inference problems and demonstrate that in most cases it yields faster convergence and better minima. All the code, including our efficient GPU mean-field inference framework, will be made publicly available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Baselines and Variants</head><p>We compare several variants of our approach to some of the baselines we introduced in the related work section. The baselines we consider are as follows:</p><p>• SWEEP. As discussed in Section 2.2.1, it involves sequential coordinate descent <ref type="bibr" target="#b4">[5]</ref> and is not always computationally tractable for large problems. • ADHOC. As discussed in Section 2.2.2, it performs parallel updates with the ad hoc damping parameter η of Eq. 9 chosen manually. • FULL-PARALLEL. As also discussed in Section 2.2.2, it relies on the inference described in <ref type="bibr" target="#b23">[24]</ref>. For example, the popular densecrf framework <ref type="bibr" target="#b22">[23]</ref> uses this approach.</p><p>We compare to these the following variants of our approach:</p><p>• OURS-FIXED. Damping occurs in the space of natural parameters instead of mean ones as described in Section 3.2. • OURS-ADAPTIVE. Adaptive and anisotropic damping in the space of natural parameters as described in Section 3.3. • OURS-MOMENTUM. Similar to OURS-ADAPTIVE, but using the momentum method instead of ordinary gradient descent, as described in Section 3.4. We use the same parameter value γ 1 = 0.95 for all datasets. • OURS-ADAM. Similar to OURS-ADAPTIVE but using the ADAM method instead of ordinary gradient as described in Section 3.5. We use the same parameters as in the original publication <ref type="bibr" target="#b19">[20]</ref>, γ 1 = 0.99, γ 2 = 0.999 and ǫ =1E-8 for all datasets.</p><p>All four methods involve a parameter η = 1 1+d , defined in Eq. 16 for OURS-FIXED, Eq. 18 for OURS-ADAPTIVE, Eq. 20 for OURS-MOMENTUM and Eq. 22 for OURS-ADAM. Additionally, in Section 4.3 and <ref type="figure">Fig. 2</ref> we demonstrate that our method is less sensitive to the choice of this parameter than its competitors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Experimental Setup</head><p>We evaluated all the methods first on a set of standardized benchmarks <ref type="bibr" target="#b13">[14]</ref>: DBN, containing 108 instances of deep belief networks (on average 920 variables), GRID, containing 21 instances of two-dimensional grids (1600 variables), and SEG, containing 100 instances of segmentation problems (230 variables), where each instance is represented as a binary pairwise random field.</p><p>We then consider three realistic Computer Vision tasks that all involve minimizing a functional of the form given in Eq. 7. We describe them below.</p><p>Characters Inpainting We consider character inpainting, formulated as a binary pairwise random field, Decision Tree Fields (DTF, <ref type="bibr" target="#b26">[27]</ref>). The dataset contains 100 test instances of occluded characters, and the goal is to restore the occluded part, as shown in the last row of <ref type="figure">Fig. 1</ref>. We use precomputed potentials provided by the authors of <ref type="bibr" target="#b26">[27]</ref>. Note, that this model consists of data-driven potentials, and includes both short and long-range interactions, which makes it particularly interesting from the optimization perspective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>People Detection</head><p>We consider detecting upright people in a multi-camera settings, using the Probabilistic Occupancy Map approach (POM, <ref type="bibr" target="#b12">[13]</ref>), that relies on a random field with high-order repulsive potentials, which models background subtraction signal given the presences of people in the environment. We evaluate it on the ISSIA <ref type="bibr" target="#b8">[9]</ref> dataset, which contains 3000 frames of a football game, captured by 6 cameras located on two sides of the field. The original work <ref type="bibr" target="#b12">[13]</ref> does not explicitly mention it, but the publicly available implementation uses the ADHOC damping method. We implement all our methods and remaining baselines directly in this code of <ref type="bibr" target="#b12">[13]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Semantic Segmentation</head><p>We consider semantic segmentation on PASCAL VOC 2012 dataset <ref type="bibr" target="#b11">[12]</ref>, which defines 20 object classes and 1 background class. We based our evaluation on DeepLab-CRF model <ref type="bibr" target="#b7">[8]</ref>, which is currently one of the best-performing methods. This model uses CNNs to obtain unary potentials, and then employs densecrf of <ref type="bibr" target="#b23">[24]</ref> with dense pairwise potentials. However, this basic CRF model does not contain any strong repulsive terms, and thus we expect densecrf's standard inference, FULL-PARALLEL, to work well. To improve performance, we additionally introduced co-occurrence potentials <ref type="bibr" target="#b31">[32]</ref>, which, as we will show, violate the conditions assumed in densecrf, but can still be successfully handled by our method. Intuitively, these co-occurrence terms put priors on the sets of classes that can appear together. We made minor modifications of densecrf to support both our inference and co-occurrence potentials.</p><p>We performed all the experiments on Intel(R) Xeon(R) CPU E5-2680 2.50GHz, and a GPU GeForce GTX TITAN X (12GB GRAM).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparative Results</head><p>In order to understand how the methods behave in practical settings, when the available computational time is limited, we evaluate all methods for several computational bud-  <ref type="bibr" target="#b13">[14]</ref>: DBN (deep belief networks), GRID (two-dimensional grids), SEG (binary segmentation). All the numbers are KL divergence (lower is better) averaged over the instances. gets. The shortest budget corresponds to the early-stopping scenario after few iterations, the longest one roughly models the time until convergence, and the middle one is around 20-30% of the longest.</p><p>Benchmarks Quantitative results are given in <ref type="table">Table 1</ref>.</p><p>Our methods systematically outperform the ADHOC damping method. The SWEEP method usually provides good performance, but is generally slow due to its sequential nature. <ref type="figure">Fig. 2</ref> shows that our methods are less sensitive to damping parameter changes than ADHOC. In <ref type="figure">Fig. 2</ref>, the vertical orange lines corresponds to the choice of the damping parameter according to d = L, which can be computed directly by the power-method. Interstingly, for the GRID dataset, which includes strong repulsive potentials, algorithms do not produce reasonable results when no damping is applied. On the other hand, for the segmentation task, SEG, all the algorithms work well even without damping, in accordance with the results of <ref type="bibr" target="#b23">[24]</ref> or Section 3.2.</p><p>Characters Inpainting Quantitative results in terms of average pixel accuracy and KL-divergence are given in Table 2 and <ref type="figure">Fig. 3 (a)</ref>. Our method, especially when used with more advanced gradient descent schemes, outperforms all the baselines. SWEEP shows relatively good performance, but does not scale as well in terms of the running time. See the bottom row of <ref type="figure">Fig. 1</ref> for an example of a result.</p><p>People Detection Quantitative results, presented in Table 3 and <ref type="figure">Fig. 3 (b)</ref>, demonstrate that our method with a fixed step size, OURS-FIXED, brings both faster convergence and better performance. Thanks to our optimization scheme, the time required to get a Multiple Object Detection Accuracy (MODA, <ref type="bibr" target="#b3">[4]</ref>) within 3% of the value at convergence is reduced by a factor of two. This can be of big practical importance for surveillance applications of the algorithm <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b1">2]</ref>, in which it is required to run in real-time. SWEEP exhibits much worse performance than our parallel method because of its greedy behavior.  <ref type="table">Table 4</ref>. Results for semantic segmentation problem <ref type="bibr" target="#b11">[12]</ref> based on DeepLab-CRF <ref type="bibr" target="#b7">[8]</ref>. For all the budgets, our method obtains better segmentation accuracy. Again, FULL-PARALLEL obtains lower KL faster, with a price of reduced performance. On the top, we provide results for the original DeepLab-CRF model without co-occurrence potentials (denoted by [o]), for which the KL divergence has therefore a different meaning and is not shown.</p><p>Semantic Segmentation Quantitative results are presented in <ref type="table">Table 4</ref> and <ref type="figure">Fig. 3 (c)</ref>. We observe that a similar oscillation issue as noted by <ref type="bibr" target="#b31">[32]</ref> starts happening when the FULL-PARALLEL method is used in conjunction with co-occurrence potentials, producing even worse results than without those. Using our convergent inference method fixes oscillations and provides an improvement of 0.5% in the average Intersection over Union measure (I/U) compared to the basic method without co-occurrence. This is a significant improvement that would be sufficient to increase the position of an algorithm by 2 or 3 places in the official ranking <ref type="bibr" target="#b11">[12]</ref>. What it represents is a big improvement in performance, as the ones shown in <ref type="figure">Fig 1,</ref> for at least 30-40 images out of total 1449. Note also, that we obtain this improvement with minimal changes in the original code. By contrast, authors <ref type="bibr" target="#b7">[8]</ref> get similar or smaller improvements by significantly augmenting the training set or by exploiting multi-scale features, which leads to additional computational burden.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion and Future Work</head><p>We have presented a principled and efficient way to do parallel mean-field inference in discrete random fields. We have demonstrated that proximal gradient descent is a powerful theoretical framework for mean-field inference, which unifies and sheds light on existing approaches. Moreover, it naturally allows to incorporate existing adaptive gradient descent techniques, such as ADAM, to mean-field methods. As shown in our experiments, it often brings dramatic improvements in performance. Additionally, we have demonstrated, that our approach is less sensitive to the choice of parameters.</p><p>Our method makes it possible to use mean-field inference with a wider range of potential functions, which was previously unachievable due to the lack of convergent optimization. Thus, there is a large amount of possible future applications of our approach, especially in the tasks where higher-order and repulsive potentials can be useful, not only in segmentation, but also in object localization.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .Figure 3 .</head><label>23</label><figDesc>Sensitivity of OURS-FIXED (red) and OURS-ADAPTIVE (dashed red) vs ADHOC (blue) to the damping parameter η = 1 1+d . We report KL-divergence (lower is better) vs the value of the parameter, both in log-space. Convergence results. (a) OURS-ADAM and OURS-MOMENTUM converge very fast to a much better minima. (b) OURS-FIXED outperforms ADHOC both in terms of speed of convergence and the value of the objective. (c) OURS-ADAM and OURS-FIXED show the best performance.The former converges a bit slower, but in the end provide slightly better minima. ADHOC for this dataset converges rather fast, but fails to find a better optima.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>Table 2. Results for characters inpainting problem<ref type="bibr" target="#b26">[27]</ref> based on DTFs. PA is the pixel accuracy for the occluded region (bigger is better). Our methods outperform the baselines by a margin of 3-5%. Since FULL-PARALLEL is not damped, it gets to low KL-divergence value quickly, however the actual solution is significantly worse.Table 3. Results for people detection task<ref type="bibr" target="#b8">[9]</ref> based on POM<ref type="bibr" target="#b12">[13]</ref>. OURS-FIXED outperforms the baselines and adaptive methods. This means that this problem does not require more sophisticated parameter exploration techniques.</figDesc><table>0.05s 

0.3s 
3s 
method 

KL 
PA 
KL 
PA 
KL 
PA 
SWEEP 

-6342.56 
54.57 
-25233.54 
58.38 
-49519.33 
62.50 

FULL-PARALLEL 

-49516.98 
60.99 
-49519.27 
62.00 
-49519.33 
62.05 

ADHOC 

-49514.27 
61.46 
-49520.09 
62.15 
-49520.20 
62.17 

OURS-FIXED 

-49505.59 
60.99 
-49520.33 
62.26 
-49521.71 
62.35 

OURS-ADAPTIVE 

-49503.43 
60.93 
-49520.14 
62.32 
-49522.49 
62.60 

OURS-MOMENTUM 

-49513.57 
63.69 
-49536.67 
65.26 
-49540.76 
65.95 

OURS-ADAM 

-49516.02 
65.36 
-49538.84 
67.03 
-49544.58 
67.12 

0.5s 
1.3s 
5s 
method 

KL 
MODA 
KL 
MODA 
KL 
MODA 
SWEEP 

1865.43 
0.630 
1795.66 
0.656 
1795.60 
0.656 

FULL-PARALLEL 

2573.79 
0.000 
2573.79 
0.000 
8500.90 
0.030 

ADHOC 

2573.79 
0.308 
1760.02 
0.781 
1753.71 
0.829 

OURS-FIXED 

1783.63 
0.626 
1754.55 
0.802 
1753.63 
0.829 

OURS-MOMENTUM 

1931.36 
0.040 
1797.19 
0.650 
1753.83 
0.826 

OURS-ADAM 

2008.52 
0.021 
1813.66 
0.501 
1754.52 
0.824 

5s 
15s 
50s 
method 

KL 
I/U 
KL 
I/U 
KL 
I/U 
FULL-PARALLEL [o] 

− 
67.18 
− 
67.70 
− 
68.00 

OURS-ADAM [o] 

− 
66.45 
− 
67.50 
− 
68.07 

FULL-PARALLEL 

-3129799 
67.21 
-3134437 
67.72 
-3133010 
68.01 

ADHOC 

-3129469 
67.19 
-3134557 
67.73 
-3136865 
68.04 

OURS-FIXED 

-3100079 
67.76 
-3135225 
68.18 
-3138206 
68.44 

OURS-MOMENTUM 

-3060405 
66.20 
-3128121 
67.39 
-3136543 
68.18 

OURS-ADAM 

-3091787 
67.08 
-3131624 
68.02 
-3138335 
68.47 

</table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Natural Gradient Works Efficiently in Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-I</forename><surname>Amari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="251" to="276" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Probability Occupancy Maps for Occluded Depth Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bagautdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Fleuret</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multi-Commodity Network Flow for Tracking Multiple People</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Benshitrit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Berclaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Fleuret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1614" to="1627" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Evaluating Multiple Object Tracking Performance: the Clear Mot Metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bernardin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EURASIP Journal on Image and Video Processing</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bishop</surname></persName>
		</author>
		<title level="m">Pattern Recognition and Machine Learning</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Fast approximate energy minimization via graph cuts. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Boykov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Veksler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zabih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1222" to="1239" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Fully-connected crfs with non-parametric pairwise potential</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Subr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Semantic image segmentation with deep convolutional nets and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference for Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A Semi-Automatic System for Ground Truth Generation of Soccer Video Sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Orazio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Leo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mosca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Spagnolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">L</forename><surname>Mazzeo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Advanced Video and Signal Based Surveillance</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="559" to="564" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Composite objective mirror descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shalev-Shwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tewari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLT</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">The PASCAL Visual Object Classes Challenge 2012 (VOC2012) Results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<ptr target="http://www.pascal-network.org/challenges/VOC/voc2012/workshop/index.html.6,8" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Multi-Camera People Tracking with a Probabilistic Occupancy Map</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Fleuret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Berclaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lengagne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="267" to="282" />
			<date type="published" when="2008-02-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Simple MAP Inference via Low-Rank Relaxations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Frostig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="3077" to="3085" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Sampling-based approaches to calculating marginal densities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">E</forename><surname>Gelfand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American statistical association</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="issue">410</biblScope>
			<biblScope unit="page" from="398" to="409" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Submodularization for binary pairwise energies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gorelick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Boykov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Veksler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Ben Ayed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Delong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1154" to="1161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Stochastic Variational Inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Paisley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2013-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A comparative study of modern inference techniques for structured discrete energy minimization problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Kappes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Hamprecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schnörr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">X</forename><surname>Kausler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kröger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lellmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="155" to="184" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Kullback-Leibler Proximal Variational Inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Baqué</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Fleuret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization. CoRR, abs/1412</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6980</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Probabilistc Graphical Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Friedman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A new look at reweighted message passing. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kolmogorov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="919" to="930" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Efficient Inference in Fully Connected CRFs with Gaussian Edge Potentials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Parameter Learning and Convergent Inference for Dense Random Fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Expectation propagation for approximate bayesian inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">P</forename><surname>Minka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventeenth conference on Uncertainty in artificial intelligence</title>
		<meeting>the Seventeenth conference on Uncertainty in artificial intelligence</meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="362" to="369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Loopy Belief Propagation for Approximate Inference: An Empirical Study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Onference on Uncertainty in Artificial Intelligence</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="467" to="475" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Decision Tree Fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bagon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sharp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kholi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Some methods of speeding up the convergence of iteration methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">T</forename><surname>Polyak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">USSR Computational Mathematics and Mathematical Physics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1" to="17" />
			<date type="published" when="1964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Application of the Mean Field Methods to MRF Optimization in Computer Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Okatani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Deguchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012-06" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Real-Time Landing Place Assessment in Man-Made Environments. Machine Vision and Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Christoudias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Entropic proximal mappings with applications to nonlinear programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Teboulle</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="670" to="690" />
		</imprint>
	</monogr>
	<note>Mathematics of Operations Research</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Filter-Based Mean-Field Inference for Random Fields with Higher-Order Terms and Product Label-Spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Warrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="290" to="307" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
		<title level="m">Graphical Models, Exponential Families, and Variational Inference. Foundations and Trends in Machine Learning</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Variational Message Passing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bishop</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="661" to="694" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jayasumana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
		<title level="m">Conditional random fields as recurrent neural networks. In International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
