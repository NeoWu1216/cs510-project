<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Track and Transfer: Watching Videos to Simulate Strong Human Supervision for Weakly-Supervised Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krishna</forename><forename type="middle">Kumar</forename><surname>Singh</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Davis</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fanyi</forename><surname>Xiao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Davis</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><forename type="middle">Jae</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Davis</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Track and Transfer: Watching Videos to Simulate Strong Human Supervision for Weakly-Supervised Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:47+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The status quo approach to training object detectors requires expensive bounding box annotations. Our framework takes a markedly different direction: we transfer tracked object boxes from weakly-labeled videos to weakly-labeled images to automatically generate pseudo ground-truth boxes, which replace manually annotated bounding boxes. We first mine discriminative regions in the weakly-labeled image collection that frequently/rarely appear in the positive/negative images. We then match those regions to videos and retrieve the corresponding tracked object boxes. Finally, we design a hough transform algorithm to vote for the best box to serve as the pseudo GT for each image, and use them to train an object detector. Together, these lead to state-of-the-art weakly-supervised detection results on the PASCAL 2007 and 2010 datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Object detection is a fundamental problem in computer vision. While tremendous advances have been made in recent years, existing state-of-the-art methods <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b10">11]</ref> are trained in a strongly-supervised fashion, in which the system learns an object category's appearance properties and precise localization information from images annotated with bounding boxes. However, such carefully labeled exemplars are expensive to obtain in the large numbers that are needed to fully represent a category's variability, and methods trained in this manner can suffer from unintentional biases or errors imparted by annotators that hinder the system's ability to generalize to new, unseen data <ref type="bibr" target="#b34">[35]</ref>.</p><p>To address these issues, researchers have proposed to train object detectors with relatively inexpensive weak supervision, in which each training image is only weaklylabeled with an image-level tag (e.g., "car", "no car") that states an object's presence/absence but not its location <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b2">3]</ref>. These methods typically mine discriminative visual patterns in the training data that frequently occur in the images that contain the object and rarely in the images that do not. However, due to scene clutter, intra-class appearance variation, and occlusion, the discriminative patterns often do not tightly fit the object-of-Tracked objects in weakly-labeled videos tagged with "car"</p><p>Weakly-labeled training images tagged with "car" For each discriminative region, we find its best matching region across all videos, and retrieve its overlapping tracked object box (yellow dotted box) back to the image. The retrieved boxes are used as pseudo ground-truth to train an object detector. Our approach improves object localization by expanding the initial visual region beyond a small object part (bottom-left) or removing the surrounding context (bottomright). In practice, we combine the retrieved boxes from multiple visual regions in an image to produce its best box.</p><p>interest; they either correspond to a small part of the object such as a car's wheel instead of the entire car, or include the surrounding context such as a car with portions of the surrounding road ( <ref type="figure" target="#fig_0">Fig. 1 bottom, green boxes)</ref>. Consequently, the detector that is trained using these patterns performs substantially worse than strongly-supervised algorithms. Main idea. So, how can we create accurate object detectors that do not require expensive bounding box annotations? Our key idea is to use motion cues from videos as a substitute for strong human supervision. Given a weaklylabeled image collection and videos retrieved using the same weak-label (e.g., "car"), we first automatically track and localize candidate objects in the videos, and then transfer their relevant tracked object boxes to the images. We transfer the object boxes by mining discriminative visual regions in the image collection, and then matching them to regions in the videos. See <ref type="figure" target="#fig_0">Fig. 1</ref>.</p><p>Since temporal contiguity and motion signals are lever-aged to localize and track the objects in video, their transferred boxes can provide precise object localizations in the weakly-labeled images. Specifically, they can expand the initial discovered region to provide a fuller coverage of the object, or decrease the spatial extent of the initial discovered region to remove the surrounding context ( <ref type="figure" target="#fig_0">Fig. 1</ref> bottom, yellow boxes). We then use the transferred boxes to generate pseudo ground-truth bounding boxes on the weaklylabeled images to train an object detector, replacing standard human-annotated bounding boxes. To account for noise in the discovered discriminative visual regions, video tracking, and image-to-video matches, we retrieve a large set of object boxes and combine them with a hough transform algorithm to produce the best boxes. What is the advantage of transferring object boxes to images instead of directly learning from videos? In general, images provide more diverse intra-category appearance information than videos, especially given the same amount of data (e.g., a 1000-frame video with a single object instance vs. 1000 images with âˆ¼1000 different object instances), and are often of higher quality since frames from real-world (e.g., YouTube) videos typically suffer from motion blur and compression artifacts. Importantly, in this way, our framework opens up the possibility to leverage the huge static imagery available online, much of which is already weakly-labeled.</p><p>Contributions. In contrast to existing stronglysupervised object detection systems that require expensive bounding box annotations, or weakly-supervised systems that rely solely on appearance-based grouping cues within the image dataset, we instead transfer tracked object boxes from videos to images to serve as pseudo ground-truth to train an object detector. This eliminates the need for expensive bounding box annotations, and compared to existing weakly-supervised algorithms, our approach provides more complete and tight localizations of the discovered objects in the training data. Using videos from the YouTube-Objects dataset <ref type="bibr" target="#b27">[28]</ref>, we demonstrate that this leads to state-of-the-art weakly-supervised object detection results on the PASCAL VOC 2007 and 2010 datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Weakly-supervised object detection. While recent stateof-the-art strongly-supervised methods <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b10">11]</ref> using deep convolutional neural networks (CNN) <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b16">17]</ref> have shown great object detection accuracy, they require thousands of expensive bounding-box annotated images.</p><p>To alleviate expensive annotation costs, weaklysupervised methods <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b2">3]</ref> train models on images labeled only with object presence/absence labels, without any location information of the object. Early efforts <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b9">10]</ref> focused on simple datasets with a single prominent object in each image (e.g., Caltech-101). Since then, a number of methods <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b24">25]</ref> learn detectors on more realistic and challenging datasets (e.g., PASCAL VOC <ref type="bibr" target="#b26">[27]</ref>). The main idea is to identify discriminative regions that frequently appear in positive images and rarely in negative ones. However, their central weakness is that due to large intra-category appearance variations, occlusion, and background clutter, they often mislocalize the objects in the training images, which results in sub-optimal detectors. We address this challenge by matching the discriminative regions to videos to retrieve automatically-tracked object boxes back to the images. This results in better localization on the weakly-labeled training set, which leads to more accurate object detectors. Learning with videos. Video offers something that static images cannot: it provides motion information, a strong cue for grouping objects (the "law of common fate" in Gestalt psychology). Existing methods learn part-based animal models <ref type="bibr" target="#b28">[29]</ref>, learn detectors from images while using video patches for regularization <ref type="bibr" target="#b19">[20]</ref>, or augment training data from videos for single-image action recognition <ref type="bibr" target="#b1">[2]</ref>. While some work consider learning object category models directly from (noisy) internet videos <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b22">23]</ref>, we are exploring a rather different problem: we use video data to simulate human annotations, but ultimately use image data to train our models. Critically, this allows our framework to potentially take advantage of the huge static image data available on the Web, which existing video-only learning methods cannot.</p><p>Finally, recent work uses videos for semi-supervised object detection with bounding box annotations as initialization <ref type="bibr" target="#b20">[21]</ref>, or trains a CNN for feature learning using tracking as supervision and fine-tuning the learned representation with bounding box annotations for detection <ref type="bibr" target="#b37">[38]</ref>. In contrast, we do not require any bounding box annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach</head><p>We are given a weakly-labeled image collection S I ={I 1 ,...,I N }, in which images that contain the objectof-interest (e.g., "car") are labeled as positive and the remaining images are labeled as negative. We are also given a weakly-labeled video collection S V ={V 1 ,...,V M } whose videos contain the positive object-of-interest, but where and when in each video it appears is unknown.</p><p>There are three main steps to our approach: (1) identifying discriminative visual regions in S I that are likely to contain the object-of-interest; (2) matching the discriminative regions to tracked objects in videos in S V and retrieving the tracked objects' boxes back to the images in S I ; and (3) training a detector using the images in S I with the retrieved object boxes as supervision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Mining discriminative positive visual regions</head><p>We first mine discriminative visual regions in the image collection S I that frequently appear in the positive images and rarely in the negative ones; these regions will likely correspond to the object-of-interest or a part of it. For this, we follow a similar approach to <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34]</ref>. For each image in S I , we generate âˆ¼2000 object proposals (rectangular regions) using selective search <ref type="bibr" target="#b35">[36]</ref>, and describe each proposal with a pool5 activation feature using AlexNet <ref type="bibr" target="#b16">[17]</ref> pre-trained for ImageNet classification. For each region, we find its best matching (nearest neighbor) region in each image in S I (regardless of image label) using cosine similarity. Each region and its k closest nearest neighbors form a cluster. We then rank the clusters in descending order of the number of cluster instances that are from the positive images. Since we create clusters for every region in every image, many will be redundant. We therefore greedily remove near-duplicate clusters that contain many near-identical regions to any higher-ranked cluster, as measured by spatial overlap of more than 25% IOU between 10% of their cluster members. Finally, for each remaining cluster, we discard any negative regions.</p><p>Let P be the set of all positive regions in the top-C ranked clusters. While P contains many diverse and discriminative regions of the object-of-interest (see <ref type="figure" target="#fig_1">Fig. 2</ref>), most of the regions will not tightly localize the object for three main reasons: (1) the most discriminative regions usually correspond to object-parts, which tend to have less appearance variation than the full-object (e.g., face vs. fullbody of a cat), (2) co-occurring "background" objects are often included in the region (e.g., airplane with sky), and (3) most of the initial object proposals are noisy and do not tightly fit any object to begin with. Thus, the regions in P will be sub-optimal for training an object detector, since they are not well-localized; this is the central weakness of all existing weakly-supervised methods. We next explain how to use videos labeled with the same weak-label (e.g., "car") to improve the localization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Transferring tracked object boxes</head><p>For now, assume that we have a (noisy) object track in each video in S V , which fits a bounding box around the positive object in each frame that it appears. In Sec. 3.4,we explain how to obtain these tracks.</p><p>For each positive image region in P, we search for its n best matching video regions across all videos in S V and return their corresponding tracked object boxes to improve the localization of the object in its image. There is an important detail we must address to make this practical: matching with fc7 features (of AlexNet <ref type="bibr" target="#b16">[17]</ref>) can be prohibitively expensive, since each candidate video region (e.g., selective search proposal) would need to be warped to 227x227 and propagated through the deep network, and there can be âˆ¼2000 such candidate regions in every frame, and millions of frames. Instead, we perform matching with conv5 features, which allows us to forward-propagate an entire video frame just once through the network since convolutional layers do not require fixed-size inputs. To compute the conv5 feature maps, we use deep pyramid <ref type="bibr" target="#b12">[13]</ref>, which creates an image pyramid with 7 levels (where the scale factor between levels is 2 âˆ’1/2 ) and computes a conv5 feature map for each level (for the 1st level, the input frame is resized such that its largest dimension is 1713 pixels). We then match each positive image region to each frame in each video densely across location and scale in a slidingwindow fashion in conv5 feature space, using cosine similarity. Note that this restricts matching between regions with similar aspect ratios, which can also help reduce false positive matches.</p><p>Given a positive image region's n best matching video regions, we return each of their frame's tracked object bounding box (if it has any spatial overlap with the matched video region) back to the positive region's image, while preserving relative translation and scale differences. Specifi-  Denote a positive image region as r, its matched video region as v, and the corresponding overlapping tracked region as t. Then, the returned bounding box r â€² is: <ref type="figure" target="#fig_3">Fig. 4</ref>. We repeat this for all n best matching video regions, and for each positive region in P.</p><formula xml:id="formula_0">(a) (b) (c) (d) (a) (b) (c) (d)</formula><formula xml:id="formula_1">r â€² = r +(t âˆ’ v). See</formula><p>Each positively-labeled image in S I (that has at least one positive region) now has a set of retrieved bounding boxes, up to n from each positive region in the image. Some will tightly fit the object-of-interest, while others will be noisy due to incorrect matches/tracks. We thus use the hough transform to vote for the best box in each image. Specifically, we create a 4-dimensional hough space in which each box casts a vote for its [x min ,y min ,x max ,y max ] coordinates. We select high density regions in the continuous hough space with mean-shift clustering <ref type="bibr" target="#b4">[5]</ref>, which helps the voting be robust to noise and quantization errors <ref type="bibr" target="#b18">[19]</ref>. The total vote for box coordinate l is a weighted sum of the votes in its spatial vicinity:</p><formula xml:id="formula_2">vote(l)= i vote(r â€² i ) Â· K l âˆ’ r â€² i b ,<label>(1)</label></formula><p>where the kernel K is a radially symmetric, non-negative function centered at zero and integrating to one, b is the mean-shift kernel bandwidth, i indexes over the positive regions in the image, and vote(r â€² i )=1, âˆ€i. We select l with the highest vote as the final box for the image. If the highest vote is less than a threshold Î¸ =20, then there is not enough evidence to trust the box so we discard it. See <ref type="figure" target="#fig_2">Fig. 3 (c-d)</ref> for example distributions of the transferred bounding boxes and final selected bounding box. We repeat this hough voting process for each positively-labeled image in S I .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Training an object detector</head><p>We can now treat the final selected boxes as pseudo ground-truth (GT)-as a substitute for manually annotated boxes-to train an object detector, with any algorithm developed for the strongly-supervised setting. We use the state-of-the-art Regions with CNN (R-CNN) system <ref type="bibr" target="#b11">[12]</ref>. Briefly, R-CNN computes CNN features over selective search <ref type="bibr" target="#b35">[36]</ref> proposals, trains a one-vs-all linear SVM (with GT boxes as positives and proposals that have less than 0.3 intersection-over-union overlap (IOU) with any GT box as negatives) to classify each region, and then performs bounding box regression to refine the object's detected location.</p><p>There are three considerations to make when adapting R-CNN to our work: (1) each positively-labeled image has at most one pseudo GT box, which means that negative regions from the same image must be carefully selected since the image could have multiple positive instances (e.g., multiple cars in a street scene) but our pseudo GT may only be covering one of them; (2) some positively-labeled images may have no pseudo GT box (i.e., if there were not enough votes), which means that we would not be making full use of all the positive images; and (3) some pseudo GT boxes may be inaccurate even after hough voting due to noise in the matching or tracking. These can all lead to a sub-optimal detector if not handled carefully.</p><p>To address the first issue, we train an R-CNN model with the pseudo GT boxes as positives, and any selective search proposal that has an IOU less than 0.3 and greater than 0.1 with a pseudo GT box as negatives. In this way, we minimize the chance of mistakenly labeling a different positive instance in the image as negative, but at the same time, select mis-localized regions (that have some overlap with a pseudo GT) as hard-negatives. We treat all selective search proposals in any negatively-labeled image in S I as negative.</p><p>To address the second and third issues, we perform a latent SVM (LSVM) update <ref type="bibr" target="#b8">[9]</ref> given the initial R-CNN model from above to update the pseudo GT boxes. For images that do not have a pseudo GT box, we fire the R-CNN model and take its highest-scoring detection in the image as the pseudo GT box. For images that already have a pseudo GT box, we take the highest-scoring detection that has at least 0.5 IOU with it, which prevents the updated box from changing too much from the initial box. We then re-train the R-CNN model with the updated pseudo GT boxes.</p><p>Finally, we also fine-tune the R-CNN model to update not only the classifier but also the features using our pseudo GT boxes, which results in an even greater boost in detection accuracy (as shown in Sec. 4.3). Fine-tuning CNN features has not previously been demonstrated in the weaklysupervised detection setting, likely due to existing methods producing too many false detections in the training data. Our discovered pseudo GT boxes are often quite accurate, making our approach amenable for fine-tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Unsupervised video object tracking</head><p>Our framework requires an accurate unsupervised video object tracker, since its tracked object boxes will be used to generate the pseudo GT boxes on the weakly-labeled images. For this, we use the unsupervised tracking method of <ref type="bibr" target="#b39">[40]</ref>, which creates a diverse and representative set of spatial-temporal object proposals in an unannotated video. Each spatial-temporal proposal is a sequence of boxes fitting an object over multiple frames in time. <ref type="bibr" target="#b0">1</ref> Briefly, the method begins by leveraging appearance and motion objectness to score a set of static object proposals in each frame, and then groups high-scoring proposals across frames that are similar in appearance and frequently appear throughout the video. Each group is then ranked according to the average objectness score of its instances. For each group, the method trains a discriminative tracking model with the group's instances as positives and all nonoverlapping regions in their frames as negatives, and tracks the object in each instance's adjacent frames. The model is then retrained with the newly tracked instances as positives, and the process iterates until all frames are covered. The output is a set of ranked spatio-temporal tracks that fit a box around the objects in each frame that they appear. The method also has a pixel-segmentation refinement step, but we skip it for speed. See <ref type="bibr" target="#b39">[40]</ref> for details.</p><p>For each video in S V , we take the 9 highest-ranked tracks generated by <ref type="bibr" target="#b39">[40]</ref>. Not all of these tracks will correspond to the object-of-interest. We therefore use our mined positive regions in P to try to select the relevant one in each frame. Specifically, given frame f , we match each positive region r i to it in a sliding-window fashion in conv5 feature space (as in Sec. 3.2), and record its best matching box v f i in the frame. We score a tracked box t f j in frame f as:</p><formula xml:id="formula_3">score(t f j )= i IOU(v f i , t f j ) Ã— sim(r i , v f i ),</formula><p>where i indexes the positive regions in P, j is the index of a tracked video box, and sim is cosine similarity. We choose the tracked box with the highest score, and discard the rest. Our selection criterion favors choosing a box in each video frame that has high-overlap with many good matches from discriminative positive regions. See <ref type="figure" target="#fig_4">Fig. 5</ref> for examples. The selected video boxes are provided as input to the videomatching module described in Sec. 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We analyze: (1) localization accuracy of our discovered pseudo GT boxes on the weakly-labeled training images, (2) detection performance of our trained models on the test images, (3) ablation studies analyzing the different components of our approach, and (4) our selection criterion for choosing the relevant object track in each video frame. Datasets. We use videos from YouTube-Objects <ref type="bibr" target="#b27">[28]</ref> and images from PASCAL VOC 2007 and 2010. We evaluate on their 10 shared classes (treating each as a positive in turn): aeroplane, bird, boat, car, cat, cow, dog, horse, motorbike, train. YouTube-Objects contains 9-24 videos per class; each video is 30-180 sec; 570K total frames. We only use each video's weak category-label (i.e., we do not know in which frames or regions the object appears). Each video is divided into shots with similar color <ref type="bibr" target="#b27">[28]</ref>; we generate object tracks for each shot using <ref type="bibr" target="#b39">[40]</ref>. VOC 2007 is used by all existing state-of-the-art weakly-supervised detection algorithms; VOC 2010 is used by <ref type="bibr" target="#b3">[4]</ref> the train+val (5011 imgs) and train set (4998 imgs), respectively, to discover the pseudo GT boxes. For both datasets, we report detection results on the test set using average precision. In contrast to existing weakly-supervised methods (except <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b33">34]</ref>), we do not discard instances labeled as pose, difficult, truncated, and restrict the supervision to the image-level object presence/absence labels to mimic a more realistic (difficult) weakly-supervised scenario. Implementation details. For mining discriminative regions, we take k=(# positive images)/2 nearest neighbors, and top C=200 clusters. When matching a positive region to video, we adjust its box to have roughly 48 conv5 cells using a sizing heuristic <ref type="bibr" target="#b21">[22]</ref>, and compute matches in every 8th frame for speed. For the mean-shift bandwidth b,w e train separate detection models for b=[100, 250, 500, 1000] and validate detection accuracy over our automatically selected object tracks on YouTube-Objects (i.e., we treat them as noisy GT); even though the discovered tracks can be noisy, we find they produce sufficiently good results for cross-validation. To compute deep features, we use AlexNet pre-trained on ILSVRC 2012 classification, using Caffe <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b15">16]</ref>. We do not use the R-CNN network finetuned on PASCAL data <ref type="bibr" target="#b11">[12]</ref>.</p><p>To fine-tune our detector, we take our discovered pseudo GT boxes over all 10 categories to fine-tune the CNN (AlexNet pre-trained on ILSVRC2012 classification) by replacing its 1000-way classification layer with a randomlyinitialized 11-way classification layer (10 categories plus background). We treat all selective search proposals with 0.6â‰¥IOU with a pseudo GT box as positives for that box's category, and all proposals with 0.1â‰¤IOUâ‰¤0.3 with a pseudo GT box as negatives. All proposals from images not belonging to any of the 10 categories are also treated as negatives. We start SGD at a learning rate of 0.001 and decrease by Ã— 1 10 after 20,000 iterations. In each SGD iteration, 32 positives (over all classes) and 96 negatives are uniformly sampled to construct a mini-batch. We perform 40,000 SGD iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Pseudo ground-truth localization accuracy</head><p>We first analyze the localization accuracy of our discovered pseudo GT boxes on the VOC 2007 train+val dataset. We use the correct localization (CorLoc) measure <ref type="bibr" target="#b6">[7]</ref>, which is the fraction of positive training images in which the predicted object box has an intersection-overunion overlap (IOU) greater than 50% with any groundtruth box. As mentioned in <ref type="bibr" target="#b3">[4]</ref>, CorLoc is not consistently measured across previous studies, due to changes in the training sets (for example, we do not exclude the images annotated as pose, difficult, truncated). Thus, we only use it to analyze our own pseudo GT boxes, and use detection accuracy to compare against the state-of-the-art. <ref type="table">Table 1</ref> shows the results. Our initial pseudo GT boxes produce an average CorLoc score of 40.9% across all categories (first row). However, we initially miss discovering a pseudo GT box in 12% of the images, which pulls down the average. (Recall we only keep the most confident box in each image that has at least Î¸ =2 0votes.) If we only consider the images in which a pseudo GT is initially found, then our average increases to 51.1% (second row). By detecting the missed pseudo GT boxes and updating the existing ones using the R-CNN model trained with the initial pseudo GT boxes (via an LSVM update), our final CorLoc average improves to 51.7% (third row). For the boat category, our low performance is due to boats often occurring with water; since water seldom appears in other categories, many water regions are mistakenly found to be discriminative, which leads to inaccurate localizations of the boat. (See supp. material for a further detailed breakdown of the error cases per class.) For the remaining categories, our pseudo GT boxes localize the objects well, and we will see in Sec. 4.3 that they lead to robust object detectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Pseudo ground-truth visualization</head><p>We next visualize our discovered pseudo GT on the VOC 2007 train+val set. In each image pair in <ref type="figure">Fig. 6</ref>, we display a heatmap of the transferred video object boxes and the final selected pseudo GT box. Our method accurately localizes the object-of-interest in many images, even in difficult cases where the object is in an atypical pose (1st dog), partiallyoccluded (2nd car), or in a highly-cluttered scene (2nd cat). The last column shows some failure cases. The most prominent failure case is when there are multiple instances of the same object category that are spatially close to each other. This is due to a sub-optimal mean-shift bandwidth parameter b, which is used in the voting of the pseudo GT box. Although we automatically select b via cross-validation on the video tracks (see implementation details), it is fixed percategory. Using an adaptive bandwidth <ref type="bibr" target="#b5">[6]</ref> to automatically find an optimal value per-image may help to alleviate such errors. Importantly, these errors occur in only a few images. See the supp. material for results on all images.</p><p>Overall, the qualitative results demonstrate that by transferring object boxes from automatically tracked objects in video, we can accurately discover the objects' full spatial extent in the weakly-labeled image collection. <ref type="figure">Figure 6</ref>. Qualitative results on the VOC 2007 train+val set. In each image pair, the first image shows a heatmap of the transferred video object boxes and the second image shows the final selected pseudo ground-truth box. Our approach accurately discovers the spatial extent of the object-of-interest in most of the images. The last column shows mis-localized examples. Our approach can fail when there are multiple instances of the same object category in the image (e.g., aeroplane, dog, horse, train) or when the object's appearance is very different from that found in videos (e.g., car). Best viewed on pdf.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Weakly-supervised detection accuracy</head><p>We next compute detection accuracy using the R-CNN model trained using our pseudo GT boxes. We compare with state-of-the-art weakly-supervised detection methods <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b3">4</ref>] that use the same AlexNet CNN features pre-trained on ILSVRC 2012. Note that our approach and the previous methods all use the same PASCAL VOC training images to train the detectors. Our use of videos is only to get better pseudo GT boxes on the training images. <ref type="table" target="#tab_1">Tables 2 and 3 show results on the VOC 2007 and 2010</ref> test sets, respectively. Our approach produces the best results with a mAP of 41.9% and 40.1%, respectively. The baselines all share the same high-level idea of mining discriminative patterns that frequently/rarely appear in the pos-itive/negative images. In particular, the detection results produced by <ref type="bibr" target="#b32">[33]</ref> is similar to what we would get if we were to train a detector directly on our initially-mined discriminative positive regions. Since those regions often correspond to an object-part (e.g., car wheel) or include surrounding context (e.g., car with road) (recall <ref type="figure" target="#fig_1">Fig. 2</ref>), these methods have difficulty producing good localizations on the training data, which in turn degrades detection performance. While <ref type="bibr" target="#b33">[34]</ref> tries to combine pairs of discriminative regions to provide better spatial coverage of the object, it is still limited by the mis-localization error of each individual region. We instead transfer automatically tracked object boxes from weakly-labeled videos to images, which produces more accurate localizations on the training data and leads to higher detection performance. Our low detection accuracy on cow   can be explained by the poor video tracks produced by <ref type="bibr" target="#b39">[40]</ref> (see supp. material), which confirms the need for good object tracks. Overall, our results suggest a scalable application for object detection, since we can greatly reduce human annotation costs and still obtain reliable detection models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation studies</head><p>In this section, we conduct ablation studies to tease apart the contribution of each component of our algorithm. <ref type="table" target="#tab_3">Table 4</ref> shows the results. The first and second rows show mAP detection accuracy produced by the R-CNN models trained using the initial and updated (via LSVM update) pseudo GT boxes, respectively. The initial R-CNN model produces 34.4% mAP. Retraining the model with the updated pseudo GT boxes leads to 36.9% mAP, which shows that the extra positive instances and corrected instances are helpful. The third row shows bounding box regression results, which further boosts performance to 39.7% mAP. This confirms that our pseudo GT boxes are well-localized, since the trained bounding box regressor <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b11">12]</ref> is able to adjust the initial detections to better localize the object.</p><p>The last row shows fine-tuning results. Training an R-CNN model with our fine-tuned features improves results on all 10 categories to 41.9% mAP for VOC 2007. The improvement is not as significant as in the fully-supervised case, which resulted in a âˆ¼9% point increase for VOC 2007 (see <ref type="table" target="#tab_1">Table 2</ref> in <ref type="bibr" target="#b11">[12]</ref>). Since our pseudo GT boxes are not perfect, any noise seems to have a more prominent effect than in the fully-supervised case, which has perfect GT boxes. Still, this result confirms our discovered pseudo GT boxes are quite accurate, making our approach amenable for fine-tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Video track selection accuracy</head><p>Finally, we evaluate our selection criterion in choosing the relevant object box among the 9 tracks produced by the unsupervised video tracking algorithm <ref type="bibr" target="#b39">[40]</ref>. For this, we compute the IOU between the tracked object boxes and the ground-truth boxes on the YouTube-Objects dataset <ref type="bibr" target="#b27">[28]</ref>. Our automatically selected tracks produce a mean IOU of 45.1 over all 10 categories (see the supp. material for percategory results). While this is lower than the upper-bound mean IOU of 61.9 (i.e., the max IOU among the 9 proposals in each frame) they are sufficiently accurate to produce high-quality pseudo GT boxes. Furthermore, since we selectively retrieve a video object box only if it is overlapping with one of the top n =20matching video regions of a discriminative positive region, and then further aggregate those transferred boxes through hough voting, we can effectively filter out most of the noisy transferred tracks (as was shown in <ref type="figure">Fig. 6</ref>). Overall, we find that <ref type="bibr" target="#b39">[40]</ref> produces sufficiently good boxes, and our selection criterion is in many cases able to choose the relevant one. These lead to accurate pseudo GT boxes on the weakly-labeled images.</p><p>Conclusions. We introduced a novel weakly-supervised object detection framework that tracks and transfers object boxes from weakly-labeled videos to images to simulate strong human supervision. We demonstrated state-of-theart-results on PASCAL 2007 and 2010 datasets for the 10 categories of the YouTube-Objects dataset <ref type="bibr" target="#b27">[28]</ref>.</p><p>Our framework assumes that we have a way to track the object-of-interest in videos, so that we can delineate its box and transfer it to images. This is easier if the object is able to move on its own, but could also work for static objects, as long as the camera is moving. We plan to investigate this in the future. Finally, we intentionally trained our detectors using only the weakly-labeled images, in order to make our results comparable to previous weakly-supervised methods. It would be interesting to explore combining the video tracks with our pseudo GT image boxes for training the object detectors.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Main idea. (top) Automatically tracked objects (yellow and blue boxes) in weakly-labeled videos without any human initialization.( bottom) Discriminative visual regions (green boxes) mined in weakly-labeled training images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Example positive regions in the top-4 automatically mined discriminative clusters for aeroplane, bird, boat, car, cat, cow, dog, horse, motorbike, and train. While the discovered regions are relevant to the positively-labeled object category, most of them do not localize the object well, capturing only an object-part (e.g., cat, cluster 1) or including the surrounding context (e.g., aeroplane, cluster 2).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>(a) Weakly-labeled positive image for aeroplane, bird, boat, car, cat, cow, dog, horse, motorbike, and train. (b) Heatmap showing the distribution of the initial discriminative positive regions found in the image. (c) Heatmap showing the distribution of the transferred video object boxes in the image. (d) Our automatically discovered pseudo ground-truth box. Notice how the initial discriminative regions focus more on object-parts, whereas the transferred boxes focus more on the full object. This leads to better localization of the object in the weakly-labeled positive image. Best viewed on pdf. Results for all images can be found in the supplementary material.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>We match a positive image region (a) to all video frames in a sliding-window fashion, and for the best matching vido region (green box) (b), we retrieve its overlapping tracked object box (yellow dotted box) back to the image (c). cally, we can parameterize any region with its top-left and bottom-right coordinate values: [x min ,y min ,x max ,y max ].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Examples showing the spatio-temporal boxes generated with<ref type="bibr" target="#b39">[40]</ref> (blue), and our automatically selected box (yellow).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>. For VOC 2007 and 2010, we useTable 1. Localization accuracy in terms of CorLoc on the VOC 2007 train+val set. We evaluate our initial and updated pseduo GT boxes. The final boxes (third row) provide very good localizations in the training data, which leads to accurate training of object detectors.</figDesc><table>VOC 2007 train+val 

aero 
bird 
boat 
car 
cat 
cow 
dog 
horse mbike train 
mean CorLoc 
Initial pseudo GT (with all images) 
48.8 
33.9 
13.3 57.3 
46.5 
32.2 
44.4 
40.8 
48.2 
43.7 
40.9 
Initial pseudo GT (excluding missed images) 58.8 
49.6 
17.7 64.7 
60.4 
44.8 
52.8 
55.3 
54.3 
53.0 
51.1 
Updated pseudo GT (with all images) 
58.8 
49.6 
15.4 64.9 
59.0 
43.2 
51.2 
57.5 
63.1 
54.4 
51.7 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 2 .</head><label>2</label><figDesc>Detection average precision on the VOC 2007 test set. We compare our approach to state-ofthe-art weakly-supervised methods.</figDesc><table>VOC 2007 test 
aero 
bird 
boat 
car 
cat 
cow 
dog 
horse mbike train 
mAP 
Song et al., 2014 [33] 
27.6 
19.7 
9.1 
39.1 
33.6 
20.9 
27.7 
29.4 
39.2 
35.6 
28.2 
Song et al., 2014 [34] 
36.3 
23.3 
12.3 46.6 
25.4 
23.5 
23.5 
27.9 
40.9 
37.7 
29.7 
Bilen et al., 2014 [1] 
42.2 
23.1 
9.2 
45.1 
24.9 
24.0 
18.6 
31.6 
43.6 
35.9 
29.8 
Wang et al., 2014 [37] 48.9 
26.1 
11.3 40.9 
34.7 34.7 34.4 
35.4 
52.7 
34.8 
35.4 
Cinbis et al., 2015 [4] 
39.3 
28.8 20.4 47.9 
22.1 
33.5 29.2 
38.5 
47.9 
41.0 
34.9 
Ours w/o fine-tune 
50.7 
36.6 
13.4 53.1 
50.8 
21.6 
37.6 
44.0 
46.1 
43.4 
39.7 
Ours 
53.9 
37.7 13.7 56.6 
51.3 24.0 38.5 
47.9 
47.0 
48.4 
41.9 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 .</head><label>3</label><figDesc>Detection average precision on the VOC 2010 test set.</figDesc><table>VOC 2010 test 
aero 
bird 
boat 
car 
cat 
cow 
dog 
horse mbike train 
mAP 
Cinbis et al., 2015 [4] 44.6 
25.5 14.1 36.3 
23.2 
26.1 29.2 
36.0 
54.3 
31.2 
32.1 
Ours w/o fine-tune 
50.9 
35.8 
8.1 
40.5 
45.9 
26.0 
36.4 
39.0 
45.7 
39.4 
36.8 
Ours 
53.5 
37.5 
8.0 
44.2 
49.4 
33.7 43.8 
42.5 
47.6 
40.6 
40.1 

VOC 2007 test 
aero 
bird 
boat 
car 
cat 
cow 
dog 
horse mbike train 
mAP 
Inital pseudo GT 
43.4 
30.5 
11.9 50.2 
39.6 
16.7 
31.6 
36.7 
42.2 
40.7 
34.4 
Updated pseudo GT 
48.0 
34.2 
12.2 51.3 
43 
21.9 33.4 
39.1 
43.8 
42.2 
36.9 
Updated pseudo GT + bbox-reg 
50.7 
36.6 
13.4 53.1 
50.8 
21.6 
37.6 
44.0 
46.1 
43.4 
39.7 
Updated pseudo GT + fine-tune + bbox-reg 53.9 
37.7 
13.7 56.6 
51.3 
24.0 
38.5 
47.9 
47.0 
48.4 
41.9 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 4 .</head><label>4</label><figDesc>Detection average precision on the VOC 2007 test set to evaluate the different components of our approach. See text for details.</figDesc><table></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We also tried the video segmentation method of<ref type="bibr" target="#b23">[24]</ref>. However, it fails to produce good segmentations when an object is not moving. Ultimately, transferring its object boxes resulted in a slightly worse detector.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. This work was supported in part by an Amazon Web Services Education Research Grant and GPUs donated by NVIDIA.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Weakly supervised object detection with posterior regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pedersoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Watching Unlabeled Video Helps Learn New Human Actions from Very Few Labeled Snapshots</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multi-fold MIL Training for Weakly Supervised Object Localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cinbis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Weakly supervised object localization with multi-fold multiple instance learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cinbis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.00949</idno>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Mean shift: A robust approach toward feature space analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Comaniciu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Meer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<publisher>PAMI</publisher>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="603" to="619" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The variable bandwidth mean shift and data-driven scale selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Comaniciu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Meer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Localizing objects while learning their appearance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Alexe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">What Makes Paris Look like Paris?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<idno>2012. 3</idno>
		<imprint>
			<publisher>SIGGRAPH</publisher>
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Object Detection with Discriminatively Trained Part Based Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>PAMI</publisher>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1627" to="1645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Object Class Recognition by Unsupervised Scale-Invariant Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deformable part models are convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Weakly Supervised Learning of Object Segmentations from Web-Scale Video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hartmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Grundmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kwatra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Madani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Essa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rehg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Caffe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<title level="m">Convolutional architecture for fast feature embedding</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Imagenet Classification with Deep Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jackel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Computation</title>
		<imprint>
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Robust object detection with interleaved categorization and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Leonardis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">IJCV</biblScope>
			<biblScope unit="page" from="259" to="289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Improving Classifiers with Unlabeled Weakly-Related Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Leistner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Godec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sakari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Werlberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Computational baby learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.2861</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Ensemble of exemplar-svms for object detection and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Malisiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Watch and learn: Semi-supervised learning of object detectors from videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Segmentation of moving objects by long term video analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ochs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>PAMI</publisher>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1187" to="1200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Is object localization for free? Weakly-supervised learning with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Scene Recognition and Weakly Supervised Object Localization with Deformable Part-Based Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<title level="m">The PASCAL Visual Object Classes Challenge Results</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning Object Class Detectors from Weakly Annotated Video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Prest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Leistner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Civera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Building Models of Animals from Video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Barnard</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>PAMI</publisher>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="1319" to="1334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Overfeat: Integrated recognition, localization and detection using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Unsupervised Discovery of Mid-level Discriminative Patches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">In Defence of Negative Mining for Annotating Weakly Labelled Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Siva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">On Learning to Localize Objects with Minimal Supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">O</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Weaklysupervised Discovery of Visual Pattern Configurations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">O</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Unbiased Look at Dataset Bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Selective Search for Object Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Weakly supervised object localization with latent category learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations using videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Unsupervised Learning of Models for Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Track and segment: An iterative unsupervised approach for video object proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
