<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Compact Binary Descriptors with Unsupervised Deep Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Lin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Information Science</orgName>
								<orgName type="institution">Academia Sinica</orgName>
								<address>
									<settlement>Taipei</settlement>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
							<email>lujiwen@tsinghua.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Automation</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chu-Song</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Information Science</orgName>
								<orgName type="institution">Academia Sinica</orgName>
								<address>
									<settlement>Taipei</settlement>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
							<email>jzhou@tsinghua.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Automation</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Compact Binary Descriptors with Unsupervised Deep Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:42+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we propose a new unsupervised deep learning approach called DeepBit to learn compact binary descriptor for efficient visual object matching. Unlike most existing binary descriptors which were designed with random projections or linear hash functions, we develop a deep neural network to learn binary descriptors in an unsupervised manner. We enforce three criterions on binary codes which are learned at the top layer of our network: 1) minimal loss quantization, 2) evenly distributed codes and 3) uncorrelated bits. Then, we learn the parameters of the networks with a back-propagation technique. Experimental results on three different visual analysis tasks including image matching, image retrieval, and object recognition clearly demonstrate the effectiveness of the proposed approach.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Feature descriptor plays an important role in computer vision <ref type="bibr" target="#b27">[28]</ref>, which has been widely used in numerous computer vision tasks such as object recognition <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b41">42]</ref>, image classification <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b51">52]</ref> and panorama stitching <ref type="bibr" target="#b4">[5]</ref>. A desirable feature descriptor should fulfill two essential properties: (1) high quality representations, and (2) low computational cost. A feature descriptor is desired to capture important and distinctive information in an image <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b27">28]</ref> and also to be robust to various image transformations <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27]</ref>. On the other hand, highly efficient descriptor enables machines to run in real-time, which is also important for retrieving image in a large corpus <ref type="bibr" target="#b36">[37]</ref>, or detecting objects with mobile devices <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b49">50]</ref>.</p><p>Over the past decade, high quality descriptors such as the rich features learned from the deep Convolutional Neural Networks (CNN) <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b31">32]</ref>, and the representative SIFT descriptor <ref type="bibr" target="#b25">[26]</ref>, have been widely explored. These descriptors demonstrate superior discriminability, and bridge the gap between low-level pixels and high-level semantic informa-  <ref type="figure">Figure 1</ref>: The basic idea of our proposed method. We enforce three criterions on the binary descriptors, and optimize the parameters of the network with back-propagation. Our approach dose not require labeled training data and is more practical to real-world applications in comparison to supervised binary descriptors.</p><p>tion <ref type="bibr" target="#b43">[44]</ref>. However, they are high-dimensional real-valued descriptors, and usually require high computational cost.</p><p>In order to reduce the computational complexity, several lightweight binary descriptors have been recently proposed such as BRIEF <ref type="bibr" target="#b5">[6]</ref>, ORB <ref type="bibr" target="#b32">[33]</ref>, BRISK <ref type="bibr" target="#b21">[22]</ref>, and FREAK <ref type="bibr" target="#b0">[1]</ref>. These binary descriptors are highly efficient to storing and matching. Given compact binary descriptors, one can rapidly measure the similarity of the images by computing the Hamming distance between binary descriptors via XOR bitwise operations. Since these early binary descriptors are computed by simple intensity comparisons, they are usually unstable and sensitive to scales, rotations, and noises. Some previous works <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b53">54]</ref> improved the binary descriptors by encoding the similarity relationship during optimization. However, the success of these methods is mainly attributed to pair-wised similarity labels. In other words, their methods is unfavourable in the case when training data do not have label annotations.</p><p>In this work, we raise a question -can we learn binary descriptor from data without labels? Inspiring from the recent advancement of deep learning, we propose an effective deep learning approach, dubbed DeepBit, to learn compact binary descriptors. We enforce three important criterions on the learned binary descriptor, and optimize the parameters of the network with back-propagation. We employ our approach on three different visual analysis tasks including image matching, image retrieval and object recognition. Experimental results clearly demonstrate that our proposed method outperforms state-of-the-arts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Binary Descriptors: Earlier works related to binary descriptors can be traced back to BRIEF <ref type="bibr" target="#b5">[6]</ref>, ORB <ref type="bibr" target="#b32">[33]</ref>, BRISK <ref type="bibr" target="#b21">[22]</ref>, and FREAK <ref type="bibr" target="#b0">[1]</ref>. These binary descriptors are built upon hand-crafted sampling patterns, and a set of pairwise intensity comparisons. While these descriptors are efficient, their performance is limited because pairwise intensity comparison is sensitive to the scale and geometric transformation. To address these limitations, several supervised approaches <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b52">53]</ref> have been proposed to learn binary descriptors. D-BRIEF <ref type="bibr" target="#b40">[41]</ref> encodes the desired similarity relationships and learns a project matrix to compute discriminative binary features. On the other hand, Local Difference Binary (LDB) <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b50">51]</ref> applies Adaboost to select optimal sampling pairs. Linear Discriminat Analysis (LDA) is also applied to learn binary descriptors <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b37">38]</ref>. Recently proposed BinBoost <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b39">40]</ref> learns a set of projection matrix using the boosting algorithm, and achieves state-of-the-art performance on patches matching. While these approaches have achieved impressive performance, their success is mainly attributed to pair-wise learning with similarity labels, and is unfavorable for the case when transferring the binary descriptor to a new task.</p><p>Unsupervised hashing algorithms learn compact binary descriptors whose distance is correlated to the similarity relationship of the original input data <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b45">46]</ref>. Locality Sensitive Hashing (LSH) <ref type="bibr" target="#b1">[2]</ref> applies random projections to map original data into a low-dimensional feature space, and then performs a binarization. Semantic hashing (SH) <ref type="bibr" target="#b33">[34]</ref> builds a multi-layers Restricted Boltzmann Machines (RBM) to learn compact binary codes for text and documents. Spectral hashing (SpeH) <ref type="bibr" target="#b45">[46]</ref> generates efficient binary codes by spectral graph partitioning. Iterative qauntization (ITQ) <ref type="bibr" target="#b13">[14]</ref> uses iterative optimization strategy to find projections with minimal binarization loss. Even if these approaches have been proved effective, the binary codes are still not as accurate as the real-valued equivalents.</p><p>Deep Learning: Deep Learning has drawn increasing attention in visual analysis since Krizhevsky et al. <ref type="bibr" target="#b19">[20]</ref> demonstrated the outstanding performance of the deep CNN on the 1, 000 class image classification. Their success is attributed to training a deep CNN to learn rich midlevel image representations on millions of images. Oquab et al. <ref type="bibr" target="#b30">[31]</ref> showed that transferring the mid-level image representations to a new domain can be achieved with a few amount of training data. Chatfield et al. <ref type="bibr" target="#b6">[7]</ref> showed that the fine-tuned domain-specific deep features yield better performance than the non-finetuned ones. Several visual analysis tasks have been greatly improved via pre-trained deep CNN and deep transfer learning, such as object detection <ref type="bibr" target="#b11">[12]</ref>, image segmentation <ref type="bibr" target="#b24">[25]</ref>, and image search <ref type="bibr" target="#b22">[23]</ref>. Among the recent studies of deep learning and binary codes learning, Xia et al. <ref type="bibr" target="#b46">[47]</ref> and Lai et al. <ref type="bibr" target="#b20">[21]</ref> take deep CNN to learn a set of hash functions, but they require pair-wised similarity labels or triplets training data. SSDH <ref type="bibr" target="#b48">[49]</ref> constructs hash functions as a latent layer in the deep CNN and achieves state-of-the-art image retrieval performance, but their method belongs to supervised learning. Deep Hashing (DH) <ref type="bibr" target="#b23">[24]</ref> builds three layers hierarchical neural networks to learn discriminative projection matrix, but their method does not take the advantage of deep transfer learning, thus makes the binary codes less effective. In contrast, the proposed DeepBit not only transfers the mid-level image representations pre-trained from ImageNet to the target domain, but also learns compact yet discriminative binary descriptor without label information. We will show that our method achieves better or comparable performance than state-of-the-art descriptors on three public datasets. <ref type="figure">Figure 2</ref> shows the learning framework of our proposed method. We introduce an unsupervised deep learning approach, dubbed DeepBit, to learn compact yet discriminative binary descriptors. Unlike previous works <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b38">[39]</ref><ref type="bibr" target="#b39">[40]</ref><ref type="bibr" target="#b40">[41]</ref> that optimize the projection matrix with hand-crafted features and pair-wised similarity information, DeepBit learns a set of non-linear projection functions to compute compact binary descriptors. We enforce three important objectives on the binary descriptors, and optimize the parameters of the proposed network with the stochastic gradient descent technique. Note that our method does not require labeled training data, and is more practical than the supervised approaches. In this section, we first give an overview of our approach, and then describe the proposed learning objectives in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Overall Learning Objectives</head><p>The proposed DeepBit computes the binary descriptor by applying the projections to the input image and then bi-  <ref type="figure">Figure 2</ref>: We enforce three objectives on the neurons at the top layer of the network to learn compact yet discriminative binary descriptor. The training procedure includes two alternative stages. The top row shows the first stage; We optimize the parameters of the network by minimizing the quantization error and enforcing binary codes to be evenly distributed. The bottom row shows the second stage; We augment the training data with different rotations, and update the parameters of the network by minimizing the distance between binary descriptors that describe the reference image and the rotated one. The alternative stages will be repeated until the stopping criterion is satisfied.</p><p>narizes the results:</p><formula xml:id="formula_0">b = 0.5 × (sign(F(x; W)) + 1),<label>(1)</label></formula><p>where x represents the input image, and b is the resulting binary descriptor in the vector form. sign(k) = 1 if k &gt; 0 and −1 otherwise. F(x; W) is a composition of number of non-linear projection functions which can be written as:</p><formula xml:id="formula_1">F(x; W) = f k (· · · f 2 (f 1 (x; w 1 ); w 2 ) · · · ; w k ),<label>(2)</label></formula><p>where f i takes the data x i and parameter w i as inputs, and produces the projection result x i+1 . The proposed approach aims to learn a set of non-linear projection parameters W = (w 1 , w 2 , ..., w k ) that quantizes the input image x into a compact binary vector b while preserving the information from the input. In order to learn compact yet discriminative binary descriptor, we enforce three important criterions to learn W. First, the learned compact binary descriptor should preserve the local data structure of the activations of the last layer. The quantization loss should be as less as possible after projection. Second, we encourage the binary descriptor to be evenly distributed, so that the binary string will convey more discriminative messages. The third is to make the descriptor invariant to rotations and noises, and thus the binary descriptor will tend to capture more uncorrelated information from the input image. To achieve these objectives, we formulate the following optimization problem to learn a set of non-linear projection parameters W using the proposed deep neural networks:</p><formula xml:id="formula_2">min W L(W) = αL 1 (W) + βL 2 (W) + γL 3 (W) = α N n=1 ||(b n − 0.5) − F(x n ; W)|| 2 + β M m=1 ||(µ m − 0.5)|| 2 + γ N n=1 R θ=−R C(θ)||b n,θ − b n || 2 ,<label>(3)</label></formula><p>where N is the number of training data for each mini-batch, M is the bit length of the binary codes, and R represents the image rotation angle. b n,θ is the binary descriptor projected from image x n with rotation angle θ, and C(θ) is the cost function which penalizes the training data according to its rotation degree. Moreover, α, β, and γ are three parameters to balance different objectives.</p><p>To give a better understanding of the proposed objectives, we describe the physical meaning of (3) as below. First, L 1 minimizes the quantization loss between the binary descriptor and the original input image. Then, L 2 encourages the binary descriptor to be evenly distributed to maximize the information capacity of the binary descriptor. Finally, L 3 tolerates the rotation transformations by minimizing the Hamming distance between the descriptors that describe the reference image and the rotated ones. We elaborate the details of each proposed objective as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Learning Discriminative Binary Descriptors</head><p>The proposed DeepBit seeks to learn the projections that maps the input image into a binary string while preserving the discriminative information of the original input. The soul idea to keep the binary descriptors informative is to minimize the quantization loss by rewriting (1) as follows:</p><formula xml:id="formula_3">(b − 0.5) = F(x; W),<label>(4)</label></formula><p>the smaller the quantization loss is, the better the binary descriptor will preserve the original data information. Different from the previous work <ref type="bibr" target="#b12">[13]</ref> that addresses this problem by iteratively updating W and b with two alternating steps, we formulate this optimization problem as the neural networks training objective. Since then, the goal of the proposed network becomes learning the W that minimizes the quantization loss between the binary descriptor and the original input image. To this end, we optimize the parameters W of the proposed network through back-propagation and stochastic gradient descent (SGD) using the following loss function:</p><formula xml:id="formula_4">min W L(W) = N n=1 ||(b n − 0.5) − F(x n ; W)|| 2 .<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Learning Efficient Binary Descriptors</head><p>To increase the information capacity of the binary descriptors, we maximize the usage of each bin in the binary string. Considering the variance for each bin, the higher the entropy is, the more information the binary codes express. Accordingly, we enhance the binary descriptor by making each bit has 50% probability of being one or zero. In other words, there is no preference for each bit to be one or zero, and the resulting binary string will convey the information as much as possible. To achieve this goal, we keep the binary descriptors to be evenly distributed by formulating the following objective, and minimizing the loss computed by the forward pass of the network:</p><formula xml:id="formula_5">min W L(W) = M m=1 ||(µ m − 0.5)|| 2 ,<label>(6)</label></formula><p>where M represents the bit length of the binary string. For each bin we compute the average response µ m using:</p><formula xml:id="formula_6">∀ m∈1,...,M µ m = 1 N N n=1 b n (m),<label>(7)</label></formula><p>where N is the number of training data, and function b(m) produces the binary value at m-th bin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Learning Rotation Invariant Binary Descriptors</head><p>Since rotation invariant is essential for a local descriptor, we hope to enhance this property during optimization. We address this issue by minimizing the difference between binary descriptors that describe the reference image and the rotated one. Considering the estimation error between images, the estimation error may become larger when increasing the rotation degree. Hence, we mitigate the estimation error by penalizing the training loss of the network according to the rotation degree. We formulate the proposed objective as a cost-sensitive optimization problem as follow:</p><formula xml:id="formula_7">min W L(W) = N n=1 R θ=−R C(θ)||b n,θ − b n || 2 ,<label>(8)</label></formula><p>where θ ∈ (−R, R) is the rotation angle. b n,θ denotes the descriptor mapping from input x n with rotation θ. C(θ) provides the cost information to reflect the relationship of binary descriptors between different rotation transformations.</p><p>In this paper, we mitigate the estimation error by setting:</p><formula xml:id="formula_8">C(θ) = exp − (θ − µ) 2 2σ 2 ,<label>(9)</label></formula><p>where C(θ) is the Gaussian distribution, and µ = 0, σ = 1 in our experiments. We implement our approach using the open source Caffe <ref type="bibr" target="#b17">[18]</ref>, and Algorithm 1 summarizes the detail procedure of the proposed DeepBit. The proposed approach includes two main components. The first is network initialization. Second is the optimization step. We initialize our network with the pre-trained weights from the 16 layers VGGNet <ref type="bibr" target="#b35">[36]</ref>, which is trained on the ImageNet large scale dataset. Then, we replace the classification layer of the VGGNet with a new fully connected layer, and enforce the neurons in this layer to learn binary descriptor. To this end, we use stochastic gradient descent (SGD) method and backpropagation to train our network, and optimize W using the proposed objectives (see <ref type="formula" target="#formula_2">(3)</ref>). Other settings are listed below. α = 1.0, β = 1.0, γ = 0.01. We rotate the image by 10, 5, 0, −5, −10 degrees, respectively. Mini-batch size is 32, and the bit-length of our binary descriptor is 256. Images are normalized to 256 × 256 and then center-cropped to 224 × 224 as the network input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1: DeepBit</head><p>Input: Training set X = [x 1 , x 2 , ..., x n ] Output: A set of non-linear projection parameters W</p><p>Step 1 (Initialization):</p><p>Initialize W with pre-trained weights from ImageNet;</p><p>Step 2 (Optimization): while iter &lt; max iter do</p><p>Fix W update b n using (1); while iter1 &lt; max iter1 do Fix b n update W by minimizing the sum of (5) and <ref type="formula" target="#formula_5">(6)</ref>;</p><p>Fix W update b n using <ref type="formula" target="#formula_0">(1)</ref>; </p><formula xml:id="formula_9">while iter2 &lt; max iter2 do</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results</head><p>We conduct experiments on three challenging datasets, the Brown gray-scale patches <ref type="bibr" target="#b3">[4]</ref>, the CIFAR-10 color images <ref type="bibr" target="#b18">[19]</ref>, and the Oxford 17 category flowers <ref type="bibr" target="#b28">[29]</ref>. We provide extensive evaluations of the proposed binary descriptor, and demonstrate its performance on various tasks, including image matching, image retrieval, and image classification. We start with introducing the datasets and then present our experimental results as well as the comparative evaluations with other state-of-the-arts. Nor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>Yos. <ref type="figure">Figure 5</ref>: Correctly matched patches and mismatched ones from the Brown dataset. Top row shows the patches from Liberty classified as matched pairs; the first three are correctly classified, but the fourth is mismatched, which describes different architectures. Middle row shows the image pairs from Notredame classified as the matched pairs; the fourth is mismatched although both of them share similar pattern. Bottom row shows the patches from Yosemite classified as matched pairs; the last one is mismatched, which are visually similar but belong to different locations. and 10, 000 test pairs (5, 000 matched, and 5, 000 nonmatched pairs), respectively.</p><p>• CIFAR-10 Dataset <ref type="bibr" target="#b18">[19]</ref> contains 10 object categories and each class consists of 6, 000 images, resulting in a total of 60, 000 images. The dataset is split into training and test sets, with 50, 000 and 10, 000 images respectively.</p><p>• The Oxford 17 Category Flower Dataset <ref type="bibr" target="#b28">[29]</ref> contains 17 categories and each class consists of 80 images, resulting in a total of 1, 360 images. The dataset is split into the training (40 images per class), validation (20 images per class), and test (20 images per class) sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Results on Image Matching</head><p>To evaluate the performance of local descriptors, we compare the proposed DeepBit with several state-of-theart binary descriptors, including unsupervised (BRIEF <ref type="bibr" target="#b5">[6]</ref>, ORB <ref type="bibr" target="#b32">[33]</ref>, BRISK <ref type="bibr" target="#b21">[22]</ref>, and Boosted SSC <ref type="bibr" target="#b34">[35]</ref>), and supervised methods (D-BRIEF <ref type="bibr" target="#b40">[41]</ref>, LDAHash <ref type="bibr" target="#b37">[38]</ref>).</p><p>Following the settings in <ref type="bibr" target="#b39">[40]</ref>, <ref type="figure">Figure 4</ref> shows the ROC curves for DeepBit and the compared methods, and <ref type="table">Table 1</ref>   <ref type="figure">Figure 4</ref>: ROC curves of the proposed DeepBit descriptors and the compared binary descriptors, across all the splits of training and testing configurations on the Brown datasets. In parentheses: the bit length of the binary descriptor (b), and the 95% error rates. <ref type="table">Table 1</ref>: Comparison of the proposed binary descriptor to the state-of-the-art binary descriptors, in terms of 95% error rates (ERR) across all the splits of training and testing configurations. For reference, we also provide the results of real-valued descriptor SIFT <ref type="bibr" target="#b25">[26]</ref>. The proposed method achieves better performance than the unsupervised binary descriptors in most cases, while remaining competitive to supervised approaches (D-BRIEF and LDAHash). ploy similarity information (matched and non-matched labels) to optimize the projection matrix, our learning process does not require the training labels and still performs more favorably against the supervised ones such as D-BRIEF and LDAHash. We further visualize the image matching results on the Brown dataset in <ref type="figure">Figure 5</ref>. As can be seen, the proposed method successfully matches pairs of patches when they are visually similar, as shown in the first three columns of <ref type="figure">Figure 5</ref>. Our method could also mismatch some patches as shown in the fourth column of <ref type="figure">Figure 5</ref>. It is worth noting that the mismatched patches are still visually similar although they are from different scenes or locations. More specifically, the patches from Liberty and Notredame describe the local structure of the statue and architecture, where the visual similarity between different patches is usually weak. Our approach achieves more favorable performance in these two datasets. However, the patches from Yosemite depict the surface of a mountain. Different local patches (such as snow and forest) could generate visually similar patterns, making them difficult to be distinguished. This could be the reason why our approach, which tends to match patterns that are visually similar, performs less favorable than some methods for the Yosemite dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Real-valued Binary</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Results on Image Retrieval</head><p>To evaluate the discriminability of the proposed binary descriptor, we further test our method on the task of image retrieval. We compare DeepBit with several unsupervised hashing methods, including LSH <ref type="bibr" target="#b1">[2]</ref>, ITQ <ref type="bibr" target="#b13">[14]</ref>, PCAH <ref type="bibr" target="#b44">[45]</ref>, Semantic Hashing (SH) <ref type="bibr" target="#b33">[34]</ref>, Spectral hashing (SpeH) <ref type="bibr" target="#b45">[46]</ref>), Spherical hashing (SphH) <ref type="bibr" target="#b16">[17]</ref>, KMH <ref type="bibr" target="#b15">[16]</ref>, and Deep Hashing (DH) <ref type="bibr" target="#b23">[24]</ref> on the CIFAR-10 dataset. Among these eight unsupervised approaches, Deep Hashing (DH), like our approach, takes advantage of deep neural networks for learning compact binary codes.</p><p>Following the settings in <ref type="bibr" target="#b23">[24]</ref>, <ref type="table" target="#tab_3">Table 2</ref> shows the CIFAR-10 retrieval results based on the mean Average Precision (mAP) of the top 1, 000 returned images with respect to different bit lengths. DeepBit improves previous best retrieval performance by 3.26%, 8.24%, and 10.77% mAP with respect to 16, 32, and 64 hash bits, respectively. According to the results, we found that the longer the hash bits, the better performance DeepBit achieves. Moreover, <ref type="figure">Figure 6</ref> shows the Precision/Recall curves of different unsupervised hashing methods with 16, 32, 64 hash bits, respectively. As can be seen, DeepBit constantly outperforms previous unsupervised methods. This indicates the proposed method is effective to learn binary descriptors. It is worth to note that DH <ref type="bibr" target="#b23">[24]</ref> takes three layers hierarchical neural networks to learn binary hash codes; however, DH dose not take advantage of the deep transfer learning during training. In contrast, the proposed DeepBit not only transfers the midlevel image representations pre-trained from ImageNet to the target domain, but also learns binary descriptor with desirable criterions. The experiments reveal that deep transfer learning with the proposed objectives can improve the unsupervised hashing performance.  Colour <ref type="bibr" target="#b28">[29]</ref> 60.9 ± 2.1% 3 Shape <ref type="bibr" target="#b28">[29]</ref> 70.2 ± 1.3% 4 Texture <ref type="bibr" target="#b28">[29]</ref> 63.7 ± 2.7% 3 HOG <ref type="bibr" target="#b7">[8]</ref> 58.5 ± 4.5% 4 HSV <ref type="bibr" target="#b29">[30]</ref> 61.3 ± 0.7% 3 SIFT-Boundary <ref type="bibr" target="#b29">[30]</ref> 59.4 ± 3.3% 5 SIFT-Internal <ref type="bibr" target="#b29">[30]</ref> 70.6 ± 1.6% 4</p><p>DeepBit 75.1 ± 2.5% 0.07</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Results on Object Recognition</head><p>Unlike previous binary descriptors that require matched/non-matched labels during training, the proposed DeepBit learns compact binary descriptors in an unsupervised manner; thus, DeepBit is practical and flexible for various applications. In this section, we extend the evaluation to object recognition and show that the proposed binary descriptor performs more favorably against several real-valued descriptors such as HOG <ref type="bibr" target="#b7">[8]</ref>, and SIFT <ref type="bibr" target="#b25">[26]</ref>.</p><p>Flower classification is a classic visual analysis task, and it is challenging due to the variation of shapes, color distributions, and pose deformations. Besides, the computation cost becomes demanding while one wants to recognize the flowers in the wild using mobile devices. We test our binary descriptors on the flower recognition. Following the setting in <ref type="bibr" target="#b28">[29]</ref>, we train the multi-class SVM classifier with the proposed binary descriptor. <ref type="table" target="#tab_4">Table 3</ref> compares the classification accuracy of the 17 categories flowers using different descriptors proposed in <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30]</ref>, in- cluding low-level (Colour, Shape, Texture), and high level (SIFT, and HOG) features. The proposed binary descriptor improves previous best recognition accuracy by around 4.5% (75.1% vs. 70.6%). In addition, DeepBit greatly reduces the computational complexity during SVM classifier training. Our training process is 71.42x faster than the one trained with SIFT because the dimension of DeepBit is lower than that of SIFT. <ref type="figure">Figure 8</ref> and <ref type="figure">Figure 9</ref> shows some visualization results. DeepBit demonstrates its efficiency and efficacy, and performs more favourably against various existing descriptors including Colour <ref type="bibr" target="#b28">[29]</ref>, Shape <ref type="bibr" target="#b28">[29]</ref>, Texture <ref type="bibr" target="#b28">[29]</ref>, HOG <ref type="bibr" target="#b7">[8]</ref>, HSV <ref type="bibr" target="#b29">[30]</ref>, and SIFT <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b29">30]</ref>. This indicates the proposed method is effective to learn discriminative and compact binary codes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this paper, we have presented an unsupervised deep learning framework to learn compact binary descriptor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Confusion Matrix of Flower 17 Category Classification</head><p>Target Class 5 <ref type="bibr">10 15</ref> Output Class  <ref type="figure">Figure 9</ref>: Confusion matrix of Oxford 17 flower classification using the proposed DeepBit. Classification results indicate that the proposed learning method is effective to learn compact but informative binary descriptor.</p><p>We employ three criterions to learn the binary codes and estimate the parameters of the deep neural network to obtain binary descriptor. Our approach does not require labeled data during learning, and is more practical to real-world applications compared to supervised binary descriptors. Experiments on three benchmark databases include gray-scale local patches, color images, and flowers in the wild demonstrate that our method achieves better performance than the state-of-the-art feature descriptors in most cases.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fix b n update W using ( 8 Figure 3 :</head><label>83</label><figDesc>Sample images from the Brown dataset, CIFAR10 dataset, and Oxford flower dataset, respectively. We test our approach on a wide range of image types, including grayscale local patches, color category images, and flowers in the wild.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 7 :</head><label>7</label><figDesc>Top 20 retrieved images from CIFAR10 dataset by DeepBit with 32 bit length.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :Figure 8 :</head><label>68</label><figDesc>Precision/Recall curves of different unsupervised hashing methods on the CIFAR-10 dataset with respect to 16Correctly classified test images and misclassified ones. The top row shows images classified as Cowslip; the first two are correctly classified but the correct category of the third is Buttercup. The bottom row shows images classified as Pansy; the third is misclassified, which belongs to Crocus.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>summarizes the 95 percent error rates for the Brown dataset. As can be seen, the overall performance of the proposed method achieves 40.67% error rate when recall rate is 95%, which outperforms BRIEF, ORB, BRISK, Boosted SSC with 15.56%(= 56.23% − 40.67%), 15.56%(= 56.23% − 40.67%), 35.14%(= 75.81% − 40.67%), 32.84%(= 73.51% − 40.67%) lower error rate over the different training and testing configurations of the Brown dataset, respectively. It is important to point out that unlike several previous works[3, 9, 38-41, 50, 53]  that em-</figDesc><table>0.2 

0.3 

0.4 

0.5 

0.6 

0.7 

0.8 

0.9 

1 

0 
0.1 
0.2 
0.3 
0.4 
0.5 
0.6 
0.7 
0.8 

True Positive Rate 

False Positive Rate 

Train: Notre Dame, Test: Liberty 

Boosted SSC (128b, 70.35%) 
BRISK (512b, 79.36%) 
ORB (256b, 59.15%) 
BRIEF (256b, 59.15%) 
LDAHash (128b, 49.66%) 
D-BRIEF (32b, 51.30%) 
DeepBit (256b, 32.06%) 

0.2 

0.3 

0.4 

0.5 

0.6 

0.7 

0.8 

0.9 

1 

0 
0.1 
0.2 
0.3 
0.4 
0.5 
0.6 
0.7 
0.8 

True Positive Rate 

False Positive Rate 

Train: Yosemite, Test: Liberty 

Boosted SSC (128b, 71.59%) 
BRISK (512b, 79.36%) 
ORB (256b, 59.15%) 
BRIEF (256b, 59.15%) 
LDAHash (128b, 49.66%) 
D-BRIEF (32b, 53.39%) 
DeepBit (256b, 34.41%) 

0.2 

0.3 

0.4 

0.5 

0.6 

0.7 

0.8 

0.9 

1 

0 
0.1 
0.2 
0.3 
0.4 
0.5 
0.6 
0.7 
0.8 

True Positive Rate 

False Positive Rate 

Train: Notre Dame, Test: Yosemite 

Boosted SSC (128b, 76.00%) 
BRISK (512b, 73.21%) 
ORB (256b, 54.96%) 
BRIEF (256b, 54.96%) 
LDAHash (128b, 52.95%) 
D-BRIEF (32b, 46.22%) 
DeepBit (256b, 63.68%) 

0.2 

0.3 

0.4 

0.5 

0.6 

0.7 

0.8 

0.9 

1 

0 
0.1 
0.2 
0.3 
0.4 
0.5 
0.6 
0.7 
0.8 

True Positive Rate 

False Positive Rate 

Train: Yosemite, Test: Notre Dame 

Boosted SSC (128b, 72.20%) 
BRISK (512b, 74.88%) 
ORB (256b, 54.57%) 
BRIEF (256b, 54.57%) 
LDAHash (128b, 51.58%) 
D-BRIEF (32b, 43.96%) 
DeepBit (256b, 29.60%) 

0.2 

0.3 

0.4 

0.5 

0.6 

0.7 

0.8 

0.9 

1 

0 
0.1 
0.2 
0.3 
0.4 
0.5 
0.6 
0.7 
0.8 

True Positive Rate 

False Positive Rate 

Train: Liberty, Test: Yosemite 

Boosted SSC (128b, 77.99%) 
BRISK (512b, 73.21%) 
ORB (256b, 54.96%) 
BRIEF (256b, 54.96%) 
LDAHash (128b, 52.95%) 
D-BRIEF (32b, 47.29%) 
DeepBit (256b, 57.61%) 

0.2 

0.3 

0.4 

0.5 

0.6 

0.7 

0.8 

0.9 

1 

0 
0.1 
0.2 
0.3 
0.4 
0.5 
0.6 
0.7 
0.8 

True Positive Rate 

False Positive Rate 

Train: Liberty, Test: Notre Dame 

Boosted SSC (128b, 72.95%) 
BRISK (512b, 74.88%) 
ORB (256b, 54.57%) 
BRIEF (256b, 54.57%) 
LDAHash (128b, 51.58%) 
D-BRIEF (32b, 43.10%) 
DeepBit (256b, 26.66%) 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="true"><head>Table 2 :</head><label>2</label><figDesc>Performance comparison (mAP, %) of different unsupervised hashing algorithms on the CIFAR-10 dataset. This table shows the mean Average Precision (mAP) of top 1, 000 returned images with respect to different number of hash bits.</figDesc><table>Method 
16 bit 
32 bit 
64 bit 

KMH [16] 
13.59 
13.93 
14.46 
SphH [17] 
13.98 
14.58 
15.38 
SpeH [46] 
12.55 
12.42 
12.56 
SH [34] 
12.95 
14.09 
13.89 
PCAH [45] 
12.91 
12.60 
12.10 
LSH [2] 
12.55 
13.76 
15.07 
PCA-ITQ [13] 
15.67 
16.20 
16.64 
DH [24] 
16.17 
16.62 
16.96 

DeepBit 
19.43 
24.86 
27.73 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc>The categorization accuracy (mean±std%) for different features on the Oxford 17 Category Flower Dataset<ref type="bibr" target="#b10">[11]</ref>.</figDesc><table>Descriptors 
Accuracy 
Training Time (sec) 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Freak: Fast retina keypoint</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ortiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Near-optimal hashing algorithms for approximate nearest neighbor in high dimensions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Andoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Indyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. FOCS</title>
		<meeting>FOCS</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Bold-binary online learned descriptor for efficient image matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Balntas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Discriminative learning of local image descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Winder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="43" to="57" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Automatic panoramic image stitching using invariant features. Int&apos;l J. Computer Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="page" from="59" to="73" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Brief: Binary robust independent elementary features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Calonder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Strecha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Return of the devil in the details: Delving deep into convolutional nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chatfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. BMVC</title>
		<meeting>BMVC</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Receptive fields selection for binary feature description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Trzcinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Proc</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2583" to="2595" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Object class recognition by unsupervised scale-invariant learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">On feature combination for multiclass object classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Iterative quantization: A procrustean approach to learning binary codes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Iterative quantization: A procrustean approach to learning binary codes for large-scale image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gordo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The pyramid match kernel: Discriminative classification with sets of image features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">K-means hashing: An affinitypreserving quantization method for learning binary compact codes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Spherical hashing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-P</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Yoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM MM</title>
		<meeting>ACM MM</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Simultaneous feature learning and hash coding with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Brisk: Binary robust invariant scalable keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Leutenegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">Y</forename><surname>Siegwart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep learning of binary hash codes for fast image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Hsiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-S</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR Workshops</title>
		<meeting>CVPR Workshops</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep hashing for compact binary codes learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">E</forename><surname>Liong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Moulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int&apos;l J. Computer Vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Indexing based on scale invariant interest points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A performance evaluation of local descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1615" to="1630" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A visual vocabulary for flower classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-E</forename><surname>Nilsback</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Automated flower classification over a large number of classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-E</forename><surname>Nilsback</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICVGIP</title>
		<meeting>ICVGIP</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning and transferring mid-level image representations using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">CNN features off-the-shelf: An astounding baseline for recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Razavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Azizpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Carlsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR Workshops</title>
		<meeting>CVPR Workshops</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Orb: an efficient alternative to sift or surf</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rublee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Rabaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Konolige</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bradski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Semantic hashing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Approx. Reasoning</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Learning task-specific similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
		<respStmt>
			<orgName>Massachusetts Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Content-based image retrieval at the end of the early years</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Smeulders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Worring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Santini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1349" to="1380" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Ldahash: Improved matching with smaller descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Strecha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Boosting binary keypoint descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Trzcinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Christoudias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning image descriptors with boosting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Trzcinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Christoudias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Efficient discriminative projections for compact binary descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Trzcinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Object recognition with informative features and linear classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vidal-Naquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ullman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Robust real-time object detection. Int&apos;l</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Computer Vision</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="51" to="52" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Deep learning for content-based image retrieval: A comprehensive study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C H</forename><surname>Hoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM MM</title>
		<meeting>ACM MM</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Semi-supervised hashing for scalable image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<title level="m">Spectral hashing. In Proc. NIPS</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Supervised hashing for image retreieval via image representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Osri: A rotationally invariant binary descriptor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Proc</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2983" to="2995" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Supervised learning of semantics-preserving hashing via deep neural networks for large-scale image search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-S</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.00101</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Ldb: An ultra-fast feature for scalable augmented reality on mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-T</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ISMAR</title>
		<meeting>ISMAR</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Local difference binary for ultrafast and distinctive feature description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-T</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="188" to="194" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Local features and kernels for classification of texture and object categories: A comprehensive study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marszałek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int&apos;l J. Computer Vision</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="213" to="238" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Usb: ultrashort binary descriptor for fast visual matching and retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Proc</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Coupled binary embedding for large-scale image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Proc</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3368" to="3380" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
