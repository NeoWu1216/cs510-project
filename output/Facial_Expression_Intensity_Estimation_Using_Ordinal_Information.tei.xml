<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Facial Expression Intensity Estimation Using Ordinal Information</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Electrical</orgName>
								<orgName type="department" key="dep2">Computer &amp; Systems Engineering</orgName>
								<orgName type="institution">Rensselaer Polytechnic Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Gan</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">University of Science and Technology of China</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shangfei</forename><surname>Wang</surname></persName>
							<email>sfwang@ustc.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">University of Science and Technology of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Qiang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Electrical</orgName>
								<orgName type="department" key="dep2">Computer &amp; Systems Engineering</orgName>
								<orgName type="institution">Rensselaer Polytechnic Institute</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Facial Expression Intensity Estimation Using Ordinal Information</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:40+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Previous studies on facial expression analysis have been focused on recognizing basic expression categories. There is limited amount of work on the continuous expression intensity estimation, which is important for detecting and tracking emotion change. Part of the reason is the lack of labeled data with annotated expression intensity since expression intensity annotation requires expertise and is time consuming. In this work, we treat the expression intensity estimation as a regression problem. By taking advantage of the natural onset-apex-offset evolution pattern of facial expression, the proposed method can handle different amounts of annotations to perform frame-level expression intensity estimation. In fully supervised case, all the frames are provided with intensity annotations. In weakly supervised case, only the annotations of selected key frames are used. While in unsupervised case, expression intensity can be estimated without any annotations. An efficient optimization algorithm based on Alternating Direction Method of Multipliers (ADMM) is developed for solving the optimization problem associated with parameter learning. We demonstrate the effectiveness of proposed method by comparing it against both fully supervised and unsupervised approaches on benchmark facial expression datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Facial expression provides rich information in understanding a person's emotional state, feeling and attitude (see <ref type="bibr" target="#b7">[8]</ref> for a survey). So far the majority of expression analysis work focus on recognition of basic expression categories including anger, happy, fear, surprise, sadness, disgust, contempt, etc. Recently, there is an increasing interest in a more fine-grained analysis, namely the facial expression intensity estimation. For instance, the pain intensity is used to evaluate the extent of discomfort in a healthcare application <ref type="bibr" target="#b12">[13]</ref>.</p><p>Automatic expression intensity estimation is challenging * Corresponding author partially due to the lack of standard rule for expression intensity labeling. One way to define expression intensity uses the intensity of Action Units (AUs), which is a set of atomic facial muscle actions defined by Facial Action Coding System (FACS). <ref type="bibr" target="#b5">[6]</ref>. For example, Prkachin and Solomon <ref type="bibr" target="#b20">[21]</ref> used the intensity summation of four AUs: brow lowering, orbital tightening, levator contraction and eye closure to define the pain intensity. However, manually recognizing AU and annotating its intensity is a time consuming process with requirement of domain expertise. While automatic AU intensity estimation is still an open research problem <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b23">24]</ref>.</p><p>Another way to define expression intensity uses relative difference between facial images presented at different stages of an expression. For example, Hess et al. <ref type="bibr" target="#b10">[11]</ref> defined the expression intensity as the relative degree of displacement away from a neutral or relaxed facial expression. Dhall and Goecke <ref type="bibr" target="#b4">[5]</ref> divided the dynamic process of smile into 6 stages. Despite the simple definition, there is no accurate way of determining different intensity levels except manual labeling, which requires substantial labor and expertise. Due to this reason, there are few datasets that come with expression intensity labels. One exception is <ref type="bibr" target="#b17">[18]</ref>.</p><p>In this work, we introduce a method that learns a framelevel expression intensity estimator by exploiting the ordinal information among different frames and intensity labels of selected frames, if available, in the training image sequences. Our approach is based on the observation that the temporal evolution of facial expression usually follows a particular order. Starting from a neutral stage where no expression is observed, we consider the expression intensity reaching its lowest level. Then we observe the onset of expression followed by apex, when the intensity reaches its peak. After reaching its peak, the expression intensity starts to reduce until it is back to neutral status. Even though the duration of each stage may vary under different occasions for different subjects, the general trend of the evolution remains the same. <ref type="figure" target="#fig_0">Figure 1</ref> shows some expression sequence examples with such evolution trend. Similar idea of temporal evolution pattern has been applied to other computer vision task such as action recognition <ref type="bibr" target="#b8">[9]</ref>. Obtaining the labels for apex and onset/offset frames is usually less expensive. Some dataset has the setting of only recording the expression changing from onset to apex, which readily provides us with some relative intensity information. Existing work on intensity estimation usually fall at two ends of machine learning paradigms. In a fully supervised setting, Lee and Xu <ref type="bibr" target="#b13">[14]</ref> adopted definition of the expression intensity in <ref type="bibr" target="#b10">[11]</ref> and used Support Vector Regression (SVR) to model the facial expression intensity. However the model is subject dependent and needs to be trained for different subjects. More recently, Kaltwang et al. <ref type="bibr" target="#b12">[13]</ref> performed continuous pain intensity estimation using Relevance Vector Regression (RVR) without considering ordinal information of the expression change. Rudovic et al. <ref type="bibr" target="#b21">[22]</ref> proposed a Conditional Random Field (CRF) based model to combine the topology and ordinal state of facial affect data for joint expression recognition and intensity estimation. However, the inference is performed at the sequence level and the output only support discrete intensity values. Some follow-up works by the same authors <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref> learned pain intensity estimator in a fully supervised manner where the intensity labels of all the frames are provided.</p><p>On the unsupervised setting, Yang et al. <ref type="bibr" target="#b30">[31]</ref> proposed a RankBoost based framework which learns a ranking model using ordinal relationship among image frames to do intensity estimation and expression recognition. However, a specially ordered sequence is required to be constructed for model learning and the relative intensity level among different sequences are different. Whitehill et al. <ref type="bibr" target="#b28">[29]</ref> trained a binary classifier using GentleBoost for smile detection where the output score of classifier is used for intensity estimation. The learning is performed using individual images, where no ordinal information of an expression is used.</p><p>Our contributions include the following aspects. First, we propose a regression approach for expression intensity estimation which exploits both ordinal relationship among different frames within an expression sequence and absolute intensity labels if available. Second, we introduce a unified max-margin learning framework to simultaneously exploit the two sources of information. An efficient algorithm to solve the optimization problem is developed. Third, our method can generalize to different learning settings depending on the availability of expression intensity annotations.</p><p>As for the rest of this paper, a formal definition of the problem and used assumptions are described in section 2. We review some basic components of our method in section 3 and explain the proposed method in details in section 4. Experimental evaluation is provided in section 5, followed by conclusion and future work in section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Problem Statement</head><p>In this section, we define the problem and state the assumptions we used. Our goal is to learn frame-level expression intensity estimator using expression sequences as training data with or without any intensity annotations.</p><p>Denote an expression sequence as X = {x i ∈ R d |i = 1, ..., |X|}, where x i is the i th individual frame, d is the feature dimension of a frame and |X| is the length of the sequence. Denote intensity labels associated with X as Y = {y i ∈ R|i ∈ V}, where V ⊆ {1, ..., |X|} is a subset of indices for the sequence. We are interested in exploring different settings on V. For a fully supervised problem, V contains all the frame indices. For a weakly supervised problem, V only contains selected frame indices. Finally, for a fully unsupervised problem, V is an empty set.</p><p>We assume that within a sequence, the expression intensity either increases monotonically until it reaches its peak or decreases monotonically after passing its peak, where peak is attained at apex frames. Specifically, let p be the index of apex frame, then the following inequalities hold</p><formula xml:id="formula_0">y i ≥ y j , ∀(i, j) ∈ E (1) where E = {(i, j)|1 ≤ j &lt; i ≤ p or p ≤ i &lt; j ≤ |X|}</formula><p>is a set specifying pairwise ordinal relationship. Intuitively speaking, the expression can only evolve in an onset-apexoffset fashion. During training, we are provided with multiple sequences and additional information on intensity annotations D = {X n , Y n , V n , E n }, n = 1, ..., N , where N is the number of sequences. Intensity label set V n may vary depending on learning settings. In fully supervised setting, V n = {1, ..., |X n |} i.e. all the frames. In weakly supervised setting, V n = {1, p, |X n |} i.e. onset, apex and offset frames, assuming the first and the last frame are onset and offset frame respectively. In unsupervised setting, V n = ∅. In each setting, E n is available given V n .</p><p>Our goal is to learn a regression function f : R d → R applied on frame-level under different learning settings of V n . During evaluation, given an image of expression, we perform expression intensity estimation as</p><formula xml:id="formula_1">y = f (x; θ)<label>(2)</label></formula><p>where θ is the parameter of the function f and y is the ground truth expression intensity. For dataset without intensity annotation, we use relative intensity as substitution. We will define relative intensity specifically later in section 5. We consider different types of expressions separately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Background</head><p>We briefly review Support Vector Regression (SVR) and Ordinal Regression (OR), on which our method is based.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Support Vector Regression</head><p>As a modification of Support Vector Machine (SVM) <ref type="bibr" target="#b3">[4]</ref>, SVR <ref type="bibr" target="#b26">[27]</ref> learns a regression model given data-label pair {y i , x i }. One of the most commonly used variant called ǫinsensitive SVR learns the model parameter θ = {w, b} by solving the following optimization problem.</p><formula xml:id="formula_2">min θ,η + ,η − 1 2 ||w|| 2 + γ i (η + i + η − i ) (3) s.t. w T φ(x i ) + b − y i ≤ ǫ + η + i y i − w T φ(x i ) − b ≤ ǫ + η − i η + i , η − i ≥ 0, ∀i</formula><p>where ǫ is a constant which defines the maximum deviation allowed for a prediction to be considered as correct, φ : X → F is a mapping from input space X to some feature space F and γ is a constant balancing between the regularization and regression loss. Solution to Eq.(3) can be obtained by solving its dual problem, which avoids explicit computation of φ(x i ) using kernel trick <ref type="bibr" target="#b27">[28]</ref>. An important property inherited from SVM is that the solution of SVR is sparse in the sense that the model parameter can be determined using only a subset of data points namely the support vectors. For further details, readers are refered to <ref type="bibr" target="#b27">[28]</ref>. Since SVR uses the label information associated with each data point, it can only be trained using frames with known intensity labels. We will use SVR as our baseline method for comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Ordinal Regression</head><p>In OR, the target value of each data point is an ordinal variable, which gives a ranking among different data points. It is widely used in information retrieval task such as ranking data according to their relevance to the query <ref type="bibr" target="#b15">[16]</ref>. In such scenario, the ordinal relationship rather than the absolute regression value is of primary interest. We introduce the formulation proposed by Herbrich et al. <ref type="bibr" target="#b9">[10]</ref>. Given datalabel pair {y i , x i }, where y i is a discrete ordinal variable, the model parameter w is learned by solving the following optimization problem.</p><formula xml:id="formula_3">min w,ξ 1 2 ||w|| 2 + γ (i,j)∈E ξ ij (4) s.t. w T (φ(x i ) − φ(x j )) ≥ 1 − ξ ij ξ ij ≥ 0, ∀(i, j) ∈ E where E = {(i, j)|y i ≥ y j } is the ordinal set.</formula><p>Eq.(4) tries to find a regression function f (x; θ) which minimizes the number of swapped pairs in training data. In general, the number of constraints is O(n 2 ) where n is the total number of data points, which can be problematic for large scale problems. More recently, Joachims <ref type="bibr" target="#b11">[12]</ref> proposed a formulation which condenses the number of constraints used in Eq.(4) and solved it using cutting-plane algorithm. The complexity is reduced to O(n d ), d &lt; 2. Different from SVR, OR does not need the intensity annotations of any frames in a sequence except the ordinal position of the frame which comes with the sequence itself. We consider this as an unsupervised approach and will use OR as our second baseline method for comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Proposed Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Ordinal Support Vector Regression</head><p>Our model is motivated by the two baseline methods which sit at the two ends of regression model learning. SVR only uses intensity labels of annotated frames while ignoring the temporal order and OR only focuses on ordinal relationship without using labeled intensity values. We propose a max-margin based regression model which incorporates the benefits of both models by taking advantage of some labeled frames and readily available ordinal information comes with the sequence that satisfying the assumptions stated in section 2. We also develop an efficient algorithm to solve the optimization problem for model learning.</p><p>We</p><formula xml:id="formula_4">use linear model f (x; θ) = w T x + b with model pa- rameter θ = {w, b}. Given D = {X n , Y n , V n , E n }, n = 1, ...N , we solve the following optimization problem. min θ,η,ξ 1 2 ||w|| 2 + γ 1 N n=1 k∈Vn (l 1 (η (n)+ k ) + l 1 (η (n)− k )) (5) + γ 2 N n=1 (i,j)∈En l 2 (ξ (n) ij ) s.t. w T x (n) k + b − y (n) k ≤ ǫ + η (n)+ k y (n) k − w T x (n) k − b ≤ ǫ + η (n)− k w T (x (n) i − x (n) j ) ≥ 1 − α ij ξ (n) ij η (n)+ k , η (n)− k , ξ (n) ij ≥ 0 ∀k ∈ V n , (i, j) ∈ E n , n = 1, ..., N</formula><p>where γ 1 , γ 2 , ǫ &gt; 0 are constants. l 1 and l 2 are some functions applied on slack variables η and ξ respectively. V n and E n are the intensity label set and ordinal set associated with the n th sequence as defined in section 2.</p><p>The first two sets of constraints are adopted from SVR in order to restrict regression function to fit the provided intensity labels. The third set of constraints is adapted from OR in order to take advantage of the temporal evolution pattern of the sequence. The additional parameter</p><formula xml:id="formula_5">α = {α ij } = { 1</formula><p>|i−j| }, ∀i = j is introduced to encourage temporal smoothness, namely similar feature values between temporally close frames. Noticing that setting all α ij = 1 reduces to the same constraints used in OR.</p><p>Intuitively, Eq.(5) tries to find a regression function which balances the regression loss and ordinal loss at the same time. More importantly, this formulation is flexible in handling different cases of label annotations i.e. fully annotated, partially annotated and un-annotated. Most of previous work has been focusing on either fully supervised approach <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b23">24]</ref> or unsupervised approach <ref type="bibr" target="#b30">[31]</ref>, our method can in addition to handle a weakly supervised setting where only selected key frames are annotated with intensity. To the best of our knowledge, this is the first work that addresses continuous expression intensity estimation under a variety of annotation settings. We refer to this method as OSVR.</p><p>For the choices of function l 1 and l 2 , we consider two different configurations. The first one we set l i (x) = x, which gives us hinge loss on both regression and ordinal constraints. The second one we set l i (x) = x 2 , which gives us squared hinge loss. However, the formulation is completely general and can be extended to apply different choices of loss functions. Noticing parameters γ 1 and γ 2 effectively balance two types of losses. By setting extreme large value to either γ 1 or γ 2 will forces regression loss dominates ordinal loss or vice versa. Their values can be determined by cross-validation in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Alternating Direction Method of Multipliers</head><p>The number of constraints in Eq. <ref type="formula">(5)</ref> is O(n 2 ), where n is the number of frames. In our experiment, n is usually at the order of 10 2 ∼ 10 3 , resulting the number of constraint 10 4 ∼ 10 6 . We resort to ADMM due to its compactness in handling optimization problem with large number of constraints. The use of augmented Lagrangian multipliers further improves the efficiency in terms of fast convergence rate. Adopting the notation used in [1], we consider the minimization problem with respect to variables u ∈ R n , z ∈ R m .</p><formula xml:id="formula_6">min u,z f (u) + g(z)<label>(6)</label></formula><p>s.t. Au + Bz = c where f and g are some convex functions, A ∈ R p×n , B ∈ R p×m , c ∈ R p are matrix or vector with proper dimension, the augmented Lagrangian can be written as follows</p><formula xml:id="formula_7">L ρ (u, z, v) = f (u) + g(z) + v T (Au + Bz − c)<label>(7)</label></formula><formula xml:id="formula_8">+ ρ/2||Au + Bz − c|| 2 2</formula><p>where v ∈ R p is the ordinary Lagrangian multipliers corresponding to p equality constraints. ρ &gt; 0 is the augmented Lagrangian multiplier. ADMM solves the optimization problem by iteratively updating variables as follows</p><formula xml:id="formula_9">u k+1 := arg min u L ρ (u, z k , v k )<label>(8)</label></formula><formula xml:id="formula_10">z k+1 := arg min z L ρ (u k+1 , z, v k ) (9) v k+1 := v k + ρ(Au k+1 + Bz k+1 − c)<label>(10)</label></formula><p>Since f and g are convex functions, L ρ is also convex. Therefore, in general we can find unique optimal solution in Eq. <ref type="formula" target="#formula_9">(8)</ref> and Eq. <ref type="bibr" target="#b8">(9)</ref>. The convergence of these updates can be proved under fairly mild assumptions <ref type="bibr" target="#b0">[1]</ref>, where neither f nor g has to be differentiable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Solving OSVR using ADMM</head><p>We now reformulate Eq.(5) and derive the needed updates in order to apply ADMM. Define variable u = θ ≡ [w; b] ∈ R d+1 , z ∈ R M1+M2 , where d is the feature dimension. M 1 and M 2 are the total number of constraints corresponding to regression loss (the first two sets of constraints in Eq.(5)) and ordinal loss (the third set of constraints in Eq. <ref type="formula">(5)</ref>). Define an element-wise operator ⌊·⌋ 0 which truncates negative value to 0. We can rewrite Eq.(5) as min u,z</p><formula xml:id="formula_11">1 2 u T Λu + µ T l(⌊z⌋ 0 )<label>(11)</label></formula><formula xml:id="formula_12">s.t. Au + c = z</formula><p>where l is selected loss function and Λ ∈ R (d+1)×(d+1) is a diagonal matrix. µ ∈ R M1+M2 is a vector whose first M 1 entries are γ 1 and the last M 2 entries are γ 2 α. A ∈ R (M1+M2)×(d+1) is a matrix and c ∈ R M1+M2 is a vector whose values are chosen in the way to resemble the first three sets of linear constraints in Eq.(5) with inequality replaced by equality. Specifically,</p><formula xml:id="formula_13">A =   X V 1 −X V −1 −X E 0   , c =   −ǫ1 − y −ǫ1 + y 1  </formula><p>where X V is a matrix whose rows are data samples with known intensity labels and X E is a matrix whose rows are the difference between two data samples whose frame indices belong to the ordinal set. y is a vector of known intensity labels. 1 and 0 are vectors with proper dimension containing all 1s and 0s respectively.</p><p>Noticing that z is not restricted to be positive in Eq. <ref type="bibr" target="#b10">(11)</ref>, which allows the equality to be attained. However, the objective function remains unaffected due to the negative truncation operator on z. In other words, ⌊z⌋ 0 correspond to the non-negative slack variables η + , η − , ξ introduced in Eq. <ref type="bibr" target="#b4">(5)</ref>. u by definition gives the values of parameter θ. Therefore, the optimal solution to Eq.(11) is the same as that to Eq.(5).</p><p>The augmented Lagrangian has quadratic form with respect to u, z and is linear to v. Even though we have non-differentiable term ⌊z⌋ 0 , we can still compute a simple close-form solution with a compact soft thresholding operator <ref type="bibr" target="#b0">[1]</ref>. We provide the detailed derivation in the supplementary material and only list the main results here. In the hinge loss case, the updates corresponding to Eq.(8)-Eq.(10) are given by</p><formula xml:id="formula_14">u k+1 := [ 1 ρ Λ + A T A] −1 A T (z k − 1 ρ v k − c) (12) z k+1 i := S µ i 2ρ (a i )<label>(13)</label></formula><formula xml:id="formula_15">v k+1 := v k + ρ(Au k+1 − z k+1 + c)<label>(14)</label></formula><p>where a = 1</p><formula xml:id="formula_16">ρ v k + Au k+1 + c − 1 2ρ µ ≡ {a i } and S κ (a i ) =      a i − κ, if a i &gt; κ 0, if |a i | ≤ κ a i + κ, if a i &lt; −κ<label>(15)</label></formula><p>is the soft thresholding operator for some constant κ &gt; 0 and the subscript i is referring to the i th entry in each vector.</p><p>For the squared hinge loss case, updates on u and v remain the same. Update on z is given by</p><formula xml:id="formula_17">z k+1 i := ρai ρ+2µi , if a i ≥ 0 a i , if a i &lt; 0<label>(16)</label></formula><p>where a = 1 ρ v k + Au k+1 + c ≡ {a i }. As the algorithm converges to the optimal solution we have</p><formula xml:id="formula_18">z k+1 − z k → 0 (17) Au k+1 − z k+1 + c → 0<label>(18)</label></formula><p>We stop the updates if the LHS of both Eq.(17) and Eq.(18) become smaller than some tolerance value or reaches the maximum number of iterations. The overall process of ADMM is summarized in Algorithm 1 1 . After solving the optimization problem, we can predict the intensity given new frame x ′ as y ′ = w T x ′ + b.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiment</head><p>We perform intensity estimation under different learning settings depending on the use of ground truth intensity labels. For dataset with complete frame-level intensity annotation, we learn models under fully supervised, <ref type="bibr" target="#b0">1</ref>  update v using Eq.(14) 7: until convergence or reach maximum iteration number 8: return u weakly supervised and unsupervised settings and evaluate them against ground truth intensities (GTI). For dataset without frame-level intensity annotation, we introduce ordinal relationship based relative intensity (RI) as substitution to ground truth. Our models are learned under weakly supervised setting and are evaluated against RI. The overall experiment process is shown in <ref type="figure">Figure 2</ref>. The specific setting for each dataset is listed in <ref type="table" target="#tab_1">Table 1</ref>.</p><p>For each frame, we extract three types of features namely facial landmark points, local binary pattern (LBP) <ref type="bibr" target="#b19">[20]</ref> and Gabor wavelet coefficients <ref type="bibr" target="#b18">[19]</ref>. Specifically, we use In-traFace <ref type="bibr" target="#b29">[30]</ref> to track 66 facial landmark points. Then each frame is aligned by performing an affine transformation such that landmark points connecting two eyes are horizontal. We crop the face location and resize it to 100 × 100 pixels, from where the LBP and Gabor features are extracted. We choose uniform LBP with 8-neighbourhood pixels. The image is divided into 5 equally sized nonoverlapping patches. LBP histograms are extracted from each patch, resulting 1475-dimensional vectors. Gabor features are extracted from the same patches, yielding 1000dimensional vectors. We apply PCA to each type of features separately to keep up to 95% energy. Final feature vector concatenates PCA results of each set of feature. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Datasets</head><p>We select three datasets in our experiment. Some sample sequences from each dataset are shown in <ref type="figure" target="#fig_0">Figure 1</ref>.</p><p>UNBC-McMaster shoulder pain <ref type="bibr" target="#b17">[18]</ref> (PAIN) dataset captures expressions from subjects suffering from shoulder pain. This dataset contains 200 spontaneous expression sequences, and the sequences are FACS coded frame-byframe. The dataset provides pain intensity value calculated using the Prkachin and Solomon pain intensity (PSPI) met-  <ref type="figure">Figure 2</ref>. A diagram showing the experiment process. Depending on the experiment setting, different amounts of intensity annotation information are fed into model learning process, resulting different models. Training is performed using complete expression sequences while testing is performed on each frame of a sequence.</p><p>ric <ref type="bibr" target="#b20">[21]</ref> for every frame. We consider this as ground truth intensity.</p><p>Extended CK <ref type="bibr" target="#b16">[17]</ref> (CK+) dataset contains 593 posed expression sequences from 123 subjects aged from 18 to 30 years old. Each sequence begins with an onset frame and ends at an apex frame. No intensity annotation is provided. Subjects are requested to perform 7 basic categories of expressions: anger, contempt, disgust, fear, happy, sadness and surprise.</p><p>BU-4DFE <ref type="bibr" target="#b31">[32]</ref> dataset records 606 3D dynamic facial sequences (called 4D data) from 101 subjects aged from 18 to 45 years old. Subjects are requested to perform six expressions including anger, disgust, fear, happy, sadness and surprise. Each sequence starts and ends with neutral face and the expression evolves by onset-apex-offset temporal pattern. No intensity annotation is provided.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Experiment on PAIN dataset</head><p>We select 191 out of 200 sequences with total number of frames 6497, excluding one subject whose expressions do not have noticeable pain. The average length of selected sequences is about 240. Each frame is labeled with discrete intensity level from 0 to 15. The vast majority (≈ 81.6%) of the data contains no noticeable pain where the intensity is labeled as 0. On the other hand, severe pain (level≥6) is also rare (≈ 1.4%). In order to make a dataset with more balanced intensity levels, we perform the following pre-processing on each sequence. First, we perform additional quantization on the intensity level by aggregating different intensity levels in the same way as <ref type="bibr" target="#b22">[23]</ref>. Second, we perform an adaptive down-sampling on the entire sequence. If the intensity level remains the same for more than 5 consecutive frames, we choose the first one as representative frame. We find this strategy is effective in both preserving intensity pattern and reducing redundant samples especially the ones with intensity level 0. Finally, we segment the sequences into subsequences where the starting and ending frame has intensity level 0. The final distribution of intensity levels is listed in <ref type="table" target="#tab_3">Table 2</ref>.</p><p>During training phase, we randomly select 60% of sequences as training set and remaining as validation set for the purpose of selecting constant γ 1 , γ 2 . We fix the value of ǫ = 0.1 and ρ = 0.1, which we find insensitive to the final result. During testing phase, we perform leave-one-subjectout test and the results are averaged over all subjects.  <ref type="bibr" target="#b23">[24]</ref> N/A 0.6400 0.8200 SVR <ref type="bibr" target="#b26">[27]</ref> 0.5659 0.5045 0.8538 SVOR <ref type="bibr" target="#b2">[3]</ref> 0.5483 0.3726 0.9366 RVR <ref type="bibr" target="#b12">[13]</ref> 0 We use Pearson correlation coefficient (PCC), intra-class correlation (ICC) and mean absolute error (MAE) as evaluation metrics. PCC measures how well the prediction can capture the trend of intensity change. We use ICC(3,1) as defined in <ref type="bibr" target="#b25">[26]</ref> to measure the consistency within each intensity level. Both PCC and ICC are numbers between 0 and 1 and the larger the better. MAE is a positive number measuring the deviation between prediction and actual value and the smaller the better. We compare our method with RVR <ref type="bibr" target="#b12">[13]</ref>, SVOR <ref type="bibr" target="#b2">[3]</ref>, KCORF <ref type="bibr" target="#b22">[23]</ref>, csCORF <ref type="bibr" target="#b23">[24]</ref> and Rankboost <ref type="bibr" target="#b30">[31]</ref>. The baseline methods SVR and OR are im- plemented by liblinear <ref type="bibr" target="#b6">[7]</ref> and rank-SVM <ref type="bibr" target="#b11">[12]</ref> respectively. The results are shown in <ref type="table" target="#tab_4">Table 3</ref>.</p><p>In fully supervised setting, both OSVR-L1 and OSVR-L2 achieve higher PCC and ICC comparing to SVR, RVR and SVOR. This shows that using additional ordinal information help increase the fitting of the trend of intensity change. The MAE values of OSVR are net effect of two factors. On one hand, the use of intensity label for each frame diminishes the effect of ordinal information and the intensity level distribution is uneven with a large portion of low intensity values. On the other hand, we reinforce ordinal information by encouraging temporal smoothness. Overall, OSVR-L2 achieves the best MAE result. For KCORF, we list the original results reported in <ref type="bibr" target="#b22">[23]</ref> where fixed testing set instead of leave-one-subject-out test is used for experiment. Similarly, original results of csCORF in <ref type="bibr" target="#b23">[24]</ref> are listed. Since the same intensity level annotation is used, we list the results for completeness despite that they are not directly comparable.</p><p>In a weakly supervised setting, on average, 3 out of 34 frame labels per sequence are used for learning. In other words, only 8.8% of total label information is required in such setting. Two variants of OSVR both show noticeable improvement comparing to competing methods. The results under unsupervised setting indicate OSVR are comparable to others. This is because without any intensity label, the regression loss is essentially defective. In addition, some sequences are labeled as 0 for all the frames, which makes apex and onset frames indifferent and thus reduces the effectiveness of pure ordinal information based approach.</p><p>Comparing OSVR learned under different settings, the results are better with more label information available. The advantage of OSVR lies in exploiting absolute intensity labels (if available) and ordinal information. It can adapt to fully supervised and unsupervised settings. In particular, OSVR demonstrates superiority especially under weakly supervised setting. <ref type="figure" target="#fig_1">Figure 3</ref> shows the predicted values by different methods under different settings of some consecutive frames versus ground truth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Experiment on CK+ and BU-4DFE datasets</head><p>For CK+ dataset, we select 327 sequences from 118 subjects, excluding 266 sequences without expression labels. Total number of selected frames is 5876. The first and last frame of each sequence are onset and apex frame respectively. For BU-4DFE dataset, we select 120 sequences from 20 subjects and manually identify the apex frame for each sequence, yielding 2289 frames in total.</p><p>For both datasets, we define the minimum and maximum relative intensity of each sequence as I l = 0 and I h = 10 respectively. In addition, I l is attained at onset or offset frame. I h is attained at apex frame. I l and I h are the same for different sequences so that we can compare intensity values across different subjects. To evaluate the performance of intensity estimation, we artificially assign the intensity label y j of a frame using its relative distance to the corresponding apex frame p of the sequence using <ref type="bibr" target="#b18">(19)</ref> where j = 1, ..., m and m is the length of the sequence. δ is the indicator function. Eq. <ref type="bibr" target="#b18">(19)</ref> essentially produces intensity curve change linearly between onset/offset and apex frames. We perform intensity estimation separately for each type of emotion. One limitation of such relative intensity is ignoring the within class variation of expression intensity. We use the same evaluation metrics, namely PCC, ICC and MAE. For CK+, we perform 10-fold cross subjects test. For BU-4DFE, we perform leave-one-subject-out test. The results are computed using all the testing frames for each type of emotion. For comparison, we use SVR <ref type="bibr" target="#b26">[27]</ref>, RVR <ref type="bibr" target="#b12">[13]</ref>, SVOR <ref type="bibr" target="#b2">[3]</ref>, GPOR <ref type="bibr" target="#b1">[2]</ref>, Rankboost <ref type="bibr" target="#b30">[31]</ref> and OR <ref type="bibr" target="#b9">[10]</ref>. All the methods except Rankboost and OR are trained given relative intensity of key frames. Rankboost and OR are trained using ordinal information only. For evaluation, relative intensity of testing frames are used for all the methods. The results of two datasets are shown in <ref type="figure" target="#fig_3">Figure 4</ref>. We use bar graph for compactness, the exact values of each method are listed in the supplementary material.  In experiment, we found setting temporal smoothness coefficients α = 1 yield slightly better results. Therefore, we report the results obtained with α = 1. For CK+ dataset, one variant of OSVR outperforms the competing methods on all three metrics. In particular, averaging over different expressions, OSVR-L2 improves PCC, ICC and MAE by 3.5%, 3.6% and 5.7% respectively comparing to the best competing method SVR. For BU-4DFE dataset, both variants of OSVR are better than or equal to competing methods on all metrics. Comparing to the second best method, OSVR-L2 improves PCC, ICC and MAE by 2.8%, 4.4% and 33.8% respectively. OSVR-L1 improves PCC and MAE by 1.7% and 24.0% respectively. Although the improvements on PCC and ICC are comparable to CK+, the improvement on MAE is substantial. Considering BU-4DFE as a more challenging case where the expression intensity changes in different ways before and after apex frames, the proposed method achieves both higher PCC, ICC and lower MAE at the same time.</p><formula xml:id="formula_19">y j = j − 1 p − 1 (I h − I l )δ j&lt;p + m − j m − p (I h − I l )δ j≥p</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion and Future Work</head><p>In this work, we formalized a regression problem for frame-level expression intensity estimation. By exploiting the intrinsic ordinal relationship among different frames in an expression sequence, the model learning is compatible with different levels of supervision on expression intensity annotation. This is very useful in case intensity annotation is not available for all the frames. In particular, we use key frames including onset, apex and offset as weak supervision on regression model learning. Results on three different benchmark datasets with different intensity annotation information show that the proposed OSVR method outperforms existing approaches supplied with only key frames' annotation. OSVR can adapt to fully supervised and unsupervised learning setting with comparable performance to other methods. We consider several extensions including introducing kernel to proposed method and adding dynamic information such as expression transition as additional constraints. More sophisticated temporal smoothness scheme can be introduced. We also plan to extend current framework to AU intensity estimation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Sample expression sequences from top: UNBC-McMaster shoulder pain [18], middle: CK+ [17] and bottom: BU-4DFE [32] datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Predicted intensity and annotated intensity on selected consecutive frames for different methods under different annotation settings. Left: Fully supervised setting. Middle: Weakly supervised setting. Right: Unsupervised setting. (Best view in color)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Results on CK+ (left column) and BU-4DFE (right column) datasets using different methods for each emotion type. From top to bottom are PCC, ICC and log(MAE) values (Best view in color). The exact values of the results are listed in the supplementary material.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>Code available from http://bit.ly/OrdinalSVR Algorithm 1 OSVR learning by ADMM Input: X: expression sequences, Y: intensity values, V:</figDesc><table>intensity label set, E: ordinal set 
Output: Model parameters 
1: Construct Λ, µ, A, c using X, Y, V, E 
2: u ← 0, z ← 0, v ← 0 
3: repeat 

4: 

update u using Eq.(12) 

5: 

update z using Eq.(13) or Eq.(16) 

6: 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 1 .</head><label>1</label><figDesc>Different learning and evaluation settings.</figDesc><table>Learning 
Evaluation 
Setting 
Intensity label 
PAIN 
CK+ 
BU-4DFE 
Fully supervised 
All frames GTI All 
frames 
GTI 

All 
frames 
RI 

All 
frames 
RI 

Weakly supervised Key frames GTI 
Unsupervised 
None 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="true"><head>Table 2 .</head><label>2</label><figDesc>Aggregated pain intensity level and the portion of each level over total number of frames</figDesc><table>Quantized 
Annotated 
Pain description Portion (%) 
0 
0 
None 
68.5 
1 
1 
Mild 
10.7 
2 
2 
Discomforting 
8.9 
3 
3 
Distressing 
5.6 
4 
4 ∼ 5 
Intense 
4.1 
5 
6 ∼ 15 
Excruciating 
2.2 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="true"><head>Table 3 .</head><label>3</label><figDesc>Results of different methods on PAIN dataset.</figDesc><table>Setting 
Method 
PCC 
ICC 
MAE 

Fully super-
vised 

KCORF h [23] 
N/A 
0.7030 0.8000 
csCORF wh </table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>This paper was supported in part by a contract from the General Electric and by the National Science Foundation of China (Grant No. 61473270). We thank Dr. Kristin P. Bennett for the discussion on optimization methods.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Distributed optimization and statistical learning via the alternating direction method of multipliers. Foundations and Trends R in Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Peleato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Eckstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Gaussian processes for ordinal regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="1019" to="1041" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">New approaches to support vector ordinal regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Keerthi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Support-vector networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="273" to="297" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Group expression intensity estimation in videos via gaussian processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dhall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goecke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3525" to="3528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Manual for the facial action coding system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ekman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">V</forename><surname>Friesen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1978" />
			<publisher>Consulting Psychologists Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Liblinear: A library for large linear classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R.-E</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1871" to="1874" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Automatic facial expression analysis: a survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fasel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luettin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern recognition</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="259" to="275" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Modeling video evolution for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Oramas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ghodrati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Large margin rank boundaries for ordinal regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Herbrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Graepel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Obermayer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The intensity of emotional facial expressions and decoding accuracy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Hess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Blairy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Kleck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Nonverbal Behavior</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="241" to="257" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Training linear svms in linear time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Continuous pain intensity estimation from facial expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kaltwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Rudovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Visual Computing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Real-time estimation of facial expression intensity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="2567" to="2572" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A unified probabilistic framework for measuring the intensity of spontaneous facial action units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Mavadati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Mahoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FG</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Learning to rank for information retrieval. Foundations and Trends in Information Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="225" to="331" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The extended cohn-kanade dataset (ck+): A complete dataset for action unit and emotion-specified expression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Saragih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ambadar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPRW</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Painful data: The unbc-mcmaster shoulder pain expression archive database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Prkachin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">E</forename><surname>Solomon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FG</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="57" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Coding facial expressions with gabor wavelets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lyons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Akamatsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kamachi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gyoba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FG</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="200" to="205" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multiresolution gray-scale and rotation invariant texture classification with local binary patterns. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ojala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pietikäinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mäenpää</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="971" to="987" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The structure, reliability and validity of pain expression: Evidence from patients with shoulder pain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Prkachin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">E</forename><surname>Solomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pain</title>
		<imprint>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Multi-output laplacian dynamic ordinal regression for facial expression recognition and intensity estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Rudovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pavlovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2634" to="2641" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Automatic pain intensity estimation with heteroscedastic conditional ordinal random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Rudovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pavlovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Visual Computing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Context-sensitive dynamic ordinal regression for intensity estimation of facial action units. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Rudovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pavlovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Regression-based intensity estimation of facial action units. Image and Vision Computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Savran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sankur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Bilge</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">E</forename><surname>Shrout</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Fleiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intraclass correlations: uses in assessing rater reliability</title>
		<imprint>
			<date type="published" when="1979" />
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page">420</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Support vector regression machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A tutorial on support vector regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics and computing</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="199" to="222" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Toward practical smile detection. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Whitehill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Littlewort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Fasel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Movellan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2106" to="2111" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Supervised descent method and its applications to face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>De La</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torre</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="532" to="539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Rankboost with l1 regularization for facial expression recognition and intensity estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A highresolution 3d dynamic facial expression database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Worm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Reale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FG</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
