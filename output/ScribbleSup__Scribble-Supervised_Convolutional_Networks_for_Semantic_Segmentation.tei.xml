<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ScribbleSup: Scribble-Supervised Convolutional Networks for Semantic Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Lin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese Univeristy of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese Univeristy of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">ScribbleSup: Scribble-Supervised Convolutional Networks for Semantic Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:45+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Large-scale data is of crucial importance for learning semantic segmentation models, but annotating per-pixel masks is a tedious and inefficient procedure. We note that for the topic of interactive image segmentation, scribbles are very widely used in academic research and commercial software, and are recognized as one of the most userfriendly ways of interacting. In this paper, we propose to use scribbles to annotate images, and develop an algorithm to train convolutional networks for semantic segmentation supervised by scribbles. Our algorithm is based on a graphical model that jointly propagates information from scribbles to unmarked pixels and learns network parameters. We present competitive object semantic segmentation results on the PASCAL VOC dataset by using scribbles as annotations. Scribbles are also favored for annotating stuff (e.g., water, sky, grass)  that has no well-defined shape, and our method shows excellent results on the PASCAL-CONTEXT dataset thanks to extra inexpensive scribble annotations. Our scribble annotations on PASCAL VOC are available at</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recent success of semantic segmentation lies on the end-to-end training of convolutional networks (e.g., <ref type="bibr" target="#b20">[21]</ref>) and large-scale segmentation annotations (e.g., <ref type="bibr" target="#b17">[18]</ref>). Even though semantic segmentation models are being improved rapidly, large-scale training data still have apparent benefits for accuracy, as evidenced in <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b19">20]</ref>.</p><p>But it is painstaking to annotate precise, mask-level labels on large-scale image data. For example, powerful interactive tools <ref type="bibr" target="#b0">[1]</ref> are adopted for annotating the MS COCO dataset <ref type="bibr" target="#b17">[18]</ref>, but it still takes minutes for an experienced annotator labeling one image <ref type="bibr" target="#b17">[18]</ref>. The interactive tools of <ref type="bibr" target="#b0">[1]</ref> require annotators to draw polygons along object boundaries, and the number of polygon vertexes in an image can * This work was done when Di Lin was an intern at Microsoft Research. be a few tens. This time-consuming task may limit the amount of data that have mask-level labels.</p><p>The procedure of annotating segmentation masks is very similar to interactive image segmentation <ref type="bibr" target="#b3">[4]</ref>, which has been a widely studied problem in the past decade <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b5">6]</ref>. Among various forms of user interactions, scribbles are particularly popular <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b5">6]</ref> and have been recognized as a user-friendly way of interacting. A commercial implementation of scribble-based interactive segmentation is the Quick Selection 1 tool in Adobe Photoshop, which is a prevalent tool for selecting complicated regions in user photos.</p><p>Driven by the popularity of using scribbles for interactive image segmentation, we believe that it is more efficient to use scribbles to annotate images. By dragging the cursor in the center of the objects, the annotators need not carefully outline the object boundaries. It is also easier to use scribbles to annotate "stuff" (water, sky, grass, etc.) that may have ambiguous boundaries and no well-defined shape. <ref type="figure" target="#fig_0">Fig. 1</ref> shows an example of scribble annotations.</p><p>An obvious way of using scribbles to ease annotating is to perform interactive image segmentation (e.g., Quick Selection) when the annotators are interacting. But we argue that to obtain precise masks that play as ground truth, the annotators may still have to do many "touch-ups" on the imperfect outcome, which are inefficient. Instead of doing so, we consider to directly use the sparse scribbles for training semantic segmentation models. The annotators just need to provide a few scribbles on the regions which they feel confident and easy to draw.</p><p>With this scenario, in this paper we develop an algorithm that exploits scribble annotations to train convolutional networks for semantic segmentation. This problem belongs to the category of weakly-supervised learning, and occupies a middle ground between image-level supervision and boxlevel supervision. Comparing with image-level annotations <ref type="bibr" target="#b23">[24]</ref>, scribbles provide location information at a few pixels, which should lead to better results; comparing with boxlevel annotations <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b6">7]</ref>, scribbles are lack of determinate bounds of objects and so are more ambiguous.</p><p>We tackle scribble-supervised training by optimizing a graphical model. The graphical model propagates the information from the scribbles to the unmarked pixels, based on spatial constraints, appearance, and semantic content. Meanwhile, a fully convolutional network (FCN) <ref type="bibr" target="#b20">[21]</ref> is learned, which is supervised by the propagated labels and in turn provides semantic predictions for the graphical model. We formulate this model as a unified loss function, and develop an alternating method to optimize it.</p><p>To evaluate our method, we use the Amazon Mechanical Turk platform to annotate scribbles on the PASCAL VOC datasets. Using training images with sparse scribble annotations, our method shows reasonable degradation compared to the strongly-supervised (by masks) counterpart on PASCAL VOC 2012. Compared with other weakly-supervised methods that are based on image-level or box-level annotations, our scribble-supervised method has higher accuracy. Furthermore, by exploiting inexpensive scribble annotations on PASCAL VOC 2007 (which has no mask annotations), our method achieves higher accuracy on the PASCAL-CONTEXT dataset (that involves objects and stuff) than previous methods that are not able to harness scribbles. These results suggest that in practice scribble annotations can be a cost-effective solution, and accuracy can be increased by larger amount of inexpensive training data. Our scribble annotations on PASCAL VOC are available at http://research.microsoft.com/en-us/ um/people/jifdai/downloads/scribble_sup.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Scribble-Supervised Learning</head><p>An annotated scribble (e.g., <ref type="figure" target="#fig_0">Fig. 1</ref>) is a set of pixels with a category label. The scribbles are sparsely provided, and the pixels that are not annotated are considered as unknown. Our training algorithm uses a set of images with annotated scribbles, and trains fully convolutional networks <ref type="bibr" target="#b20">[21]</ref> for semantic segmentation.</p><p>We argue that scribble-based training is more challenging than previous box-based training <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b6">7]</ref>. A box annotation can provide determinate bounds of the objects, but scribbles are most often labeled on the internal of the ob-jects. In addition, box annotations imply that all pixels outside of the boxes are not of the concerned categories. This implication is not available for scribbles. In the case of scribbles, we need to propagate information from the scribbles to all other unknown pixels.</p><p>Our training algorithm accounts for two tasks. For the first task, our training algorithm propagates the semantic labels from the scribbles to other pixels and fully annotates the images; for the second task, our training algorithm learns a convolutional network for semantic segmentation. These two tasks are dependent on each other. We formulate the training as optimizing a unified loss function that has two terms. The dependency between the two tasks is made explicit in an alternating training solution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Objective Functions</head><p>We use a graphical model to propagate information from scribbles to unknown pixels. We build a graph on the superpixels of a training image. A vertex in the graph represents a super-pixel, and an edge in the graph represents the similarity between two super-pixels (see <ref type="figure" target="#fig_1">Fig. 2</ref>). We use the method of <ref type="bibr" target="#b9">[10]</ref> to generate super-pixels.</p><p>We denote a training image as X, and its set of nonoverlapping super-pixels as {x i } satisfying i x i = X and x i x j = ∅, ∀i, j. The scribble annotations of this image are S = {s k , c k } where s k is the pixels of a scribble k and 0 ≤ c k ≤ C is the scribble's category label (assuming there are C categories and c k = 0 for background). For a superpixel x i , we want to find a category label 0 ≤ y i ≤ C. The set of {y i } is denoted as Y . The labeling Y provides full annotations of the image. Our objective function is:</p><formula xml:id="formula_0">i ψ i (y i |X, S) + i,j ψ ij (y i , y j |X),<label>(1)</label></formula><p>where ψ i is a unary term involving the super-pixel x i , and ψ ij is a pairwise term involving a pair of super-pixels x i and x j . Formulation in this form is widely used for interactive image segmentation <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b18">19]</ref>. In our scenario, the unary term ψ i has two parts. The first part is based on scribbles and denoted as ψ scr i . We define this term as follows:</p><formula xml:id="formula_1">ψ scr i (y i ) =      0 if y i = c k and x i s k = ∅ − log( 1 |{c k }| ) if y i ∈ {c k } and x i S = ∅ ∞ otherwise</formula><p>(2) In this formulation, the first condition means that if a superpixel x i overlaps with a scribble s k , then it has zero cost when being assigned the label c k . In the second condition, if a super-pixel x i does not overlap with any scribble, it can be assigned any label annotated on this image with equal probability, but must not be assigned to those categories that are absent in this image. Here |{c k }| denotes the number of categories annotated for this image. This exclusive information is useful for reducing false positive predictions.</p><p>The second part of the unary term respects the output of a fully convolutional network. We denote this unary term as ψ net i , and define it as:</p><formula xml:id="formula_2">ψ net i (y i ) = − log P (y i |X, Θ),<label>(3)</label></formula><p>Here Θ represents the parameters of the network. log P (y i |X, Θ) denotes the log probability of predicting x i to have the label y i . It is simply the summation of the pixelwise log probability of all pixels in the super-pixel x i . The two parts of the unary terms are illustrated in <ref type="figure" target="#fig_1">Fig. 2</ref>. The unary term ψ i is simply ψ scr + ψ net , for which an implicit balance weight of 1 is used. The pairwise term ψ ij in Eqn.(1) models the similarity between two super-pixels. We only adopt a pairwise term to adjacent super-pixels. We consider simple appearance similarities for adjacent super-pixels. We build color and texture histograms for x i . The color histogram h c (x i ) on x i is built on the RGB space using 25 bins for each channel. The texture histogram h t (x i ) is built on the gradients at the horizontal and the vertical orientations, where 10 bins are used for each orientation. All bins are concatenated and normalized in the color/texture histograms respectively. We define the pairwise term as:</p><formula xml:id="formula_3">ψ ij (y i , y j |X) =[y i = y j ] exp − h c (x i ) − h c (x j ) 2 2 δ 2 c − h t (x i ) − h t (x j ) 2 2 δ 2 t .</formula><p>(4) Here [·] is 1 if the argument is true and 0 otherwise. The parameters δ c and δ t are set as 5 and 10 respectively. This definition means that for the adjacent super-pixels assigned different labels, the cost is higher if their appearance is closer.</p><p>With these definitions, we have the optimization problem in this form:</p><formula xml:id="formula_4">i ψ scr i (y i |X, S)+ i − log P (y i |X, Θ)+ i,j ψ ij (y i , y j |X),<label>(5)</label></formula><p>where there are two sets of variables to be optimized: Y = {y i } for labeling all super-pixels, and Θ for the fully convolutional network's parameters. Eqn.(5) is for one training image. The total loss function sums over the loss functions for all training images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Optimization</head><p>We present an alternating solution to optimize the above loss function. We fix Θ and solve for Y , and vice versa. The two alternating steps have clear intuitions: (i) with Θ fixed, the solver propagates labels to unmarked pixels, based on scribbles, appearance, and also network predictions; (ii) with Y fixed, the solver learns a fully-convolutional network for pixel-wise semantic segmentation.</p><p>Propagating scribble information to unmarked pixels. With Θ fixed, the unary term of ψ i = ψ scr i + ψ net i can be easily evaluated by enumerating all possible labels 0 ≤ y i ≤ C. The pairwise terms can be pre-computed as a look-up table. With these evaluated unary and pairwise terms, the optimization problem in Eqn. <ref type="bibr" target="#b4">(5)</ref> can be solved by the graph cuts solution <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b1">2]</ref>. We use the multi-label graph cuts solver in  <ref type="figure">Figure 3</ref>. The propagated labels of a training image during the training process. Because of the network predictions that are aware of the high-level semantics, the propagated labels become more accurate.</p><p>[2] with publicly available code 2 . This solver assigns a category label 0 ≤ y i ≤ C for each super-pixel x i , and thus effectively propagate information to all unmarked pixels.</p><p>Optimizing network parameters. With the labeling Y fixed for all super-pixels, solving for the network parameters Θ is equivalent to optimizing a network using the full-image annotation Y as the supervision. In this paper, we adopt the FCN <ref type="bibr" target="#b20">[21]</ref> as our network model. Given the the labeling Y , every pixel in the training image has been assigned a category label. So the FCN is directly trainable for handling this pixel-wise regression problem. The last layer of FCN outputs the per-pixel log probability, which are used to update the unary term in the graph.</p><p>The alternating solution is initialized from the graph cut step without using network prediction, and then iterates between the two steps. For every network optimizing step, we fine-tune the network with a learning rate of 0.0003 for 50k mini-batches and 0.0001 for the next 10k mini-batches, using a mini-batch size of 8. The network is (re-)initialized by an ImageNet <ref type="bibr" target="#b27">[28]</ref> pre-trained model (e.g., VGG-16 <ref type="bibr" target="#b29">[30]</ref>) at the beginning of every network optimizing step. We have also experimented with going on training with the network parameters learned by the last iteration, and observed slightly poorer accuracy. We conjecture that this is because when the labels are not reliable, the network parameters might be tuned to a poorer local optimum. <ref type="figure">Fig. 3</ref> shows the evolution of the propagated labels on a training image. With the network being updated, the sematic information from the network becomes more reliable, and the propagated labels are getting more accurate. These propagated labels in turn improve network learning. We empirically find that the alternating algorithm converges after three iterations, and more iterations have negligible improvements.</p><p>2 http://vision.csd.uwo.ca/code/gco-v3.0.zip For inference, we only need to apply the FCN model (represented by Θ) on the image. We note that the superpixels and their graphical model are for training only, and are not needed for inference. Following <ref type="bibr" target="#b4">[5]</ref>, we adopt a CRF to post-process the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Related Work</head><p>Graphical models for segmentation. Graphical models are widely used for interactive image segmentation <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b18">19]</ref> and semantic segmentation <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14]</ref>. Graphical models in general involves unary and pairwise terms, and are particularly useful for modeling local and global spatial constraints. Interestingly, FCN <ref type="bibr" target="#b20">[21]</ref>, as one of the recent most successful method for semantic segmentation, performs pixel-wise regression and thus only has explicit unary terms. But graphical models (e.g., CRF/MRF) are later developed for FCN as post-processing <ref type="bibr" target="#b4">[5]</ref> or joint training <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b19">20]</ref>. The graphical models in <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b19">20]</ref> are developed for strongly-supervised training and their effects are mainly in improving mask boundaries <ref type="bibr" target="#b4">[5]</ref>, whereas our graphical model plays a more important role in our scenario for propagating information into the unknown pixels of the training images. The graphical models in <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b19">20]</ref> operates on pixels, in contrast to ours that operates on superpixels. Pixel-based models are preferred for refining boundaries, but our super-pixel-based model can propagate labels more easily into regions that are distant from the scribbles.</p><p>Weakly-supervised semantic segmentation. There have been a series of methods <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b26">27]</ref> on weakly-supervised learning CNN/FCN for semantic segmentation. Image-level annotations are more easily to obtain, but semantic segmentation accuracy <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b23">24]</ref> by using only image-level labels lags far behind strongly-supervised results. Results based on box-level annotations <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b6">7]</ref> are much closer to stronglysupervised ones. Alternating solvers are adopted in <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b6">7]</ref> for addressing box-supervised training, as is also used by our method. But box annotations provide the object bounds and confident background regions, so it is not demanded for the box-supervised training to propagate information. Pairwise terms as ours are not considered in <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b6">7]</ref>, and graphical models are not used in their weakly-supervised training (except for CRF refinements). On the contrary, we will show by experiments that information propagation is influential in our scribble-supervised case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Annotating Scribbles</head><p>To evaluate our method, we use the Amazon Mechanical Turk (AMT) platform to obtain scribble annotations on the PASCAL VOC datasets. We annotate the PASCAL VOC 2012 set <ref type="bibr" target="#b8">[9]</ref> that involves 20 object categories and one background category, and the PASCAL-CONTEXT dataset <ref type="bibr" target="#b21">[22]</ref> that involves 59 categories of objects and stuff. We further annotate the PASCAL VOC 2007 set using the 59 categories (which include the 20 object categories). We note that the 2007 set has no available mask-level annotations. So although the scribble annotations for the 2012 and CON-TEXT sets are mainly for the investigation purpose (as these sets have available masks), the scribble annotations for the 2007 set can be actually exploited to improve results.</p><p>Our scribble annotations were labeled by 10 annotators from the AMT platform. Each image is annotated by one annotator, and is checked by another annotator (if necessary, to add missing scribbles and modify imprecise scribbles). The annotators are asked to draw scribbles on the regions which they feel confident and easy to draw; the object boundaries or ambiguous regions are not needed to annotated. However, we require that every existing object (of the related categories) in an image must be labeled by at least one scribble, so missing objects are not allowed (verified by the checker annotator). According to our record, it takes an annotator on average 25 seconds to label an image with 20 object categories, and 50 seconds with 59 object/stuff categories. Compared with annotating per-pixel masks that takes several minutes per image <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b17">18]</ref>, the annotation effort by scribbles is substantially reduced. <ref type="figure" target="#fig_2">Fig. 4</ref> shows some annotated scribbles. The scribbles have an average length of ∼ 70% of the longer side of the object's bounding box.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Experiments on PASCAL VOC 2012</head><p>Following <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b6">7]</ref>, we train the models on the 10,582 (denoted as 11k) training images <ref type="bibr" target="#b11">[12]</ref> and evaluate on the 1,449 validation images. The accuracy is evaluated by the mean Intersection-over-Union (mIoU) score. We adopt the DeepLab-MSc-CRF-LargeFOV <ref type="bibr" target="#b4">[5]</ref> as our strongly(mask)supervised baseline (using VGG-16 <ref type="bibr" target="#b29">[30]</ref>). Our implementation has 68.5% mIoU score, reasonably close to 68.7% reported in <ref type="bibr" target="#b4">[5]</ref>. This network architecture also serves as our FCN structure in our scribble-supervised models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Strategies of utilizing scribbles</head><p>Our method jointly propagates information into unmarked pixels and learns network parameters. A simpler solution is to first use any existing interactive image segmentation method to generate masks based on scribbles, and then use these masks to train FCNs. In <ref type="table">Table 1</ref> we compare with this two-step solution.</p><p>We investigate two popular interactive image segmentation algorithms for generating masks from scribbles: Grab-Cut 3 <ref type="bibr" target="#b25">[26]</ref> and LazySnapping <ref type="bibr" target="#b16">[17]</ref>. In our scenario, the difference between <ref type="bibr" target="#b25">[26]</ref> and <ref type="bibr" target="#b16">[17]</ref> lies on their definitions of the unary and pairwise terms. Training FCNs using the masks  generated by these methods shows inferior semantic segmentation accuracy <ref type="table">(Table 1</ref>, all post-processed by CRFs of <ref type="bibr" target="#b4">[5]</ref>). This is because the terms in these traditional methods <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b16">17]</ref> only model low-level color/sptial information and are unaware of semantic content. The generated masks are not reliable "ground-truth" for training the networks. On the contrary, our scribble-supervised method achieves a score of 63.1%, about 10% higher than the twostep solutions. This is because when the network gradually learns semantic content, the high-level information can help with the graph-based scribble propagation. This behavior is shown in <ref type="figure">Fig. 3</ref>: at the beginning of training when the network is not reliable, the propagated labels tend to rely on low-level color/texture similarities; but the erroneous propagation can be corrected with the network feedback.</p><p>In <ref type="table">Table 1</ref> we also show that the pairwise term is important for propagating information. We experiment with a case without using the pairwise terms, which leads to a lower score of 60.5%. In this case, the unmarked pixels are ambiguous at initialization (as their cost of taking any available label is equal), so we initialize them by the background label. Without the pairwise term, the problem in Eqn. <ref type="bibr" target="#b4">(5)</ref> only involves unary terms, and the graph cuts step actually degrades to winner-take-all selection of labels based on network predictions. As such, the information propagation is only performed by the "fully convolutional" behavior (sliding a large receptive field) that may implicitly impose local coherency. But this way of propagation is insufficient as shown by the lower accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sensitivities to scribble quality</head><p>The quality of the annotated scribbles is subject to the behavior and experience of the annotators. So we investigate how sensitive our method is to the scribble quality. Because method annotations mIoU (%) MIL-FCN <ref type="bibr" target="#b24">[25]</ref> image-level 25.1 WSSL <ref type="bibr" target="#b23">[24]</ref> image-level 38.2 point supervision <ref type="bibr" target="#b26">[27]</ref> spot 46.1 WSSL <ref type="bibr" target="#b23">[24]</ref> box 60.6 BoxSup <ref type="bibr" target="#b6">[7]</ref> box 62.0 ours spot 51.6 ours scribble 63.1 <ref type="table">Table 3</ref>. Comparisons of weakly-supervised methods on the PAS-CAL VOC 2012 validation set, using different ways of annotations. All methods are trained on the PASCAL VOC 2012 training images using VGG-16, except that the annotations are different.</p><p>the "quality" of scribbles is somehow subjective and using different sets of scribbles requires extra annotation efforts, we use synthesized scribbles based on the user annotations. We focus on the length of the scribbles. Given any user annotated scribble, we reduce the length of this scribble by a ratio. For a shortened scribble, one of its end-points is randomly chosen from the original scribble, and the other endpoint is determined by the reduced length. <ref type="figure" target="#fig_3">Fig. 5</ref> shows examples of the shortened scribbles, where a ratio of 0 means a spot is used. <ref type="table" target="#tab_2">Table 2</ref> shows the results of our scribble-supervised algorithm using scribbles of different lengths. Our method performs gracefully when the scribbles are shortened, suggesting that our method is reasonably robust to the quality of scribbles. To the extreme, when the length approaches 0 and the scribbles become spots, our method is still applicable and has a score of 51.6%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparisons with other weakly-supervised methods</head><p>In <ref type="table">Table 3</ref> we compare with other weakly-supervised methods using different ways of annotations. We note that while image-level annotations are the most economical, their weakly-supervised learning accuracy (e.g., 38.2% of WSSL <ref type="bibr" target="#b23">[24]</ref>) lags far behind other ways of annotations. On the other hand, our method achieves accuracy on par with box-supervised methods (60.6% of of WSSL <ref type="bibr" target="#b23">[24]</ref> and 62.0% of BoxSup <ref type="bibr" target="#b6">[7]</ref>), indicating that scribbles can be well exploited as a user-friendly alternative to boxes. Comparing with a recent point-supervised method <ref type="bibr" target="#b26">[27]</ref> (46.1%), our spot-only result (51.6%) is over 5% higher (but we note that the our annotations are different from those used in <ref type="bibr" target="#b26">[27]</ref>). method data/annotations mIoU (%) CFM <ref type="bibr" target="#b7">[8]</ref> 5k w/ masks 34.4 FCN <ref type="bibr" target="#b20">[21]</ref> 5k w/ masks 35.1 Boxsup <ref type="bibr" target="#b6">[7]</ref> 5k w/ masks + 133k w/ boxes (COCO+VOC07) 40.5 baseline 5k w/ masks 37.7 ours, weakly 5k w/ scribbles 36.1 ours, weakly 5k w/ scribbles + 10k w/ scribbles <ref type="bibr">(VOC07)</ref> 39.3 ours, semi 5k w/ masks + 10k w/ scribbles (VOC07) 42.0  <ref type="table">Table 4</ref>. Comparisons of our method using different annotations on the PASCAL VOC 2012 validation set. The term "total" shows the number of training images, "# w/ masks" shows the number of training images with mask-level annotations, and "# w/ scribbles" shows the number of training images with scribble annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparisons with using masks</head><p>In <ref type="table">Table 4</ref> we compare our weakly-supervised results based on scribbles and strongly-supervised results based on masks. When replacing all mask-level annotations with scribbles, our method has a degradation of about 5 points. We believe this is a reasonable gap considering the challenges of exploiting scribbles.</p><p>Our method can also be easily generalized to semisupervised learning that uses both mask annotations and scribble annotations. For the training images that have mask-level annotations, the graphical model is not applied, and these images only contribute to the network training step. To evaluate our semi-supervised results, we use the mask-level annotations on the PASCAL VOC 2012 set, and the scribble annotations on the PASCAL VOC 2007 set (that has no available mask annotation). Our semi-supervised result is a score of 71.3%, showing a gain of 2.8% higher than the baseline. This gain is due to the extra scribble annotations from the 2007 set. As a comparison, <ref type="bibr" target="#b23">[24]</ref> reports a strongly-supervised result of 71.7% on this validation set using extra 123k COCO <ref type="bibr" target="#b17">[18]</ref> images with masks. Our semisupervised result is on par with their strongly-supervised result, but we only use 10k VOC 2007 scribble-level annotations as the extra data. <ref type="figure" target="#fig_4">Fig. 6</ref> shows some results.</p><p>We further evaluate our method on the PASCAL VOC 2012 test set. By semi-supervised training using the masklevel annotations of the PASCAL VOC 2012 train and validation sets, as well as the scribble annotations on the VOC 2007 set, the trained model has a score of 73.1%. This number is behind but close to the current state-of-the-art results on the test set 4 , without using the 123k COCO data. The <ref type="bibr" target="#b3">4</ref> Using only the VOC data, the strongly-supervised methods of <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b19">20]</ref> competitive accuracy of our method suggests that using inexpensive scribbles to increase the data size can be a practical solution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Experiments on PASCAL-CONTEXT</head><p>We perform more experiments on the PASCAL-CONTEXT dataset <ref type="bibr" target="#b21">[22]</ref> with 59 categories. The images of this dataset are fully annotated by <ref type="bibr" target="#b21">[22]</ref>, providing pixellevel masks on objects and stuff. We evaluate on the 5k validation images, and train the models on the 5k training images or extra data. We note that scribbles are particularly favored for annotating stuff, and annotating precise outlines of stuff can be more time-consuming than objects. <ref type="table" target="#tab_3">Table 5</ref> compares the results.</p><p>Our reproduced strongly-supervised baseline has a score of 37.7% on this dataset. Our weakly-supervised method based on scribbles has a score of 36.1%, showing a graceful degradation of 1.6 point. This gap is smaller than that for PASCAL VOC 2012 ( <ref type="table">Table 4</ref>), suggesting that it is easier to propagate stuff labels as stuff in general has uniform appearance. When using the extra (object+stuff) scribbles annotated on the PASCAL VOC 2007 set, our weakly-supervised result is boosted to 39.3%, demonstrating the effects of exploiting more training data (which are yet efficient to annotate). Finally, our semi-supervised method that uses the provided 5k mask-level annotations and extra 10k VOC 2007 scribble-level annotations achieves a score of 42.0%. This number compares favorably with BoxSup <ref type="bibr" target="#b6">[7]</ref> (40.5%) that exploits extra 133k box-level annotations (×10 of ours). We note that box-level annotations are only applicable for objects, but is not for stuff. So even though BoxSup uses much larger amount of data than ours, its accuracy is lower than ours on this dataset that involves stuff. To the best of our knowledge, our accuracy is the current state of the art on this dataset. <ref type="figure" target="#fig_6">Fig. 7</ref> shows some example results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusion and Future Work</head><p>We have presented a weakly-supervised method for semantic segmentation based on scribbles. Our method optihave test set accuracy of over 74.8% and 74.1%. The improvements of <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b19">20]</ref> are orthogonal to our method, and these strongly-supervised methods can be swapped into our system, in place of the FCN baseline.  <ref type="table">Table 4</ref>.   <ref type="table" target="#tab_3">Table 5</ref>. mizes a graphical model for propagating information from scribbles. Although our formulation is motivated by the usage of scribbles, it is applicable for many other types of weak supervision including box-level or image-level annotations. We plan to investigate these issues in the future.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>An image annotated with per-pixel masks (middle) and sparse scribbles (right). Scribbles are more user-friendly for annotators.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Overview of our graphical model. The graph is built on the super-pixels of the training image. The scribble-based unary term imposes constraints from the user annotations. The networkbased unary term respects the predictions from a fully convolutional network. The pairwise terms are used to propagate information into the unmarked pixels. In this figure, the super-pixels are for the illustration purpose and are not their actual shapes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Examples of annotated scribbles via the crowdsourcing AMT platform. Left: PASCAL VOC 2012 that has 20 object categories. Right: PASCAL-CONTEXT that has 59 categories of objects and stuff.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Scribbles of different lengths for investigating our method's sensitivities to scribble quality. See also</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>Our results on the PASCAL VOC 2012 validation set. The training data and annotations are: (c) 11k images with masks on VOC 2012; (d) 11k images with scribbles on VOC 2012; (e) 11k images with masks on VOC 2012 and 10k images with scribbles on VOC 2007. See also</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .</head><label>7</label><figDesc>Our results on the PASCAL-CONTEXT validation set. The training data and annotations are: (c) 5k images with masks on PASCAL-CONTEXT; (d) 5k images with scribbles on PASCAL-CONTEXT; (e) 5k images with masks on PASCAL-CONTEXT and 10k images with scribbles on VOC 2007. See also</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 2 .</head><label>2</label><figDesc></figDesc><table>length ratio 
mIoU (%) 
1 
63.1 
0.8 
61.8 
0.5 
58.5 
0.3 
54.3 
0 (spot) 
51.6 

Table 2. Sensitivities to scribble length evaluated on the PASCAL 
VOC 2012 validation set. The shorter scribbles are synthesized 
from the annotated scribbles, reducing their length by a ratio. A 
ratio of 0 means spots are used. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="true"><head>Table 5 .</head><label>5</label><figDesc>Comparisons on the PASCAL-CONTEXT validation set.</figDesc><table>supervision # w/ masks # w/ scribbles total mIoU (%) 
weakly 
-
11k 
11k 
63.1 
strongly 
11k 
-
11k 
68.5 
semi 
11k 
10k (VOC07) 
21k 
71.3 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://helpx.adobe.com/photoshop/using/ making-quick-selections.html</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Although GrabCut used boxes as it was originally developed, its definitions of unary/pairwise terms can be directly applied with scribbles.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>This work is in part supported by a grant from the Research Grants Council of the Hong Kong SAR (project No. 413113).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Opensurfaces: A richly annotated catalog of surface appearance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Upchurch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">An experimental comparison of min-cut/max-flow algorithms for energy minimization in vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Boykov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kolmogorov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Fast approximate energy minimization via graph cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Boykov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Veksler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zabih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Interactive graph cuts for optimal boundary &amp; region segmentation of objects in nd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Y</forename><surname>Boykov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-P</forename><surname>Jolly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Semantic image segmentation with deep convolutional nets and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sharp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pérez</surname></persName>
		</author>
		<title level="m">Geodesic image and video editing. SIGGRAPH</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Boxsup: Exploiting bounding boxes to supervise convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Convolutional feature masking for joint object and stuff segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<title level="m">The PASCAL Visual Object Classes (VOC) Challenge. IJCV</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Efficient graphbased image segmentation. IJCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Huttenlocher</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Random walks for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Grady</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Semantic contours from inverse detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Robust higher order potentials for enforcing label consistency. IJCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="302" to="324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Efficient inference in fully connected crfs with gaussian edge potentials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">C</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">A closed form solution to natural image matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Lazy snapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-K</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Paint selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIG-GRAPH</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page">69</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Semantic image segmentation via deep parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The role of context for object detection and semantic segmentation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N.-G</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning deconvolution network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Weakly-and semi-supervised learning of a dcnn for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Fully convolutional multi-class multiple instance learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.7144</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Grabcut: Interactive foreground extraction using iterated graph cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kolmogorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">What&apos;s the point: Semantic segmentation with point supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Bearman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-F</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.02106</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0575</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Imagenet large scale visual recognition challenge</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Textonboost: Joint appearance, shape and context modeling for mulit-class object recognition and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jayasumana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
		<title level="m">Conditional random fields as recurrent neural networks. In ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
