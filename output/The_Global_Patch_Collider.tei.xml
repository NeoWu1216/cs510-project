<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">The Global Patch Collider</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenlong</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Toronto</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><forename type="middle">Ryan</forename><surname>Fanello</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Toronto</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Rhemann</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Toronto</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahram</forename><surname>Izadi</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Toronto</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Toronto</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">The Global Patch Collider</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:47+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper proposes a novel extremely efficient, fullyparallelizable, task-specific algorithm for the computation of global point-wise correspondences in images and videos. Our algorithm, the Global Patch Collider, is based on detecting unique collisions between image points using a collection of learned tree structures that act as conditional hash functions. In contrast to conventional approaches that rely on pairwise distance computation, our algorithm isolates distinctive pixel pairs that hit the same leaf during traversal through multiple learned tree structures. The split functions stored at the intermediate nodes of the trees are trained to ensure that only visually similar patches or their geometric or photometric transformed versions fall into the same leaf node. The matching process involves passing all pixel positions in the images under analysis through the tree structures. We then compute matches by isolating points that uniquely collide with each other ie. fell in the same empty leaf in multiple trees. Our algorithm is linear in the number of pixels but can be made constant time on a parallel computation architecture as the tree traversal for individual image points is decoupled. We demonstrate the efficacy of our method by using it to perform optical flow matching and stereo matching on some challenging benchmarks. Experimental results show that not only is our method extremely computationally efficient, but it is also able to match or outperform state of the art methods that are much more complex.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Correspondence estimation ie. the task of estimating how parts of visual signals (images or volumes) correspond to each other, is an important and challenging problem in Computer Vision. Point-wise correspondences between images or 3D volumes can be used for tasks such as camera pose estimation, multi-view stereo, structure-from-motion, co-segmentation, retrieval, and compression etc. Due to its wide applicability, many variants of the general correspondence estimation problem like stereo and optical flow have been extensively studied in the literature.</p><p>There are two key challenges in matching visual content across images or volumes. First, robust modelling of the photometric and geometric transformations present in real-world data, such as occlusions, large displacements, viewpoints, shading, and illumination change. Secondly, and perhaps more importantly, the hardness of performing inference in the above-mentioned model. The latter stems from the computational complexity of performing search in the large space of potential correspondences and is a major impediment in the development of real time algorithms. A popular approach to handle the problem involves detecting 'interest or salient points' in the image which are then matched based on measuring the euclidean distance between hand specified <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b6">7]</ref> or learned <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b30">31]</ref> descriptors that are designed to be invariant to certain classes of transformations and in some cases can also work across different modalties. While these methods generate accurate matches, the computational complexity (quadratic in the number of interest points) of matching potential interest points restricts their applicability to small number of key-points.</p><p>An effective strategy to generate dense correspondences is to limit the search space of possible correspondences. For instance, in the case of optical flow by only searching for matches in the immediate vicinity of the pixel location. However, this approach fails to detect large motions/displacements. Methods like <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b21">22]</ref> overcome this problem by adaptively sampling the search space and have been shown to be very effective for optical flow and disparity estimation <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4]</ref>. However, they rely on the implicit assumption that the correspondence field between images is smooth and fail when this assumption is violated. Techniques based on algorithms for finding approximate nearest neighbors such as KD-Tree <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b1">2]</ref> and hashing <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b8">9]</ref> can be used to search large-displacement correspondences and have been used for initializing optical flow algorithms <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b24">25]</ref>. However, these approaches search for candidate matches based on the appearance similarity and they are not robust in scenarios when geometric and photometric transformations occurs (see <ref type="figure" target="#fig_2">Fig. 2</ref>).</p><p>In this paper, we address the problem of efficiently generating correspondences that can (1) have arbitrary distribution of magnitudes, <ref type="bibr" target="#b1">(2)</ref> and that are between image elements affected by task-dependent geometric and photometric transformations. We propose a novel fullyparallelizable, learned matching algorithm called Global Patch Collider (GPC) to enable extremely efficient computation of global point-wise correspondences. GPC is based on detecting unique collisions between image points using a collection of learned tree structures that act as conditional hash functions. In contrast to conventional approaches that isolate matches by computing distances between pairs of image elements, GPC detects matches by finding which pixel pairs hit the same leaf during traversal through multiple learned tree structures.</p><p>The split functions stored at the intermediate nodes of the trees are trained to ensure that visually similar patches fall into the same terminal node. The matching process involves passing all pixel positions in the images under analysis through the tree structures. We then compute matches by isolating points that uniquely collide with each other ie. fell in the same empty leaf in multiple trees. We also incorporate a multi-scale top-bottom architecture, which significantly reduces the number of outliers. Content-aware motion patterns are learned for each leaf node, in order to increase the recall of the retrieved matches.</p><p>Unlike existing feature matching algorithms, the proposed global patch collider does not require any pairwise comparisons or key-point detection, thus it tackles the matching problem with linear complexity with respect to the number of pixels. Furthermore, its computational complexity can be made independent of the number of pixels by using a parallel computation architecture as the tree traversal for individual image points is decoupled.</p><p>We demonstrate the efficacy of our method by applying it on a number of challenging vision tasks, including optical flow and stereo. Not only is GPC extremely computationally efficient, but it is also able to match or outperform more complex state of the art algorithms. To summarize, our contributions are two-fold: firstly, we propose a novel learning based matching algorithm that conducts global correspondence with linear complexity; secondly, we develop a novel hashing scheme by training decision trees designed for seeking collisions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Our work is similar to correspondence estimation algorithms based on approximate nearest neighbor (ANN) methods, such as KD-Tree <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b1">2]</ref> or hashing <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b8">9]</ref>. However, there are two notable differences: (1) GPC is trained to be robust to various geometric and photometrics transformations in the training data, and (2) it isolates potential matches by looking for unique collisions in leaves of deci-  The growing availability of real and synthetic datasets for correspondence problems have led to the proposal of a number of learning based approaches. In one of the earliest works along this direction, Roth and Black <ref type="bibr" target="#b26">[27]</ref> showed how optical flow estimates can be improved by incorporating a statistical prior on the distribution of flow in a Field of Experts model. As the size of the available datasets have grown, researchers have started to use high capacity models such as deep convolutional neural network to either learn the pair-wise similarity <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b37">38]</ref> or learn the end-toend pipeline directly <ref type="bibr" target="#b15">[16]</ref>.</p><p>The computational architecture of GPC is similar to decision forests <ref type="bibr" target="#b9">[10]</ref>. Decision trees have been widely used in various fields of computer vision, such as pose estimation <ref type="bibr" target="#b27">[28]</ref>, image denoising <ref type="bibr" target="#b13">[14]</ref>, image classification <ref type="bibr" target="#b4">[5]</ref>, object detection <ref type="bibr" target="#b20">[21]</ref>, depth estimation <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b14">15]</ref>, etc. However, unlike all these applications, our method does not require classification or regression labels. Our objective function has been especially designed to ensure that visually similar patches (or their perspective transformed versions) will follow the same path in the trees and fall into the same leaf node.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Global Patch Collider</head><p>GPC is a matching algorithm based on finding unique collisions using decision trees as hash function evaluators. Each tree learns to map patches that are in correspondence into the same leaf while separating them from other patches (see <ref type="figure" target="#fig_1">Fig. 1</ref>). We provide the formal description of the Global Patch Collider (GPC) below. We can see that correspondences are task dependent with different type of variations, e.g. non-rigid transform, scaling, intensity change, rotation, background change, etc. It is difficult to propose a generic descriptor that is robust to all kinds of variations, whereas our approach is able to learn those variations directly from the training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Formulation</head><p>Single Tree Prediction. Given two images I and I ′ , our target is to find distinctive local correspondences between pixel positions. Given a local patch x with center coordinate p from an image I, we pass it into a decision tree T until it reaches one terminal node (leaf). The id of the leaf is just a hash key for the image patch and is denoted as T (x).</p><p>After processing all the patches, for each leaf j, GPC stores a set of patches from source image denoted as S j as well as a set of patches from the target image, denoted as S ′ j . We will consider two patches to be a correspondence pair if and only if they fall into the same leaf and this leaf contains only one target patch and one source patch. More formally, the set of correspondences could be written as C T (I,</p><formula xml:id="formula_0">I ′ ) = {(x, x ′ )|T (x) = T (x ′ ) and |S T (x) | = |S ′ T (x) | = 1}</formula><p>. This decision tree approach can be considered as a hashing function, where correspondent patches are picked by finding distinctive collisions between source and target image in the hash table. Simple binary hash functions can be used instead of decision trees but they would not have the conditional execution structure that decision trees have as only one split function needs to be evaluated at every layer.</p><p>Forest Prediction. It is worth noting that a simple tree is not discriminative enough to generate a large amount of distinctive pairs. For example, given a 16-layer binary tree, the maximum number of states is 32768, but, if we consider megapixel images, there are millions of patches from one image. Moreover, due to the content similarity, most patches within one image will fall into a small fraction of the leaves (between 6000 to 10000 on Sintel dataset). If we merely increase the depth of the tree we will bring additional computational and storage burdens for training the decision trees. This motivates us to extend the single-tree approach to a hashing forest scheme.</p><p>Specifically, instead of searching distinctive pairs that fall into the same leaf, our method seeks distinctive pairs that fall into the same leaf across all the trees in the forest. In particular, two patches are considered as a distinctive match if they reach the same leaves for all the trees and there is not any other patch from both source and target image reach exactly the same leaves. Given two images I and I ′ and a random forest F, the set of correspondence is formulated as C F (I,</p><formula xml:id="formula_1">I ′ ) = {(x, x ′ )|F(x) = F(x ′ ) and |S F (x) | = |S ′ F (x) | = 1}. F(x)</formula><p>is a sequence of leaf nodes {j 1 , ..., j T } where x falls in this forest, and S L represents a set of patches that fall into the ordered leaf nodes sequence L. For a forest with T trees and L layers for each tree, the number of states in total is 2 L(T −1) . In practice, the number of states is between 50k to 200k for a 16-layer-8-tree forest on 0.4-megapixel image from Sintel dataset <ref type="bibr" target="#b7">[8]</ref>. Note that our method only seeks unique matched pairs, thus no re-ranking or pairwise comparison is needed.</p><p>Split Function. Each split node contains a set of learned parameters θ = (w, τ ), where w is a hyper-plane and τ represents a threshold value. The split function f is evaluated at a patch x as:</p><formula xml:id="formula_2">f (x; θ) = sign(w T φ(x) − τ )<label>(1)</label></formula><p>where φ(x) is the features for x, we will introduce our patch-based features for each task individually. This hyperplane classifier is commonly adopted in decision forest <ref type="bibr" target="#b9">[10]</ref>. Note the sparse hyper-planes can be used to increase efficiency, since only a small fraction of the feature is tested. Furthermore, the nature of random forest allows us to easily process patches and trees in parallel and independently for each pixel. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Training.</head><p>Training Data. Each tree in the forest is trained independently on a random subset S of the training data. For our correspondence problem, the set S contains triplet samples (x, x pos , x neg ) where x is a patch in a training source image, x pos is the ground-truth correspondent patch in the target image and x neg is a negative patch sampled around the ground-truth location in the target image with a random offset.</p><p>Training Objective. Intuitively, for each node we want to choose the optimal parameters that keep positive and reference patches into the same child node, and that split them from the negative patches. Therefore, for each internal node, we propose to choose the parameters that maximize weighted harmonic mean between precision and recall:</p><formula xml:id="formula_3">max θ precision(S, θ) · recall(S, θ) w 1 precision(S, θ) + w 2 recall(S, θ)<label>(2)</label></formula><p>where w 1 + w 2 = 1. The optimization task is equivalent to maximize precision if w 1 = 0, w 2 = 1 and maximize recall if w 1 = 1, w 2 = 0. In practice we choose a small w 1 ∈ [0, 0.3], since we prefer high-precision matches due to the nature of correspondence problem. The optimization is conducted in a greedy manner. We randomly sample hyper-planes and for each hyper-plane we choose the best threshold through line-search. Each node selects the hyper-plane and the threshold that maximize our objective function. To further improve the efficiency of the training we share features across nodes within the same layer and updating threshold for each node only. This technique is known in literature as Random Ferns <ref type="bibr" target="#b4">[5]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Extensions.</head><p>The described method is very efficient and retrieves very accurate and sparse matches. However, some applications require a denser coverage in order to incorporate smoothness constraints within neighbor pixels. To do so we propose three possible extensions that do not introduce any additional cost in the overall running time. First, we design a multiscale version of the algorithm to increase the coverage across the image and improve the recall of the matched pairs. Secondly, hard pairs are sampled with higher probability during the training stage. Finally we learn motion prior over the patches: this gives a low compute way to disambiguate non-unique matches without performing any expensive re-ranking steps.</p><p>Multi-scale Learning. Many feature matching methods have difficulties in finding all reliable matches at a fixed scale. For instance, for small local patches, matches are ambiguous due to repetitive patterns or smoothing regions due to the lack of context. This motivates us to utilize information from multiple scales. However, simply stacking multiple features will dramatically increasing the dimension of the hyper-plane which brings difficulty for optimization. Therefore, we proposed a multi-layer learning scheme where the decision trees are organized in a coarse-to-fine manner. The first several layers are required to focus on features at a coarse resolution and they will look into finer resolutions as the tree goes deeper. Tab. 1 shows precisionrecall of single-scale approach and multiple scales methods and <ref type="figure" target="#fig_3">Fig. 3</ref> depicts the matching results. Compared with a single-scale approach with the same tree architecture, this multi-scale approach achieves better recall at the same level of precision.</p><p>Mining Hard Pairs. One of the drawback of the greedy training approach is that difficult positive pairs are discarded early once they are split into different internal nodes. In the context of optical flow and stereo, we found these samples are mostly due to large motion. Therefore, when DeepMatching has the best coverage but also generates some outliers (background and the arm on top, the man's head on bottom). Our method is almost outlier-free and has most matches. sampling training patches we give higher probability to large-displacement patches.</p><p>Motion Pattern Learning. Our method could be further extended to learn priors of motion patterns. To be specific, at each terminal node, we train a six-layer decision tree to predict whether two patches are true correspondent simply according to the relative motion. This is based on the motivation that motions are highly correlated with local content of images. For instance, boundary patches are more possible to move along the direction perpendicular to the edges than along the edge direction. <ref type="figure" target="#fig_6">Fig. 6</ref> depicts the motion priors over different leaves. As we can see the patterns of motion diverse significantly, which justify our approach to using motion features to further boost performance. In the testing stage, we could further utilize non-distinctive patches by predicting whether two patches are likely to be a good match.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Complexity Analysis.</head><p>The run-time complexity of the algorithm depends linearly on the size of the image I. For instance, in optical flow task, the total complexity of our matching algorithm is</p><formula xml:id="formula_4">O(dT LN ) + O(N )<label>(3)</label></formula><p>where N is the number of patches, d is the number of features examined in each split function, T is the number </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results</head><p>This section presents the results for the proposed Global Patch Collider to the following tasks: (i) optical flow, (ii) structured light stereo matching and (iii) feature matching  for widebaseline stereo. For the first problem, we perform evaluations using the popular MPI-Sintel benchmark <ref type="bibr" target="#b7">[8]</ref> and the KITTI 2015 Optical flow dataset <ref type="bibr" target="#b16">[17]</ref>. We compare our method with current state-of-the-art algorithms. The structured light stereo matching task is conducted over a sequence of infrared stereo images, and compared with the patch-matching stereo algorithm <ref type="bibr" target="#b3">[4]</ref>. Finally we adopt the fountain dataset <ref type="bibr" target="#b33">[34]</ref> to validate domain transfer ability for the proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Optical Flow</head><p>For optical flow experiments, we evaluate our method on the challenging MPI-Sintel dataset <ref type="bibr" target="#b7">[8]</ref>. We first split the training dataset into training (sequence 1-12) and validation (sequence <ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref>, where we evaluate the performance of the sparse matching and pick the best hyper-parameters. Reference patches are randomly sampled with higher-probability over large-motion patches (pixel larger than 10). A positive patch is chosen according to the ground-truth flow of center pixel and a negative patch is randomly sampled around ground-truth location with an offset between 3 to 20. This configuration would generate very difficult negative samples due to the local appearance similarity of images. In total we have 4-million patches for training and 1.25 million patches for validation. Three scales are selected for the multi-scale patch collider <ref type="bibr">(</ref> use Walsh-Hadamard transform (WHT) as feature due to its efficiency and representation power 1 . For each rgb channel we pick the first 9 components, thus our feature dimension in total is 81 for multi-scale collider and 27 for single-scale collider.</p><p>Precision-Recall. We first report precision-recall on validation triplets with multiple-configurations in Tab. 1. The balance of precision-recall could be achieved via adjusting the number of layers and the number of intersected trees. As the model becomes complex we could achieve higher precision and lower recall. We compare our method with a random balanced tree baseline with exactly same features and tree architecture but randomly generated hyper-plane. This is essentially equivalent to locality sensitive hashing method <ref type="bibr" target="#b10">[11]</ref>. Tab. 1 shows that our learning-based approach clearly out-performs the random baseline in terms both single-tree and forest setting. Furthermore, under the same level of recall, we can see that a multi-scale learning achieves higher precision than the single-scale approach.</p><p>Sparse Matching. We conduct sparse matching experiments on a subset of our validation data (every 5 frame). Tab. 3 reports the results in terms of endpoint error, inlier percentage as well as number of matches per image. We consider pixel-wise motion estimation with endpoint error larger than 3 pixels as outliers. We also report our algorithm under multiple configurations, namely single-scale, multiscale and mutli-scale plus motion learning. Several matching methods are picked as competing algorithm. Coherency sensitive hashing <ref type="bibr" target="#b21">[22]</ref> is a hashing based PatchMatch algorithm which is designed for dense nearest-neighbor field 2 .</p><p>SiftMatching <ref type="bibr" target="#b23">[24]</ref> is a baseline for sparse matching 3 . Lib-Viso2 <ref type="bibr" target="#b19">[20]</ref> is a fast feature matching algorithm which is designed for sparse correspondence with applications in <ref type="figure">Figure 8</ref>. Domain transfer ability for wide-baseline stereo matching. Left to right: matching results across 1, 2, 3 frames respectively. Green lines are inlier and blue lines are outlier.</p><p>SLAM, optical flow and stereo 4 . DeepMatching <ref type="bibr" target="#b35">[36]</ref> is the state-of-the-art matching algorithm specially designed for optical flow task <ref type="bibr" target="#b4">5</ref> . From this table we can see that our algorithm achieves the lowest endpoint error and outliers percentage with more number of matched points on average. In terms of coverage in the most difficult case, our method outperforms other feature matching algorithms. Compared with single-scale approach, mutli-scale GPC significantly reduces the endpoint error and outlier percentage, but also decreases the number of matched points in the worst case.</p><p>If we also consider non-unique hits with motion learning, the proposed method reaches the same accuracy level with multi-scale method while keeps a reasonable number of matches. Qualitative comparisons of sparse matching are shown in <ref type="figure" target="#fig_5">Fig. 5</ref>. Two failure cases of our global patch collider are shown in <ref type="figure" target="#fig_1">Fig. 10</ref>. Although motion and multi-scale learning is introduced to increase coverage, in some cases (e.g. in presence of motion blur and rotation) our method may fail in capturing some large transformed regions.</p><p>Dense Flow. Once we compute sparse matches, we use the state-of-the-art interpolation method EpicFlow <ref type="bibr" target="#b25">[26]</ref> to generate dense flow results on the Sintel testing benchmark. Tab. 2 shows the qualitative results of top-8 optical flow methods on Sintel testing benchmark (final) as well as other three popular algorithms <ref type="bibr" target="#b5">6</ref> . Please note all the top-5 methods use EpicFlow as post-processing and the original EpicFlow uses DeepMatching <ref type="bibr" target="#b35">[36]</ref> as sparse initialization. The proposed GPC is ranked second among all the optical flow methods. In particular our proposed approach achieves the best results over pixels with motion between 10 pixels and 40 pixels. It is worth noting that our matching algorithm is the only method which does not need pairwise similarity comparison or re-ranking from multiple proposals. <ref type="figure" target="#fig_4">Fig. 4</ref> shows qualitative comparison of all the competing algorithms over the testing dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">KITTI Optical Flow.</head><p>We evaluate our algorithm on the KITTI 2015 Optical flow dataset <ref type="bibr" target="#b16">[17]</ref>. To be specific we follow the same configuration used in the Sintel dataset. In the training stage, we trained a GPC with 8 trees, with 12 layers. Each layer learns a specific scale from (7×7, 15×15 and 31×31) with  27 dimensional feature for each scale. In the testing stage, our sparse matching is conducted with GPC and we used EpicFlow <ref type="bibr" target="#b25">[26]</ref> to obtain the final dense optical flow, with the standard hyper-parameters for the KITTI dataset. The average number of matches per image is 14563, whereas the minimum number of matches is 2542. Tab. 4 shows the results. In general our method is comparable with sparse matching + EpicFlow, but orders of magnitude faster in the matching stage. We also show some qualitative results in <ref type="figure">Fig. 9.</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Structured Light Stereo Matching</head><p>For the stereo matching task, we collected 2200 infrared stereo images in indoor scene with a Kinect depth sensor based on structured illumination. The reference pattern is recovered using the calibration procedure in <ref type="bibr" target="#b14">[15]</ref>. The pattern and Kinect images are rectified so that disparity is along horizontal line <ref type="bibr" target="#b14">[15]</ref>. We used 1000 frames as training set and the rest as test test. The GPC patch size is set to 7 × 7.</p><p>For this scenario, we use the following pixel-wise difference test as split function: f (x, θ) = sign(x(i) − x(j) − τ ). Each internal node calculates the pixel-wise intensity different at two pixel offsets (i, j) and the binary decision is made whether the difference is smaller than the threshold τ . This relative feature is illumination invariant and requires little computation. In the training stage, we trained a 10-tree forest with 16 layers for each tree over 1 million triplets ran-   domly collected from the training set. In practice we found choosing the first 7 layers will already ensure a good balance between precision and recall for this task, since the number of possible matches is greatly reduced by epipolar constraint. For each internal node, we generate 1024 random proposals and pick the best one which maximizes our objective defined in Eq. (2). Tab. 5 shows the average endpoint error and outlier percentage under different configurations. Pixels with disparity error larger than 1 are considered as outliers. In this table 'Ours' represents our standard unique-collisions based matching, 'Ours (high-recall)' represents reducing the complexity of the tree architecture (6-layer, 8-tree) in order to generate comparable number of matches before inducing 'motion' prior. Please note that in this 1-dimensional matching case, 'motion' learning is conducted simply as training a 1D-Gaussian binary classifier with disparity as input for each node. With this prior and non-unique collisions our method further increases both accuracy and recall. We also conducted dense stereo reconstruction experiment by using our sparse matching as initialization for PatchMatch based stereo <ref type="bibr" target="#b3">[4]</ref>. In <ref type="figure" target="#fig_8">Fig. 7</ref>, we show PatchMatch results after 1-iteration with our method initialization and random initialization respectively. As we can see our method could generate more completed results and even comparable with the quality of full-iteration of PatchMatch. Quantitative comparisons are shown in Tab. 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Feature Matching for Wide-baseline Stereo</head><p>We also conduct an experiment to show the domain transfer ability of GPC. To be more specific, we trained a Global Patch Collider over Sintel dataset for optical flow  <ref type="table">Table 7</ref>. Quantitative analysis of our method's domain transfer ability on wide-baseline stereo matching (trained on Sintel).</p><p>task and used it for matching correspondence on EPFL wide-baseline stereo data <ref type="bibr" target="#b33">[34]</ref>. Given the camera poses, we use our patch collider to find matches across two images then discard those violating the epipolar constraints. Errors are measured in the 3D space by projecting the two matched points back into world coordinate using the GT depth. A match pair is considered as outlier if the ℓ 2 -error is larger than 0.15m in 3D space. <ref type="figure">Fig. 8</ref> depicts an example of matches across 1 frame, 2 frames and 3 frames respectively. Green lines are inliers and blue lines are outliers. We also consider Sift as a baseline approach and reported the quantitative results on average over all frames on 'Fountain' subset in Tab. 7. From the table and figure we can see that our method achieves better results for small baseline cases, but the performance dropped over wide-baseline case. This is expected since the GPC model is trained on the Sintel dataset, where large viewpoint changes and significant patch deformations barely happen on adjacent frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>This paper proposes a novel algorithm, the Global Patch Collider, for the computation of global point-wise correspondences in images. The proposed method is based on detecting unique collisions between image points using a collection of learned tree structures that act as conditional hash functions. Our algorithm is extremely efficient, fully-parallelizable, task-specific and does not require any pairwise comparison. Experiments on optical flow and stereo matching validates the performance of the proposed method. Future work includes high level applications such as hand tracking, and nonrigid reconstruction of deformable objects.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>Global Patch Collider (GPC). Local patches traverse each tree in the decision forest, reaching different leaves. If two patches from source and target image hit the same leaf across all trees without collisions with other patches, they are considered as a distinctive correspondence. For instance, source patch 4 and target patch 1 hit the same leaves of all the trees and there is no other patch hit exactly the same leaves across all trees with them, thus it is a distinctive correspondence. sion trees. These leaves act like conditional hash functions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Examples of matched local patches. From left to right: Sintel, Kitti, active stereo, MVS, synthetic.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Sparse matching with w/o multi-scale learning. From top-left to bottom right: 7 × 7, 15 × 15, 31 × 31, multi-scale.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Qualitative comparisons among top-5 algorithms on Sintel optical flow benchmark. Top to bottom: input image (average of two), ground-truth, EpicFlow<ref type="bibr" target="#b25">[26]</ref>, CPM, FlowFields<ref type="bibr" target="#b0">[1]</ref>, Glob-alPatchCollider (ours).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Qualitative comparisons for sparse matching on Sintel flow dataset (zoom-in for better quality). Left to right: Sift, LibViso, CSH, DeepMatching, Ours. Number of matches in Sift and LibViso are not dense enough. CSH and Sift generates too many outliers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 .</head><label>6</label><figDesc>Motion histogram for ten randomly picked leaves. Histogram bins are divided according to motion radius (0, 1, 3, 10) and angle (−π to π). of trees, and L is the layer of the each tree. To be specific, the forest prediction stage requires O(dT LN ) operations and matching stage requires a linear pass over all nonzero states with a maximum number of N . Therefore, our method considers all the possible matches globally in linear time and does not require any pairwise comparison. In practice, the parameters for our algorithm are T = 8, L = 12, d = 27 for optical flow and T = 7, L = 12, d = 2 for stereo. As comparison, KD-tree based matching will takes O(dN logN ) + O(dN logN ) + O(dmN ) with an additional tree building step and deep matching approximate takes O(N N ) operations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Algorithm 1</head><label>1</label><figDesc>Global Patch Collider Input: Image I and I ′ and the trained decision forest F. -Get all local patch features {x} and {x ′ } from source and target images respectively. -Initialize C(I, I ′ ) with empty set. -For each patch x encode and store the forest status F(x) according to Sec. 3.1. -Enumerate all the forest status with non-zero number of hits. If there is only one source patch and target patch, add this distinctive pair (x i , x ′ j ) into the correspondence set C(I, I ′ ). Output: C(I, I ′ )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 .</head><label>7</label><figDesc>Qualitative comparisons for structured light stereo. Top to bottom: left input, full-iter (random initialization), 1-iter (random initialization), 1-iter (ours). Our initialization can help achieve better results within only one iteration, e.g. regions of the computer and the table on the right side of two images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 .</head><label>10</label><figDesc>Failure cases of Global Patch Collider. Left: compared with deep matching, which explicitly models rotation, our method failed in capturing rotation of the basket; Right: both deep matching and our patch collider failed in capturing non-rigid deformation of the bat.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>Table 1. Precision at %k recall under different configurations. Our baseline is a random balanced tree with hyper-plane split function.</figDesc><table>Dataset 
Optical Flow Sintel (Final, Pr at k% recall) 
Method 
1% 
5% 
10% 
25% 
50% 
Locality Sensitive Hashing 
-
-
-
85.2% 
76.6% 
Global Patch Collider (single-tree) 
-
-
-
94.5% 
89.5% 
Global Patch Collider (multi-tree) 
99.8% 
99.3% 
97.5 
93.6% 
89.5% 
Global Patch Collider (+multi-scale) 
99.8% 
99.5% 
99.3 
98.1% 
94.7% 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>7 × 7, 15 × 15, 31 × 31). We</figDesc><table>EPE All S0-10 S10-40 
S40+ 
FlowFields [1] 
5.810 
1.157 
3.739 
33.890 
GlobalPatchCollider 
6.040 
1.102 
3.589 
36.455 
CPM 
6.078 
1.201 
3.814 
35.748 
DiscreteFlow [25] 
6.077 
1.074 
3.832 
36.339 
EpicFlow [26] 
6.285 
1.135 
3.727 
38.021 
TF+OFM [19] 
6.727 
1.512 
3.765 
39.761 
Deep+R [12] 
6.769 
1.157 
3.837 
41.687 
DeepFlow2 [36] 
6.928 
1.182 
3.859 
42.854 
MDP-Flow2 [37] 
8.445 
1.420 
5.449 
50.507 
LDOF [7] 
9.116 
1.485 
4.839 
57.296 
Classic+NL [33] 
9.153 
1.113 
4.496 
60.291 

Table 2. Optical Flow Leader-board on Sintel (final) benchmark. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 .</head><label>3</label><figDesc>Sparse matching performance on Sintel Dataset.Table 4. Performance on KITTI flow 2015 benchmark</figDesc><table>Figure 9. Results on KITTI optical flow. From top to bottom: input 
image, flow estimation, flow error. 

Error 
Fl-bg 
Fl-fg 
Fl-all 
All / All 30.60 % 33.09 % 31.01 % 
Noc / All 20.09 % 28.92 % 21.69 % 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 6 .</head><label>6</label><figDesc>PatchMatch based on dense stereo results w/o initialization with global patch collider.</figDesc><table></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Since 2 n × 2 n patch size is required for WHT, we extrapolate the additional row and column with padding.<ref type="bibr" target="#b1">2</ref> We use the author's implementation. In order to ensure a fair comparison for dense approaches, we only compare pixels available at 'Ours 15 × 15' approach when calculating endpoint errors and inlier percentage.<ref type="bibr" target="#b2">3</ref> We use the implementation in VLFeat and set PeakThres to be 0 for DoG based keypoint detection.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">We use the author's implementation.<ref type="bibr" target="#b4">5</ref> We use the author's implementation.<ref type="bibr" target="#b5">6</ref> This is a snapshot of Sintel benchmark on Nov. 10 2015. For latest results, please refer to http://sintel.is.tue.mpg.de/.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Flow fields: Dense correspondence fields for highly accurate large displacement optical flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bailer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Taetz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Stricker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV. 1</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Fast edge-preserving patchmatch for large displacement optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TIP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">The generalized patchmatch correspondence algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Goldman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Finkelstein</surname></persName>
		</author>
		<idno>ECCV. 2010. 1</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Patchmatch stereostereo matching with slanted support windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bleyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rhemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Image classification using random forests and ferns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bosch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Munoz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Dense semantic correspondence where every pixel is a classifier</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bristow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Large displacement optical flow: descriptor matching in variational motion estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PAMI</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A naturalistic open source movie for optical flow evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Butler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Large displacement optical flow from nearest neighbor fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Decision forests: A unified framework for classification, regression, density estimation, manifold learning and semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Konukoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Now</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Locality-sensitive hashing scheme based on p-stable distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Datar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Immorlica</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Indyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">S</forename><surname>Mirrokni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SOCG</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Combinatorial regularization of descriptor matching for optical flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Drayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning to be a depth camera for close-range human capture and interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Fanello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Keskin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Izadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sweeney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Paek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGGRAPH and Transaction On Graphics</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Filter forests for learning data-dependent convolutional kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Fanello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Keskin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Izadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Pattacini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Paek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Hyperdepth: Learning depth from structured light without matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Fanello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rhemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Tankovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kowdle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Escolano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Izadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Flownet: Learning optical flow with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Häusser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hazırbaş</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Van Der Smagt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Computing nearest-neighbor fields via propagation-assisted kd-trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Optical flow with geometric occlusion estimation and fusion of multiple frames</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMM-CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Visual odometry based on stereo image sequences with ransac-based outlier rejection scheme</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lategahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IV</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Context-sensitive decision forests for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kontschieder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Bulò</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pelillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Coherency sensitive hashing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Korman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Avidan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Learning the matching function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ladickỳ</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Häne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.00652</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Object recognition from local scale-invariant features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Discrete optimization for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Menze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Heipke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">GCPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Epicflow: Edge-preserving interpolation of correspondences for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">On the spatial statistics of optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Real-time human pose recognition in parts from single depth images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fitzgibbon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sharp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Finocchio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kipman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Discriminative learning of deep convolutional feature point descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Simo-Serra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Trulls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ferraz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">M</forename><surname>Noguer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning local feature descriptors using convex optimisation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PAMI</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Unsupervised discovery of mid-level discriminative patches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Ldahash: Improved matching with smaller descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Strecha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PAMI</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Secrets of optical flow estimation and their principles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Daisy: An efficient dense descriptor applied to wide-baseline stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PAMI</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Boosting binary keypoint descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Trzcinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Christoudias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deepflow: Large displacement optical flow with deep matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Motion detail preserving optical flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Matsushita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PAMI</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning to compare image patches via convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Computing the stereo matching cost with a convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Žbontar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
