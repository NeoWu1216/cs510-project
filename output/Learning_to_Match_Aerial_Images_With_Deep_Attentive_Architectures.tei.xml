<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning to Match Aerial Images with Deep Attentive Architectures</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hani</forename><surname>Altwaijry</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Cornell University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Cornell Tech</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Trulls</surname></persName>
							<affiliation key="aff2">
								<orgName type="laboratory">Computer Vision Laboratory</orgName>
								<orgName type="institution">École Polytechnique Fédérale de Lausanne (EPFL</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
							<affiliation key="aff3">
								<orgName type="department" key="dep1">School of Interactive Computing</orgName>
								<orgName type="department" key="dep2">College of Computing</orgName>
								<orgName type="institution">Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
							<affiliation key="aff2">
								<orgName type="laboratory">Computer Vision Laboratory</orgName>
								<orgName type="institution">École Polytechnique Fédérale de Lausanne (EPFL</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Cornell University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Cornell Tech</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning to Match Aerial Images with Deep Attentive Architectures</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:42+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image matching is a fundamental problem in Computer</head><p>Vision. In the context of feature-based matching, SIFT and its variants have long excelled in a wide array of applications. However, for ultra-wide baselines, as in the case of aerial images captured under large camera rotations, the appearance variation goes beyond the reach of SIFT and RANSAC. In this paper we propose a data-driven, deep learning-based approach that sidesteps local correspondence by framing the problem as a classification task. Furthermore, we demonstrate that local correspondences can still be useful. To do so we incorporate an attention mechanism to produce a set of probable matches, which allows us to further increase performance. We train our models on a dataset of urban aerial imagery consisting of 'same ' and  'different' pairs, collected  for this purpose, and characterize the problem via a human study with annotations from Amazon Mechanical Turk. We demonstrate that our models outperform the state-of-the-art on ultra-wide baseline matching and approach human accuracy.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Finding the relationship between two images depicting a 3D scene is one of the fundamental problems of Computer Vision. This relationship can be examined at different granularities. At a coarse level, we can ask whether two images show the same scene. At the other extreme, we would like to know the dense pixel-to-pixel correspondence, or lack thereof, between the two images. These granularities are directly related to broader topics in Computer Vision; in particular, one can look at the coarse-grained problem as a recognition/classification task, whereas the pixel-wise problem can be viewed as one of segmentation. Traditional geometry-based approaches live in a middle ground, relying on a multi-stage process that typically involves keypoint matching and outlier rejection, where image-level correspondence is derived from local correspondence.  In this paper we focus on pairs of oblique aerial images acquired by distant cameras from very different angles, as shown in <ref type="figure" target="#fig_1">Fig. 1</ref>. These images are challenging for geometry-based approaches for a number of reasons-chief among them are dramatic appearance distortions due to viewpoint changes and ambiguities due to repetitive structures. This renders methods based on local correspondence insufficient for ultra-wide baseline matching.</p><p>In contrast, we follow a data-driven approach. Specifically, we treat the problem from a recognition standpoint, without appealing specifically to hand-crafted, featurebased approaches or their underlying geometry. Our aim is to learn a discriminative representation from a large amount of instances of same and different pairs, which separates the genuine matches from the impostors.</p><p>We propose two architectures based on Convolutional Neural Networks (CNN). The first architecture is only concerned with learning to discriminate image pairs as same or different. The second one extends it by incorporating a Spatial Transformer module <ref type="bibr" target="#b15">[16]</ref> to propose possible matching <ref type="bibr">Figure 2</ref>. Sample pairs from one of our datasets, collected from Google Maps <ref type="bibr" target="#b12">[13]</ref> 'Birds-Eye' view. Pairs show an area or building from two widely separated viewpoints.</p><p>regions, in addition to the classification task. We learn both networks given only same and different pairs, i.e., we learn the spatial transformations in a semi-supervised manner.</p><p>To train and validate our models, we use a dataset with 49k ultra-wide baseline pairs of aerial images compiled from Google Maps specifically for this problem: example pairs are shown in <ref type="figure">Fig. 2</ref>. We benchmark our models against multiple baselines, including human annotations, and demonstrate state-of-the-art performance, close to that of the human annotations.</p><p>Our main contributions are as follows. First, we demonstrate that deep CNNs offer a solution for ultra-wide baseline matching. Inspired by recent efforts in patch matching <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b30">31]</ref> we build a siamese/classification hybrid model using two AlexNet networks <ref type="bibr" target="#b18">[19]</ref>, cut off at the last pooling layer. The networks share weights, and are followed by a number of fully-connected layers embodying a binary classifier. Second, we show how to extend the previous model with a Spatial Transformer (ST) module, which embodies an attention mechanism that allows our model to propose possible patch matches (see <ref type="figure" target="#fig_1">Fig. 1</ref>), which in turn increases performance. These patches are described and compared with MatchNet <ref type="bibr" target="#b13">[14]</ref>. As with the first model, we train this network end-to-end, and only with same and different training signal, i.e., the ST module is trained in a semisupervised manner. In sections 3.2 and 4.6 we discuss the difficulties in training this network, and offer insights in this direction. Third, we conduct a human study to help us characterize the problem, and benchmark our algorithms against human performance. This experiment was conducted on Amazon Mechanical Turk, where participants were shown pairs of images from our dataset. The results confirm that humans perform exceptionally while responding relatively quickly. Our top-performing model falls within 1% of human accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Correspondence Matching</head><p>Correspondence matching has been long dominated by feature-based methods, led by SIFT <ref type="bibr" target="#b22">[23]</ref>. Numerous de-scriptors have been developed within the community, such as SURF <ref type="bibr" target="#b4">[5]</ref>, BRIEF <ref type="bibr" target="#b7">[8]</ref>, and DAISY <ref type="bibr" target="#b35">[36]</ref>. These descriptors generally provide excellent performance in narrow baselines, but are unable to handle the large distortions present in ultra-wide baseline matching <ref type="bibr" target="#b24">[25]</ref>.</p><p>Sparse matching techniques typically begin by extracting keypoints, e.g., Harris Corners <ref type="bibr" target="#b14">[15]</ref>; followed by a description step, e.g., computing SIFT descriptors; then a keypoint matching step, which gives us a pool of probable keypoint matches. These are then fed into a model-estimation technique, e.g., RANSAC <ref type="bibr" target="#b10">[11]</ref> with a homography model. This pipeline assumes certain limitations and demands assumptions to be made. Relying on keypoints can be limitingdense techniques have been successful in wide-baseline stereo with calibration data <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b39">40]</ref>, scene alignment <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b39">40]</ref> and large displacement motion <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b39">40]</ref>.</p><p>The descriptor embodies assumptions about the topology of the scene, e.g., SIFT is not robust against affine distortions, a problem addressed by Affine-SIFT <ref type="bibr" target="#b41">[42]</ref>. Further assumptions are made in the matching step: do we consider only unique keypoint matches? What about repetitive structures? Finally, the robust model estimation step is expected to tease out a correct geometric model. We believe that these assumptions play a major role in why featurebased approaches are currently incapable of matching images across very wide baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Ultra-wide Baseline Feature-Based Matching</head><p>Ultra-wide baseline matching generally falls under the umbrella of correspondence matching problems. There have been several works on wide-baseline matching <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b23">24]</ref>. For urban scenery, Bansal et al. <ref type="bibr" target="#b3">[4]</ref> presented the Scale-Selective Self-Similarity (S 4 ) descriptor which they used to identify and match building facades for image geolocalization purposes. Altwaijry and Belongie <ref type="bibr" target="#b0">[1]</ref> matched urban imagery under ultra-wide baseline conditions with an approach involving affine invariance and a controlled matching step. Chung et al. <ref type="bibr" target="#b8">[9]</ref> calculate sketch-like representations of buildings used for recognition and matching. In general, these approaches suffer from poor performance due to the difficulty of the problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Convolutional Neural Networks</head><p>Neural Networks have a long history in the field of Artificial Intelligence, starting with <ref type="bibr" target="#b29">[30]</ref>. Recently, Deep Convolutional Neural Networks have achieved state-of-the-art results and become the dominant paradigm in multiple fronts of Computer Vision research <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b11">12]</ref>.</p><p>Several works have investigated aspects of correspondence matching with CNNs. In <ref type="bibr" target="#b21">[22]</ref>, Long et al. shed some light on feature localization within a CNN, and determine that features in later stages of the CNN correspond to features finer than the receptive fields they cover. Toshev and Szegedy <ref type="bibr" target="#b36">[37]</ref> determine the pose of human bodies using CNNs in a regression framework. In their setting, the neural network is trained to regress the locations of body joints in a multi-stage process. Lin et al. <ref type="bibr" target="#b19">[20]</ref> use a siamese CNN architecture to put aerial and ground images in a common embedding for ground image geo-localization.</p><p>The literature has seen a number of approaches to learning descriptors prior to neural networks. In <ref type="bibr" target="#b6">[7]</ref>, Brown et al. introduce three sets of matching patches obtained from structure-from-motion reconstructions, and learn descriptor representations to match them better. Simonyan et al. <ref type="bibr" target="#b31">[32]</ref> learn the placement of pooling regions in image-space and dimensionality reduction for descriptors. However, with the rise of CNNs, several lines of work investigated learning descriptors with deep networks. They generally rely on a two-branch structure inspired by the siamese network of <ref type="bibr" target="#b5">[6]</ref>, where two networks are given pairs of matching and nonmatching patches. This is the approach followed by <ref type="bibr">Han et al.</ref> with MatchNet <ref type="bibr" target="#b13">[14]</ref>, which relies on a fully connected network after the siamese structure to learn the comparison metric. DeepCompare <ref type="bibr" target="#b42">[43]</ref> uses a similar architecture and focuses on the center of the patch to increase performance. In contrast, Simo-Serra et al. <ref type="bibr" target="#b30">[31]</ref> learn descriptors that can be compared with the L 2 distance, discarding the siamese network after training. These three methods relied on data from <ref type="bibr" target="#b6">[7]</ref> to learn their representations. They assume that salient regions are already determined, and deliver a better approach to feature description for feature-based correspondence matching techniques. The question of obtaining CNN-borne correspondences between two input pairs, however, remains unexplored.</p><p>Lastly, attention models <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b2">3]</ref> have been developed to recognize objects by an attention mechanism examining sub-regions of the input image sequentially. In essence, the attention mechanism embodies a saliency detector. In <ref type="bibr" target="#b15">[16]</ref>, the Spatial Transformer (ST) network was introduced as an attention mechanism capable of warping the inputs to increase recognition accuracy. In section 3.2 we discuss how we employ an ST module to let the network produce guesses for probable region matches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Deep-Learning Architectures</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Hybrid Network</head><p>We introduce an architecture which, given a pair of images, estimates the likelihood that they belong to the same scene. Inspired by the recent success of patch-matching approaches based on CNNs <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b30">31]</ref>, we use a hybrid siamese/classification network. The network comprises two parts: two feature extraction arms that share weights (the siamese component) and process each input image separately, and a classifier component that produces the matching probability. For the siamese component we use the convolutional part of AlexNet <ref type="bibr" target="#b18">[19]</ref>, i.e., cutting off the fully connected layers. For the classifier we use a set of fully- connected layers that takes as input the concatenation of the siamese features and ends with a binary classifier, for which we minimize the binary cross-entropy loss. <ref type="figure" target="#fig_2">Fig. 3</ref> illustrates the structure of the 'Hybrid' network.</p><p>The main motivation behind this design is that it allows features with local information from both images to be considered jointly. This is achieved where the two convolutional features are concatenated. At that layer, the features from both images retain correspondence to specific regions within the input images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Hybrid++</head><p>Unlike traditional geometry-based approaches, the hybrid network proposed in the previous section does not model local similarity explicitly, making it difficult to draw conclusions about corresponding image regions. We would like to determine whether modeling local similarities more explicitly can produce more discriminative models.</p><p>We therefore sought to expand our hybrid architecture to allow for predictions of probable region matches, in addition to the classification task. To accomplish this, we leverage the Spatial Transformer (ST) network described in <ref type="bibr" target="#b15">[16]</ref>. Spatial transformers consist of a network used for localization, which takes as input the image and produces the parameters for a pre-determined transformation model (e.g., translation, affine, etc.) which is used in turn to transform the image. It relies on a grid generator and a differentiable sampling kernel to keep track of the gradient propagation to the localization network. The model can be trained with standard back-propagation, unlike the attention mechanisms of <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b25">26]</ref> that relied on reinforcement learning techniques. The spatial transformer is typically a standard CNN followed by a set of fully-connected layers with the required number of outputs, i.e., the number of transformation parameters, e.g., two for translation, six for affine.</p><p>The spatial transformer allows for any transformation as long as it is differentiable. However, in this work we only consider extracting patches at a fixed scale, i.e., translations, which are used to generate patch proposals over both images-richer models, such as perspective transformations, can potentially be more descriptive, but are also more difficult to train. We build the spatial transformer with the same convolutional network used for the 'arms' of the siamese component of our hybrid network, plus a set of fully connected layers that regress the transformation parameters Θ = {Θ 1 , Θ 2 }, which are used to transform the input images, effectively sampling patches. Note that patch locations for each individual image are a function of both images. The number of extracted patches is reflected in the number of regressed parameters specified. <ref type="figure" target="#fig_3">Fig. 4</ref> illustrates how the spatial transformer module operates.</p><p>The spatial transformer modules allow us to explicitly model regions within each input image, permitting the network to propose similar regions given an architecture that demands such a goal. The overall structure of this model, which we call 'Hybrid++', is shown in <ref type="figure" target="#fig_4">Fig. 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Describing Patches</head><p>In our model, we pair a ST module which produces a predetermined number of fixed-scale patch proposals with our hybrid network. The extracted patches are given to a Match-Net <ref type="bibr" target="#b13">[14]</ref> network, which was trained with interest points from Structure-from-Motion data <ref type="bibr" target="#b6">[7]</ref> and thus already has a measure of invariance against perspective changes built-in.</p><p>MatchNet has two components in its network, a feature extractor modeled as a series of convolutional layers, and a classifier network that takes the outputs of two feature extractors and produces a similarity score. We pass each extracted patch, after converting it to grayscale, through the MatchNet feature extractor network (MatchNet-Feat) and arrive at a 4096-dimensional descriptor vector.</p><p>These descriptors are then used for three different objectives. The first objective is to supplement the global feature description extracted by the original hybrid architecture. In this manner, the extracted descriptors provide the classifier with information extracted at a dedicated higher-resolution mode. The second objective is to match patches in the other  image. This objective encourages the network to use the spatial transformer to focus on similar patches in both images simultaneously. The third objective is for the patch to not match other patches extracted from the same image, which we mainly use to discourage the network from collapsing onto a single patch. For the last two tasks, we use the MatchNet classification network (MatchNet-Classify).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Optimization</head><p>Combining the image-wise classification objective with the regional descriptor objectives yields an objective function with four components:</p><formula xml:id="formula_0">(1) L = 1 N N i=1 L class + αL patch + βL pairwise + γL bounds</formula><p>where N is the size of the training batch and α, β, γ are used to adjust the weights. The first component of the loss function encodes the image classification objective:</p><formula xml:id="formula_1">(2) L class = y i log p i + (1 − y i ) log(1 − p i )</formula><p>where p i is the probability of the images matching and y i ∈ {0, 1} is the label. The second component encodes the match of each pair of patches across both images:</p><formula xml:id="formula_2">(3) L patch = 1 M M m=1 y i log q m + (1 − y i ) log(1 − q m )</formula><p>where M is the number of patches, and q m is the probability of patch x 1 m on image 1 matching patch x 2 m on image 2. The third component is a pairwise penalty function that discourages good matches among the patches within the same image, to prevent the network from collapsing the transformations on top of each other:</p><formula xml:id="formula_3">(4) L pairwise = 4 M (M − 1) 2 t=1 M m=1 M k=m+1 log(1 − u t m,k )</formula><p>where u t m,k is the probability of patch x t m matching patch x t k on image t = {1, 2}. The last component is a penalty function that discourages spatial transformations that fall out of bounds:</p><formula xml:id="formula_4">(5) L bounds = 2 M 2 t=1 M m=1 f (x t m )</formula><p>where f (x t m ) is a function that computes the ratio of pixels sampled out of bounds for patch x t m . The out-of-bounds loss term discourages the model from stepping outside the image, which may minimize the patch-matching loss, given an appropriate weight-with this penalty function we gain more control over the optimization process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Training Procedure</head><p>To train the hybrid network, we follow a standard training procedure by fine-tuning the model after loading pretrained AlexNet weights into the convolutional arms only. However, training the Hybrid++ network is more subtle, as the network needs to get started on the right foot. We initially train the non-ST and ST sides separately with the global yes/no matching signal. Afterwards, we train the networks jointly. We learned this is necessary to prevent the network from shutting off one side while minimizing the objective. Similar to the Hybrid case, we use pre-trained weights for the convolutional arms.</p><p>We use MatchNet as a pure feature descriptor, with frozen weights, i.e., no learning. This is primarily done to prevent the network from minimizing the loss by changing the descriptors themselves without moving the attention mechanism. Our training procedure does not have pixelto-pixel correspondence labels, and hence we do not know if the network is examining similar patches. We rely on the power provided by MatchNet to determine patch similarity. The global matching label in turn becomes a semisupervised cue. Therefore, the network can only minimize the loss component for patch matching by moving the attention mechanism to examine patches that appear to be similar, as per MatchNet.</p><p>The reliance on MatchNet is a double-edged sword, as it is our only means of moving the attention mechanism without explicit knowledge of labeled patch correspondences. That means if MatchNet cannot find correspondence for two patches that do match, then the attention mechanism cannot learn to look for these two patches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Dataset</head><p>We compiled 49,271 matching pairs (98,542 images) of oblique aerial imagery through Google Maps <ref type="bibr" target="#b12">[13]</ref>. The images were collected using an automated process that looks for planar surfaces such that the normal vector of the surface is within 40 • to 75 • of one cardinal direction. This guarantees the visibility of the surface from two different viewpoints. The pairs were collected non-uniformly from: San Francisco, Boston and Milan. Those locations were chosen with a goal of diversifying the scenery.</p><p>We split the dataset into roughly ∼39K/∼10K training/testing positive pairs. For training we generate samples in an online manner by sampling from the reservoir of positive matching pairs. The sampling procedure is set to produce samples with a 1:1 positive:negative ratio. Therefore, a random classifier would score 50% on the test-set. We call this the 'aerial' dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Human Performance</head><p>We ask ourselves: How well do humans perform when matching such images? To this end, we conducted a small experiment with human participants on Amazon Mechanical Turk <ref type="bibr" target="#b1">[2]</ref>. We picked a subset of 1,000 pairs from our test set and presented them to the human subjects. Each participant was shown 10 pairs of different images, and was asked to determine whether each pair showed the same area or building, as a binary question. We show a screenshot of the interface presented to the participants in <ref type="figure" target="#fig_5">Fig. 6</ref>. Each pair of images was presented at least 5 times to different participants, giving us a total of 5000 labels, 5 per pair.</p><p>Our interface was prone to adversarial participants, those answering randomly or giving a constant answer all the time. To mitigate the effect of unfaithful workers, we took the majority vote of the 5 labels per-pair. Human accuracy was then calculated to be 93.3%, with a precision of 98% and a recall of 89.4%.</p><p>We observed that the average response time for humans was less than 4.5 seconds/pair, with a minimum re-  <ref type="figure" target="#fig_6">Fig. 7</ref> and False-Negatives in <ref type="figure" target="#fig_7">Fig. 8</ref>. Most of the False-Positive pairs have a similar general structure, a cue that humans relied on hastilynotice that these examples require deliberate correspondence matching. This is a non-trivial, time-consuming task, which explains why the human subjects, who operate in an environment that favors lower response times, labeled them as False. This is also corroborated by the high precision and lower recall of the human labelers, which is another indication that humans are performing high-level image comparisons. All in all, we believe this indicates that the human participants were relying mostly on global appearance cues, which indicates the need for local correspondence matching.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Training Framework</head><p>We train our networks with Torch7 <ref type="bibr" target="#b9">[10]</ref>. We transplant weights in our models from the pre-trained reference model CaffeNet available from Caffe <ref type="bibr" target="#b17">[18]</ref>. For the convolutional feature arms, we keep the AlexNet layers up to 'pool5' and discard the rest. The fully connected layers of our classifier component are trained from scratch. For the patch descriptor network, i.e., MatchNet <ref type="bibr" target="#b13">[14]</ref>, we transplant the 'feature'network and the 'classification'-network as-is and freeze the learning for both.</p><p>We use Rectified Linear Units (ReLU) for all our nonlinearities, and train the networks with Stochastic Gradient Descent. The spatial transformer modules are trained specifically without momentum.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Spatial Transformer Details</head><p>The spatial transformer regresses |Θ|= 4n parameters, where n is the number of patches per image. Each 2 parameters are taken for an x-y location in the image plane in the range [−1, 1]. We specify a fixed-scale interpretation, where extracted patches are always 64 × 64, the resolution  In the Hybrid++ network, we remove the 'pool5' and 'conv5' layers provided by AlexNet from the convolutional arms, and learn a new 1 × 1 convolutional layer with an output size of 64 × 13 × 13, performing dimensionality reduction from the 384-channel output of 'conv4'. The localization network takes a 2 × 64 × 13 × 13 input from the two convolutional arms and follows up with 3 fully-connected layers as follows: 21632 → 1024 → 256 → 4n. The initialization of the last fully-connected layer is not random; as recommended in <ref type="bibr" target="#b15">[16]</ref>, we initialize it with a zero-weight matrix and a bias specifying initial locations for the patches. In our experiments, we predict M = 6 patches per image, initialized to non-overlapping grid locations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Matching Results</head><p>We compare our CNN models with a variety of baselines on the 'aerial' dataset. Our first baseline was a feature-based correspondence-matching method. We chose A-SIFT <ref type="bibr" target="#b41">[42]</ref> as it offers all the capabilities of SIFT with the addition of affine invariance. In aerial images we mainly observe affine distortion effects, which makes A-SIFT's invariance properties particularly relevant. We use the implementation offered by the authors, which computes the matches and performs outlier rejection to estimate the fundamental matrix between the views, providing a yes/no answer, given a threshold. The accuracy of A-SIFT is better than random by 11%, but suffers from low accuracy for the positive samples (i.e., low recall), as it is unable to find enough correspondences to perform the fundamental matrix estimation for a large number of positive pairs. This illustrates the difficulty of this problem with local correspondence matching.</p><p>Our second set of baselines are a measure of the performance of holistic representation methods used in the image classification and retrieval literature. We chose to compare the performance of GIST <ref type="bibr" target="#b26">[27]</ref>, Fisher Vectors <ref type="bibr" target="#b27">[28]</ref>, and VLAD <ref type="bibr" target="#b16">[17]</ref>. The GIST-based classifier predicted most image pairs to be non-matching. Fisher Vectors surpassed A-SIFT performance by showing a better ability to recognize positive matches, but performed worse than A-SIFT in distinguishing negative pairs. VLAD performed the best out of these three holistic approaches with an average accuracy of 78.6%. For GIST we use the authors' implementation, and for Fisher Vectors and VLAD we use VLFeat <ref type="bibr" target="#b38">[39]</ref>.</p><p>The third set of baselines are vanilla CNN models used in a siamese fashion (without fine-tuning). We compare against AlexNet <ref type="bibr" target="#b18">[19]</ref>, trained on ImageNet, and PlacesCNN <ref type="bibr" target="#b43">[44]</ref>, which is an instance of the AlexNet architecture trained on the Places205 dataset <ref type="bibr" target="#b43">[44]</ref>. We extract the 'fc7' layer outputs as descriptor vectors for input images, and use the L 2 distance as a similarity metric. This group of baselines explores the applicability of pre-trained networks as generic feature descriptors, for which there is mounting evidence <ref type="bibr" target="#b28">[29]</ref>. Both CNNs performed well, considering the lack of fine-tuning. We note that while VLAD surpassed the performance of these two CNN approaches, both VLAD and Fisher Vectors require training with our dataset. This shows the power of CNNs generalizing to other domains.</p><p>Finally we measure the classification accuracy of our proposed architectures. Our Hybrid CNN outperforms all the baselines. A variant of the Hybrid CNN was trained without the 'conv5' and 'pool5' layers, with a 1 × 1 convolution layer after 'conv4' to reduce the dimensionality of its output. This variant outperforms the base Hybrid CNN by a small margin. Our Hybrid++ model with Spatial Transformers gives us a further boost, and performs nearly as well as the human participants in our study. <ref type="table">Table 1</ref> summarizes the accuracy for every method, and <ref type="figure" target="#fig_8">Fig. 9</ref> shows precision/recall curves, along with the average precision, expressed as a percentage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Insights and Discussion</head><p>One of the main difficulties in the application of CNNs to real-world problems lies in designing and training the networks. This is particularly true for complex architectures with multiple components, such as our Hybrid++ network. In this section we discuss our experience and attempt to of-  <ref type="bibr" target="#b26">[27]</ref> .549 .242 .821 .553 Fisher Vectors <ref type="bibr" target="#b27">[28]</ref> .659 .605 .713 .722 VLAD <ref type="bibr" target="#b16">[17]</ref> . fer insights that may not be immediately obvious.</p><p>We obtained a small improvement by removing the 'pool5' layer from the AlexNet model, and replacing 'conv5' by a 1 × 1 dimensionality reduction convolution. We believe this is mainly due to the increased resolution of 13 × 13 presented to the classifier. This resolution would typically allow for more local detail to be considered jointly. In particular, this detail appears to be crucial to training the Hybrid++ model, as it provided the Spatial Transformer module with more resolution to work with. In <ref type="figure" target="#fig_1">Fig. 10</ref> we show a sample of matched images with probable patch matches highlighted. Even with the increase in resolution, the receptive field for each neuron is still quite large in the original image space. This suggests that higher resolution features would be needed for finer localization of similar patches. This aspect is reflected in the network learning regions of interest for each of its attention mechanisms.</p><p>We attempted to use transformations with more degrees of freedom with the Spatial Transformer module, such as affine transforms, but we found the task increasingly difficult without higher levels of supervision and additional constraints. This was the origin of our 'out-of-bounds' penalty term. For example, the network would learn to stretch parts of each image into seemingly similar looking patches, effectively minimizing the pairwise patch similarity loss term.</p><p>To train the pairwise patch similarity portion of the network, we only have the image-level match label, with no information regarding pixel-wise correspondence. It might seem unclear what target labels should be presented to the pairwise similarity loss. However, by studying the loss function we can see that the attention mechanism would not be able to find matching patches unless we actively look for correspondences; hence it is sensible to use the image-level label for patch correspondence. Given that MatchNet modules are frozen, the network will not induce a high loss for non-corresponding patches over negative samples, but only for non-corresponding patches over positive samples.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7.">Investigating the Spatial Transformers</head><p>The patch proposal locations of <ref type="figure" target="#fig_1">Fig. 10</ref> are meaningful from pair to pair, and across the images for a given pair. However, while the baseline between the two images in a pair is very large, it does not change much from pair to pair-an inevitable artifact of the dataset collection process. This results in patch proposals with similar configurations and raises questions about the Spatial Transformers.</p><p>We thus set up a second experiment to study the effect of varying viewpoint changes explicitly. To this end we used several high-resolution aerial images from the city of Lausanne, Switzerland, to build a Structure-from-Motion dataset <ref type="bibr" target="#b40">[41]</ref> and extract corresponding patches, with 8.7k training pairs and 3.6k test pairs. Patches were extracted around SIFT locations and are thus significantly easier to match than those in the 'aerial' dataset. However, the viewpoint changes from pair to pair are much more pronounced.</p><p>We followed the same methodology as before to train our models on this new dataset. In <ref type="figure" target="#fig_1">Fig. 11</ref> we show different pairs from the new dataset, along with the probable patch matches suggested by the model. The model learns to predict patch locations that are consistent with the change in perspective, while also differing from pair to pair. Match-Net results on the proposals corroborate the findings when the contents of those patches do match (non-red boxes), and when they do not (red boxes). Numerical results are provided in <ref type="table">Table 2</ref>. As this data is significantly easier, the baselines (notably A-SIFT) perform much better, but our method achieves the highest accuracy of 96%. The performance gain from Hybrid to Hybrid++ is however negligible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions and Future Work</head><p>We present two neural network architectures to address the problem of ultra-wide baseline image matching. First, we fine-tune a pre-trained AlexNet model over aerial data, with a siamese architecture for feature extraction, and a binary classifier. This network proves capable of discerning image-level correspondence, but is agnostic to local corre-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Acc. Acc. pos Acc. neg AP A-SIFT <ref type="bibr" target="#b41">[42]</ref> .947 .896 .998 .968 GIST <ref type="bibr" target="#b26">[27]</ref> .856 .798 .914 .937 Fisher Vectors <ref type="bibr" target="#b27">[28]</ref> .769 .723 .816 .867 VLAD <ref type="bibr" target="#b16">[17]</ref> . spondence. We then show how to integrate Spatial Transformer modules to predict probable patch matches in addition to the classification task, which further boosts performance. Our models achieve state-of-the-art accuracy in ultra-wide baseline matching, and close the gap with human performance. We also demonstrate the adaptability of our approach on a new dataset with varied viewpoint changes which the ST modules can adapt to. This work is a step towards bridging the gap between neural networks and traditional image-matching techniques based on local correspondence, in a framework that is trainable end-to-end. We intend to build on it in the following directions. First, we plan to explore means to increase the resolution of the localization network to obtain finergrained patch proposals. Second, we plan to replace Match-Net with 'descriptor' networks trained for this specific purpose. Third, we are interested in richer transformations for the ST modules, e.g., affine, and in exploring constraints in order to do so. Finally, we want to study the use of higher supervision for a better feature-localization step, bringing neural networks closer to local correspondence techniques.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>Matching ultra-wide baseline aerial images. Left: The pair of images in question. Middle: Local correspondence matching approaches fail to handle this baseline and rotation. Right: The CNN matches the pair and proposes possible region matches.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>The siamese/classification Hybrid network. Weights are shared between the convolutional arms. ReLU and LRN (Local Response Normalization) layers are not shown for brevity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Overview of a Spatial Transformer module operating on a single image. The module uses the regressed parameters Θ to generate and sample a grid of pixels in the original image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>The 'Hybrid++' Network. Spatial Transformer modules are incorporated into the 'Hybrid' model to predict probable patch matches.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>The user interface presented to our human subjects through Amazon Mechanical Turk. sponse time of half a second. This quick response average prompted us to examine mislabeled pairs: we show examples of False-Positives in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .</head><label>7</label><figDesc>False-Positive pairs from the human experiment.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 .</head><label>8</label><figDesc>False-Negative pairs from the human experiment. required by MatchNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 .</head><label>9</label><figDesc>Precision/Recall curves for the 'aerial' dataset. The number between parenthesis denotes the average precision (%).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 .</head><label>10</label><figDesc>Image pairs from 'aerial', matched with Hybrid++. The overlaying boxes indicate patch proposals. Red boxes denote patches that do not match, according to MatchNet. Boxes with colors other than red indicate matches, with the color encoding the correspondence.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 11 .</head><label>11</label><figDesc>Image pairs from 'Lausanne', matched with Hybrid++. Color coding follows the same conventions are the figure above.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head></head><label></label><figDesc>Table 2. Classification performance on the 'Lausanne' dataset.</figDesc><table>898 .867 
.930 .965 
Siamese PlacesCNN [44] .690 .626 
.754 .958 
Siamese AlexNet [19] .754 .697 
.811 .968 

Hybrid CNN 
.959 .960 
.957 .992 
Hybrid++ 
.959 .962 
.956 .992 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank Kevin Matzen and Tsung-Yi Lin for their valuable input. This work was supported by the KACST Graduate Studies Scholarship and EU FP7 project MAGELLAN under grant number ICT-FP7-611526.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Ultra-wide baseline aerial imagery matching in urban environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Altwaijry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Amazon mechanical turk</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Amazon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Com</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multiple object recognition with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Ultra-wide baseline facade matching for geo-localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sawhney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">SURF: Speeded Up Robust Features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Signature verification using a &quot;siamese&quot; time delay neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bromley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sckinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Discriminative learning of local image descriptors. PAMI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Winder</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">BRIEF: Computing a local binary descriptor very fast</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Calonder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ozuysal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Trzcinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Strecha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Building recognition using sketch-based representations and spectral graph matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Torch7: A MATLAB-like environment for machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Farabet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BigLearn, NIPS Workshop</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Random sample consensus: A paradigm for model fitting with applications to image analysis and automated cartography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Fischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Bolles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of ACM</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Google Inc. Google maps</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">MatchNet: Unifying feature and metric learning for patchbased matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A combined corner and edge detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stephens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fourth Alvey Vision Conference</title>
		<imprint>
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<title level="m">Spatial transformer networks. NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Aggregating local descriptors into a compact image representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jegou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<title level="m">Caffe: Convolutional architecture for fast feature embedding</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning deep representations for ground-to-aerial geolocalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">SIFT Flow: Dense correspondence across different scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Do convnets learn correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Object recognition from local scale-invariant features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Robust wide baseline stereo from maximally stable extremal regions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Urban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A performance evaluation of local descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Recurrent models of visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Modeling the shape of the scene: A holistic representation of the spatial envelope</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Fisher kernels on visual vocabularies for image categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dance</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">CNN features off-the-shelf: an astounding baseline for recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Razavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Azizpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Carlsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshop</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">The perceptron: a probabilistic model for information storage and organization in the brain. Psychological review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Rosenblatt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Discriminative learning of deep convolutional feature point descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Simo-Serra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Trulls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ferraz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning local feature descriptors using convex optimisation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">DeepFace: Closing the gap to human-level performance in face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Combining appearance and topology for wide baseline matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Carlsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Daisy: An efficient dense descriptor applied to wide-baseline stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">DeepPose: Human pose estimation via deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Dense segmentation-aware descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Trulls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sanfeliu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">VLFeat: An open and portable library of computer vision algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fulkerson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimedia</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">DeepFlow: Large displacement optical flow with deep matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Towards linear-time incremental structure from motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">ASIFT: An Algorithm for Fully Affine Invariant Comparison</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-M</forename><surname>Morel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Processing On Line</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning to compare image patches via convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning deep features for scene recognition using places database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
