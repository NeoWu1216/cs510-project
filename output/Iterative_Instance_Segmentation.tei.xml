<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Iterative Instance Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Li</surname></persName>
							<email>ke.li@eecs.berkeley.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
								<orgName type="institution">UC Berkeley</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
							<email>bharathh@fb.com</email>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
								<orgName type="institution">UC Berkeley</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
							<email>malik@eecs.berkeley.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
								<orgName type="institution">UC Berkeley</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Iterative Instance Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:42+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Existing methods for pixel-wise labelling tasks generally disregard the underlying structure of labellings, often leading to predictions that are visually implausible. While incorporating structure into the model should improve prediction quality, doing so is challenging -manually specifying the form of structural constraints may be impractical and inference often becomes intractable even if structural constraints are given. We sidestep this problem by reducing structured prediction to a sequence of unconstrained prediction problems and demonstrate that this approach is capable of automatically discovering priors on shape, contiguity of region predictions and smoothness of region contours from data without any a priori specification. On the instance segmentation task, this method outperforms the state-of-the-art, achieving a mean AP r of 63.6% at 50% overlap and 43.3% at 70% overlap.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In computer vision, the objective of many tasks is to predict a pixel-wise labelling of the input image. While the intrinsic structure of images constrains the space of sensible labellings, existing approaches typically eschew leveraging such cues and instead predict the label for each pixel independently. Consequently, the resulting predictions may not be visually plausible. To mitigate this, a common strategy is to perform post-processing on the predictions using superpixel projections <ref type="bibr" target="#b15">[16]</ref> or conditional random fields (CRFs) <ref type="bibr" target="#b18">[19]</ref>, which ensures the final predictions are consistent with local appearance cues like colour and texture but fails to account for global object-level cues like shape.</p><p>Despite its obvious shortcomings, this strategy enjoys popularity, partly because incorporating global cues requires introducing higher-order potentials in the graphical model and often makes inference intractable. Because inference in general graphical models is NP-hard, extensive work on structured prediction has focused on devising efficient inference algorithms in special cases where the higherorder potentials take on a particular form. Unfortunately, <ref type="figure">Figure 1</ref>: A challenging image in which object instances are segmented incorrectly. While pixels belonging to the category are identified correctly, they are not correctly separated into instances. this restricts the expressive power of the model. As a result, care must be taken to formulate the cues of interest as higher-order potentials of the desired form, which may not be possible. Moreover, low-energy configurations of the potentials often need to be specified manually a priori, which may not be practical when the cues of interest are complex and abstract concepts like shape.</p><p>In this paper, we devise a method that learns implicit shape priors and use them to improve the quality of the predicted pixel-wise labelling. Instead of attempting to capture shape using explicit constraints, we would like to model shape implicitly and allow the concept of shape to emerge from data automatically. To this end, we draw inspiration from iterative approaches like auto-context <ref type="bibr" target="#b31">[32]</ref>, inference machines <ref type="bibr" target="#b25">[26]</ref> and iterative error feedback (IEF) <ref type="bibr" target="#b5">[6]</ref>. Rather than learning a model to predict the target in one step, we decompose the prediction process into multiple steps and allow the model to make mistakes in intermediate steps as long as it is able to correct them in subsequent steps. By learning to correct previous mistakes, the model must learn the underlying structure in the output implicitly in order to use it to make corrections.</p><p>To evaluate if the method is successful in learning shape constraints, a perfect testbed is the task of instance segmentation, the goal of which is to identify the pixels that belong to each individual object instance in an image. Because the unit of interest is an object instance rather than an entire object category, methods that leverage only local cues have difficulty in identifying the instance a pixel belongs to in scenes with multiple object instances of the same category that are adjacent to one another, as illustrated in Figure 1. We demonstrate that the proposed method is able to successfully learn a category-specific shape prior and correctly suppresses pixels belonging to other instances. It is also able to automatically discover a prior favouring contiguity of region predictions and smoothness of region contours despite these being not explicitly specified in the model. Quantitatively, it outperforms the state-of-the-art and achieves a mean AP r of 63.6% at 50% overlap and 43.3% at 70% overlap.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Yang et al. <ref type="bibr" target="#b32">[33]</ref> first described the task of segmenting out individual instances of a category. The metrics we use in this paper were detailed by Tighe et al. <ref type="bibr" target="#b29">[30]</ref>, who proposed non-parametric transfer of instance masks from the training set to detected objects, and by Hariharan et al. <ref type="bibr" target="#b13">[14]</ref> who used convolutional neural nets (CNNs) <ref type="bibr" target="#b19">[20]</ref> to classify region proposals. We use the terminology and metrics proposed by the latter in this paper. Dai et al. <ref type="bibr" target="#b7">[8]</ref> used ideas from <ref type="bibr" target="#b16">[17]</ref> to speed up the CNN-based proposal classification significantly.</p><p>A simple way of tackling this task is to run an object detector and segment out each detected instance. The notion of segmenting out detected objects has a long history in computer vision. Usually this idea has been used to aid semantic segmentation, or the task of labeling pixels in an image with category labels. Borenstein and Ullman <ref type="bibr" target="#b2">[3]</ref> first suggested using category-specific information to improve the accuracy of segmentation. Yang et al. <ref type="bibr" target="#b32">[33]</ref> start from object detections from the deformable parts model <ref type="bibr" target="#b9">[10]</ref> and paste figure-ground masks for each detected object. Similarly, Brox et al. <ref type="bibr" target="#b4">[5]</ref> and Arbeláez et al. <ref type="bibr" target="#b0">[1]</ref> paste figureground masks for poselet detections <ref type="bibr" target="#b3">[4]</ref>. Recent advances in computer vision have all but replaced early detectors such as DPM and poselets with ones based on CNNs <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b10">11]</ref> and produced dramatic improvements in performance in the process. In the CNN era, Hariharan et al. <ref type="bibr" target="#b15">[16]</ref> used features from CNNs to segment out R-CNN detections <ref type="bibr" target="#b11">[12]</ref>.</p><p>When producing figure-ground masks for detections, most of these approaches predict every pixel independently. However, this disregards the fact that pixels in the image are hardly independent of each other, and a figure-ground labeling has to satisfy certain constraints. Some of these constraints can be simply encoded as local smoothness: nearby pixels of similar color should be labeled similarly. This can be achieved simply by aligning the predicted segmentation to image contours <ref type="bibr" target="#b4">[5]</ref> or projecting to superpixels <ref type="bibr" target="#b15">[16]</ref>. More sophisticated approaches model the problem using CRFs with unary and pairwise potentials <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b18">19]</ref>. Later work considers extending these models by incorporating higher-order potentials of specific forms for which inference is tractable <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b20">21]</ref>. A related line of work explores learning a generative model of masks <ref type="bibr" target="#b8">[9]</ref> using a deep Boltzmann machine <ref type="bibr" target="#b27">[28]</ref>. Zheng et al. <ref type="bibr" target="#b34">[35]</ref> show that inference in CRFs can be viewed as recurrent neural nets and trained together with a CNN to label pixels, resulting in large gains. Another alternative is to use eigenvectors obtained from normalized cuts as an embedding for pixels <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b21">22]</ref>.</p><p>However, images contain more structure than just local appearance-dependent smoothness. For instance, one high informative form of global cue is shape; in the case of persons, it encodes important constraints like two heads cannot be part of the same person, the head must be above the torso and so on. There has been prior work on handling such constraints in the pose estimation task by using graphical models defined over keypoint locations <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b30">31]</ref>. However, in many applications, keypoint locations are unknown and such constraints must be enforced on raw pixels. Explicitly specifying these constraints on pixels is impractical, since it would require formulating potentials that are capable of localizing different parts of an object, which itself is a challenging task. Even if this could be done, the potentials that are induced would be higher order (which arises from the relative position constraints among multiple parts of an object) and non-submodular (due to mutual exclusivity constraints between pixels belonging to two different heads). This makes exact inference and training in these graphical models intractable.</p><p>Auto-context <ref type="bibr" target="#b31">[32]</ref> and inference machines <ref type="bibr" target="#b25">[26]</ref> take advantage of the observation that performing accurate inference does not necessarily require modelling the posterior distribution explicitly. Instead, these approaches devise efficient iterative inference procedures that directly approximate message passing. By doing so, they are able to leverage information from distant spatial locations when making predictions while remaining computationally efficient. In a similar spirit, other methods model the iterative process as recurrent neural nets <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b34">35]</ref>. IEF <ref type="bibr" target="#b5">[6]</ref> uses a related approach on the task of human pose estimation by directly refining the prediction rather than approximating message passing in each iteration. While this approach shows promise when the predictions lie in a lowdimensional space of possible 2D locations of human joints, <ref type="figure">Figure 2</ref>: The proposed method decomposes the prediction process into multiple steps, each of which consists of performing unconstrained inference on the input image and the prediction from the preceding step. The diagram above illustrates a threestep prediction procedure when a convolutional neural net is used as the underlying model, as is the case with our method when applied to instance segmentation.</p><p>it is unclear if it will be effective when the output is highdimensional and embeds complex structure like shape, as is the case with tasks that require a pixel-wise labelling of the input. In this paper, we devise an iterative method that supports prediction in high-dimensional spaces without a natural distance metric for measuring conformity to structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Task and Setting</head><p>The objective of the instance segmentation task, also known as simultaneous detection and segmentation (SDS), is to predict the segmentation mask for each object instance in an image. Typically, an object detection system is run in the first stage of the pipeline, which generates a set of candidate bounding boxes along with the associated detection scores and category labels. Next, non-maximum suppression (NMS) is applied to these detections, which are then fed into the segmentation system, which predicts a heatmap for each bounding box representing the probability of each pixel inside the bounding box belonging to the foreground object of interest. The heatmaps then optionally undergo some form of post-processing, such as projection to superpixels. Finally, they are binarized by applying a threshold, yielding the final segmentation mask predictions. We use fast R-CNN <ref type="bibr" target="#b10">[11]</ref> trained on MCG <ref type="bibr" target="#b1">[2]</ref> bounding box proposals as our detection system and focus on designing the segmentation system in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Segmentation System</head><p>For our segmentation system, we use a CNN that takes a 224 × 224 patch as input and outputs a 50 × 50 heatmap prediction. The architecture is based on that of the hypercolmumn net proposed by Hariharan et al. <ref type="bibr" target="#b15">[16]</ref>, which is designed to be sensitive to image features at finer scales and relative locations of feature activations within the bounding box. Specifically, we use the architecture based on the VGG 16-layer net <ref type="bibr" target="#b28">[29]</ref> (referred to as "O-Net" in <ref type="bibr" target="#b15">[16]</ref>), in which heatmaps are computed from the concatenation of upsampled feature maps from multiple intermediate layers, known as the hypercolumn representation. The CNN is trained end-to-end on the PASCAL VOC 2012 training set with ground truth instance segmentation masks from the Semantic Boundaries Dataset (SBD) <ref type="bibr" target="#b12">[13]</ref> starting from an initialization from the weights of a net finetuned for the detection task using R-CNN <ref type="bibr" target="#b11">[12]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Algorithm</head><p>We would like to incorporate global cues like shape when making predictions. Shape encodes important structural constraints, such as the fact that a person cannot have two heads, which is why humans are capable of recognizing the category of an object from its silhouette almost effortlessly. So, leveraging shape enables us to disambiguate region hypotheses that all correctly cover pixels belonging to the category of interest but may group pixels into instances incorrectly.</p><p>Producing a heatmap prediction that is consistent with shape cues is a structured prediction problem, with the structure being shape constraints. The proposed algorithm works by reducing the structured prediction problem to a sequence of unconstrained prediction problems. Instead of forcing the model to produce a prediction that is consistent with both the input and the structure in a single step, we allow the model to disregard structure initially and train it to correct its mistakes arising from disregarding structure over multiple steps, while ensuring consistency of the prediction with the input in each step. The final prediction is therefore consistent with both the input and the structure. Later, we demonstrate that this procedure is capable to learning a shape prior, a contiguity prior and a contour smoothness prior purely from data without any a priori specification to bias the learning towards finding these priors.</p><p>At test time, in each step, we feed the input image and the prediction from the previous step, which defaults to constant prediction of 1/2 in the initial step, into the model and take the prediction from the last step as our final prediction. In our setting, the model takes the form of a CNN. Please see <ref type="figure">Figure 2</ref> for a conceptual illustration of this procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Training Procedure</head><p>Require: D is a training set consisting of (x, y) pairs, where x and y denote the instance and the ground truth labelling respectively, and f is the model</p><formula xml:id="formula_0">function TRAIN(D, f ) // p (t)</formula><p>x is the predicted labelling of x in the t th stage p (0)</p><formula xml:id="formula_1">x ← 1/2 · · · 1/2 T ∀ (x, y) ∈ D for t = 1 to N do // Training set for the current stage T ← x p (i) x , y (x, y) ∈ D, i &lt; t Train model f on T starting from the current parameters of f p (t) x ← f x p (t−1) x ∀ (x, y) ∈ D end for return f end function Algorithm 2 Testing Procedure Require: f is the model and x is an instance function TEST(f , x) //ŷ (t) is the predicted labelling of x after t iterationŝ y (0) ← 1/2 · · · 1/2 T for t = 1 to M dô y (t) ← f x y (t−1) end for returnŷ (M ) end function</formula><p>Training the model is straightforward and is done in stages: in the first stage, the model is trained to predict the ground truth segmentation mask with the previous heatmap prediction set to 1/2 for all pixels and the predictions of the model at the end of training are stored for later use. In each subsequent stage, the model is trained starting from the parameter values at the end of the previous stage to predict the ground truth segmentation mask from the input image and a prediction for the image generated during any of the preceding stages.</p><p>Pseudocode of the training and testing procedures are shown in Algorithms 1 and 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Discussion</head><p>Modelling shape constraints using traditional structured prediction approaches would be challenging for three reasons. First, because the notion of shape is highly abstract, it is difficult to explicitly formulate the set of structural constraints it imposes on the output. Furthermore, even if it could be done, manual specification would introduce biases that favour human preconceptions and lead to inaccuracies in the predictions. Therefore, manually engineering the form of structural constraints is neither feasible or desirable. Hence, the structural constraints are unknown and must be learned from data automatically. Second, because shape imposes constraints on the relationship between different parts of the object, such as the fact that a person cannot have two heads, it is dependent on the semantics of the image. As a result, the potentials must be capable of representing high-level semantic concepts like "head" and would need to have complex non-linear dependence on the input image, which would complicate learning. Finally, because shape simultaneously constrains the labels of many pixels and enforce mutual exclusivity between competing region hypotheses, the potentials would need to be of higher order and non-submodular, often making inference intractable.</p><p>Compared to the traditional single-step structured prediction paradigm, the proposed multi-step prediction procedure is more powerful because it is easier to model local corrections than the global structure. This can be viewed geometrically -a single-step prediction procedure effectively attempts to model the manifold defined by the structure directly, the geometry of which could be very complex. In contrast, our multi-step procedure learns to model the gradient of an implicit function whose level set defines the manifold, which tends to have much simpler geometry. Because it is possible to recover the manifold, which is a level set of an implicit function, from the gradient of the function, learning the gradient suffices for modelling structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Implementation Details</head><p>We modify the architecture introduced by Hariharan et al. <ref type="bibr" target="#b15">[16]</ref> as follows. Because shape is only expected to be consistent for objects in the same category, we make the weights of the first layer category-dependent by adding twenty channels to the input layer, each corresponding to a different object category. The channel that corresponds to the category given by the detection system contains the heatmap prediction from the previous step, and channels corresponding to other categories are filled with zeros. To prepare the input to the CNN, patches inside the bounding boxes generated by the detection system are extracted and anisotropically scaled to 224 × 224 and the ground truth segmentation mask is transformed accordingly. Because the heatmap prediction from the preceding step is 50 × 50, we upsample it to 224 × 224 using bilinear interpolation before feeding it in as input. To ensure learning is wellconditioned, the heatmap prediction is rescaled and centred element-wise to lie in the range [−127, 128] and the weights corresponding to the additional channels are ini-tialized randomly with the same standard deviation as that of the weights corresponding to the colour channels.</p><p>The training set includes all detection boxes that overlap with the ground truth bounding boxes by more than 70%. At training time, boxes are uniformly sampled by category, and the weights for upsampled patches are set proportionally to their original areas for the purposes of computing the loss. The weights for all layers that are present in the VGG 16-layer architecture are initialized from the weights finetuned on the detection task and the weights for all other layers are initialized randomly. The loss function is the sum of the pixel-wise negative log likelihoods of the ground truth. The net is trained end-to-end using SGD on mini-batches of 32 patches with a learning rate of 5 × 10 −5 and momentum of 0.9. We perform four stages of training and train for 30K, 42.5K, 50K and 20K iterations in stages one, two, three and four respectively. We find that the inference procedure typically converges after three steps and so we use three iterations at test time.</p><p>We can optionally perform post-processing by projecting to superpixels. To generate region predictions from heatmaps, we colour in a pixel or superpixel if the mean heat intensity inside a pixel or superpixel is greater than 40%. Finally, we can rescore the detections in the same manner as <ref type="bibr" target="#b15">[16]</ref> by training support vector machines (SVMs) on features computed on the bounding box and the region predictions. To construct the training set, we take all bounding box detections that pass non-maximum suppression (NMS) using a bounding box overlap threshold of 70% and include those that overlap with the ground truth by more than 70% as positive instances and those by less than 50% as negative instances. To compute the features, we feed in the original image patch and the patch with the region background masked out to two CNNs trained as described in <ref type="bibr" target="#b14">[15]</ref>. To obtain the final set of detections, we compute scores using the trained SVMs and apply NMS using a region overlap threshold of 30%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Evaluation</head><p>We evaluate the proposed method in terms of region average precision (AP r ), which is introduced by <ref type="bibr" target="#b14">[15]</ref>. Region average precision is defined in the same way as the standard average precision metric used for the detection task, with the difference being the computation of overlap between the prediction and the ground truth. For instance segmentation, overlap is defined as the pixel-wise intersection-over-union (IoU) of the region prediction and the ground truth segmentation mask, instead of the IoU of their respective bounding boxes. We evaluate against the SBD instance segmentation annotations on the PASCAL VOC 2012 validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>First, we visualize the improvement in prediction accuracy as training progresses. In <ref type="figure" target="#fig_0">Figure 3</ref>, we show the pixelwise heatmap predictions on image patches from the PAS-CAL VOC 2012 validation set after each stage of training. As shown, prediction quality steadily improves with each successive stage of training. Initially, the model is only able to identify some parts of the object; with each stage of training, it learns to recover additional parts of the object that were previously missed. After four stages of training, the model is able to correctly identify most parts belonging to the object. This indicates that the model is able to learn to make local corrections to its predictions in each stage. After four stages of training, the predictions are reasonably visually coherent and consistent with the underlying structure of the output space. Interestingly, the model gradually learns to suppress parts of other objects, as shown by the predictions on the bicycle and horse images, where the model learns to suppress parts of the pole and the other horse in later stages. Next, we compare the performance of the proposed method with that of existing methods. As shown in <ref type="table" target="#tab_1">Table  1</ref>, the proposed method outperforms all existing methods in terms of mean AP r at both 50% and 70%. We analyze performance at a more granular level by comparing the proposed method to the state-of-the-art method, the hypercolumn net <ref type="bibr" target="#b15">[16]</ref>, under three settings: without superpixel projection, with superpixel projection and with superpixel projection and rescoring. As shown in <ref type="table">Table 2</ref>, the proposed method achieves higher mean AP r at 50% and 70% than the state-of-the-art in each setting. In particular, the proposed method achieves an 9.3-point gain over the state-of-   the-art in terms of its raw pixel-wise prediction performance at 70% overlap. This indicates the raw heatmaps produced by the proposed method are more accurate than those produced by the vanilla hypercolumn net. As a result, the proposed method requires less reliance on post-processing. We confirm this intuition by visualizing the heatmaps in <ref type="figure" target="#fig_1">Figure 4</ref>. When superpixel projection is applied, the proposed method improves performance by 1.7 points and 3.8 points at 50% and 70% overlaps respectively. With rescoring, the proposed method obtains a mean AP r of 63.6% at 50% overlap and 43.3% at 70% overlap, which represent the best performance on the instance segmentation task to date. We break down performance by category under each setting in the supplementary material. We examine heatmap and region predictions of the proposed method and the vanilla hypercolumn net, both with and without applying superpixel projection. As shown in <ref type="figure" target="#fig_1">Figure 4</ref>, the pixel-wise heatmap predictions produced by the proposed method are generally more visually coherent than those produced by the vanilla hypercolumn net. In particular, the proposed method predicts regions that are more consistent with shape. For example, the heatmap predictions produced by the proposed method for the sportscaster <ref type="bibr">Method</ref>   <ref type="table">Table 2</ref>: Performance comparison of the proposed method and the state-of-the-art under different settings. and the toddler images contain less noise and correctly identify most foreground pixels with high confidence. In contrast, the heatmap predictions produced by the hypercolumn net are both noisy and inconsistent with the typical shape of persons. On the bicycle image, the proposed method is able to produce a fairly accurate segmentation, whereas the hypercolumn net largely fails to find the contours of the bicycle. On the horse image, the proposed method correctly identifies the body and the legs of the horse. It also incorrectly hallucinates the head of the horse, which is actually occluded; this mistake is reasonable given the similar appearance of adjacent horses. This effect provides some evidence that the method is able to learn a shape prior successfully; because the shape prior discounts the probability of seeing a headless horse, it causes the model to hallucinate a head. On the other hand, the hypercolumn net chooses to hedge its bets on the possible locations of the head and so the resulting region prediction is noisy in the area near the expected location of the head. Notably, the region predictions generated by the proposed method also tend to contain fewer holes and have smoother contours than those produced by the hypercolumn net, which is apparent in the case of the sportscaster and toddler images. This suggests that the model is able to learn a prior favouring the contiguity of regions and smoothness of region contours. More examples of heatmap and region predictions can be found in the supplementary material. Applying superpixel projection significantly improves the region predictions of the vanilla hypercolumn net. It effectively smoothes out noise in the raw heatmap predictions by averaging the heat intensities over all pixels in a superpixel. As a result, the region predictions contain fewer holes after applying superpixel projection, as shown by the predictions on the sportscaster and toddler images. Superpixel projection also ensures that the region predictions conform to the edge contours in the image, which can result in a significant improvement if the raw pixel-wise region prediction is very poor, as is the case on the bicycle image. On the other hand, because the raw pixel-wise predictions of the proposed method are generally less noisy and have more accurate contours than those of the hypercolumn net, superpixel projection does not improve the quality of predictions as significantly. In some cases, it may lead to a performance drop, as pixel-wise prediction may capture details that are missed by the superpixel segmentation. As an example, on the bicycle image, the seat is originally segmented correctly in the pixel-wise prediction, but is completely missed after applying superpixel projection. Therefore, superpixel projection has the effect of masking prediction errors and limits performance when the quality of pixel-wise predictions becomes better than that of the superpixel segmentation.</p><p>We find that the proposed method is able to avoid some of the mistakes made by the vanilla hypercolumn net on images with challenging scene configurations, such as those depicting groups of people or animals. On such images, the hypercolumn net sometimes includes parts of adjacent persons in region predictions. Several examples are shown in <ref type="figure" target="#fig_2">Figure 5</ref>, in which region predictions contain parts from different people or animals. The proposed method is able to suppress parts of adjacent objects and correctly exclude them from region predictions, suggesting that the learned shape prior is able to help the model disambiguate region hypotheses that are otherwise consistent with local appearance cues.</p><p>We now analyze the improvement in overlap between region predictions and the ground truth segmentation masks at the level of individual detections. In <ref type="figure">Figure 6</ref>, we plot the maximum overlap of the pixel-wise region prediction produced by the proposed method with the ground truth against that of the region prediction generated by the vanilla hypercolumn net for each of the top 200 detections in each category. So, in this plot, any data point above the diagonal represents a detection for which the proposed method produces a more accurate region prediction than the hypercolumn net. We find overlap with ground truth improves for 76% of the detections, degrades for 15.6% of the detections and remains the same for the rest. This is reflected in the plot, where the vast majority of data points lie above the diagonal, indicating that the proposed method improves the accuracy of region predictions for most detections.</p><p>Remarkably, for detections on which reasonably good overlap is achieved using the vanilla hypercolumn net, which tend to correspond to bounding boxes that are welllocalized, the proposed method can improve overlap by 15% in many cases. Furthermore, the increase in overlap tends to be the greatest for detections on which the hypercolumn net achieves 75% overlap; when the proposed method is used, overlap for these detections at times reach more than 90%. This is particularly surprising given that improving upon good predictions is typically challenging.</p><p>Such a performance gain is conceptually difficult to achieve without leveraging structure in the output. This suggests that the proposed method is able to use the priors it learned to further refine region predictions that are already very accurate. Overlap between Prediction of Proposed Method and Ground Truth <ref type="figure">Figure 6</ref>: Comparison of maximum overlap of region predictions produced by the vanilla hypercolumn net and the proposed method with the ground truth. Each data point corresponds to a bounding box detection and the colour of each data point denotes the category of the detection. Points that lie above the diagonal represent detections for which the region predictions produced by the proposed method are more accurate than those produced by the hypercolumn net.</p><p>Finally, we conduct an experiment to test whether the proposed method is indeed able to learn a shape prior more directly. To this end, we select an image patch from the PASCAL VOC 2012 validation set that contains little visually distinctive features, so that it does not resemble an object from any of the categories. We then feed the patch into the model along with an arbitrary category label, which essentially forces the model to try to interpret the image as that of an object of the particular category. We are interested in examining if the model is able to hallucinate a region that is both consistent with the input image and resembles an object from the specified category. <ref type="figure">Figure 7</ref> shows the input image and the resulting heatmap predictions under different settings of category. As shown, when the category is set to bird, the heatmap prediction resembles the body and the wing of a bird. When the category is set to horse, the model hallucinates the body and the legs of a horse. Interestingly, the wing of the bird and the legs of the horse are hallucinated even though there are no corresponding contours that resemble these parts in the input image. When the category is set to bicycle, the model interprets the edges in the input image as the frame of a bicycle, which contrasts with the heatmap prediction when the category is set to television, which is not sensitive to thin edges in the input image and instead contains a large contiguous box that resembles the shape of a television set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bird Horse</head><p>Bicycle TV <ref type="figure">Figure 7</ref>: Heatmap predictions of the proposed method under different settings of category. As shown, the model is able to hallucinate plausible shapes that correspond to the specified categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We presented a method that is able to take advantage of the implicit structure that underlies the output space when making predictions. The method does not require manual specification of the form of the structure a priori and is able to discover salient structure from the data automatically. We applied the method to the instance segmentation task and showed that the method automatically learns a prior on shape, contiguity of regions and smoothness of region contours. We also demonstrated state-of-the-art performance using the method, which achieves a mean AP r of 63.6% and 43.3% at 50% and 70% overlaps respectively. The method is generally applicable to all tasks that require the prediction of a pixel-wise labelling of the input image; we hope the success we demonstrated on instance segmentation will encourage application to other such tasks and further exploration of the method.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Heatmap predictions on images from the PAS-CAL VOC 2012 validation set after each stage of training. Best viewed in colour.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Comparison of heatmap and region predictions produced by the proposed method and the vanilla hypercolumn net on images from the PASCAL VOC 2012 validation set. Best viewed in colour.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Region predictions on images with challenging scene configurations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 :</head><label>1</label><figDesc>Performance of the proposed method compared to existing methods.</figDesc><table></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. This work was supported by ONR MURI N00014-09-1-1051 and ONR MURI N00014-14-1-0671. Ke Li thanks the Natural Sciences and Engineering Research Council of Canada (NSERC) for fellowship support. The authors also thank NVIDIA Corporation for the donation of GPUs used for this research.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Semantic segmentation using regions and parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multiscale combinatorial grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="328" to="335" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Class-specific, top-down segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Borenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ullman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Detecting people using mutually consistent poselet activations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<idno>ECCV. 2010. 2</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Object segmentation by alignment of poselet activations to image contours</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Human pose estimation with iterative error feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.06550</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Semantic segmentation with second-order pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caseiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Batista</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
		<idno>ECCV. 2012. 6</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Convolutional feature masking for joint object and stuff segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The shape boltzmann machine: a strong model of object shape</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="155" to="176" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained partbased models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">9</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Semantic contours from inverse detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV), 2011 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="991" to="998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Simultaneous detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Simultaneous detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2014</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Hypercolumns for object segmentation and fine-grained localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A principled deep random field model for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Osokin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2013 IEEE Conference on</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1971" to="1978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Efficient inference in fully connected CRFs with Gaussian edge potentials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Exploring compositional high order pattern potentials for structured output learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2013 IEEE Conference on</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="49" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Object detection and segmentation from joint embedding of parts and pixels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Biased normalized cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">K</forename><surname>Vishnoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The truth about cats and dogs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jawahar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Recurrent convolutional neural networks for scene labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H O</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning message-passing inference machines for structured prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Munoz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Bagnell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Grabcut: Interactive foreground extraction using iterated graph cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kolmogorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="309" to="314" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on artificial intelligence and statistics</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="448" to="455" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Scene parsing with object instances and occlusion handling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tighe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niethammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Joint training of a convolutional network and a graphical model for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Auto-context and its application to highlevel vision tasks and 3D brain image segmentation. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1744" to="1757" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Layered object models for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hallman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">9</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Articulated human detection with flexible mixtures of parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">12</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jayasumana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03240</idno>
		<title level="m">Conditional random fields as recurrent neural networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
