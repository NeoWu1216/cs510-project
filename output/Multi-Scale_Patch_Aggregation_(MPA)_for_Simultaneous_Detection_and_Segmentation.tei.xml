<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-scale Patch Aggregation (MPA) for Simultaneous Detection and Segmentation *</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
							<email>sliu@cse.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="laboratory">SenseTime Group Limited</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
							<email>xjqi@cse.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="laboratory">SenseTime Group Limited</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
							<email>shijianping@sensetime.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">SenseTime Group Limited</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Zhang</surname></persName>
							<email>hzhang@cse.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="laboratory">SenseTime Group Limited</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">SenseTime Group Limited</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Multi-scale Patch Aggregation (MPA) for Simultaneous Detection and Segmentation *</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:43+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Aiming at simultaneous detection and segmentation (SD-S), we propose a proposal-free framework, which detect and segment object instances via mid-level patches. We design a unified trainable network on patches, which is followed by a fast and effective patch aggregation algorithm to infer object instances. Our method benefits from end-to-end training. Without object proposal generation, computation time can also be reduced. In experiments, our method yields results 62.1% and 61.8% in terms of mAP r on VOC2012 segmentation val and VOC2012 SDS val, which are stateof-the-art at the time of submission. We also report results on Microsoft COCO test-std/test-dev dataset in this paper.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Object detection and semantic segmentation have been core tasks of image understanding for long time. Object detection focuses on generating bounding boxes for objects. These boxes may not be accurate enough to localize objects. Meanwhile semantic segmentation is to predict a more detailed mask in pixel-level for different classes. It however ignores existence of single-object instances.</p><p>Recently, simultaneous detection and segmentation (S-DS) <ref type="bibr" target="#b13">[14]</ref> becomes a promising direction to generate pixellevel labels for every object instance, naturally leading to the next-generation object recognition <ref type="bibr" target="#b23">[24]</ref> goal. Accurate and efficient SDS can be used in a lot of disciplines as a fundamental tool, where both pixel-wise label and object instance information can help build robotics, achieve automatic driving, enhance surveillance systems, construct intelligent home, to name a few.</p><p>SDS is more challenging than object detection and semantic segmentation separately. In this task, instance-level information and pixel-wise accurate mask for objects are to be estimated. Nearly all previous work <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b2">3]</ref> took * This work is supported by a grant from the Research Grants Council of the Hong Kong SAR <ref type="bibr">(project No. 413113</ref>). the bottom-up segment-based object proposals <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b30">31]</ref> as input and modeled the system as classifying proposals with the help of powerful deep convolutional neural networks (DCNNs). Classified proposals are either output or refined in post-processing to produce final results.</p><p>Issues of Object Proposals in SDS It has been noticed that systems with object-proposal input may be accompanied by a few shortcomings. First, generating segmentbased proposals takes time. The high-quality proposal generator <ref type="bibr" target="#b30">[31]</ref> that was employed in previous SDS work <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b2">3]</ref> takes about 40 seconds to process one image. It was discussed in <ref type="bibr" target="#b4">[5]</ref> that using previous faster segmentbased proposals decreases performance. The newest proposal generators <ref type="bibr" target="#b29">[30]</ref> are not evaluated yet for SDS.</p><p>Second, the overall SDS performance is bounded by the quality of proposals since they only select provided proposals. Object proposals inevitably contain noise regarding missing objects and errors inside each proposal. Last but not least, if a SDS system is independent of object proposal generation, end-to-end parameter tuning is impossible. Consequently, the system loses the chance to learn feature and structure information from images directly, which however could be important to further improve the system performance with information feedback.</p><p>Our End-to-End SDS Solution To address these issues, we propose a systematically feasible scheme to integrate object proposal generation into the networks, enabling end-to-end training from images to pixel-level labels for instance-aware semantic segmentation.</p><p>Albeit beautiful in concept, practically establishing suitable models is difficult due to various scales, aspect ratios, and deformation of objects. In our work, instead of segmenting objects directly, we propose segmenting and classifying part of or entire objects using many densely located patches. The mask of an object is then generated by aggregating masks of the overlapping patches in a postprocessing step, as shown in <ref type="figure" target="#fig_1">Fig. 1</ref>. This scheme shares the spirit of mid-level representation <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b35">36]</ref> and part-based  model <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b15">16]</ref>. It is yet different by nature in terms of system construction and optimization. In our scheme, overlapped patches gather different levels of information for final object segmentation, which makes the result more robust than prediction from only one input. Our end-to-end trainable SDS system is thus with output of semantic segment labels in patches.</p><p>Our Contributions Our framework to tackle the SDS problem makes the following main contributions.</p><p>• We propose the strategy to generate dense multi-scale patches for object parsing.</p><p>• Our unified end-to-end trainable proposal-free network can achieve segmentation and classification simultaneously for each patch. By sharing convolution in the network, computation time is reduced and good quality results are produced.</p><p>• We develop an efficient algorithm to infer the segmentation mask for each object by merging information from mid-level patches.</p><p>We evaluated our method on PASCAL VOC 2012 segmentation validation and VOC 2012 SDS validation benchmark datasets. Our method yields state-of-the-art performance with reasonably short running time. We also evaluated it on Microsoft COCO test-std and test-dev data. Decent performance is achieved based on the VGG-16 network structure without network ensemble.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>The SDS task is closely related to object detection, semantic segmentation, and proposal generation. We briefly review them in this section.</p><p>Object Detection Object detection has a long history in computer vision. Before DCNN shows its great ability for image classification <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b32">33]</ref>, part-based models <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b36">37]</ref> were popular. Recent object detection frameworks <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b9">10]</ref> are based on DCNN <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b32">33]</ref> to classify object proposals. These methods either take object proposals as independent input <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b33">34]</ref>, or use the entire image and pool features for each proposal <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b9">10]</ref>. Different from these methods, Ren et al. <ref type="bibr" target="#b31">[32]</ref> unified proposal generation and classification with shared convolution feature maps. It saves time to generate object proposals and yields good performance.</p><p>Semantic Segmentation DCNNs <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b32">33]</ref> also boost performance of semantic segmentation <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b24">25]</ref>. Related methods can be categorized into two streamsone utilizes DCNNs to classify segment proposals <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b4">5]</ref> and the other line is to use fully convolutional networks <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b24">25]</ref> for dense prediction. CRF can be applied in post-processing <ref type="bibr" target="#b1">[2]</ref> or incorporated in the network <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b24">25]</ref> to refine segment contours.</p><p>SDS SDS is a relatively new topic. Hariharan et al. <ref type="bibr" target="#b13">[14]</ref> presented pioneer work. It took segment-based object proposals <ref type="bibr" target="#b30">[31]</ref> as input similar to object detection. Two networks -one for bounding boxes and one for masks -were adopted to extract features. Then features from these networks were concatenated and classified by SVM <ref type="bibr" target="#b3">[4]</ref>.</p><p>Hariharan et al. <ref type="bibr" target="#b14">[15]</ref> used hyper-column representation to refine segment masks. But updating all proposals is computationally too costly, especially when complex networks, such as VGG <ref type="bibr" target="#b32">[33]</ref>, are deployed. So the method made use of detection results <ref type="bibr" target="#b11">[12]</ref> and a final rescore procedure was adopted. Chen et al. <ref type="bibr" target="#b2">[3]</ref> developed an energy minimization framework incorporating top-down and bottom-up informa- tion to handle occlusion <ref type="bibr" target="#b13">[14]</ref>. Dai et al. <ref type="bibr" target="#b4">[5]</ref> resolved the efficiency problem by pooling segments and bounding box features from convolutional feature maps shared by all proposals. All methods rely on proposal generation <ref type="bibr" target="#b30">[31]</ref> and conduct separate classification afterwards.</p><p>Liang et al. <ref type="bibr" target="#b21">[22]</ref> proposed a proposal-free network to tackle SDS. In <ref type="bibr" target="#b21">[22]</ref>, the category-level segmentation mask is first generated by the method of <ref type="bibr" target="#b1">[2]</ref>. Then another network is used to assign pixels to objects by predicting location of objects for every pixel. Finally, post-processing is performed to generate the instance-level mask. It is notable that we use a completely different system. Instead of having these separate steps, we aggregate mid-level patch segment prediction results for SDS. Our unified framework is thus more effective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Our Method</head><p>We solve the SDS problem via aggregation of local segment prediction results. We generate multi-scale dense patches, and classify and segment them in a network. We infer objects based on these patches. In the following, we first motivate our SDS network and give an overview.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Motivation</head><p>An object consists of patches corresponding to parts. This concept was extensively explored in mid-level representation work <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b35">36]</ref> and found useful to extract and organize structural information. Intuitively, by dividing ob-jects into semantic patches, as shown in <ref type="figure" target="#fig_2">Fig. 2</ref>, it is easier to model and highlight object variation in local regions.</p><p>Different from the traditional way <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b10">11]</ref>, which classifies sliding-windows or proposals as objects, our method regards semantic patches as part of an object. Previous proposal-classification frameworks, contrarily, are based on the assumption that most objects are already there in proposals and what remains to do is to pick them out. They do not search for missing objects and thus greatly depend on the quality of object proposals. Our strategy to utilize patches to represent objects is more flexible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Network Structure</head><p>Our network is illustrated in <ref type="figure" target="#fig_4">Fig. 3</ref>. It jointly learns the classification label and segmentation mask on each candidate patch. The key components are shared convolution layers, multi-scale patch generator, multi-class classification branch, and the segmentation branch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Convolution Layers</head><p>In our method, convolution layers are shared among subsequent classification and segmentation branches. It greatly decreases model complexity comparing to extracting features for these two branches separately. The following classification and segmentation branches can be regarded as multi-task training <ref type="bibr" target="#b10">[11]</ref>, which enhances the generalization ability of the network. In our case, the segmentation branch seeks precise localization and instance masks while the classification branch infers semantic meaning of patches. They benefit each other via the shared convolution parameters.</p><p>We adopt 13 convolution layers interleaved with ReLU and pooling layers, similar to those of VGG-16 <ref type="bibr" target="#b32">[33]</ref>. We abandon the last pooling layer to achieve our goal. We denote G as the last shared convolution feature map. With four pooling layers, the network stride is 16 -that is, the input image is down-sampled by a factor of 16.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Multi-Scale Patch Generator</head><p>One of our major contributions is multi-scale patch generator as illustrated in <ref type="figure" target="#fig_5">Fig. 4</ref>, which is essential to construct the patch-based framework. This part aims at generating multiscale patches from the original image, naturally cropping their corresponding feature grids from G, and aligning these grids to improve the generalization ability. Also, a new strategy is proposed to assign the classification and segment labels to these patches.</p><p>Candidate Patch Generation We break objects into parts and aggregate them to infer objects. This scheme is easier to achieve compared with previous proposal-based methods that require objects tightly covered by proposals. For high  recall, we use four scales of patches, i.e., 48 × 48, 96 × 96, 192×192, and 384×384 respectively. Sliding-windows are used to generate these patches with stride 16, which make each object overlapped with multiple patches in appropriate scales. Each patch P i is represented by a four-tuple (r, c, h, w), where (r, c) is the coordinate of its top-left corner while h and w are height and width. Each P i corresponds to a down-sampled feature grid G i on the feature map G. Following the design of <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b16">17]</ref>, each G i is represented as</p><formula xml:id="formula_0">( r 16 , c 16 , h 16 , w 16 ).</formula><p>The scaling factor 16 is the network stride. Then G i for all i are small grids with stride 1 on G.</p><p>By generating multi-scale patches from the single-scale input image, we get several levels of feature grids. They are cropped from the shared convolution feature map, which do not increase computation cost. The related work of learning to segment object candidates <ref type="bibr" target="#b29">[30]</ref> takes multi-scale input and utilizes single-scale windows to search for objects. The major difference is that our method is more flexible for multi-scale recognition tasks. It also saves computation time consumed by convolution layers.</p><p>Scale Alignment Note that the above feature grids G i s are with different spatial resolutions. We map them to the same scale to achieve scale-invariant generalization ability of the network for both classification and segmentation branches.</p><p>As explained above, the spatial scales of G i on G are 3 × 3, 6 × 6, 12 × 12, and 24 × 24 respectively since we apply stride 16. We use deconvolution and pooling layers to calibrate them to the same size 12×12, which well balances efficiency and effectiveness. As shown in <ref type="figure" target="#fig_5">Fig. 4</ref>, we have four cases, each corresponding to one scale. For scales 3×3 and 6 × 6, we use deconvolution layers <ref type="bibr" target="#b25">[26]</ref>   them to scale 12 × 12. The corresponding kernel-size-overstride values are 4×4 4 and 2×2 2 respectively. The deconvolution layer enriches spatial information, which is crucial for the segmentation branch. We further apply 2 × 2/2 maxpooling to map the 24 × 24 case back to the standard size. The patches with spatial scale 12 × 12 are left unchanged.</p><p>Label Assignment During Training Each patch P i should be associated with a class label l i and a binary mask M i for system training. Since P i may be just part of an object, naively cropping labels from the ground-truth mask is not optimal due to possibly complex object appearance and boundary shape. We thus design the following rules to more appropriately assign labels. We give one positive label to P i if it satisfies the following constraints.</p><p>1. Center of the patch is located on an object O n and 2. area of O n inside P i is larger than half of O n area and 3. area of O n inside P i is larger than one fifth of P i area. Only when all these constraints are satisfied, we assign the object label of O n to l i and object segment of O n contained in P i to M i . This strategy reduces noise during training. To enable multi-scale training in one pipeline and reduce computation, we down-sample the patch mask to resolution 48 × 48, which is the smallest scale of input patches.</p><p>By making patch P i only responsible for the center object O n , we are able to distinguish among individual instances. If there are multiple objects overlapped with P i , only the label and mask of O n will be predicted. In other words, for each patch, we segment only one object that the patch is responsible for, rather than multiple objects overlapped with it to minimize ambiguity. The second and third constraints are to make sure enough semantic information is involved and the scale is appropriate. This simple strategy is empirically very effective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Multi-class Classification Branch</head><p>This branch takes G i s from the multi-scale patch generator as input and predicts semantic label l i of P i . First, we apply 2 × 2 max-pooling to reduce the complexity of our model. Similar to other standard design, we utilize three fully connected layers to classify patches. The predicted score for patch P i is denoted as f c (P i ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4">Segmentation Branch</head><p>Similar to multi-class classification branch, segmentation branch also takes G i s as input. It segments part of or entire object O n in patch P i . Note that we constrain that each patch P i is only responsible for one object O n according to the center location of P i .</p><p>The intuition is similar to representing objects by the center of bounding boxes <ref type="bibr" target="#b29">[30]</ref>. But the major difference is on relaxing the constraint that the entire object is enclosed by the bounding box. The method of <ref type="bibr" target="#b29">[30]</ref> aims at generating object proposals, which needs independent classifiers for classification. Our network achieves simultaneous segmentation and classification of mid-level patches. These for P k ∈ {P i } do for P j ∈ S r (P k ) do if x k = x j then Calculate o kj , i.e., the overlap score of P k and P j ; Store P k , P j and o kj as a triple in H; Break; end if end for Calculate o kj with P j ∈ S c (P k ) in the same way; Store P k , P j and o kj as a triple in H; end for while there exist (o ij ∈ H) and (o ij &gt; τ ) do if P i and P j do not belong to any object then Create a new object Q m = Y i ∪ Y j ; else if either P i or P j belongs to an object Q n then Q n = Q n ∪ P j (or Q n = Q n ∪ P i ); else if P i ∈ Q m and P j ∈ Q n then Q m = Q m ∪ Q n ; end if Remove P i , P j and o ij from H; end while Output: inferred objects in this scale, {Q i } with both the mask and class label.</p><p>patches are not necessarily objects. After the final simple aggregation step described in Sec. 3.3, objects are inferred. We further model this branch as a pixel-wise classifier to predict each element of mask M i based on G i directly. We yield 48 × 48 classifiers, each for an element in M i composed of two fully connected layers interleaved with ReLU, as shown in <ref type="figure" target="#fig_4">Fig. 3</ref>. The final prediction vector is reshaped into the 48 × 48 score map. After segmentation, we update the size of predicted score map f s (P i ) to that of P i . We denote the resized score map asf s (P i ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.5">Training Loss and Strategy</head><p>In the training stage, we incorporate the loss of classification and segmentation branches as</p><formula xml:id="formula_1">L(w) = i [− log(f l i c (Pi)) + λI(li = 0) N j − log(f j s (Pi))],<label>(1)</label></formula><p>where f li c (P i ) is the prediction from the classification branch for P i belonging to class l i . f j s (P i ) is the prediction from the segmentation branch at location j of M i given j ∈ [0, 48 2 ]. I(l i = 0) is the indicator function, which is one if l i = 0 and zero otherwise. With this definition, we only calculate the segmentation loss for foreground objects. N is the number of elements in M i . w is the parameter to update. The first and second terms correspond to classification and segmentation. λ is set to 10 for trade-off of these two branches.</p><p>We back-propagate the gradient of this loss function to update the parameters in the network. Different from the structure of <ref type="bibr" target="#b31">[32]</ref>, which has four steps to train a two-branch network, we train the two branches simultaneously to increase stability and optimality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Patch Aggregation</head><p>After network prediction, our patches are assigned with predicted labels and segmented masks denoted as x i and Y i . It is observed that if largely overlapped patches cover an object instance, corresponding segment masks also overlap heavily. For different instances, the segmented masks are distinct from each other. This property is illustrated in <ref type="figure" target="#fig_6">Fig.  5</ref>, very common in images and usable to infer objects.</p><p>With the predicted semantic label for each patch, we prevent errors accumulated from bottom-up grouping procedures. By merging segmentation masks in nearby patches, we optimize the recall for our method.</p><p>Our method to aggregate patches is sketched in Alg. 1. For each P i , we calculate the overlap score on segment masks for neighboring patches that have the same class label. The overlap score o ij of P i and P j is defined as the intersection over union (IoU) <ref type="bibr" target="#b7">[8]</ref> of the segmented mask in respective patches. The row search range for P i is the consecutive L patches located on the left hand side of P i . We denote patches in this range as S r (P i ). The column search range includes the consecutive L patches located on top of P i , denoted as S c (P i ). We only need to search one direction along the row and column since we iterate over all patches.</p><p>The patch pair with the highest overlap score is selected where corresponding segment masks are merged. This process iterates until no existing patch pair has the overlap score higher than threshold τ . For each inferred object, we take the maximum score of those patches as its prediction score. Note that our computation is performed for different scales independently. To deal with duplicate detection results, we apply non-maximum suppression <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b13">14]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Evaluation</head><p>We evaluate our method on the same benchmark datasets <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b21">22]</ref>. We also give running-time discussion and error analysis. Our multi-scale patch aggregation framework is denoted as MPA in following experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and Evaluation Metrics</head><p>Three benchmark datasets are used. The first is VOC 2012 segmentation val <ref type="bibr" target="#b7">[8]</ref> subset. There are 1, 449 images with high quality annotations. Note that for VOC test subset <ref type="bibr" target="#b7">[8]</ref>, there is no testing server for the SDS task. We thus compare our method with PFN <ref type="bibr" target="#b21">[22]</ref> with the same training data: 10, 582 training images and annotations are from the train subset and SBD <ref type="bibr" target="#b12">[13]</ref>; images in val subset are excluded. The second dataset is the SDS val subset of VOC 2012 and the annotations are from SBD <ref type="bibr" target="#b12">[13]</ref>. There are 5, 732 images in this subset, on which we compare with the methods of <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b4">5]</ref> with the same 5, 623 training images in the train subset. We also evaluate our method on Microsoft COCO <ref type="bibr" target="#b23">[24]</ref> dataset, which is newer and more complex for SDS.</p><p>For training and fine-tuning our network, our system is built upon the Caffe platform <ref type="bibr" target="#b18">[19]</ref>. We use the released VGG-16 <ref type="bibr" target="#b32">[33]</ref> model to initialize the convolution layers in our network. While for other new layers, we randomly initialize them by sampling from a zero-mean Gaussian distribution. The initial learning rate is 0.001 and the batch size is set to 10. The momentum value is 0.9 and weight decay ratio is 0.001. We randomly pick a scale from {0. <ref type="bibr" target="#b5">6</ref>  <ref type="table">Table 3</ref>. Experimental results (in % AP r ) on VOC 2012 segmentation val by gradually increasing the IoU threshold. The entries with the best AP r s are bold-faced. posed in <ref type="bibr" target="#b13">[14]</ref>. Similar to standard mAP <ref type="bibr" target="#b7">[8]</ref>, AP r is based on the IoU of prediction and ground-truth. The difference is that AP r calculates IoU in terms of masks instead of bounding boxes. AP r vol is the average of AP r on 9 IoU thresholds, which is more comprehensive than AP r where the latter uses IoU threshold 0.5 only.</p><p>During the course of inference, we apply 1-scale or 3scale input of the image to our network. The 3 scales are 0.6, 1 and 1.4 times of the original image resolution. We use the same parameters on all datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Results on VOC 2012 Segmentation Val</head><p>We compare with PFN <ref type="bibr" target="#b21">[22]</ref> that produces state-of-the-art results on VOC 2012 segmentation val subset at the time of submission. This method is proposal-free, and yet relies on an independent segmentation network.</p><p>The results are listed in Tables 1 and 2. The 'PFN unified' system is trained by merging the segmentation and instance branches to share convolution layers while in Method mean AP r mean AP r vol SDS <ref type="bibr" target="#b13">[14]</ref> 49.7 41.4 Hypercolumn <ref type="bibr" target="#b14">[15]</ref> 56.5 -Hypercolumn-rescore <ref type="bibr" target="#b14">[15]</ref> 60.0 -CFM <ref type="bibr" target="#b4">[5]</ref> 60.  'PFN independent' scheme, two independent networks are trained. CRF <ref type="bibr" target="#b19">[20]</ref> has been used to improve segments for 'PFN'. As shown in <ref type="bibr" target="#b1">[2]</ref>, the improvement can be around 4 points under the quality measure of semantic segmentation. Thus PFN enjoys this bonus naturally.</p><p>It is noteworthy that our results are produced without CRF optimization. Even without this effective strategy, the statistics shown in the two tables manifest the high quality of our generated masks for instances. We further compare our results with PFN measured by AP r with IoU threshold ranging from 0.6 to 0.9 in <ref type="table">Table 3</ref>. At all thresholds our method performs better. Our single-scale system already achieves state-of-the-art performance, while extra 2% improvement is yielded from our 3-scale structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Results on VOC 2012 SDS Val</head><p>We compare with CFM <ref type="bibr" target="#b4">[5]</ref>, SDS <ref type="bibr" target="#b13">[14]</ref> and Hypercolumn <ref type="bibr" target="#b14">[15]</ref> on this dataset. The results are listed in <ref type="table" target="#tab_4">Table 4</ref>. Since the methods we compare with only provide mean of perclass results, we use them directly.</p><p>In particular, Hypercolumn <ref type="bibr" target="#b14">[15]</ref> utilized an independent detection system to rescore the refined mask. Our system can similarly do that by using the tight bounding boxes of our inferred objects for rescore. More specifically, we train a Fast RCNN <ref type="bibr" target="#b10">[11]</ref> on the same train subset. In the duration of testing, for each inferred object, we take detection scores from Fast RCNN. That is, we keep the label and update detection scores of our inferred objects. The results in the '3-scale-rescore' row manifest that our method works decently, 1.8% higher than Hypercolumn-rescore and 1.1% higher than CFM.</p><p>We show a few segmentation results of our method on this dataset in <ref type="figure">Fig. 6</ref>. Large and small object instances are detected and segmented out. It handles the situation where instances occlude each other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Running-time Analysis</head><p>We compare running time in this section. The experiments are conducted by randomly picking 100 images on val subset and run our system on a PC with an NVIDIA GeForce Titan X display card and Intel Core i7 3.50GHZ CPU with a single thread. Running time for other methods are quoted from respective papers.</p><p>As shown in <ref type="table" target="#tab_5">Table 5</ref>, proposal-based systems of <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b2">3]</ref> take much longer time due to the high-quality proposal generation procedure <ref type="bibr" target="#b30">[31]</ref>. While for PFN <ref type="bibr" target="#b21">[22]</ref>, the semantic segmentation step can be regarded as one kind of proposal generation, more efficient than that of <ref type="bibr" target="#b30">[31]</ref>. Our method with the single-scale input takes less than 2 seconds involving all computation. Our patch aggregation (Algorithm 1) needs only 0.1s to complete. For our 3-scale input, the system takes less than 10 seconds, which is still more efficient than previous proposal-based methods with highquality results generated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Proposal CNN Patch Agg. Total SDS <ref type="bibr" target="#b13">[14]</ref> 42.2s <ref type="bibr" target="#b30">[31]</ref> 17.9s -60.1s Hypercolumn <ref type="bibr" target="#b14">[15]</ref> 42.2s <ref type="bibr" target="#b30">[31]</ref> --&gt; 42.2s CFM <ref type="bibr" target="#b4">[5]</ref> 42.2s <ref type="bibr" target="#b30">[31]</ref> 2.1s -44.3s PFN <ref type="bibr" target="#b21">[22]</ref> 0.6s --  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Error Analysis</head><p>We utilize the tool of <ref type="bibr" target="#b13">[14]</ref> to analyze our results. As shown in <ref type="figure" target="#fig_8">Fig. 7</ref>, mis-localization has a huge impact on the performance, similar to other apporaches <ref type="bibr" target="#b13">[14]</ref>. It implies that localization accuracy of masks still has much room to improve. The impact of other two errorsi.e., confusion with similar classes or background -is much smaller. These small errors suggest that the classification ability of our network is strong.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Results on Microsoft COCO Dataset</head><p>We finally evaluate our method on Microsoft COCO dataset <ref type="bibr" target="#b23">[24]</ref>, which has 80 object classes and a large set of complex images. We train our network on trainval subset, which consists of 80k+40k images. We report our results on test-std and test-dev in terms of mAP r @IoU = [0.5 : 0.95] (COCO metric), mAP r @IoU = 0.5 (VOC metric) and mAP r @IoU = 0.75 (strict metric) in <ref type="table" target="#tab_6">Table 6</ref>. It is notable we still use the VGG-16 <ref type="bibr" target="#b32">[33]</ref> structure without network ensemble and do not conduct rescore step.</p><p>As shown in <ref type="table" target="#tab_6">Table 6</ref>, compared with contemporary work MNC-16 <ref type="bibr" target="#b5">[6]</ref>, which is also based on VGG-16 without model ensemble, our method performs decently. We expect using the 101-layer ResNet <ref type="bibr" target="#b17">[18]</ref> would yield further performance improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Concluding Remarks</head><p>We have accomplished a new SDS system based on a unified end-to-end trainable network. It takes raw images as input, and classifies and segments patches. A simple aggregation process is then adopted to infer objects from the patch output of the network. We evaluated our method on several datasets for SDS. Our future work will be to incorporate graphical models to further reduce mis-localization errors.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>Objects overlapped with many densely localized patches. After segmenting objects in different patches, aggregation can be used to infer complete objects.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Objects consist of many different patches. This example shows many semantically meaningful human body and car regions. Part of or the entire objects can be in a patch.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 .</head><label>3</label><figDesc>Overview of our network. The cubes represent feature maps and rectangles represent operations. It takes a complete image as input. Then a multi-scale patch generator is applied to produce patches on different scales and align corresponding feature grids. Segmentation and classification branches are responsible for segmenting and classifying patches respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>Illustration of patch generation and alignment. Four scales of patches correspond to the four scales of feature grids. We align them to the same resolution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 .</head><label>5</label><figDesc>Illustration of patch overlap. Segment masks of the same person are shown in rows with content overlap. For different persons, segment masks generally do not overlap.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Algorithm 1</head><label>1</label><figDesc>Patch Aggregation Input: all patches {P i } of the same scale, patch labels {x i }, patch masks {Y i }. Procedure:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 .</head><label>7</label><figDesc>Impact of errors to performance measured by AP r according to mislocalization ('L' bar); false positives when confused with similar classes ('S' bar); and detection on background ('B' bar). This figure shows increase of AP r if we remove one type of errors. (a) corresponds to the one-scale results on VOC 2012 segmentation val and (b) corresponds to the one-scale results on VOC 2012 SDS val.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>to up-sample</figDesc><table>3×3 
6×6 
12×12 
24×24 

conv feature map 

sliding window 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>VOC 2012 seg. val aero bike bird boat bottle bus car cat chaircow table dog horse mbike person plant sheep sofa train tv mean Table 1. Experimental results (in % AP r ) on VOC 2012 segmentation val. The entries with the best AP r s for each object class are bold-faced.</figDesc><table>PFN unified [22] 
72.9 18.1 78.8 55.4 23.2 63.6 17.8 72.1 14.7 64.1 44.5 69.5 71.5 63.3 39.1 9.5 27.9 47.7 72.1 57.0 49.1 
PFN independent [22] 76.4 15.6 74.2 54.1 26.3 73.8 31.4 92.1 17.4 73.7 48.1 82.2 81.7 72.0 48.4 23.7 57.7 64.4 88.9 72.3 58.7 
MPA 1-scale 
79.2 13.4 71.6 59.0 41.5 73.8 52.3 87.3 23.3 61.2 42.5 83.1 70.0 77.0 67.6 50.7 56.0 45.9 80.0 70.5 60.3 
MPA 3-scale 
79.7 11.5 71.6 54.6 44.7 80.9 62.0 85.4 26.5 64.5 46.6 87.6 71.7 77.9 72.1 48.8 57.4 48.8 78.9 70.8 62.1 

VOC 2012 seg. val aero bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tv mean 
PFN unified [22] 
71.2 22.8 74.1 47.3 24.2 55.1 18.5 69.8 15.4 56.2 40.1 63.7 63.0 56.2 38.1 13.2 31.5 41.6 63.8 47.1 45.6 
PFN independent [22] 70.8 21.1 66.7 47.6 26.7 65.3 27.5 83.2 17.2 64.5 45.1 74.7 67.9 64.5 41.3 22.1 48.8 56.5 76.2 58.2 52.3 
MPA 1-scale 
69.3 25.2 62.0 50.6 40.3 69.9 47.3 80.0 24.6 54.4 36.9 75.4 61.4 63.8 59.2 41.1 50.8 44.4 70.5 62.3 54.5 
MPA 3-scale 
71.0 23.7 64.3 50.4 42.5 74.7 54.9 78.3 26.9 59.1 40.8 76.7 61.2 64.2 62.8 42.4 53.7 46.7 73.3 63.1 56.5 

Table 2. Experimental results (in % AP r 
vol ) on VOC 2012 segmentation val. The entries with the best AP r 
vol s for each object class are 
bold-faced. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head></head><label></label><figDesc>Figure 6. SDS results generated by our method on VOC 2012 SDS val subset. For each input image, we show the ground-truth label and our segment result.</figDesc><table>, 0.8, 1, 1.2, 1.4} to resize the input image and 
crop patches with size 384 × 384. 
For evaluation, we use the metrics AP r and AP r 
vol pro-
input image 

ground-truth 
our result 
input image 
ground-truth 
our result 

IoU threshold 0.6 
0.7 
0.8 
0.9 
PFN [22] 
51.3 42.5 31.2 15.7 
MPA 1-scale 54.6 45.9 34.3 17.3 
MPA 3-scale 56.6 47.4 36.1 18.5 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4 .</head><label>4</label><figDesc>Experimental results (in % AP r and AP r vol ) on VOC 2012 SDS val. The best results are bold-faced.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 5 .</head><label>5</label><figDesc>Running-time comparison of different methods.</figDesc><table>≈ 1s 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 6 .</head><label>6</label><figDesc>Experimental results on Microsoft COCO test-std/test-dev in terms of mAP r at different thresholds.</figDesc><table></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Midlevel elements for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<idno>abs/1504.07284</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Semantic image segmentation with deep convolutional nets and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multi-instance object segmentation with occlusion handling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3470" to="3478" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Support-vector networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="273" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Convolutional feature masking for joint object and stuff segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3992" to="4000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Instance-aware semantic segmentation via multi-task network cascades</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno>ab- s/1512.04412</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Mid-level visual element discovery as discriminative mode seeking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="494" to="502" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The pascal visual object classes challenge: A retrospective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M A</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="98" to="136" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A discriminatively trained, multiscale, deformable part model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Object detection via a multiregion &amp; semantic segmentation-aware CNN model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<idno>abs/1505.01749</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Semantic contours from inverse detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Simultaneous detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="297" to="312" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Hypercolumns for object segmentation and fine-grained localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="447" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Detecting objects using deformation dictionaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno>abs/1406.4729</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno>abs/1512.03385</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Caffe</surname></persName>
		</author>
		<idno>arX- iv:1408.5093</idno>
		<title level="m">Convolutional architecture for fast feature embedding</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Efficient inference in fully connected crfs with gaussian edge potentials. CoRR, abs/1210</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">5644</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1106" to="1114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Proposal-free network for instance-level object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<idno>abs/1509.02636</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<idno>abs/1312.4400</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<idno>ab- s/1405.0312</idno>
		<title level="m">Microsoft COCO: common objects in context. CoRR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Semantic image segmentation via deep parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Feedforward semantic segmentation with zoom-out features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mostajabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yadollahpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.0774</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Learning deconvolution network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<idno>arX- iv:1505.04366</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Deepid-net: multi-stage and deformable deep convolutional neural networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<idno>abs/1409.3505</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Learning to segment object candidates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">O</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<idno>abs/1506.06204</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Multiscale combinatorial grouping for image segmentation and object proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.00848</idno>
		<imprint>
			<date type="published" when="2015-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition. CoRR, abs/1409.1556</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno>abs/1409.4842</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Segmentation as selective search for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">E A</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R R</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W M</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1879" to="1886" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Object detection by labeling superpixels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5107" to="5116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">segDeepM: Exploiting Segmentation and Context in Deep Neural Networks for Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
