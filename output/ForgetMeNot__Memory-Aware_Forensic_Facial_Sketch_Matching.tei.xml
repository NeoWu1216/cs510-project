<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ForgetMeNot: Memory-Aware Forensic Facial Sketch Matching</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuxin</forename><surname>Ouyang</surname></persName>
							<email>s.ouyang@qmul.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Posts and Telecommunications § Queen Mary University of London</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
							<email>t.hospedales@qmul.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Posts and Telecommunications § Queen Mary University of London</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Zhe</forename><surname>Song</surname></persName>
							<email>yizhe.song@qmul.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Posts and Telecommunications § Queen Mary University of London</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueming</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Posts and Telecommunications § Queen Mary University of London</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">†</forename><forename type="middle">†</forename><surname>Beijing</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Posts and Telecommunications § Queen Mary University of London</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">ForgetMeNot: Memory-Aware Forensic Facial Sketch Matching</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:40+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We investigate whether it is possible to improve the performance of automated facial forensic sketch matching by learning from examples of facial forgetting over time. Forensic facial sketch recognition is a key capability for law enforcement, but remains an unsolved problem. It is extremely challenging because there are three distinct contributors to the domain gap between forensic sketches and photos: The well-studied sketch-photo modality gap, and the less studied gaps due to (i) the forgetting process of the eye-witness and (ii) their inability to elucidate their memory. In this paper, we address the memory problem head on by introducing a database of 400 forensic sketches created at different time-delays. Based on this database we build a model to reverse the forgetting process. Surprisingly, we show that it is possible to systematically "un-forget" facial details. Moreover, it is possible to apply this model to dramatically improve forensic sketch recognition in practice: we achieve the state of the art results when matching 195 benchmark forensic sketches against corresponding photos and a 10,030 mugshot database.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Facial sketch recognition is an important law enforcement tool for determining the identity of criminals where only an eyewitness account of the suspect is available. In this situation, a forensic sketch artist renders the face of the suspect by hand or with compositing software based on eyewitness description. The facial sketch is then disseminated in the media, but the crucial capability is to then identify the suspect by matching it against a photo mugshot database.</p><p>Motivated by this, the computer vision <ref type="bibr" target="#b11">[12]</ref> and biometrics <ref type="bibr" target="#b1">[2]</ref> fields have extensively studied sketch to photo face matching. However, practical matching of forensic sketches to photo databases remains an unsolved question. This is because studies have primarily focused on matching viewed sketches rather than the rarer forensic sketches. Viewed sketches such as those in the popular CUHK <ref type="bibr" target="#b22">[23]</ref> database are drawn by artists while viewing a photo. As such there is no forgetting issue, and the sketches are accurate renditions of the subject. The cross-modal sketch-photo gap is thus small, and viewed sketches are relatively easy to match -resulting in benchmark performance saturated at near-perfect <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b11">12]</ref>. Forensic sketches are drawn based on eyewitness description, possibly days after the event. Despite being the practically relevant variant of the problem for law enforcement, forensic sketch matching remains both relatively unstudied and unsolved. It is a much harder and unsolved problem due to the sketch-photo gap being widened by: (i) forgotten / inaccurate memory of facial details <ref type="bibr" target="#b6">[7]</ref>, and (ii) imperfect communication of memory <ref type="bibr" target="#b4">[5]</ref> (whether to a human sketch-artist or software compositor <ref type="bibr" target="#b6">[7]</ref>). Nevertheless, it is relatively unstudied largely due to lesser availability of forensic sketch benchmark databases, which is why we introduce a new forensic sketch database.</p><p>In computer vision, facial sketch-photo matching has been studied extensively using a variety of approaches including invariant feature engineering <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b11">12]</ref>, crossmodal regression/synthesis <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref> and shared subspace learning <ref type="bibr" target="#b19">[20]</ref>. These contributions address the sketch/photo modality gap, but do not address the issue of forgotten or inaccurately remembered details due to imperfect memory. In contrast, psychology <ref type="bibr" target="#b24">[25]</ref> and forensic psychology <ref type="bibr" target="#b5">[6]</ref> have studied the reliability of different facial features in human face matching, and the fading of memory with time <ref type="bibr" target="#b6">[7]</ref>. This has provided some insights into human recognition (internal facial features are more important overall), and the reliability of human memory, for example that memory fidelity drops rapidly after a few hours <ref type="bibr" target="#b6">[7]</ref>. This means that forensic sketches are very inaccurate in practice, because they are usually taken days after the event <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref>. Thus the memory gap is the key underlying problem to solve.</p><p>Motivated by these studies in human memory and recognition, we investigate here whether it is possible to bring learning and computer vision techniques to bear to ameliorate the memory gap problem. To disentangle the three factors (cross-modal, forgetting, and imperfect communication) in the forensic sketch/photo gap, we introduce a new . Database and approach overview. We first learn a projection for "un-forgetting", as well as modality and description gap (top). We apply this projection to improve (un-forget) forensic sketches before matching against photos (below). Reconstructed sketch (red) is a closer match to the true photo (bottom left) than the input forensic sketch (bottom right) (visualisation with HOGgles <ref type="bibr" target="#b20">[21]</ref>).</p><p>facial sketch memory gap database that contains 100 subjects. Uniquely, each subject has a photo, a viewed sketch, a 1-hour delay sketch, a 24-hour delay sketch and an unviewed sketch. Based on this database, we investigate the question of whether memory transience is random (i.e., all memory errors are equally likely), or there is any systematicity in the forgetting process (i.e., misremembered details occur with some kind of predictable pattern that can be exploited). Somewhat surprisingly, we demonstrate that it is possible for a machine learning model to input a forensic sketch, and to some extent reverse the forgetting process to produce a more accurate sketch that is easier to match.</p><p>Based on our memory gap database and model, we aim to improve forensic sketch to mugshot matching: by modelling the photo-sketch modality gap, imperfect communication gap and -uniquely -by modelling a map from memories of old to recently seen faces to correct misremembered facial details. Since forgetting dynamics differ across time periods <ref type="bibr" target="#b6">[7]</ref>, it is unclear how to model the memory gap data: a single model covering forgetting across different time-periods is too coarse, but a distinct model of the forgetting in time-slice of the database is too specific. Similarly, the overall forensic sketch matching task spans modality, communication and memory gaps. An intuitive approach would therefore be to apply in sequence multiple models trained to span each of these gaps. We show that while this is effective, a better solution in practice is to apply multitask learning <ref type="bibr" target="#b23">[24]</ref> to build a single model trained to span the longer 24h memory gap, but with the others (short-term memory, modality and communication) as auxiliary tasks. Finally, we demonstrate the practical value of these contributions by transferring the model learned on our memory gap database to a realistic forensic task <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref> of matching 195 forensic sketches against corresponding photos and a 10,030-mugshot database. The results demonstrate a large improvement over the previous state of the art. An overview of our proposed framework is illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Facial sketch-photo recognition: Studies on matching facial sketches to photos can be classified based on the type of sketches used: viewed, semi-forensic and forensic, and whether the sketches are hand drawn, or computer composited. The majority of previous studies have focused on viewed sketches due to being an easier task with accessible benchmark databases. Representative approaches to viewed sketch recognition include bridging the gap with MRF-based photo-sketch synthesis, <ref type="bibr" target="#b22">[23]</ref>, learning common subspace for comparison with PLS <ref type="bibr" target="#b19">[20]</ref>, or engineered new invariant descriptors <ref type="bibr" target="#b7">[8]</ref>. For further details, we refer the reader to the survey in <ref type="bibr" target="#b16">[17]</ref>. Recognition rates on the main viewed sketch benchmarks <ref type="bibr" target="#b22">[23]</ref> have reached 100% <ref type="bibr" target="#b7">[8]</ref>, so viewed sketch recognition can be considered solved.</p><p>Forensic sketch face recognition: One of the earliest studies to discuss automatically matching forensic sketches with photos was <ref type="bibr" target="#b9">[10]</ref>. It highlighted the importance, as well as complexity and difficulty of forensic sketch based face recognition. The first significant demonstration of automated forensic sketch matching was <ref type="bibr" target="#b11">[12]</ref>, which combined feature engineering (SIFT and LBP) with a discriminative (LFDA) method to learn a weighting that maximised identification accuracy. Later studies such as <ref type="bibr" target="#b1">[2]</ref> improved these results, again combining feature engineering (Weber and Wavelet descriptors) plus the discriminative learning (genetic algorithms) strategy to maximise matching accuracy.</p><p>Unlike viewed sketches, forensic sketch databases are few and small in size. The main sketch/photo databases are 159 pairs identified by <ref type="bibr" target="#b11">[12]</ref>, and 190 pairs in the IIIT-D database <ref type="bibr" target="#b1">[2]</ref>. A realistic evaluation of sketch-based face matching should also include a large pool of mugshots to match against, in addition to the true photo corresponding to each sketch. Despite this, only a few studies have evaluated forensic sketch matching algorithms in this way. Notably <ref type="bibr" target="#b11">[12]</ref>, which trained a matching model on viewed sketches and then tested matching 159 forensic sketches against corresponding photos and a 10,030 mugshot database. In this paper we also evaluate our approach in this rigorous way, and show that the results can be significantly improved by explicitly modelling the human visual memory components.</p><p>Regression models: Regression models are widely used in cross-domain face recognition <ref type="bibr" target="#b16">[17]</ref>. For facial sketch matching, regression models may provide facial sketch↔photo synthesis <ref type="bibr" target="#b21">[22]</ref> to support matching, for example via support vector regression (SVR) <ref type="bibr" target="#b25">[26]</ref>. Alternatively, Partial Least Squares (PLS) models may be used to map images in each modality to a common subspace where they are more comparable <ref type="bibr" target="#b19">[20]</ref>. Although widely and effectively used, all prior work has focused on regression modelling to tackle the modality-gap problem rather than the memory-gap problem. In this paper, we exploit Gaussian Process regression to deal with both the memory-gap and the modality-gap components in forensic sketch matching.</p><p>Facial Attributes: Study of facial attributes <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b15">16]</ref> is a topical problem in computer vision. It is also relevant to forensic sketch recognition because encoding sketches and photos in terms of facial attributes can help to bridge the sketch/photo modality gap <ref type="bibr" target="#b17">[18]</ref>, or prune the matching space <ref type="bibr" target="#b11">[12]</ref>. However, attributes are vulnerable to forgetting as well, so the attributes of a sketch may mismatch those of the corresponding photo even if they are perfectly detectable by computer vision techniques.</p><p>Human memory and forensic sketches: Studies have shown the ability of individuals to recognise faces depends on different facial features according to the level of familiarity <ref type="bibr" target="#b24">[25]</ref>. Internal facial features are important for identification of familiar faces, and external features for unfamiliar faces <ref type="bibr" target="#b5">[6]</ref>. It remains to be seen if/how these findings translate to automatic face recognition, so we use whole face images in our study. With regards to the forgetting process, forensic psychology studies have found that memory fidelity drops dramatically between the first hour and first 24 hours after witnessing a face. However, in practice forensic sketches are rarely made within the first day <ref type="bibr" target="#b6">[7]</ref>. Thus, any mechanism capable of bridging this gap automatically is expected to both have a large impact on quantitative recognition performance and forensic police work in practice. Contributions: Overall, our contributions are as follows: (i) We present a new memory gap facial sketch database with 100 subjects each with a photo and four sketches that disentangle different aspects of the forensic sketch gap (400 sketches in total). (ii) We use this database to demonstrate that there is systematicity in facial forgetting, by showing that inaccurate forensic facial sketches can be automatically improved by machine learning methods trained to recover 'recent' from 'old' face memories. (iii) We transfer the learned memory reconstruction models to a realistic forensic sketch matching benchmark. The results significantly outperform the previous state of the art <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b14">15]</ref> at matching forensic sketches against corresponding photos and a large 10,030 mugshot database.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Memory-Aware Facial Sketch Modeling</head><p>The forensic sketch-photo matching task is complicated by three distinct challenges. Photo/sketch modality change, forgetting, and communication (of memory to sketch artist/compositing software) issues all contribute. We create a dataset designed to disentangle these issues. It contains N subjects, with photos <ref type="bibr" target="#b23">(24)</ref> hour and (u)nviewed. Each image is assumed to be represented by a d-dimensional feature vector x. The task of nearest-neighbour (NN) matching a viewed sketch x t=v to a photo database would be</p><formula xml:id="formula_0">D p = {x p i } N i=1 and sketches drawn with different conditions D s = {x t i } N i=1 , t = (v)iewed, (1) hour,</formula><formula xml:id="formula_1">i * N N = argmin i |x v − x p i | .<label>(1)</label></formula><p>Studies focusing on bridging the modality gap by linear regression-based synthesis or linear subspace projection aim to solve a similar task, after learning a suitable regression matrix W v or projections W v and W p respectively:</p><formula xml:id="formula_2">i * map = argmin i |W v x v − W p x p i | .<label>(2)</label></formula><p>Memory Modelling: Making use of our memory-gap database, we can separate contributing components of the forensic-sketch gap. For example, training W v→p in</p><formula xml:id="formula_3">W v→p = argmin W v→p i x p i − W v→p x v i 2 2 (3)</formula><p>is the conventional task of learning to bridge the modality gap between photos and viewed sketches. Training W u→v would be learning to correct the communication gap. While training W 24→v in</p><formula xml:id="formula_4">W 24→v = argmin W 24→v i x v i − W 24→v x 24 i 2 2<label>(4)</label></formula><p>is learning to correct 24 hours worth of transience, independent of the modality or communication gap. Given the conditions in our memory-gap database, there are a variety of potential tasks (10 in total) including: correcting the modality v → p or short term memory gap 1 → v; reducing or completely correcting the long-term memory gap 24 → 1 or 24 → v respectively; and full forensic sketch matching u → p (see Sec. 5.1 for full list). We will learn all 10 tasks allowed by our database.</p><p>Mapping Strategy: Rather than the most common linear projection approach to these learning tasks <ref type="bibr" target="#b19">[20]</ref>, we use Gaussian Process Regression (GPR) <ref type="bibr" target="#b18">[19]</ref>. We take this approach because: (i) GPR provides a more flexible nonlinear mapping, and importantly (ii) as a Bayesian regression framework, GPR provides a distribution over the reconstruction rather than a single point estimate. This uncertainty metric at each point of the reconstruction turns out to be important to improve matching performance, by automatically weighting each feature according to its reliability. Exploiting Multiple Models: As mentioned earlier, our memory-gap database provides 10 potential modelling tasks. The most obvious ways to use these for practical forensic sketch matching would be: (i) apply the model learned for direct forensic sketch-photo matching u → p, or (ii) given multiple models trained to correct the different sources of error, sequentially apply them to correct each source of error in turn, e.g.,</p><formula xml:id="formula_5">u → 24 → 1 → v → p.</formula><p>Clearly some of these tasks are related (e.g., tasks 1 → v, 24 → 1, 24 → v span different steps of forgetting). So an alternative approach that will turn out to be better is to learn all the tasks together in a multi-task learning framework. In this way each task shares information with -is regularised by -the others. Specifically, we will jointly learn the tasks with Multi-Task Gaussian Process Regression (MTL-GPR).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Improving Forgotten Faces with MTL-GPR</head><p>Single Task Modelling: GP regression can be applied to cross-modal/memory-gap problems such as those in Eqs. 2-4, but learning a non-linear projection. Denoting now features in input and target conditions as x and y respectively, our database provides training pairs D = {y, x}. For any query point x * the GPR prediction for y * is:</p><formula xml:id="formula_6">p(y * |x * , D) ∼ N (k T * K −1 y, k * * − k T * K −1 k * ) (5)</formula><p>where matrix K is the covariances at all pairs of train points, vector k * is the train-test covariances, k * = [κ(x * , x 1 )...κ(x * , x N )] and k * * = κ(x * , x * ). We take the most common squared-exponential kernel κ(x,</p><formula xml:id="formula_7">x ′ ) = exp(− 1 2l 2 (x − x ′ ) 2 )</formula><p>, and the kernel hyper parameter l can be tuned by gradient on the marginal likelihood <ref type="bibr" target="#b18">[19]</ref>. Multi Task Modelling: In our problem there are 10 distinct mapping tasks, which we learn together in a MTL-GPR framework. Following <ref type="bibr" target="#b2">[3]</ref>, we learn GP regression with predictions for tasks l and k correlated as:</p><formula xml:id="formula_8">&lt; f l (x)f k (x ′ ) &gt; = K f lk κ(x, x ′ )<label>(6)</label></formula><p>Here l and k index any two conditions in our memory-gap database, and K f is the 10 × 10 PSD matrix of inter-task similarities. Standard GP predictions can then be made using this covariance. Importantly, with this approach, the key task similarity matrix K f can also be learned along with the kernel hyper parameters l via the marginal likelihood <ref type="bibr" target="#b2">[3]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Matching Forgotten Sketches to Photos</head><p>Correcting Inaccurate Memory: For any task provided by our database, reconstruction is performed by computing the GP posterior of each target feature. For example, to improve an unviewed sketch u → v, we would compute the predictive distribution p(x v * |x u * , D) ∼ N (µ x * , σ 2 x * ), as given by Eq. 5. The new sketch would then be given by the mean of the posterior normal µ x * , and the confidence of each feature dimension by the corresponding variance σ 2</p><p>x * . Matching across Memory or Domain Gap: With this framework matching can be performed by calculating the likelihood of each mugshot in the gallery under the posterior predictive distribution of the probe sketch. For example, after training on our memory gap database D, we can use model u → p to match a forensic sketch x u * against a database of mugshots X p = {x p i } N i=1 as follows: • Compute the distribution over the expected photo corresponding to the forensic sketch: p(x p |x u * , D). • Pick the photo with maximum likelihood under this predictive photo distribution:</p><formula xml:id="formula_9">i * = argmax i p(x p i |x u * , D).</formula><p>• In practice, we model each dimension of the target independently with GPR, so this is equivalent to i * = argmax i k (x p ik − µ x * k ) 2 /σ 2 x * k . Where x p ik , µ x * k and σ 2</p><p>x * k respectively are the k −th dimension of the target photo, posterior predicted photo mean and variance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Memory gap database</head><p>In this section we describe our memory gap database and its creation procedure in more detail <ref type="bibr" target="#b0">1</ref> . 100 subjects are chosen from mugshots.com, which releases mugshots of real criminals. For each subject one frontal face photo is selected, and four types of sketches are drawn:</p><p>Viewed: Sketches are drawn while the artist looks directly at the mugshot photos. 1 hour: Mugshot photos are viewed by the artist, and sketches are drawn one hour later. Thus, compared to viewed sketches, the sketch is 'corrupted' by one hour worth of memory transience. 24 hours: Mugshot photos are viewed by the artist, and drawn 24-hours later. Unviewed: Sketches are drawn by an artist based on the description of an eyewitness who has seen the mugshot photo immediately before (but does not view it during the sketching). The artist does not see the photo. In this case, the memory gap is negligible, but it is the only condition in the database where the communication gap of imperfect communication between the eyewitness and artist exists.</p><p>The reason for this design of the collection procedure is so that the modality and communication gaps can be isolated (in photo-viewed and viewed-unviewed respectively) from the memory gap (24h to 1h to viewed). This potentially enables specific models to be built to address each contributing factor of the forensic sketch challenge.</p><p>To build the memory gap database, over 20 art students are selected to contribute as both sketch artists and eyewitness. Each artist is asked to draw all four kinds of sketches for each subject. This way the sketches for each mugshot do not have inter-artist variability, but the drawing order is such that forensic sketches are fully unviewed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Datasets and Settings</head><p>Databases: We study three databases: The contributed Memory Gap Database (MGDB), where we have also annotated each image with 40 binary facial attributes from the ontology provided by <ref type="bibr" target="#b17">[18]</ref>; a Forensic Composite Database with 51 forensic composite-photo pairs <ref type="bibr" target="#b6">[7]</ref>, and the Forensic Sketch and Mugshot Database (FSMD). The latter consists of two parts: 195 forensic sketch-photo pairs <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b11">12]</ref> and a large background gallery of mugshots to search against, in order to replicate a real-world scenario where a law-enforcement agency would query a large gallery of mugshot images with a forensic sketch. We use the same 195 sketch-photo pairs as <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b17">18]</ref>. The mugshot gallery used by <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref> was not released publicly, so we simulate this as best as possible by downloading 10,030 mugshots from mugshots.com (the same source used by <ref type="bibr" target="#b11">[12]</ref>).</p><p>Memory-Aware Model Training: All sketch and photo conditions (t=photo, viewed, 1 hour, 24 hour and unviewed) are used to exhaustively construct the 10 possible reconstruction tasks. For each task, sketches corresponding to two-thirds of subjects serve as training data, and the others serve as testing data. The 2/3s training subjects and 10 tasks are used to jointly train 10 models via MTL-GPR. We explore performance on the testing split of Memory Gap Database, before transferring to FSMD for final evaluation.</p><p>Overall ten regression tasks were trained: 1) viewed sketch to photo, 2) 1 hour sketch to photo, 3) 24 hour sketch to photo, 4) unviewed sketch to photo, 5) 1 hour to viewed sketch, 6) 24 hour to viewed sketch, 7) unviewed to viewed sketch, 8) 24 hour to 1 hour sketch, 9) unviewed to 1 hour sketch and 10) unviewed to 24 hour sketch. Some of these are illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Features and settings:</head><p>We normalise all photo and sketch images to 256 × 196 and align them by normalising on interocular distance. Each image is then represented with HoG features. We compute dense HoG feature over a regular grid (16×16 step size), which results in a feature vector of dimension 5,952 for each image. For each image, 40 attributes are also detected using SVM detectors trained using the ground-truth attributes on the training split <ref type="bibr" target="#b17">[18]</ref>.</p><p>Baselines: In addition to our MTL-GPR memory-aware model, we also consider alternative regression methods that could potentially model the gaps across database contexts: Nearest Neighbour (NN): Direct matching. Ignore the gap. Linear Regression (LR): Linear (L2 regularised) regression is the simplest explicit mapping approach. Polynomial Support Vector Regression (SVR): SVR was used in <ref type="bibr" target="#b25">[26]</ref> to accomplish sketch-photo synthesis. Polynomial Multi-Task Learning: We use the <ref type="bibr" target="#b23">[24]</ref> implementation of the popular GO-MTL <ref type="bibr" target="#b12">[13]</ref> multi-task learner. By exploiting task relatedness, this may perform better than SVR. In initial experiments we found polynomial MTL significantly better than linear, so we report the former. (Single Task) Gaussian Process Regression (GPR) <ref type="bibr" target="#b18">[19]</ref>: Compared to the others, GPR provides a non-parametric probabilistic prediction with an estimate of uncertainty that can be used for matching as in Sec 3.2. Sequential GPR: As mentioned in Sec 3, this is the intuitive baseline of applying a number of the 10 GPR models in sequence to correct distinct error sources.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Memory-Aware Model Analysis</head><p>In this section, we analyse the MTL-GPR reconstruction of faces, as represented by HoG features 2 . To help inter-  pret the results, we also divide the facial hog feature maps into external regions and internal regions: external, internal, eyes, nose, mouth and chin <ref type="bibr" target="#b24">[25]</ref>, as shown in <ref type="figure">Fig. 2</ref>.</p><p>To investigate whether our memory model helps to bridge the gap between photo and forensic sketch, we calculated RMSE between sketch/reconstructed sketch and the corresponding photos. The results are shown broken down by facial region and averaged over tasks (Tab. 1) and averaged over all regions broken down by tasks <ref type="figure">(Fig. 3</ref>). From these we can see that: (i) Each learned projection task in the MGDB database reduces the sketch-photo RMSE. (ii) This demonstrates that sketches drawn at different delays contain some systematic shift that it is possible to reverse, or it would not be possible to learn a model that consistently improves RMSE. (iii) Reconstruction consistently improves RMSE for each distinct semantic facial region.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Face matching: Memory gap database.</head><p>In this section we quantitatively evaluate face matching performance on the test split of the memory gap database. As outlined in Sec 5.1, we compare a variety of baselines to our proposed MTL-GPR and report the rank 1 (perfect match) accuracy for each of the 10 tasks in Tab. 2. The row and column give the MGDB image pair (training task). The column gives the MGDB sketch input for testing, and the task is always to match against photos using the corresponding training model.</p><p>Efficacy of memory-aware models: From Tab. 2, we can draw the conclusions: (i) Sketch reconstruction with linear regression does not consistently improve on direct NN matching, suggesting that a linear projection is insufficient. (ii) Every non-linear approach to bridging the modality/memory gap performs better than direct NN matching with no memory gap model, but among the baseline memory gap models, there is no clear winner or loser. (iii) Our MTL-GPR is the clear winner overall, often with significant margins over the next best (e.g., 87% vs 57% in 24 → v setting). (iv) That MTL-GPR outperforms regular GPR demonstrates that there is common information in each of the distinct tasks that can be extracted and shared. (v) In some cases the gain from an explicit un-forgetting model is vast: In the 24 → v setting, performance triples from 29% to 87% comparing NN matching with MTL-GPR.</p><p>Significance of Bayesian Memory Gap Model: One of the reasons for the GP methods' good performance is their ability to account for reconstructed feature reliability in matching (Sec 3.2). We demonstrate this in Tab. 3, where we compare performance with and without the use of the reconstruction variance. Clearly accounting for reconstruction reliability significantly benefits performance.</p><p>Qualitative Analysis: The average variance map across the database is shown in <ref type="figure" target="#fig_3">Fig. 5(right)</ref>. The model confidently predicts both internal (eyes, mouth) and external (hair, chin) facial regions <ref type="bibr" target="#b24">[25]</ref>, while giving less weight to skin regions (forehead, cheeks), where texture may not be predictable from the sketch.</p><p>The MTL-GPR framework also aims to discover task relatedness. The learned task relatedness matrix K f is shown in <ref type="figure" target="#fig_3">Fig. 5(left)</ref>. The clear block structure here shows that the tasks with sketches as target context are much more related to each other than those with photos as the targets. The 24 → 1 task is also noticeable as sharing structure with many of the other sketch predictors (cross structure within the block).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Applying Memory-Aware Models to Forensic Sketch Matching</head><p>Matching on Forensic Sketch Database: All ten learned memory-aware models are transferred to the forensic sketch database, which includes 195 forensic sketch-photo pairs. Few experiments have been done on forensic sketch database, except <ref type="bibr" target="#b17">[18]</ref> which focused on using attributes to bridge the sketch/photo gap. To compare directly with <ref type="bibr" target="#b17">[18]</ref>, we evaluate our models on the same 1/3 test split.</p><p>The results are shown in Tab. 4, from which we make the following observations: (i) All our reconstruction models perform significantly better than 9% with HoG matching alone, and almost all outperform the 21% of <ref type="bibr" target="#b17">[18]</ref>. (ii) Com-   </p><formula xml:id="formula_10">- - - - - - - - - - - - - - - - - - 42</formula><p>30 30 32 <ref type="bibr">18 21</ref>  </p><formula xml:id="formula_11">u → 24 u → 24 → 1 u → 24 → 1 → v u → 24 → 1 → v → p 54 28 20 13 24 → 1 24 → 1 → v 24 → 1 → v → p 1 → v → p 56</formula><p>39 <ref type="bibr">16 16</ref> paring STL-GPR and MTL-GPR, the models trained with photo targets perform worse when learned jointly, i.e., they suffer negative transfer from the sketch targets. However, the models trained with sketch targets generally perform better, i.e., they successfully share information about bridging the memory gap. (iii) The best model overall is MTL-GPR's 24 → 1, suggesting that the biggest single contributor to the forensic sketch gap in practice is the longer term forgetting between 1 and 24 hours. The second best is also memory related 1 → v. An intuitive alternative way to exploit the tasks learned in MGDB for forensic sketch matching is to apply the models in sequence to correct the various sources of error in forensic sketches. We conduct this experiment for a variety of possible STL-GPR model sequences (Sec 3). The results in Tab. 5 show that while all outperform the 9% of direct matching, none of the multi-step configurations outperform the best single task of 24 → 1. Which is itself outperformed by our MTL-GPR 24 → 1 in Tab. 4. Based on this analysis, we focus on the contribution of the two MTL-GPR memory models 1 → v and 24 → 1, which we denote Early and Late, in the final large-scale benchmark experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Matching on Forensic Sketch and Mugshot Database:</head><p>We now address the full problem of matching forensic sketches to a large database of mugshot photos. We compare the results of our Early and Late-Memory MTL-GPR models to the results of the state of the art LFDA <ref type="bibr" target="#b11">[12]</ref> (who also reported the results of a state of the art commercial sys- tem FaceVACS), KPS <ref type="bibr" target="#b10">[11]</ref>, and DFD <ref type="bibr" target="#b14">[15]</ref>. To provide an additional baseline, we also take the best publicly available (photo) Deep face recognition model <ref type="bibr" target="#b8">[9]</ref> and use it to extract features for matching. As <ref type="bibr" target="#b11">[12]</ref> demonstrated the value of filtering by soft biometrics, we also further combine our models with predicted attributes (trained on memory gap database) with score-level fusion.</p><p>In order to compare directly with <ref type="bibr" target="#b11">[12]</ref>, who break down results by "good" and "bad" quality sketches, we show results in Tab. 6 focusing on a good quality subset of sketches. In <ref type="figure" target="#fig_4">Fig. 6</ref>, we provide a cumulative match characteristic (CMC) curve, including results for both all 195 sketches as well as the 49 good quality sketches. From the results we can see that: (i) Our memory-gap model significantly surpasses state of the art performance, demonstrating that the model learned on our database can dramatically improve real forensic sketch matching, (ii) Of the memory-aware models, the Late-Memory model trained on the 1-24 hour memory gap performs better, reflecting forensic psychology conclusions that the first day's forgetting is significant <ref type="bibr" target="#b6">[7]</ref>, (iii) Including predicted facial attributes improves performance further, (iv) Using modern deep features with direct matching now outperforms the commercial FaceVACS result, but it is significantly worse than both LFDA <ref type="bibr" target="#b11">[12]</ref> and ours: indicating that deep features alone are insufficient to address forensic sketch matching.</p><p>Qualitative Examples: Some qualitative examples of our matching process using the forensic database are shown in <ref type="figure" target="#fig_2">Fig. 4</ref>. Photos and sketches are represented with HoG features (visualised by HOGgles <ref type="bibr" target="#b20">[21]</ref>). The learned memory reconstruction model predicts the mean and variance of photo-HOGs. Photos are chosen by their likelihood under the predicted Gaussian distribution, allowing matching to take into account the prediction reliability of each feature. Matching on Forensic Composite Database: Although our model is trained on sketch rather than software composite faces, we also evaluate whether the learned model is general enough to improve forensic composite matching. Tab. 7 shows the results of retrieving 51 composites from among the same mugshot gallery. Clearly our model still makes a significant impact on retrieval performance, despite the sketch-composite domain shift.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>We investigated two questions: Whether it is possible to improve facial sketches whose quality is impacted by a large delay between seeing the face and making the sketch; and whether such models can be used to improve practical forensic sketch recognition. We were able to demonstrate that it is indeed possible to improve facial sketches drawn after a time-delay, and that this translates into the significantly improved state of the art performance on the important task of forensic sketch matching.</p><p>One limitation of our current work is that each HoG dimension is modelled independently, so cross-pixel correlation is not exploited. In future, we would explore richer information sharing architectures, such as local patches, CRF smoothing, and multi-task among neighboring pixels. Secondly, we ultimately exploited the contributions of crossmodal and communication gaps only implicitly via MTL sharing. A richer framework more explicitly modelling the contributing factors should be explored.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1</head><label>1</label><figDesc>Photo</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .Figure 3 .</head><label>23</label><figDesc>Illustration Learned reconstruction reduces sketch/photo gap for each task in MGDB database: RMSE averaged across full face.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Qualitative results of matching in forensic sketch database. The memory reconstruction model trained on 24 → 1 hour sketches of MGDB is transferred to forensic sketch database. Reconstruction variance improves matching by focusing on reliable features. These good sketches were both retrieved at Rank 1 of 10,225 (10,030+195). Bad sketches were retrieved at Rank 1592 and 1800 respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Qualitative results of MTL-GPR model. Left: Estimated task relatedness K f . Right: Average reconstruction variance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>CMC</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><head>Table 1 .</head><label>1</label><figDesc>RMSE of sketch/reconstruction vs photo according to regions, averaged across all ten tasks in MGDB.</figDesc><table>Region 
Photo v.s. Original Sketch Photo v.s. Projected Sketch 
External 
0.20 ± 0.013 
0.16±0.025 
Chin 
0.20 ± 0.014 
0.16±0.023 
Internal 
0.18 ± 0.003 
0.16±0.015 
Mouth 
0.17 ± 0.007 
0.16±0.012 
Eyes 
0.18 ± 0.003 
0.15±0.023 
Nose 
0.18 ± 0.011 
0.14±0.018 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 .</head><label>2</label><figDesc>PM PS LR NN MG-G-PM PS LR NN MG G-PM PS LR NN MG G-PM PS LR NN</figDesc><table>Photo-sketch matching on the memory gap database (Rank 1 accuracy, %). Comparing MTL-GPR, GPR, Polynomial MTL, 
Polynomial SVR, Linear Regr. and NN. Sketch input is given by column and matched with the model trained on the corresponding cell of 
MGDB. Average accuracies over 15 random splits of 68 training and 32 testing subjects. See supplementary for standard deviations. 

Accuracy 
Viewed 
1 Hour 
24 Hour 
Unviewed 
MG G-Photo 
99 
88 88 90 53 
71 
96 
70 65 56 39 
51 
90 
55 50 52 32 
31 
86 
35 35 38 34 
21 
Viewed 
-
-
-
-
-
-
90 
58 63 66 52 
51 
86 
57 44 46 26 
31 
73 
33 32 38 24 
21 
1 Hour 
-
-
-
-
-
-
-
-
-
-
-
-
69 
41 44 45 26 
31 
63 
32 29 35 18 
21 
24 Hour 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 .</head><label>3</label><figDesc></figDesc><table>The importance of Bayesian memory modelling: Rank 1 
MGDB match results (%) without/with reconstruction confidence. 
Average accuracies over 15 random splits of 68 training and 32 
testing subjects. See supplementary for standard deviations. 

Accuracy Viewed 
1h 
24h 
Unviewed 
photo 
86 / 99 85 / 96 60 / 90 
50 / 86 
Viewed 
-
56 / 90 43 / 86 
40 / 73 
1h 
-
-
38 / 69 
36 / 63 
24h 
-
-
-
28 / 42 

Table 4. Matching results (Rank 1 accuracy, %) on forensic sketch 
database (1/3 test split) using MTL-GPR / STL-GPR. Compare: 
21% from [18] and 9% by direct HoG matching. Average accura-
cies over 15 random splits of 68 training and 32 testing subjects. 
See supplementary for standard deviations. 

Accuracy Viewed 
1h 
24h 
Unviewed 
Photo 
22 / 35 22 / 34 15 / 40 
18 / 41 
Viewed 
-
65 / 48 40 / 50 
33 / 48 
1h 
-
-
78 / 48 
54 / 40 
24h 
-
-
-
65 / 42 

Table 5. Matching results (Rank 1 accuracy, %) on forensic sketch 
database (1/3 test split) using sequence of STL-GPR models. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="true"><head>Table 6 .</head><label>6</label><figDesc>State of the art comparison. Accuracy (%) of matching 49 good forensic sketches against corresponding photos and 10,030 FSMD database mugshots. * Not directly comparable, used a different 53 sketch probe set.</figDesc><table>Accuracy 
Rank 1 Rank 10 Rank 50 
MTL-GPR Early-Mem 
23 
23 
33 
MTL-GPR Early-Mem+Attr 
25 
25 
35 
MTL-GPR Late-Mem 
33 
33 
39 
MTL-GPR Late-Mem+Attr 
38 
42 
45 
LFDA [12] 
17 
23 
33 
LFDA [12]+ gender +race 
19 
27 
45 
FaceVACS (reported by[12]) 
2 
4 
8 
KPS [11]  *  
4 
9 
21 
Deep Features [9] 
2 
6 
15 
DFD [15] 
6 
13 
19 

Table 7. Accuracy (%) of matching 51 forensic composites against 
corresponding photos and 10,030 FSMD database mugshots. 

Accuracy 
Rank 1 Rank 10 Rank 50 
HOG 
6 
14 
20 
DFD [15] 
2 
4 
4 
MTL-GPR Late-Mem 
14 
18 
26 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Available to download at http://sketchx.eecs.qmul.ac.uk/downloads.html</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">The analysis could in principle be done with pixels, but this would be computationally expensive due to higher dimensionality.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements: This project received support from the European Union's Horizon 2020 research and innovation programme under grant agreement #640891, and the Royal Society and Natural Science Foundation of China (NSFC) joint grant #IE141387 and #61511130081. We especially would like to thank the China Scholarship Council (CSC) for funding the first author to conduct the entirety of this project at Queen Mary University of London.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">On matching sketches with digital face images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Bhatt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bharadwaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vatsa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BTAS</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Memetically optimized mcwld for matching sketches with digital face images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Bhatt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bharadwaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vatsa</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>TIFS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multitask gaussian process prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">V</forename><surname>Bonilla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M A</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Data insufficiency in sketch versus photo face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Introduction to Applied Psychology, chapter Eyewitnesses and the use and application of cognitive theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Frowd</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The relative importance of external and internal features of facial composites</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Frowd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Bruce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mcintyre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hancock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">British Journal of Psychology</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A decade of evolving composite techniques: Regression-and meta-analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Frowd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Erickson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lampinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Skelton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mcintyre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hancock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Forensic Practice</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>in press</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Inter-modality face sketch recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Galoogahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICME</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">When face recognition meets with deep learning: an evaluation of convolutional neural networks for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Christmas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshops ChaLearn Looking at People</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A framework for recognizing a facial image from a police sketch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">G U</forename><genName>Jr</genName></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Da Victoria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lobo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Heterogeneous face recognition using kernel prototype similarities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">F</forename><surname>Klare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Matching forensic sketches to mug shot photos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">F</forename><surname>Klare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning task grouping and overlap in multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">D</forename><surname>Iii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Attribute and simile classifiers for face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">N</forename><surname>Belhumeur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Learning discriminant face descriptor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pietikainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A deep sum-product architecture for robust facial attributes analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">A survey on heterogeneous face recognition: Sketch, infra-red, 3d and low-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.5114</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Crossmodal face matching: Beyond viewed sketches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Gaussian processes for machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">E</forename><surname>Rasmussen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Gaussian Processes for Machine Learning</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Bypassing synthesis pls for face recognition with pose, low-resolution and sketch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Malisiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<title level="m">Hoggles: Visualizing object detection features. ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">A comprehensive survey to face hallucination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>IJCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Face photo-sketch synthesis and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A unified perspective on multi-domain and multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Matching familiar and unfamiliar faces on internal and external features. Perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">H</forename><surname>Mcweeny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">M</forename><surname>Flude</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Ellis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Face sketchphoto synthesis based on support vector regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
