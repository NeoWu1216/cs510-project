<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Highlight Detection with Pairwise Deep Ranking for First-Person Video Summarization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
							<email>tiyao@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
							<email>tmei@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Rui</surname></persName>
							<email>yongrui@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Highlight Detection with Pairwise Deep Ranking for First-Person Video Summarization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:41+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The emergence of wearable devices such as portable cameras and smart glasses makes it possible to record life logging first-person videos. Browsing such long unstructured videos is time-consuming and tedious. This paper studies the discovery of moments of user's major or special interest (i.e., highlights) in a video, for generating the summarization of first-person videos. Specifically, we propose a novel pairwise deep ranking model that employs deep learning techniques to learn the relationship between highlight and non-highlight video segments. A two-stream network structure by representing video segments from complementary information on appearance of video frames and temporal dynamics across frames is developed for video highlight detection. Given a long personal video, equipped with the highlight detection model, a highlight score is assigned to each segment. The obtained highlight segments are applied for summarization in two ways: video timelapse and video skimming. The former plays the highlight (non-highlight) segments at low (high) speed rates, while the latter assembles the sequence of segments with the highest scores. On 100 hours of first-person videos for 15 unique sports categories, our highlight detection achieves the improvement over the state-of-the-art RankSVM method by 10.5% in terms of accuracy. Moreover, our approaches produce video summary with better quality by a user study from 35 human subjects.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Wearable devices have become pervasive. People are taking first-person videos using these devices everyday and everywhere. For example, wearable camcorders such as GoPro cameras and Google Glass are now able to capture high quality first-person videos for recording our daily experience. These first-person videos are usually extremely unstructured and long-running. Browsing and editing such videos is really a tedious job. Therefore, video summariza-  tion, which produces a short summary of a full-length video and ideally encapsulates its most informative parts, is becoming increasingly important for alleviating the problem of first-person video browsing, editing and indexing.</p><p>The research on video summarization has mainly proceeded along two dimensions, i.e., keyframe or shot-based <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b17">18]</ref>, and structure-driven <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b21">22]</ref> approaches. The keyframe or shot-based method always selects a collection of keyframes or shots by optimizing diversity or representativeness of a summary, while structure-driven approach exploits a set of well-defined structures in certain domains (e.g., audience cheering, goal or score events in sports videos) for summarization. In general, existing approaches offer sophisticated ways to sample a condensed synopsis from the original video, reducing the time required for users to view all the contents.</p><p>However, defining video summarization as a sampling problem in conventional approaches is very limited as users' interests in a video are fully overlooked. As a result, the special moments may be omitted due to the visual diversity criteria of excluding redundant parts in a summary. The limitation is particularly severe when directly applying those methods to first-person videos. First-person videos captured with wearable devices record experiences from a first-person perspective in unconstrained environments, making them long, redundant and unstructured. Moreover, the continuous nature of such videos even yields no evident shot boundaries for summary; nevertheless, there should be the moments (segments) of major or special interest (i.e. highlights) in raw videos. Therefore, our goal is to provide a new paradigm for first-person video summarization by exploring the highlights in a video.</p><p>As the first step towards this goal, <ref type="figure" target="#fig_1">Figure 1</ref> demonstrates the process of retrieving highlights in raw firstperson videos. A raw video is divided into several segments. The highlight measure for each segment is equivalent to learning a function to predict the highlight score given the representations of the segment. The higher the score, the more highlighted the segment. Thus, the segments with high scores can be selected as video highlights. Furthermore, in order to incorporate both spatial and temporal information for better depicting a video segment, complementary streams on visual appearance from static frames and temporal dynamics across multiple frames are jointly exploited. As such, we devise a two-stream deep convolution neural networks (DCNN) architecture by fusing DCN-N on each stream for video highlight detection. In particular, considering that highlight score expresses only a relative degree of interest within each video, we propose to train DCNN on each stream independently with a pairwise deep ranking model, which characterizes the relative relationships by a set of pairs. Each pair contains a highlight and a non-highlight segment from the same video. The D-CNN on each stream aims to optimize the function making the detection score of highlight segment higher than that of non-highlight segment.</p><p>Then, by assigning a highlight score to each segment, the highlight-driven summary of a video can be generated in two ways: video timelapse and video skimming. The former keeps all the segments in the video while adjusting their speed rates of playing based on highlight scores (highlight segments with lower playing rate, and vice versa). The latter assembles the sequence of only highlight segments while trimming out the other non-highlight ones. We evaluate both video highlight detection and highlight-driven video summarization on a newly created dataset including about 100 hours of first-person videos captured by GoPro cameras for 15 sport categories, which is so far the largest scale first-person video dataset.</p><p>The remaining sections are organized as follows. Section 2 describes the related work. Section 3 presents the architecture of video highlight detection, while Section 4 formulates the problem of video summarization over the predicted video highlight. In Section 5, we provide empirical evaluations on both video highlight detection and video summarization, followed by the discussions and conclusions in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>The research area of first-person video summarization is gaining an increasing amount of attention recently <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b20">21]</ref>. The objective of video summarization is to explore the most important parts of long first-person video sequences. In <ref type="bibr" target="#b12">[13]</ref>, a short subsequence of the video was selected by using the importance of the objects as the decision criteria. Similar in spirit, video subshots which depict the essential events were concatenated to generate the summary <ref type="bibr" target="#b13">[14]</ref>. Later in <ref type="bibr" target="#b5">[6]</ref> and <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr">Gygli et al. and Potapov et al.</ref> formulated the problem as scoring each video segment in terms of visual importance and interestingness, respectively. Then the summary was produced by selecting the segments with highest scores. Recently, Joshi et al. created a stabilized time-lapse video by rendering, stitching and blending appropriately selected source frames for each output frame <ref type="bibr" target="#b9">[10]</ref>. In contrast, our approach explores the moments of user interest in the videos, which we show is vital to distill the essence in the original videos.</p><p>In addition to first-person video summarization, there is a large literature on summarization for general videos. Keyframe or shot-based methods use a subset of representative keyframes or segments from the original video to generate a summary. In <ref type="bibr" target="#b14">[15]</ref>, keyframes and video shots were sampled based on their scores of attention, which are measured by combining both visual and aural attention. Similar in spirit, Ngo et al. presented a video as a complete undirected graph, which is partitioned into video clusters to form a temporal graph and further detect video scenes <ref type="bibr" target="#b17">[18]</ref>. Video summarization can be generated from the temporal graph in terms of both the structure and attention information. Later in <ref type="bibr" target="#b15">[16]</ref>, subshots were first detected and classified into five categories according to the dominant camera motion. Then a number of representative keyframes, as well as structure and motion information were extracted from each subshot to generate the video summary.</p><p>Different from keyframe or shot-based methods, structure-driven approaches exploit video structure for summarization. The well-defined structure often exists in broadcast sports videos. A long sports game can be divided into parts and only a few of these parts contain certain informative segments. For instance, these segments include the score moment in soccer games or the hit moment in baseball games. Based on the well-defined structure, specifically designed audio-visual features, such as crowds, cheering, goal, etc., are used in the structure-driven methods <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b21">22]</ref>.</p><p>Most of the above methods focus on selecting frames, shots or segments independently, ignoring the relationship between them. Our work is different that we claim to learn the relationship of video segments in a pairwise manner, which characterizes the relative preferences of all the segments within a video and will benefit video summarization.  Each video segment is decomposed into spatial and temporal streams. The spatial stream is in the form of multiple frame appearance, while the temporal stream is represented by temporal dynamics in a video clip. A deep convolution neural networks architecture for highlight prediction is devised for spatial and temporal stream, respectively. The output of the two components are combined by late fusion as the final highlight score for each video segment. (c) By assigning a highlight score to each video segment, a highlight curve can be obtained for each video. The segments with highest highlight scores are regarded as "highlights" in the video. (d) Two highlight-driven video summarization methods, i.e., video timelapse and video skimming, can be easily formulated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Video Highlight Detection</head><p>In this section, we first present our highlight detection for each video segment by combining two deep convolution neural networks architectures on spatial and temporal stream, followed by the pairwise deep ranking model for the training of each DCNN structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Two-Stream DCNN for Highlight Detection</head><p>Video can be naturally decomposed into spatial and temporal components, which are related to ventral and dorsal streams for human perception respectively <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b22">23]</ref>. The ventral stream plays the major role in the identification of objects, while the dorsal stream mediates the sensorimotor transformations for visually guided actions at such objects. Therefore, we devise a novel two-stream DCNN architecture (TS-DCNN) by late fusing spatial and temporal DC-NN for video highlight detection, as shown in <ref type="figure" target="#fig_2">Figure 2</ref> (a)-(c). The spatial component depicts scenes and objects in the video by frame appearance while the temporal part conveys the temporal dynamics in a video clip (multiple frames).</p><p>Given an input video, a set of video segments can be delimited by uniform partition in temporal, shot boundary detection, or change point detection. For each video segment, spatial DCNN operates on multiple frames extracted from the segment. The static frame appearance is useful as some highlights are strongly associated with particular scenes and objects. The first stage of the architecture is to generate a fixed-length visual representation for each video segment. For this purpose, AlexNet <ref type="bibr" target="#b11">[12]</ref>, which is the re-cent advance image classification architecture, is exploited for extracting the softmax scores for multiple frames. Then, an average pooling <ref type="bibr" target="#b0">[1]</ref> is performed over all the frames to get a single 1,000 dimensional vector for each video segment. The AlexNet is pre-trained on 1.2 million images of ImageNet challenge dataset <ref type="bibr" target="#b1">[2]</ref>. The resulting 1,000 dimensional representation of video segment forms the input to a following neural networks for predicting the highlight score of this segment. The architecture of this neural networks is F 1000−F 512−F 256−F 128−F 64−F 1, which contains six fully-connected layers (denoted by F with the number of neurons). The output of the last layer is taken as the highlight score.</p><p>Unlike the spatial DCNN, the inputs to temporal DCNN architecture are comprised of multiple video clips and each video clip contains multiple frames. Such inputs explicitly describe the temporal dynamics between frames. To generate the representations of each video clip, 3D CNN is utilized. Different from traditional 2D CNN, 3D CNN architecture takes video clip as the inputs and consists of alternating 3D convolutional and 3D pooling layers, which are further topped by a few fully connected layers as described in <ref type="bibr" target="#b7">[8]</ref>. Specifically, C3D <ref type="bibr" target="#b24">[25]</ref>, which is pre-trained on Sports-1M video dataset <ref type="bibr" target="#b10">[11]</ref>, is exploited and we regard the outputs of the fc6 fully-connected layer as representations for each video clip. Similar to spatial DCNN architecture, temporal DCNN fuses the outputs of C3D on each video clip, followed by importing into a neural networks for video highlight detection. ) ) ) ) <ref type="figure">Figure 3</ref>. The training of spatial DCNN architecture with pairwise deep ranking model. The inputs are a set of highlight and nonhighlight video segment pairs, which are fed independently into two identical spatial DCNN with shared architecture and parameters. A ranking layer is on the top to evaluate the margin ranking loss of the pair. Note that the training of temporal DCNN follows the same philosophy.</p><p>By late fusing the two predicted highlight scores of spatial and temporal DCNN, we can obtain the final highlight score for each video segment and form a highlight curve for the whole video. The video segments with high scores are selected as video highlights accordingly. It is worth noticing that although the two streams used here are visual appearance and temporal dynamics, our approach is applicable to include any other stream, e.g., audio stream.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Pairwise Deep Ranking Model</head><p>As with most deep learning problems, the learning of our spatial and temporal DCNN architectures are critical for video highlight detection. Existing deep learning models for visual recognition often focus on learning category-level representation <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b11">12]</ref>. The learnt representations mainly correspond to visual semantics. Instead, in our case, a highlight score for every video segment reflects its degree of interest within a video and represents a relative measure. It is a straight forward way to formulate it as a supervised ranking problem. More importantly, the intrinsic property between visual recognition and ranking tasks is different, as visual recognition is modeled as a binary classification problem while ranking task is considered as a regression problem. As such, a good network for visual recognition may not be optimal for distinguishing video highlights.</p><p>Deriving from the idea of exploring relative relationship through ranking <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref>, we develop a pairwise deep ranking model to learn our spatial and temporal DCNN architectures for predicting video highlights. <ref type="figure">Figure 3</ref> shows the training of spatial DCNN with pairwise deep ranking model. Given a pair of highlight and non-highlight video segments, we wish to optimize our spatial DCNN architecture, which could output a higher score of highlight segment than that of non-highlight one. Formally, suppose we have a set of pairs P, where each pair (h i , n i ) consists of a highlight video segment h i and a non-highlight segment n i from an identical video. The two segments are fed separately into two identical spatial DCNN with shared architecture and parameters. A pair characterizes the relative highlight degree for the two video segments. The output f (·) of the spatial DCNN computes the highlight score of the input video segment. Our goal is to learn the DCNN architecture that assigns higher output score to the highlight segment, which can be expressed as</p><formula xml:id="formula_0">f (h i ) ≻ f (n i ), ∀ (h i , n i ) ∈ P.<label>(1)</label></formula><p>As the output scores exhibit a relative ranking order for the video segments, a ranking layer on the top is employed to evaluate the margin ranking loss of each pair, which is a convex approximation to the 0-1 ranking error loss and has been used in several information retrieval methods <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b28">29]</ref>. Specifically, it can be given by min :</p><formula xml:id="formula_1">(h i ,n i )∈P max (0, 1 − f (h i ) + f (n i )).<label>(2)</label></formula><p>The ranking layer does not have any parameters. During learning, it evaluates the model's violation of the ranking order, and back-propagates the gradients to the lower layers so that the lower layers can adjust their parameters to minimize the ranking loss. To avoid overfitting, dropout <ref type="bibr" target="#b6">[7]</ref> with a probability of 0.5 is applied to all the fully-connected layers after AlexNet in our architecture. The process of temporal DCNN training is the same as spatial DCNN. After training, the learnt spatial and temporal DCNN architectures are late fused for video highlight detection as shown in <ref type="figure" target="#fig_2">Figure 2</ref> (b).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Highlight-driven Video Summarization</head><p>After we get the highlight score for each video segment, how to use them for video summarization? Two video summarization approaches, i.e., video timelapse and video skimming <ref type="figure" target="#fig_2">(Figure 2 (d)</ref>), will be easily formulated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Video Timelapse</head><p>A simple and robust technique for video summarization is timelapse, i.e., increasing the speed of the non-highlight video segments by selecting every r th frame and showing the highlight segments in slow motion. Particularly, as all the segments are included finally and thus there is no strict demand on video segmentation, we simply divide the video into segments evenly in this case rather than analyzing video content. Let L v , L h and L n be the length of original video, highlight segments and non-highlight segments, respectively. Typically we have L h ≪ L n , L v . Without loss of generality, we consider the case when the rate of decelerating highlight segments and speeding up non-highlight parts is the same and denote r as the rate. Given a maximum length of summary L, the problem is then equivalent to find a proper rate r which satisfies the formula as</p><formula xml:id="formula_2">rL h + 1 r L n ≤ L.<label>(3)</label></formula><p>Since L h + L n = L v , we can derive that:</p><formula xml:id="formula_3">r = L 2L h + √ Y , where Y = L 2 −4L v L h +4L 2 h 4L 2 h .</formula><p>In this way, we generate a video summary by compressing the non-highlight video segments while expanding the highlight parts. In general, video timelapse has two major characteristics. First, all the video content are contained in the summary. As a result, there is no risk of omitting any important segments, making the summary more coherent and continuous in telling camera wearer's story. Furthermore, the video segments of interest are underlined and presented in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Video Skimming</head><p>Video skimming addresses the summarization problem by providing a short summary of original video which ideally includes all the important video segments. A common practice to video skimming is to first perform a temporal segmentation, followed by singling out a few segments to form an optimal summary in terms of certain criteria, e.g., interestingness and importance. Following <ref type="bibr" target="#b20">[21]</ref>, we exploit a kernel temporal segmentation approach which is originated from the idea of multiple change point detection. Readers can refer to <ref type="bibr" target="#b20">[21]</ref> for more technical details.</p><p>After the segmentation, highlight detection is applied to each video segment, producing the highlight score. Given the set of video segments S = {s 1 , . . . , s c } and each segment is associated with a highlight score f (s i ), we aim to single out a subset with a length below the maximum length L and the sum of the highlight scores is maximized. Specifically, the problem is defined as</p><formula xml:id="formula_4">max : b c i=1 b i f (s i ) s.t. c i=1 b i |s i | ≤ L ,<label>(4)</label></formula><p>where b i ∈ {0, 1} and b i = 1 indicates that the i th segment is selected. |s i | is the length of the i th segment. The maximization is a standard 0/1-knapsack problem and can be solved with dynamic programming for a global optimal solution <ref type="bibr" target="#b4">[5]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>We conducted our experiments on a newly created firstperson video dataset crawled from YouTube and evaluated our approaches on both video highlight detection and highlight-driven video summarization.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Dataset</head><p>While the research on first-person video analysis has recently received intensive attention, the public datasets to date are still small (e.g., up to 20 hours) and very specific (e.g., in a kitchen). To substantially evaluate our approach, we collect a new large dataset from YouTube for first-person video highlight detection. The dataset consists of 100 hours videos mainly captured by GoPro cameras for 15 sports related categories. In particular, we query the Y-ouTube database with "category name + GoPro" to retrieve relevant videos. Given the retrieved videos, those with visible edited traces, as with scene splicing or rendering, are removed manually. Hence, our dataset is constructed with only raw videos. <ref type="figure" target="#fig_4">Figure 4</ref> shows a representative frame selected from each category in our dataset. For each category, there are about 40 videos, each with a duration between 2 to 15 minutes.</p><p>To evaluate our highlight results, we first split the video into a set of five-second segments evenly sampled across each raw video and ask multiple evaluators to label the highlight level of each segment. We invited 12 evaluators from different education backgrounds, including linguistics, physics, business, computer science, and design. All evaluators are outdoor sports enthusiasts and some of them are from local outdoor sports club. Each video segment was annotated on a three point ordinal scale: 3-highlight; 2-normal; 1-boring. To make the annotation as objective as possible, there are three labelers assigned to each video. Only video segments with their aggregate scores at or over 8 points were selected as "highlight." Note that obtaining these annotations was very time consuming. The labelers are requested to watch the whole video before assigning labels to each segment as the highlight is a relative judgement within a video. The dataset is partitioned into training and test sets evenly on all 15 categories for our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Highlight Detection</head><p>The first experiment was conducted on our first-person video dataset to examine how our spatial and temporal DC-NNs work on highlight detection.</p><p>Compared Approaches. We compare the following approaches for performance evaluation:</p><p>(1) Rule-based model <ref type="bibr" target="#b15">[16]</ref> (Rule). The video is first segmented into a series of shots based on color information. Each shot is then decomposed into one or more subshots by a motion threshold-based approach. The highlight score for each subshot is proportion to its length, giving that the longer subshot usually contains more informative content.</p><p>(2) Importance-based model <ref type="bibr" target="#b20">[21]</ref>. A linear SVM classifier per category is trained to score importance (highlight) of each video segment. For each category, we use all the video segments of this category as positive examples and the video segments from the other categories as negatives. We adopt both improved dense trajectories motion features proposed in <ref type="bibr" target="#b25">[26]</ref> and the average of DCNN frame features for representing each video segment. The detailed settings will be presented in parameter settings. The two runs based on improved dense trajectory and DCNN are named as Im-p+IDT and Imp+DCNN, respectively.</p><p>(3) Latent ranking model <ref type="bibr" target="#b23">[24]</ref>. A latent linear ranking SVM model per category is trained to score highlight of each video segment. For each category, all the highlight and non-highlight video segment pairs within each video are exploited for training. Similarly, improved dense trajectories and the average of DCNN frame features are extracted as the representations of each segment. We refer to the two runs as LR+IDT and LR+DCNN, respectively. Parameter Settings. In the experiments, we uniformly pick up three frames every second and hence have 15 frames for each five seconds' video segment. Following <ref type="bibr" target="#b24">[25]</ref>, each video clip is composed of the first 16 continuous frames in every second and then have 5 video clips for each segment. For S-DCNN and T-DCNN training, only the segment pairs, the difference of whose aggregate scores is over 3 points, are selected and in total we have 105K pairs in training set.</p><p>To ensure the performance of these methods comparable, the representation of video segment is the average of the outputs of AlexNet on selected frames in both Imp+DCNN and LR+DCNN, which is the same as our S-DCNN. For the extraction of trajectory descriptor, we use the default parameters which results in 426 dimensions. The local descriptor is then reduced to half of the dimensions with PCA separately on each component of the descriptor. Finally, each video segment is encoded in a Fisher Vector <ref type="bibr" target="#b18">[19]</ref> based on a GMM of 256 Gaussians. Furthermore, following the setting in <ref type="bibr" target="#b23">[24]</ref>, we use the liblinear package <ref type="bibr" target="#b2">[3]</ref> to train both LR+IDT and LR+DCNN with the same stopping criteria: maximum 10K iterations and ε = 0.0001. Evaluation Metrics. We calculate the average precision of highlight detection for each video in test set and mean Average Precision (mAP) averaging the performance of all test videos is reported. In addition, since it is naturally to treat highlight detection as a problem of ranking segments in one video, we further adopt Normalized Discounted Cumulative Gain (N DCG) which takes into account the measure of multi-level highlight scores as the performance metric. Given a segment ranked list for a video, the N DCG score at the depth of d in the ranked list is defined by:</p><formula xml:id="formula_5">N DCG@d = Z d d j=1 2 r j −1 log(1+j)</formula><p>, where r j = {5 : as ≥ 8; 4 : as = 7; 3 : as = 6; 2 : as = 5; 1 : as ≤ 4} represents the rating of a segment in the ground truth and as denotes the aggregate score of each segment. Z d is a normalization constant and is chosen so that N DCG@d = 1 for perfect ranking. The final metric is the average of N DCG@d for all videos in the test set.</p><p>Performance Comparison. <ref type="figure" target="#fig_6">Figure 5</ref> shows the performances of eight runs averaged over all the test videos in our dataset. Overall, the results across different evaluation metrics consistently indicate that our TS-DCNN leads to a performance boost against others. In particular, the mAP of TS-DCNN can achieve 0.3574, which makes the improvement over LR+IDT by 10.5%. More importantly, the run time of TS-DCNN is less than LR+IDT by several dozen times and more details are given in the following run time section. Since Rule run is only based on the general subshot detection and without any highlight knowledge as a prior, it is not surprise that all the other methods exhibit significantly better performance than the Rule run.</p><p>There is a significant performance gap between S-DCNN (T-DCNN) and LR+DCNN (LR+IDT). Though both runs train the model in a pairwise manner, they are fundamentally different in the way that S-DCNN (T-DCNN) is by using a DCNN architecture, and LR+DCNN (LR+IDT) is based on ranking SVM techniques. The results basically indicate the advantage of exploiting DCNN architecture on highlight detection task. Furthermore, LR+DCNN (LR+IDT) exhibits better performance than Imp+DCNN (Imp+IDT) which formulates highlight detection as a binary classification problem by using linear SVM model. In addition, as observed in our results, using motion (temporal) features can constantly offer better performance than multiple stat-  ic frame appearances on all three models. This somewhat reveals that video highlights often appear in the segments with some special motions and hence they could be better represented by temporal features. <ref type="figure" target="#fig_7">Figure 6</ref> details the mAP performance across different categories. Different from importance-based model and latent ranking model which are category-specific, our model is general for all categories. Nevertheless, among the 15 categories, our S-DCNN, T-DCNN and TS-DCNN achieve the best performances for 11 categories, which empirically verify the merit of our model from the aspect of category independent. However, considering that all the 15 categories are sports related, it is still not clear that whether the proposed techniques are generalized to handle video contents from all domains. Moreover, the complementarity between S-DCNN and T-DCNN is generally expected. For instance, the videos of the category "fencing" is diverse in frame appearance, resulting in poor performance by S-DCNN. Instead, temporal dynamics is found to be more helpful for this category. In the case of category "golf" where motion is relatively few, visual features of frames show better performance. <ref type="figure">Figure 8</ref> further shows ten segments uniformly sampled from a video for "surfing," "skydiving," and "climbing." Each segment is represented by one sampled frame. As illustrated in the figure, the ten segments are ranked according to their predicted highlight scores by our TS-DCNN and we can easily see that the ranking order reflects the relative degree of interest within a video. Fusion Parameter. A common problem with late fusion is the need to set the parameter to tradeoff S-DCNN and T-DCNN, i.e., ω in ω × S-DCNN + (1 − ω) × T-DCNN. In the previous experiments, ω was optimally set in terms of mAP performance. Furthermore, we conducted experiments to test the performance, when the values of ω are set from 0.1 to 0.9. <ref type="figure" target="#fig_8">Figure 7</ref> shows the mAP, NDCG@1 and NDCG@5 performances with respect to different values of ω. We can see that the performance curves are relatively smooth and achieve the best result around ω = 0.3. In general, this again confirms that T-DCNN leads to better performance gain and thus is given more weights in fusion.</p><p>Run Time. <ref type="table" target="#tab_2">Table 1</ref> listed the detailed run time of each approach on predicting a five minutes' video. Note that the run time of LR+IDT and Imp+IDT, LR+DCNN and Im-p+DCNN, T-DCNN and TS-DCNN is the same respectively, only one of each is presented in the <ref type="table">Table.</ref> We see that our method has the best tradeoff between performance and efficiency. Our TS-DCNN finishes in 360 seconds, which is slightly longer than the video duration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Video Summarization</head><p>The second experiment was performed to evaluate our highlight-driven video summarization.</p><p>Compared Approaches. We compare the following approaches for performance evaluation:</p><p>(1) Uniform sampling (UNI). A simple approach by uniformly selecting K subshots throughout the video.</p><p>(2) Importance-driven summary <ref type="bibr" target="#b20">[21]</ref> (IMP). Kernel temporal segmentation is first applied to video segmentation, then an importance score is assigned to each segment by Imp+DCNN. Finally, segments are included in the summary by the order of their importance.</p><p>(3) Interestingness-driven summary <ref type="bibr" target="#b5">[6]</ref> (INT). The method starts by finding positions appropriate for a cut and High Low High Low</p><p>High Low <ref type="figure">Figure 8</ref>. Examples of segments ranking from low (right) to high (left) according to our predicted highlight score for "surfing," "skydiving," and "climbing" categories. We uniformly sampled ten segments in a video and one frame is selected to represent each segment. gets a set of segments. The interestingness score of each segment is the sum over the interestingness of its frames, which are jointly estimated by low-level (e.g., quality and saliency) and high-level (e.g., motion and person detection) features. Based on the interestingness score, an optimal subset of segments is selected to concatenate a summary.</p><p>(4) Highlight-driven summary. We designed two runs for our proposed highlight-driven video summarization approaches described in Section 4, i.e. HD-VT and HD-VS, which exploit video timelapse and video skimming techniques respectively.</p><p>Performance Comparison. We conduct subjective evaluation to compare the generated summaries. The evaluation process is as follows. First, all the evaluators are required to watch the original video. Then we show them once two summaries for that video. One is by HD-VT or HD-VS and the other is from the rest four runs. Note that we do not reveal which is ours and order the two summaries randomly. After viewing both, the evaluators are asked two questions: 1) Coverage: Which summary better covers the progress of the video? 2) Presentation: Which summary better distills and presents the essence of the video?</p><p>We randomly selected three videos in the test set from each of the 15 categories and the evaluation set is consisted of 45 original videos associated with five summaries for each. As only the comparisons between our methods and other three baselines are taken into account, we have 45 × 7 pairs of summaries to be tested in total. We invited 35 evaluators from different education backgrounds and they range from 20-52 years old. On each pair of compared approaches, the percentage of 35 evaluators' choices is averaged on all 45 videos and finally reported. <ref type="table" target="#tab_3">Table 2 and 3</ref> show the statistics of our proposed HD-VT and HD-VS, respectively. Overall, a strong majority of the evaluators prefer the summaries produced by HD-VT and HD-VS over other three methods in terms of both Coverage and Presentation criteria. The results support our point that video highlights can distill the moments of user interest and thus better summarize the first-person videos. Compared to HD-VS, HD-VT achieves more preferences in Coverage as it keeps all the video content. HD-VS, in contrast, is benefited from the way of skimming the non-highlight segments and hence gets more votes in Presentation. Furthermore, UNI which concatenates the uniformly selected subshots is not informative for the long and unstructured first-person videos in general. Though both IMP and INT involve utilization of scoring each video segment for summarization, they formulate the scoring as a classification problem and tend to include more near-duplicate segments. HD-VT and HD-VS, in contrast, treat it as a pairwise ranking problem and have a better ability in differentiating each segment, and thus allow better summarization in terms of both Coverage and Presentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Discussion and Conclusion</head><p>We have presented a new paradigm of exploring the moments of user interest, i.e., highlights, for first-person video summarization. Particularly, we propose a categoryindependent deep video highlight detection model, which incorporates both spatial and temporal streams based on deep convolution neural networks. On a large first-person video dataset, performance improvements are observed when comparing to other highlight detection techniques such as linear SVM classifier and latent linear ranking SVM model, which are both category-specific. Furthermore, together with two summarization methods, we have developed our highlight-driven video summarization system. Our user study with 35 human subjects shows that a majority of the users prefer our summaries over both importance-based and interestingness-based methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>Retrieving highlights from unconstrained first-person videos: (a) raw video, where each segment is represented by a frame sampled from the segment; (b) highlight prediction score curve. The segments with high scores are selected as highlights in red bounding boxes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Highlight-driven video summarization framework (better viewed in color). (a) The input video is split into a set of segments. (b)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>One representative frame selected from each category in our dataset. The category is given in the lower row.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>( 4 )</head><label>4</label><figDesc>Deep Convolution Neural Networks model. We designed three runs for our proposed approaches: S-DCNN, T-DCNN and TS-DCNN. The two runs S-DCNN and T-DCNN predict the highlight score of video segment by separately using spatial DCNN and temporal DCNN, respectively. The result of TS-DCNN is the weighted summation of S-DCNN and T-DCNN by late fusion.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 .</head><label>5</label><figDesc>Performance comparison of different approaches for highlight detection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 .</head><label>6</label><figDesc>Per-category mAPs of different approaches for all the 15 categories.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 .</head><label>7</label><figDesc>Performance of TS-DCNN by late fusing S-DCNN and T-DCNN with different ω.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 1 .</head><label>1</label><figDesc>Run time of different approaches on a five minutes' video. The experiments are conducted on a regular PC (Intel dual-core 3.33GHz CPU and 32 GB RAM).</figDesc><table>App. 
Rule [16] 
LR+IDT [24] 
LR+DCNN 
S-DCNN 
TS-DCNN 

Time 

25s 
5h 
65s 
72s 
360s 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 .</head><label>2</label><figDesc>Percentage of users who prefer the summary generated by HD-VT over each of other four approaches.</figDesc><table>UNI 
IMP [21] INT [6] HD-VS 
Coverage 
91.4% 
80.1% 
74.3% 
68.6% 
Presentation 85.7% 
60.2% 
64.8% 
34.3% 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 .</head><label>3</label><figDesc>Percentage of users who prefer the summary generated by HD-VS over each of other four approaches.</figDesc><table>UNI 
IMP [21] INT [6] HD-VT 
Coverage 
87.2% 
77.1% 
71.4% 
31.4% 
Presentation 88.6% 
74.3% 
82.9% 
65.7% 

</table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Scene aligned pooling for complex video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Natsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">F</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EC-CV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Liblinear: A library for large linear classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R.-E</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1871" to="1874" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Separate visual pathways for perception and action</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Goodale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Milner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in Neurosciences</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="20" to="25" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Algorithm Design: Foundation, Analysis and Internet Examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Goodrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tamassia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>John Wiley and Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Creating summaries from user videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gygli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Grabner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Riemenschneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Improving neural networks by preventing co-adaptation of feature detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno>arX- iv:1207.0580</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">3d convolutional neural networks for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on PAMI</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="221" to="231" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Real-time hyperlapse creation via optimal frame selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kienzle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Toelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Uyttendaele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Discovering important people and objects for egocentric video summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Story-driven summarization for egocentric video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A generic framework of user attention model and its application in video summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-F</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-S</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on MM</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="907" to="919" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Near-lossless semantic video summarization and its applications to video analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-S</forename><surname>Hua</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>ACM TOMCCAP</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Automatic detection of goal segments in basketball videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nepal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Reynolds</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Video summarization and scene detection by graph modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-W</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-F</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on CSVT</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="296" to="299" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Action and event recognition with fisher vectors on a compact feature set</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Oneata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Clickthrough-based cross-view learning for image search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-W</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rui</surname></persName>
		</author>
		<editor>SI-GIR</editor>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Category-specific video summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Potapov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Automatically extracting highlights for tv baseball programs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Acero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Ranking domain-specific highlights by analyzing edited videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Seitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Action recognition with improved trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning fine-grained image similarity with deep ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Etc</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning query and image similarities with ranking canonical correlation analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-W</forename><surname>Ngo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Annotation for free: Video tagging by mining user search behavior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-W</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
