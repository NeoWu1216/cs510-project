<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Hierarchical Pose-Based Approach to Complex Action Understanding Using Dictionaries of Actionlets and Motion Poselets</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Lillo</surname></persName>
							<email>ialillo@uc.cl</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
							<email>jniebles@cs.stanford.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alvaro</forename><surname>Soto</surname></persName>
							<email>asoto@ing.uc.cl</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Universidad Catolica de Chile Santiago</orgName>
								<address>
									<country key="CL">Chile</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Stanford University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Universidad del Norte</orgName>
								<address>
									<country key="CO">Colombia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Universidad Catolica de Chile Santiago</orgName>
								<address>
									<country key="CL">Chile</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Hierarchical Pose-Based Approach to Complex Action Understanding Using Dictionaries of Actionlets and Motion Poselets</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:37+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we introduce a new hierarchical model for human action recognition using body joint locations. Our model can categorize complex actions in videos, and perform spatio-temporal annotations of the atomic actions that compose the complex action being performed. That is, for each atomic action, the model generates temporal action annotations by estimating its starting and ending times, as well as, spatial annotations by inferring the human body parts that are involved in executing the action. Our model includes three key novel properties: (i) it can be trained with no spatial supervision, as it can automatically discover active body parts from temporal action annotations only; (ii) it jointly learns flexible representations for motion poselets and actionlets that encode the visual variability of body parts and atomic actions; (iii) a mechanism to discard idle or non-informative body parts which increases its robustness to common pose estimation errors. We evaluate the performance of our method using multiple action recognition benchmarks. Our model consistently outperforms baselines and state-of-the-art action recognition methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Human action recognition in video is a key technology for a wide variety of applications, such as smart surveillance, human-robot interaction, and video search. Consequently, it has received wide attention in the computer vision community with a strong focus on recognition of single actions in short video sequences <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b35">36]</ref>. As this area evolves, there has been an increasing interest to develop more flexible models that can extract useful knowledge from longer video sequences, featuring multiple concurrent or sequential actions, which we refer to as complex actions. Furthermore, to facilitate tasks such as video tagging or retrieval, it is important to design models that can identify the spatial and temporal spans of each relevant ac-  <ref type="figure">Figure 1</ref>. Sample frames from a video sequence featuring a complex action. Our method is able to identify the global complex action, as well as, the temporal and spatial span of meaningful actions (related to actionlets) and local body part configurations (related to motion poselets). tion. As an example, <ref type="figure">Figure 1</ref> illustrates a potential usage scenario, where an input video featuring a complex action is automatically annotated by identifying its underlying atomic actions and corresponding spatio-temporal spans.</p><p>A promising research direction for reasoning about complex human actions is to explicitly incorporate body pose representations. In effect, as noticed long ago, body poses are highly informative to discriminate among human actions <ref type="bibr" target="#b12">[13]</ref>. Similarly, recent works have also demonstrated the relevance of explicitly incorporating body pose information in action recognition models <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b29">30]</ref>. While human body pose estimation from color images remains elusive, the emergence of accurate and cost-effective RGBD cameras has enabled the development of robust techniques to identify body joint locations and to infer body poses <ref type="bibr" target="#b24">[25]</ref>.</p><p>In this work, we present a new pose-based approach to recognizing and provide detailed information about complex human actions in RGBD videos. Specifically, given a video featuring a complex action, our model can identify the complex action occurring in the video, as well as, the set of atomic actions that compose this complex action. Furthermore, for each atomic action, the model is also able to generate temporal annotations by estimating its starting and ending times, and spatial annotations by inferring the body parts that are involved in the action execution.</p><p>To achieve this, we propose a hierarchical compositional model that operates at three levels of abstraction: body poses, atomic actions, and complex actions. At the level of body poses, our model learns a dictionary that captures relevant spatio-temporal configurations of body parts. We refer to the components of this dictionary as motion poselets <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b25">26]</ref>. At the level of atomic actions, our model learns a dictionary that captures the main modes of variation in the execution of each action. We refer to the components of this dictionary as actionlets <ref type="bibr" target="#b31">[32]</ref>. Atoms in both dictionaries are given by linear classifiers that are jointly learned by minimizing an energy function that constraints compositions among motion poselets and actionlets, as well as, their spatial and temporal relations. While our approach can be extended to more general cases, here we focus on modeling atomic actions that can be characterized by the body motions of a single actor, such as running, drinking, or eating.</p><p>Our model introduces several contributions with respect to prior work <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b33">34]</ref>. First, it presents a novel formulation based on a structural latent SVM model <ref type="bibr" target="#b38">[39]</ref> and an initialization scheme based on self-pace learning <ref type="bibr" target="#b14">[15]</ref>. These provide an efficient and robust mechanism to infer, at test and training time, action labels for each detected motion poselet, as well as, their temporal and spatial span. Second, it presents a multi-modal approach that trains a group of actionlets for each atomic action. This provides a robust method to capture relevant intra-class variations in action execution. Third, it incorporates a garbage collector mechanism that identifies and discards idle or non-informative spatial areas of the input videos. This provides an effective method to process long video sequences. Finally, we provide empirical evidence indicating that the integration of the previous contributions in a single hierarchical model, generates a highly informative and accurate solution that outperforms state-of-the-art approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>There is a large body of work on human activity recognition in the computer vision literature <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b35">36]</ref>. We focus on recognizing human actions and activities from videos using pose-based representations and review in the following some of the most relevant previous work.</p><p>The idea of using human body poses and configurations as an important cue for recognizing human actions has been explored recurrently, as poses provide strong cues on the actions being performed. Initially, most research focused on pose-based action recognition in color videos <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b26">27]</ref>. But due to the development of pose estimation methods on depth images <ref type="bibr" target="#b24">[25]</ref>, there has been recent interest in posebased action recognition from RGBD videos <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b27">28]</ref>. Some methods have tackled the problem of jointly recogniz-ing actions and poses in videos <ref type="bibr" target="#b19">[20]</ref> and still images <ref type="bibr" target="#b36">[37]</ref>, with the hope to create positive feedback by solving both tasks simultaneously.</p><p>One of the most influential pose-based representations in the literature is Poselets, introduced by Bourdev and Malik <ref type="bibr" target="#b2">[3]</ref>. Their representation relies on the construction of a large set of frequently occurring poses, which is used to represent the pose space in a quantized, compact and discriminative manner. Their approach has been applied to action recognition in still images <ref type="bibr" target="#b18">[19]</ref>, as well as in videos <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b40">41]</ref>.</p><p>Researchers have also explored the idea of fusing posebased cues with other types of visual descriptors. For example, Cheron et al. <ref type="bibr" target="#b4">[5]</ref> introduce P-CNN as a framework for incorporating pose-centered CNN features extracted from optical flow and color. In the case of RGBD videos, researchers have proposed the fusion of depth and color features <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b13">14]</ref>. In general, the use of multiple types of features helps to disambiguate some of the most similar actions.</p><p>Also relevant to our framework are hierarchical models for action recognition. In particular, the use of latent variables as an intermediary representation in the internal layers of the model can be a powerful tool to build discriminative models and meaningful representations <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b33">34]</ref>. An alternative is to learn hierarchical models based on recurrent neural networks <ref type="bibr" target="#b5">[6]</ref>, but they tend to lack interpretability in their internal layers and require very large amounts of training data to achieve good generalization.</p><p>While most of the previous work have focused on recognizing single and isolated simple actions, in this paper we are interested in the recognition of complex, composable and concurrent actions and activities. In this setting, a person may be executing multiple actions simultaneously, or in sequence, instead of performing each action in isolation. An example of these is the earlier work of Ramanan and Forsyth <ref type="bibr" target="#b21">[22]</ref>, with more recent approaches by Yeung et al. <ref type="bibr" target="#b37">[38]</ref> and Wei et al. <ref type="bibr" target="#b34">[35]</ref>. Another recent trend aims at finegrained detection of actions performed in sequence such as those in a cooking scenario <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b15">16]</ref>. We build our model upon several of these ideas in the literature. Our method extends the state-of-the-art by introducing a model that can perform detailed annotation of videos during testing time but only requires weak supervision at training time. While learning can be done with reduced labels, the hierarchical structure of poselets and actionlets combined with other key mechanisms enable our model to achieve improved performance over competing methods in several evaluation benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Model Description</head><p>In this section, we introduce our model for pose-based recognition of complex human actions. Our goal is to build a model with the capability of annotating input videos with the actions being performed, automatically identifying the  parts of the body that are involved in each action (spatial localization) along with the temporal span of each action (temporal localization). As our focus is on concurrent and composable activities, we would also like to encode multiple levels of abstraction, such that we can reason about poses, actions, and their compositions. Therefore, we develop a hierarchical compositional framework for modeling and recognizing complex human actions. One of the key contributions of our model is its capability to spatially localize the body regions that are involved in the execution of each action, both at training and testing time. Our training process does not require careful spatial annotation and localization of actions in the training set; instead, it uses temporal annotations of actions only. At test time, it can discover the spatial and temporal span, as well as, the specific configuration of the main body regions executing each action. We now introduce the components of our model and the training process that achieves this goal.</p><formula xml:id="formula_0">FEATURES y ACTIVITY v v v v v v v v v</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Body regions</head><p>We divide the body pose into R fixed spatial regions and independently compute a pose feature vector for each re-gion. <ref type="figure" target="#fig_1">Figure 3</ref> illustrates the case when R = 4 that we use in all our experiments. Our body pose feature vector consists of the concatenation of two descriptors. At frame t and region r, a descriptor x g t,r encodes geometric information about the spatial configuration of body joints, and a descriptor x m t,r encodes local motion information around each body joint position. We use the geometric descriptor from <ref type="bibr" target="#b17">[18]</ref>: we construct six segments that connect pairs of joints at each region <ref type="bibr" target="#b0">1</ref> and compute 15 angles between those segments. Also, three angles are calculated between a plane formed by three segments <ref type="bibr" target="#b1">2</ref> and the remaining three noncoplanar segments, totalizing an 18-D geometric descriptor (GEO) for every region. Our motion descriptor is based on tracking motion trajectories of key points <ref type="bibr" target="#b30">[31]</ref>, which in our case coincide with body joint positions. We extract a HOF descriptor using 32x32 RGB patches centered at the joint location for a temporal window of 15 frames. At each joint location, this produces a 108-D descriptor, which we concatenate across all joints in each a region to obtain our motion descriptor. Finally, we apply PCA to reduce the dimensionality of our concatenated motion descriptor to 20. The final descriptor is the concatenation of the geometric and motion descriptors,</p><formula xml:id="formula_1">x t,r = [x g t,r ; x m t,r ].</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Hierarchical compositional model</head><p>We propose a hierarchical compositional model that spans three semantic levels. <ref type="figure" target="#fig_0">Figure 2</ref> shows a schematic of our model. At the top level, our model assumes that each input video has a single complex action label y. Each complex action is composed of a temporal and spatial arrangement of atomic actions with labels u = [u 1 , . . . , u T ], u i ∈ {1, . . . , S}. In turn, each atomic action consists of several non-shared actionlets, which correspond to representative sets of pose configurations for action identification, modeling the multimodality of each atomic action. We capture actionlet assignments in</p><formula xml:id="formula_2">v = [v 1 , . . . , v T ], v i ∈ {1, .</formula><p>. . , A}. Each actionlet index v i corresponds to a unique and known actomic action label u i , so they are related by a mapping u = u(v). At the intermediate level, our model assumes that each actionlet is composed of a temporal arrangement of a subset from K body poses, encoded in z = [z 1 , . . . , z T ], z i ∈ {1, . . . , K}, where K is a hyperparameter of the model. These subsets capture pose geometry and local motion, so we call them motion poselets. Finally, at the bottom level, our model identifies motion poselets using a bank of linear classifiers that are applied to the incoming frame descriptors.</p><p>We build each layer of our hierarchical model on top of BoW representations of labels. To this end, at the bottom <ref type="bibr" target="#b0">1</ref> Arm segments: wrist-elbow, elbow-shoulder, shoulder-neck, wristshoulder, wrist-head, and neck-torso; Leg segments: ankle-knee, knee-hip, hip-hip center, ankle-hip, ankle-torso and hip center-torso level of our hierarchy, and for each body region, we learn a dictionary of motion poselets. Similarly, at the mid-level of our hierarchy, we learn a dictionary of actionlets, using the BoW representation of motion poselets as inputs. At each of these levels, spatio-temporal activations of the respective dictionary words are used to obtain the corresponding histogram encoding the BoW representation. The next two sections provide details on the process to represent and learn the dictionaries of motion poselets and actionlets. Here we discuss our integrated hierarchical model.</p><p>We formulate our hierarchical model using an energy function. Given a video of T frames corresponding to complex action y encoded by descriptors x, with the label vectors z for motion poselets, v for actionlets and u for atomic actions, we define an energy function for a video as:</p><formula xml:id="formula_3">E(x,v, z, y) = Emotion poselets(z, x) + Emotion poselets BoW(v, z) + Eatomic actions BoW(u(v), y) + Emotion poselets transition(z) + Eactionlets transition(v).<label>(1)</label></formula><p>Besides the BoW representations and motion poselet classifiers described above, Equation <ref type="formula" target="#formula_3">(1)</ref> includes two energy potentials that encode information related to temporal transitions between pairs of motion poselets (E motion poselets transition ) and actionlets (E actionlets transition ). The energy potentials are given by:</p><formula xml:id="formula_4">Emot. poselet(z, x) = r,t k w r k ⊤ xt,rδ k z (t,r) + θ r δ K+1 z (t,r) (2) Emot. poselet BoW(v, z) = r,a,k β r a,k δ a v (t,r) δ k z (t,r) (3) Eatomic act. BoW(u(v), y) = r,s α r y,s δ s u(v (t,r) ) (4) Emot. pos. trans.(z) = r,k +1 ,k ′ +1 η r k,k ′ t δ k z (t−1,r) δ k ′ z (t,r) (5) Eacttionlet trans.(v) = r,a,a ′ γ r a,a ′ t δ a v (t−1,r) δ a ′ v (t,r)<label>(6)</label></formula><p>Our goal is to maximize E(x, v, z, y), and obtain the spatial and temporal arrangement of motion poselets z and actionlets v, as well as, the underlying complex action y.</p><p>In the previous equations, we use δ b a to indicate the Kronecker delta function δ(a = b), and use indexes k ∈ {1, . . . , K} for motion poselets, a ∈ {1, . . . , A} for actionlets, and s ∈ {1, . . . , S} for atomic actions. In the energy term for motion poselets, w r k are a set of K linear pose classifiers applied to frame descriptors x t,r , according to the label of the latent variable z t,r . Note that there is a special label K+1; the role of this label will be explained in Section 3.5. In the energy potential associated to the BoW representation for motion poselets, β r denotes a set of A mid-level classifiers, whose inputs are histograms of motion poselet labels at those frame annotated as actionlet a. At the highest level, α r y is a linear classifier associated with complex action y, whose input is the histogram of atomic action labels, which are related to actionlet assignments by the mapping function u(v). Note that all classifiers and labels here correspond to a single region r. We add the contributions of all regions to compute the global energy of the video. The transition terms act as linear classifiers η r and γ r over histograms of temporal transitions of motion poselets and temporal transitions of actionlets respectively. As we have a special label K + 1 for motion poselets, the summation index k +1 indicates the interval [1, . . . , K + 1].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Learning motion poselets</head><p>In our model, motion poselets are learned by treating them as latent variables during training. Before training, we fix the number of motion poselets per region to K. In every region r, we learn an independent set of pose classifiers {w r k } K k=1 , initializing the motion poselet labels using the k-means algorithm. We learn pose classifiers, actionlets and complex actions classifiers jointly, allowing the model to discover discriminative motion poselets useful to detect and recognize complex actions. As shown in previous work, jointly learning linear classifiers to identify body parts and atomic actions improves recognition rates <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b33">34]</ref>, so here we follow a similar hierarchical approach, and integrate learning of motion poselets with the learning of actionlets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Learning actionlets</head><p>A single linear classifier does not offer enough flexibility to identify atomic actions that exhibit high visual variability. As an example, the atomic action "open" can be associated with "opening a can" or "opening a book", displaying high variability in action execution. Consequently, we augment our hierarchical model including multiple classifiers to identify different modes of action execution.</p><p>Inspired by <ref type="bibr" target="#b22">[23]</ref>, we use the Cattell's Scree test to find a suitable number of actionlets to model each atomic action. Specifically, using the atomic action labels, we compute a descriptor for every video interval using normalized histograms of initial pose labels obtained with k-means. Then, for a particular atomic action s, we compute the eigenvalues λ(s) of the affinity matrix of the atomic action descriptors, which is build using χ 2 distance. For each atomic action s ∈ {1, . . . , S}, we find the number of actionlets G s as G s = argmin i λ(s) 2 i+1 /( i j=1 λ(s) j ) + c · i, with c = 2 · 10 −3 . Finally, we cluster the descriptors from each atomic action s running k-means with k = G s . This scheme generates a set of non-overlapping actionlets to model each single atomic action. In our experiments, we notice that the number of actionlets used to model each atomic action varies typically from 1 to 8.</p><p>To transfer the new labels to the model, we define u(v) as a function that maps from actionlet label v to the corresponding atomic action label u. A dictionary of actionlets provides a richer representation for actions, where several actionlets will map to a single atomic action. This behavior resembles a max-pooling operation, where at inference time we will choose the set of actionlets that best describe the performed actions in the video, keeping the semantics of the original atomic action labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">A garbage collector for motion poselets</head><p>While poses are highly informative for action recognition, an input video might contain irrelevant or idle zones, where the underlying poses are noisy or non-discriminative to identify the actions being performed in the video. As a result, low-scoring motion poselets could degrade the pose classifiers during training, decreasing their performance. To deal with this problem, we include in our model a garbage collector mechanism for motion poselets. This mechanism operates by assigning all low-scoring motion poselets to the (K + 1)-th pose dictionary entry. These collected poses are associated with a learned score lower than θ r , as in Equation <ref type="bibr" target="#b1">(2)</ref>. Our experiments show that this mechanism leads to learning more discriminative motion poselet classifiers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Learning</head><p>Initial actionlet labels. An important step in the training process is the initialization of latent variables. This is a challenging due to the lack of spatial supervision: at each time instance, the available atomic actions can be associated with any of the R body regions. We adopt the machinery of self-paced learning <ref type="bibr" target="#b14">[15]</ref> to provide a suitable solution and formulate the association between actions and body regions as an optimization problem. We constrain this optimization using two structural restrictions: i) atomic actions intervals must not overlap in the same region, and ii) a labeled atomic action must be present at least in one region. We formulate the labeling process as a binary Integer Linear Programming (ILP) problem, where we define b m r,q = 1 when action interval q ∈ {1, . . . , Q m } is active in region r of video m; and b m r,q = 0 otherwise. Each action interval q is associated with a single atomic action. We assume that we have initial motion poselet labels z t,r in each frame and region. We describe the action interval q and region r using the histogram h m r,q of motion poselet labels. We can find the correspondence between action intervals and regions using a formulation that resembles the operation ofk-means, but using the structure of the problem to constraint the labels: Here, µ r aq are the means of the descriptors with action label a q within region r. We solve P1 iteratively using a block coordinate descending scheme, alternating between solving b m r,q with µ r a fixed, which has a trivial solution; and then fixing µ r a to solve b m r,q , relaxing P1 to solve a linear program. Note that the second term of the objective function in P1 resembles the objective function of self-paced learning <ref type="bibr" target="#b14">[15]</ref>, managing the balance between assigning a single region to every action or assigning all possible regions to the respective action interval.</p><formula xml:id="formula_5">P1) min b,µ M m=1 R r=1 Qm q=1 b m r,q d(h m r,q − µ r aq ) − 1 λ b m r,q s.t. R r=1 b m r,q ≥ 1, ∀q, ∀m b m r,q 1 + b m r,q 2 ≤ 1 if q1 ∩ q2 = ∅, ∀r, ∀m b m r,q ∈ {0, 1}, ∀q, ∀r, ∀m<label>(7)</label></formula><p>Learning model parameters. We formulate learning the model parameters as a Latent Structural SVM problem <ref type="bibr" target="#b38">[39]</ref>, with latent variables for motion poselets z and actionlets v. We find values for parameters in equations (2-6), slack variables ξ i , motion poselet labels z i , and actionlet labels v i , by solving:</p><formula xml:id="formula_6">min W,ξ i , i={1,...,M } 1 2 ||W || 2 2 + C M M i=1 ξi,<label>(9)</label></formula><p>where</p><formula xml:id="formula_7">W ⊤ = [α ⊤ , β ⊤ , w ⊤ , γ ⊤ , η ⊤ , θ ⊤ ],<label>(10)</label></formula><p>and ξi = max</p><formula xml:id="formula_8">z,v,y {E(xi, z, v, y) + ∆((yi, vi), (y, v)) − max z i E(xi, zi, vi, yi)}, i ∈ [1, ...M ].<label>(11)</label></formula><p>In Equation <ref type="formula" target="#formula_3">(11)</ref>, each slack variable ξ i quantifies the error of the inferred labeling for video i. We solve Equation <ref type="formula" target="#formula_6">(9)</ref> iteratively using the CCCP algorithm <ref type="bibr" target="#b39">[40]</ref>, by solving for latent labels z i and v i given model parameters W , temporal atomic action annotations (when available), and labels of complex actions occurring in training videos (see Section 3.7). Then, we solve for W via 1-slack formulation using Cutting Plane algorithm <ref type="bibr" target="#b11">[12]</ref>. The role of the loss function ∆((y i , v i ), (y, v)) is to penalize inference errors during training. If the true actionlet labels are known in advance, the loss function is the same as in <ref type="bibr" target="#b17">[18]</ref> using the actionlets instead of atomic actions: <ref type="bibr" target="#b11">(12)</ref> where v ti is the true actionlet label. If the spatial ordering of actionlets is unknown (hence the latent actionlet formulation), but the temporal composition is known, we can compute a list A t of possible actionlets for frame t, and include that information on the loss function as ∆((yi, vi), (y, v)) = λy(yi = y) + λv 1 T T t=1 δ(vt / ∈ At) (13)</p><formula xml:id="formula_9">∆((yi, vi), (y, v)) = λy(yi = y)+λv 1 T T t=1 δ(vt i = vt),</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7.">Inference</head><p>The input to the inference algorithm is a new video sequence with features x. The task is to infer the best complex action labelŷ, and to produce the best labeling of actionletŝ v and motion poseletsẑ.</p><formula xml:id="formula_10">y,v,ẑ = argmax y,v,z E(x, v, z, y)<label>(14)</label></formula><p>We can solve this by exhaustively enumerating all values of complex actions y, and solving forv andẑ using:</p><formula xml:id="formula_11">v,ẑ|y = argmax v,z R r=1 T t=1 α r y,u(v(t,r)) + β r v (t,r) ,z (t,r) +w r z (t,r) ⊤ xt,rδ(z (t,r) ≤ K) + θ r δ K+1 z (t,r) +γ r v (t−1,r) ,v (t,r) + η r z (t−1,r) ,z (t,r) .<label>(15)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>Our experimental validation focuses on evaluating two properties of our model. First, we measure action classification accuracy on several action recognition benchmarks. Second, we measure the performance of our model to provide detailed information about atomic actions and body regions associated to the execution of a complex action.</p><p>We evaluate our method on four action recognition benchmarks: the MSR-Action3D dataset <ref type="bibr" target="#b16">[17]</ref>, Concurrent Actions dataset <ref type="bibr" target="#b34">[35]</ref>, Composable Activities Dataset <ref type="bibr" target="#b17">[18]</ref>, and sub-JHMDB <ref type="bibr" target="#b10">[11]</ref>. Using cross-validation, we set K = 100 in Composable Activities and Concurrent Actions datasets, K = 150 in sub-JHMDB, and K = 200 in MSR-Action3D. In all datasets, we fix λ y = 100 and λ u = 25. The number of actionlets to model each atomic action is estimated using the method described in Section 3.4. The garbage collector (GC) label (K + 1) is automatically assigned during inference according to the learned model parameters θ r . We initialize the 20% most dissimilar frames to the K + 1 label. In practice, at test time, the number of frames labeled as (K + 1) ranges from 14% in MSR-Action3D to 29% in sub-JHMDB.</p><p>Computation is fast during testing. In the Composable Activities dataset, our CPU implementation runs at 300 fps on a 32-core computer, while training time is 3 days, mostly due to the massive execution of the cutting plane algorithm. Using Dynamic Programming, complexity to estimate labels is linear with the number of frames T and quadratic with the number of actionlets A and motion poselets K. In practice, we filter out the majority of combinations of motion poses and actionlets in each frame, using the 400 best combinations of (k, a) according to the value of nonsequential terms in the dynamic program. Details are provided in the supplementary material. Algorithm Accuracy Our model 93.0% L. Tao et al. <ref type="bibr" target="#b25">[26]</ref> 93.6% C. Wang et al. <ref type="bibr" target="#b29">[30]</ref> 90.2% Vemulapalli et al. <ref type="bibr" target="#b27">[28]</ref> 89.5% <ref type="table">Table 1</ref>. Recognition accuracy in the MSR-Action3D dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Classification of Simple and Isolated Actions</head><p>As a first experiment, we evaluate the performance of our model on the task of simple and isolated human action recognition in the MSR-Action3D dataset <ref type="bibr" target="#b16">[17]</ref>. Although our model is tailored at recognizing complex actions, this experiment verifies the performance of our model in the simpler scenario of isolated atomic action classification.</p><p>The MSR-Action3D dataset provides pre-trimmed depth videos and estimated body poses for isolated actors performing actions from 20 categories. We use 557 videos in a similar setup to <ref type="bibr" target="#b31">[32]</ref>, where videos from subjects 1, 3, 5, 7, 9 are used for training and the rest for testing. <ref type="table">Table 1</ref> shows that in this dataset our model achieves classification accuracies comparable to state-of-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Detection of Concurrent Actions</head><p>Our second experiment evaluates the performance of our model in a concurrent action recognition setting. In this scenario, the goal is to predict the temporal localization of actions that may occur concurrently in a long video. We evaluate this task on the Concurrent Actions dataset <ref type="bibr" target="#b34">[35]</ref>, which provides 61 RGBD videos and pose estimation data annotated with 12 action categories. We use a similar evaluation setup as proposed by the authors. We split the dataset into training and testing sets with a 50%-50% ratio. We evaluate performance by measuring precision-recall: a detected action is declared as a true positive if its temporal overlap with the ground truth action interval is larger than 60% of their union, or if the detected interval is completely covered by the ground truth annotation.</p><p>Our model is tailored at recognizing complex actions that are composed of atomic components. However, in this scenario, only atomic actions are provided and no compositions are explicitly defined. Therefore, we apply a simple preprocessing step: we cluster training videos into groups by comparing the occurrence of atomic actions within each video. The resulting groups are used as complex actions labels in the training videos of this dataset. At inference time, our model outputs a single labeling per video, which corresponds to the atomic action labeling that maximizes the energy of our model. Since there are no thresholds to adjust, our model produces the single precision-recall measurement reported in <ref type="table" target="#tab_2">Table 2</ref>. Our model outperforms the state-of-the-art method in this dataset at that recall level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Precision Recall</head><p>Our full model 0.92 0.81 Wei et al. <ref type="bibr" target="#b34">[35]</ref> 0.85 0.81  <ref type="bibr" target="#b3">[4]</ref> 79.0% <ref type="table">Table 3</ref>. Recognition accuracy in the Composable Activities dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Recognition of Composable Activities</head><p>In this experiment, we evaluate the performance of our model to recognize complex and composable human actions. In the evaluation, we use the Composable Activities dataset <ref type="bibr" target="#b17">[18]</ref>, which provides 693 videos of 14 subjects performing 16 activities. Each activity is a spatio-temporal composition of atomic actions. The dataset provides a total of 26 atomic actions that are shared across activities. We train our model using two levels of supervision during training: i) spatial annotations that map body regions to the execution of each action are made available ii) spatial supervision is not available, and therefore the labels v to assign spatial regions to actionlets are treated as latent variables. <ref type="table">Table 3</ref> summarizes our results. We observe that under both training conditions, our model achieves comparable performance. This indicates that our weakly supervised model can recover some of the information that is missing while performing well at the activity categorization task. In spite of using less supervision at training time, our method outperforms state-of-the-art methodologies that are trained with full spatial supervision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Action Recognition in RGB Videos</head><p>Our experiments so far have evaluated the performance of our model in the task of human action recognition in RGBD videos. In this experiment, we explore the use of our model in the problem of human action recognition in RGB videos. For this purpose, we use the sub-JHMDB dataset <ref type="bibr" target="#b10">[11]</ref>, which focuses on videos depicting 12 actions and where most of the actor body is visible in the image frames. In our validation, we use the 2D body pose configurations provided by the authors and compare against previous methods that also use them. Given that this dataset only includes 2D image coordinates for each body joint, we obtain the geometric descriptor by adding a depth coordinate with a value z = d to joints corresponding to wrist and knees, z = −d to elbows, and z = 0 to other joints, so we can compute angles between segments, using d = 30 fixed with cross-validation. We summarize the results in <ref type="table" target="#tab_3">Table 4</ref>, which shows that our method outperforms alternative stateof-the-art techniques. Algorithm Accuracy Our model 77.5% Huang et al. <ref type="bibr" target="#b10">[11]</ref> 75.6% Chéron et al. <ref type="bibr" target="#b4">[5]</ref> 72.5%  <ref type="table">Table 5</ref>. Atomic action annotation performances in the Composable Activities dataset. The results show that our model is able to recover spatiotemporal annotations both at training and testing time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Spatio-temporal Annotation of Atomic Actions</head><p>In this experiment, we study the ability of our model to provide spatial and temporal annotations of relevant atomic actions. <ref type="table">Table 5</ref> summarizes our results. We report precision-recall rates for the spatio-temporal annotations predicted by our model in the testing videos (first and second rows). Notice that this is a very challenging task. The testing videos do no provide any label, and the model needs to predict both, the temporal extent of each action and the body regions associated with the execution of each action. Although the difficulty of the task, our model shows satisfactory results being able to infer suitable spatiotemporal annotations.</p><p>We also study the capability of the model to provide spatial and temporal annotations during training. In our first experiment, each video is provided with the temporal extent of each action, so the model only needs to infer the spatial annotations (third row in <ref type="table">Table 5</ref>). In a second experiment, we do not provide any temporal or spatial annotation, but only the global action label of each video (fourth row in <ref type="table">Table 5</ref>). In both experiments, we observe that the model is still able to infer suitable spatio-temporal annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Effect of Model Components</head><p>In this experiment, we study the contribution of key components of the proposed model. First, using the sub-JHMDB dataset, we measure the impact of three components of our model: garbage collector for motion poselets (GC), multimodal modeling of actionlets, and use of latent variables to infer spatial annotation about body regions (latent v). <ref type="table">Table 6</ref> summarizes our experimental results. Table 6 shows that the full version of our model achieves the best performance, with each of the components mentioned above contributing to the overall success of the method.</p><p>Second, using the Composable Activities dataset, we also analyze the contribution of the proposed self-paced learning scheme for initializing and training our model. We summarize our results in <ref type="table" target="#tab_4">Table 7</ref>    i) Random: random initialization of latent variables v, ii) Clustering: initialize v by first computing a BoW descriptor for the atomic action intervals and then perform k-means clustering, assigning the action intervals to the closer cluster center, and iii) Ours: initialize v using the proposed selfpaced learning scheme. Our proposed initialization scheme helps the model to achieve its best performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7.">Qualitative Results</head><p>Finally, we provide a qualitative analysis of relevant properties of our model. <ref type="figure" target="#fig_3">Figure 4</ref> shows examples of moving poselets learned in the Composable Activities dataset. We observe that each moving poselet captures a salient body configuration that helps to discriminate among atomic actions. To further illustrate this, <ref type="figure" target="#fig_3">Figure 4</ref> indicates the most likely underlying atomic action for each moving poselet. <ref type="figure">Figure 5</ref> presents a similar analysis for moving poselets learned in the MSR-Action3D dataset.</p><p>We also visualize the action annotations produced by our model. <ref type="figure" target="#fig_5">Figure 6</ref> (top) shows the action labels associated with each body part in a video from the Composable Activities dataset.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions and Future Work</head><p>We present a hierarchical model for human action recognition using body joint locations. By using a semisupervised approach to jointly learn dictionaries of motions poselets and actionlets, the model demonstrates to be very flexible and informative, to handle visual variations and to provide spatio-temporal annotations of relevant atomic actions and active body part configurations. In particular, the model demonstrates to be competitive with respect to state-of-the -art approaches for complex action recognition, while also proving highly valuable additional information. As future work, the model can be extended to handle multiple actor situations, to use contextual information such as relevant objects, and to identify novel complex actions not present in the training set.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Graphical representation of our discriminative hierarchical model for recognition of complex human actions. At the top level, activities are represented as compositions of atomic actions that are inferred at the intermediate level. These actions are, in turn, compositions of poses at the lower level, where pose dictionaries are learned from data. Our model also learns temporal transitions between consecutive poses and actions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Skeleton representation used for splitting the human body into a set of spatial regions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Motion poselet #4 -most likely action: talking on cellphone Motion poselet #7 -most likely action: erasing on board Motion poselet #19 -most likely action: waving hand</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Moving poselets learned from the Composable Activities dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 (Figure 5 .</head><label>65</label><figDesc>bottom) illustrates per-body part action annotations for a video in the Concurrent Actions dataset. These examples illustrate the capabilities of our model to correctly annotate the body parts that are involved in the execution of each action, in spite of not having that information during training. Moving poselets learned from the MSR-Action3D dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Automatic spatio-temporal annotation of atomic actions. Our method detects the temporal span and spatial body regions that are involved in the performance of atomic actions in videos.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 2 .</head><label>2</label><figDesc>Recognition accuracy in the Concurrent Actions dataset.</figDesc><table>Algorithm 
Accuracy 
Base model + GC, GEO desc. only, spatial supervision 
88.5% 
Base model + GC, with spatial supervision 
91.8% 
Our full model, no spatial supervision (latent v) 
91.1% 
Lillo et al. [18] (without GC) 
85.7% 
Cao et al. </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="true"><head>Table 4 .</head><label>4</label><figDesc>Recognition accuracy in the sub-JHMDB dataset.</figDesc><table>Videos 
Annotation inferred 
Precision 
Recall 
Testing set 
Spatio-temporal, no GC 
0.59 
0.77 
Testing set 
Spatio-temporal 
0.62 
0.78 
Training set 
Spatial only 
0.86 
0.90 
Training set 
Spatio-temporal 
0.67 
0.85 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 7 .</head><label>7</label><figDesc>by reporting action recognition accuracy under different initialization schemes: Our full model (Actionlets + GC + latent v) 77.5%Table 6. Analysis of contribution to recognition performance from each model component in the sub-JHMDB dataset. Results in Composable Activities dataset, with latent v and different initializations.</figDesc><table>Algorithm 

Accuracy 
Base model, GEO descriptor only 
66.9% 
Base Model 
70.6% 
Base Model + GC 
72.7% 
Base Model + Actionlets 
75.3% 
Initialization Algorithm Accuracy 
Random 
46.3% 
Clustering 
54.8% 
Ours 
91.1% 
Ours, fully supervised 
91.8% 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Arm plane: shoulder-elbow-wrist; Leg plane: hip-knee-ankle</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements This work was partially funded by the FONDECYT grant 1151018, from CONICYT, Government of Chile; and by the Stanford AI Lab-Toyota Center for Artificial Intelligence Research. I.L. is supported by a PhD studentship from CONICYT.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Human activity analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Ryoo</surname></persName>
		</author>
		<idno>16:1-16:43</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Detecting people using mutually consistent poselet activations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="168" to="181" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Poselets: Body part detectors trained using 3D human pose annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1365" to="1372" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Spatio-temporal triangularchain crf for activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd Annual ACM Conference on Multimedia Conference</title>
		<meeting>the 23rd Annual ACM Conference on Multimedia Conference</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1151" to="1154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chéron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>P-Cnn</surname></persName>
		</author>
		<title level="m">Pose-based CNN Features for Action Recognition. ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Hierarchical Recurrent Neural Network for Skeleton Based Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1110" to="1118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Automated vision-based recognition of construction worker actions for building interior construction operations using RGBD cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Davila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Golparvar-Fard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Construction Research Congress</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="879" to="888" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Human action recognition by sequence of movelet codewords</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="717" to="721" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Jointly learning heterogeneous features for RGB-D activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-F</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5344" to="5352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning latent structure for activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Englebienne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Krose</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Towards understanding action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zuffi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Cutting-plane training of structural SVMs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Finley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="page" from="27" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Visual perception of biological motion and a model for its analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Johansson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perception &amp; Psychophysics</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="201" to="211" />
			<date type="published" when="1973" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Bilinear heterogeneous information machine for RGB-D action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1054" to="1062" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Self-paced learning for latent variable models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Packer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Action recognition by hierarchical mid-level action elements</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4552" to="4560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Action recognition based on a bag of 3D points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="9" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Discriminative hierarchical modeling of spatio-temporally composable human activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Soto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Action recognition from a distributed representation of pose and appearance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="3177" to="3184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Joint action recognition and pose estimation from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">X</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1293" to="1301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A survey on vision-based human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Poppe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="976" to="990" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Automatic annotation of everyday movements</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Discovering discriminative action parts from mid-level video representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Raptis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1242" to="1249" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A database for fine grained activity detection of cooking activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1194" to="1201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Real-time human pose recognition in parts from a single depth image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fitzgibbon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sharp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Finocchio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kipman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Communications of the ACM</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="116" to="124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Moving Poselets : A Discriminative and Interpretable Skeletal Motion Representation for Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision Workshops</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Pose primitive based human action recognition in videos or still images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Thurau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Hlavac</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Human Action Recognition by Representing 3D Skeletons as Points in a Lie Group</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vemulapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Arrate</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A survey on activity recognition and behavior understanding in video surveillance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vishwakarma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Visual Computer</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="983" to="1009" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">An approach to posebased action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Action recognition by dense trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Klaser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="3169" to="3176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Mining actionlet ensemble for action recognition with depth cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Video Action Detection with Relational Dynamic-Poselets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="565" to="580" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning a discriminative hidden part model for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Concurrent action detection with structural prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A survey of visionbased methods for action representation, segmentation and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weinland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ronfard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Boyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="224" to="241" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Modeling Mutual Context of Object and Human Pose in Human-Object Interaction Activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="17" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.05738</idno>
		<title level="m">Every Moment Counts: Dense Detailed Labeling of Actions in Complex Videos</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning structural svms with latent variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">The concave-convex procedure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rangarajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="915" to="936" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">The Moving Pose: An Efficient 3D Kinematics Descriptor for Low-Latency Action Recognition and Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Leordeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2752" to="2759" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
