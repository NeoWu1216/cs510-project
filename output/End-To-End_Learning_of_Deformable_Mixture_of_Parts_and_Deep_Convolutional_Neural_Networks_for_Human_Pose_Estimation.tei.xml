<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">End-to-End Learning of Deformable Mixture of Parts and Deep Convolutional Neural Networks for Human Pose Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Yang</surname></persName>
							<email>wyang@ee.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electronic Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
							<email>wlouyang@ee.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electronic Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
							<email>hsli@ee.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electronic Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
							<email>xgwang@ee.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electronic Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">End-to-End Learning of Deformable Mixture of Parts and Deep Convolutional Neural Networks for Human Pose Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:40+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Recently, Deep Convolutional Neural Networks</head> (DC-NNs)  <p>have been applied to the task of human pose estimation, and have shown its potential of learning better feature representations and capturing contextual relationships. However, it is difficult to incorporate domain prior knowledge such as geometric relationships among body parts into DCNNs. In addition, training DCNN-based body part detectors without consideration of global body joint consistency introduces ambiguities, which increases the complexity of training. In this paper, we propose a novel end-to-end framework for human pose estimation that combines DC-NNs with the expressive deformable mixture of parts. We explicitly incorporate domain prior knowledge into the framework, which greatly regularizes the learning process and enables the flexibility of our framework for loopy models or tree-structured models. The effectiveness of jointly learning a DCNN with a deformable mixture of parts model is evaluated through intensive experiments on several widely used benchmarks. The proposed approach significantly improves the performance compared with state-of-the-art approaches, especially on benchmarks with challenging articulations.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Articulated human pose estimation is one of the fundamental tasks in computer vision. It solves the problem of localizing human parts in images, and has many important applications such as action recognition <ref type="bibr" target="#b44">[45]</ref>, clothing parsing <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b49">50]</ref>, and human tracking <ref type="bibr" target="#b5">[6]</ref>. The main challenges of this task are articulation, occlusion, cluttered background, and variations in clothing and lighting. Recently, state-of-the-art performance of human pose estimation has been achieved with Deep Convolutional Neural Networks (DCNNs) <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b6">7]</ref>. These approaches primarily fall into two categories: 1) regressing * Wanli Ouyang is the corresponding author. heat-maps of each body part location with DCNNs <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b39">40]</ref>; 2) learning deep structured output to further model the relationships among body joints <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b40">41]</ref>. DCNN-based heat-map regression models have shown the potential of learning better feature representations. However, geometric constraints among body parts, which are essential to ensure the joint consistency, are usually missed in training the DCNNs. As a consequence, during the training stage, these approaches may produce many imperfect results, as shown in <ref type="figure" target="#fig_0">Figure 1</ref> (b, e). For example, regions with high response to head in <ref type="figure" target="#fig_0">Figure 1</ref> (b) are heads of unannotated persons, which are reasonable but will be treated as false positives in learning the DCNN. Errors on these regions will be back propagated to penalize the features correspond to head detection, which is inappropriate. We observe that this problem could be addressed by considering global joint consistency during the training stage: the unannotated persons do not have their full bodies appearing in the image, hence can be suppressed when considering the full pose configuration, as shown in <ref type="figure" target="#fig_0">Figure 1</ref> (c). Another example is shown in <ref type="figure" target="#fig_0">Figure 1</ref> (e), where the false positive region for ankle at the background (top-left corner) will be treated as the hard negative for learning the DCNN. It is no longer a hard negative when the structure of full body is considered, as shown in <ref type="figure" target="#fig_0">Figure 1</ref> (f).</p><p>Deep structured output learning has attracted considerable attention recently, and has shown promising results in tasks such as semantic segmentation <ref type="bibr" target="#b1">[2]</ref>, scene parsing <ref type="bibr" target="#b22">[23]</ref>, object detection <ref type="bibr" target="#b43">[44]</ref>, and depth estimation <ref type="bibr" target="#b23">[24]</ref>. For human pose estimation, recent studies combine DCNNs with fully-connected Markov Random Field <ref type="bibr" target="#b40">[41]</ref> or weakly spatial histogram over body part locations <ref type="bibr" target="#b14">[15]</ref> to exploit structural constraints between body joint locations. However, the parameter space of learning spatial constraints with convolutional kernels <ref type="bibr" target="#b40">[41]</ref> is too large, which makes the learning difficult. Additionally, for persons with a large range of possible poses, e.g., the head is not always above the shoulder, these approaches will be less effective.</p><p>In vision community, domain knowledge has been proved effective in many tasks such as object recognition <ref type="bibr" target="#b10">[11]</ref>, detection <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b27">28]</ref>, and person reidentification <ref type="bibr" target="#b47">[48]</ref>. For pose estimation, the deformable mixture of parts model <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b29">30]</ref> uses domain knowledge and designs a deformable model to constrain the spatial configuration between a pair of parts with multiple appearance mixtures. By using a DCNN for feature extraction together with deformable model for spatial constraints, Chen and Yuille <ref type="bibr" target="#b3">[4]</ref> achieve a significant improvement. However, features and spatial constraints are still learned separately. Therefore, the problem in learning DCNNs as shown in <ref type="figure" target="#fig_0">Figure 1</ref> still exists.</p><p>In this paper, we propose to incorporate the DCNN and the expressive mixture of parts model into an end-to-end framework. This enables us to predict the body part locations with the consideration of global pose configurations during the training stage, hence our framework is able to predict heat-maps with less false positives, as shown in Figure 1 (c), (f). Therefore, jointly learning the DCNN with the deformable model makes the feature learning more effective in handling the negative samples that are difficult when taking the full body pose into account. In addition, we explicitly incorporate human pose priors including body part mixture types and standard quadratic deformation constraints into our model. This greatly reduces the parameters to be learned compared with the use of convolution or histogram, and still keeps the flexibility of our framework in building loopy models or tree-structured models.</p><p>We show the efficiency of the proposed framework on three widely used pose estimation benchmarks: the LSP <ref type="bibr" target="#b17">[18]</ref> dataset, the FLIC <ref type="bibr" target="#b34">[35]</ref> dataset and the Image Parse <ref type="bibr" target="#b32">[33]</ref> dataset. Our approach improves the state-of-theart on all these datasets. The generalization ability of our framework is also validated by cross-dataset experiments on the Image Parse dataset.</p><p>The main contributions of this work are three folds: • We design a novel message passing layer, which is flexible to build tree-structured models or loopy models with appearance mixtures. • An end-to-end deep CNN framework for human pose estimation is proposed. By jointly learning DCNNs with deformable mixture of parts models, global pose consistency is considered. Hence our framework is able to reduce the ambiguity and mine hard negatives effectively when learning features and part deformation. • Domain knowledge is incorporated into our framework.</p><p>Through quadratic deformation constraints, we reduce the parameter space in modeling the spatial and the appearance mixture relationships among parts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>In literature, part-based models have been widely used to model the articulated relationships between rigid human body parts. Specifically, tree-structured pictorial structures <ref type="bibr" target="#b12">[13]</ref> have been made tractable together with the development of general distance transform <ref type="bibr" target="#b10">[11]</ref>, and is popular in human pose estimation <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b34">35]</ref>. For example, Yang and Ramanan <ref type="bibr" target="#b50">[51]</ref> proposed a flexible mixture model to capture contextual co-occurrence relations between parts. Johnson and Everingham <ref type="bibr" target="#b18">[19]</ref> used a cascade of body parts detectors to obtain mixture models on the full model scale. Pishchulin et al. <ref type="bibr" target="#b29">[30]</ref> extended part-based model based on rigid body parts with Poselet <ref type="bibr" target="#b0">[1]</ref> priors. Despite efficient inference and impressive successes, tree-structured models suffer from the doublecounting problem, which often happens to limbs.</p><p>To overcome the limited expressiveness of treestructured models, there have been a lot of efforts that focused on constructing more expressive models <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b37">38]</ref>. For example, symmetry of appearance between limbs has been considered in <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b37">38]</ref>. Ferrari et al. <ref type="bibr" target="#b11">[12]</ref> proposed repulsive edges between opposite-sided arms to overcome double counting in upper-body pose estimation. These strong pose priors, however, may overfit to the statistics of some particular datasets <ref type="bibr" target="#b42">[43]</ref>. To consider higher-order part relationships beyond primitive rigid parts, Wang et al. <ref type="bibr" target="#b45">[46]</ref> incorporated hierarchical poselets for human parsing. In video pose estimation, Cherian et al. <ref type="bibr" target="#b4">[5]</ref> designed temporal links between body parts to address inconsistency between parts that across the sequences. These methods achieved better expressiveness by loopy models. Inference on such models, however, requires approximate methods such as integer programs <ref type="bibr" target="#b16">[17]</ref>, integer quadratic programs <ref type="bibr" target="#b33">[34]</ref>, or loopy belief propagation <ref type="bibr" target="#b35">[36]</ref>. Moreover, the above mentioned approaches are based on hand-crafted features (e.g., HOG <ref type="bibr" target="#b7">[8]</ref> and Shape Context <ref type="bibr" target="#b24">[25]</ref>), and may be limited by the representation ability. Deep Models for Human Pose Estimation: Recently, deep models have been successfully applied in human pose estimation. Ouyang et al. <ref type="bibr" target="#b25">[26]</ref> proposed a multi-source deep model for constructing the non-linear representation from multiple information sources. DeepPose <ref type="bibr" target="#b41">[42]</ref> estimated body part locations by learning a regressor based on DCNNs in a holistic manner. However, this method suffered from inaccuracy in the high-precision regions. Jain et al. <ref type="bibr" target="#b14">[15]</ref> used a multi-resolution DCNN and adopted motion features to improve the accuracy of body parts localization. Tompson et al. <ref type="bibr" target="#b39">[40]</ref> proposed spatial pooling to overcome the reduced localization accuracy caused by pooling operations. Chen and Yuille <ref type="bibr" target="#b3">[4]</ref> used a DCNN to learn the conditional probabilities for the presence of parts and their spatial relationships. They further proposed flexible compositions of object parts <ref type="bibr" target="#b2">[3]</ref> to handle significant occlusions in images, and showed state-of-the-art results. However, part detection scores and detectors are fixed in <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b2">3]</ref> but are not fixed in our model. The approaches in <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b2">3]</ref> learned part detectors and spatial relationships independently, while we jointly learned them. Besides, our model learns global pose configuration and is not constrained to tree models while tree models were used in <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b2">3]</ref>.</p><p>To capture contextual relationships directly within DC-NNs, some recent studies explored to combine DCNNs with Conditional Random Fields (CRFs), Markov Random Fields (MRFs), or Deformable Part Models, and showed promising results on several applications, such as depth esti-mation <ref type="bibr" target="#b23">[24]</ref>, semantic segmentation <ref type="bibr" target="#b21">[22]</ref>, and object detection <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b26">27]</ref>. For pose estimation, Tompson et al. <ref type="bibr" target="#b40">[41]</ref> jointly trained a multi-scale DCNN with an approximate MRF, which is to model the spatial relationships between body parts. Our approach is different from this approach in the following aspects. First, their model has difficulty in learning effective spatial relationships on datasets with large pose variations. Our method addresses this problem by using appearance mixtures. Second, to cover the largest body joint displacement, very large 128 × 128 convolution kernels were used in <ref type="bibr" target="#b40">[41]</ref>. Hence its parameter space is very large, and it is hard to learn when body parts are with large variations in relative locations. We take the body part articulation property into account and model spatial relations by mixture of deformation constraints, which are only 4 parameters for each pair of mixture-of-parts. Therefore, our model is better in handling large range of possible poses.</p><p>Parameterized deformation constraints are also jointly learned with DCNN for pedestrian detection in <ref type="bibr" target="#b26">[27]</ref> and object detection in <ref type="bibr" target="#b13">[14]</ref>. However, these approaches do not consider the appearance mixtures, hence are limited to body part variances, while we learn deformation constraints taking the appearance mixture into account to handle the variation. In addition, only the star model is considered in <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b13">14]</ref> while our approach is flexible for star models, treestructured models or loopy models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The Model</head><p>We formulate the human pose estimation problem by using a graph. Let G = (V, E) denote a graph with vertices V specifying the positions as well as the mixture types of body parts, and edges E ⊆ V × V indicating the spatial relationships between parts. Let K = |V | be the number of parts, and i ∈ {1, · · · , K} be the ith part. Given an image I, we denote the pixel locations of parts by</p><formula xml:id="formula_0">l = {l i } K i=1 = {(x i , y i )} K i=1 , and denote the mixture type of different spatial relationships by t = {t i } K i=1 , where t i ∈ {1, · · · , T i }.</formula><p>The full score of a pose configuration given an input image I is as follows:</p><formula xml:id="formula_1">F (l, t|I; θ, w) = i∈V φ(li, ti|I; θ) + (i,j)∈E ψ(li, lj, ti, tj|I; w t i ,t j i,j ),<label>(1)</label></formula><p>where θ and w = {w ti,tj i,j } are parameters of the model. Part Appearance Terms: Given an image patch located at l i , the unary terms φ(l i , t i |I; θ) provide local confidence of the appearance of part i with mixture type t i , which are defined as the log probability,</p><formula xml:id="formula_2">φ(li, ti|I; θ) = log p(li, ti|I; θ) = log σ(f (li, ti|I; θ)).<label>(2)</label></formula><p>The probability p(l i , t i |I; θ) is given by the softmax function σ(·), which is to predict the probability of the ith part at</p><formula xml:id="formula_3">location l i with type t i in image I. f (l i , t i |I; θ)</formula><p>is modeled by the front-end DCNN to predict a score for part i located at l i with type t i , where θ are its parameters. Appearance terms φ(l i , t i |I; θ) are obtained from the DCNN through a classification layer as shown in <ref type="figure" target="#fig_1">Figure 2</ref> (b). Spatial Relationship Terms: The pairwise terms model the spatial compatibility of two neighboring parts i and j.</p><p>We define the pairwise terms as follows:</p><formula xml:id="formula_4">ψ(li, lj, ti, tj|I; w t i ,t j i,j ) =&lt; w t i ,t j i,j , d(li − lj) &gt; .<label>(3)</label></formula><p>Here we incorporate standard quadratic deformation constraints into our model, where d(l i − l j ) is deformation feature defined as d(l i − l j ) = [∆x ∆x 2 ∆y ∆y 2 ] T , and ∆x = x i − x j and ∆y = y i − y j are the relative locations of part i with respect to part j, and w ti,tj i,j are the 4dimensional deformation weights to encode pairwise terms for mixture types (t i , t j ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Inference</head><p>Inference is to find the optimal part locations l * and mixture types t * that maximize the score function F (l, t|I; θ, w) as follows:</p><formula xml:id="formula_5">(l * , t * ) = arg max l,t F (l, t|I; θ, w).<label>(4)</label></formula><p>An overview of the inference procedure is demonstrated in <ref type="figure" target="#fig_1">Figure 2</ref> (b-d). Given an image, the heat-maps f (l i , t i |I; θ) of each part are computed by a forward pass through the DCNN. Then the log probability φ(l i , t i ) of each part with each type is obtained from f (l i , t i |I; θ) through a softmax layer and a logarithm layer. Taking φ(l i , t i ) as input, we propose to pass messages in neural networks by designing a novel message passing layer, which is flexible to build treestructured models or loopy models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Message Passing</head><p>We first give a brief review of message passing on the proposed model. Max-sum algorithm has been widely used for inferring the best configuration in graphical models. Although the max-sum algorithm is only an approximation and the convergence cannot be guaranteed on loopy structures, it still provided excellent experimental results <ref type="bibr" target="#b35">[36]</ref>.</p><p>At each iteration, a vertex sends a message to its neighbors and receives messages from its neighbors. We denote m ij (l j , t j ) as the message sent from part i to part j, and u i (l i , t i ) as the belief of part i, then the max-sum algorithm updates the messages and beliefs as follows:</p><formula xml:id="formula_6">mij(lj, tj) ← αm max l i ,t i (ui(li, ti) + ψ(li, lj, ti, tj)) ,<label>(5)</label></formula><formula xml:id="formula_7">ui(li, ti) ← αu(φ(li, ti) + k∈N(i) m ki (li, ti)),<label>(6)</label></formula><p>where α m and α u are normalization terms, and N(i) denotes the set of neighbors of part i. To simplify the notation, we omit model parameters here. <ref type="figure" target="#fig_1">Figure 2</ref> (a) gives a visualization of this message passing procedure.</p><p>The algorithm starts with all message vectors initialized to constant functions. The normalization terms in Eq.(5-6) are not necessary. However, we find that they help to make the inference more stable in practice. Maximum Score Assignment: Suppose the algorithm converges at the N th iteration, then the belief for each location and each type (l i , t i ) is the approximation of the maximum score function. Hence we can obtain the max-sum assignment (l * i , t * i ) by</p><formula xml:id="formula_8">(l * i , t * i ) = arg max l i ,t i u * i (li, ti), ∀i ∈ {1, · · · , K},<label>(7)</label></formula><p>where u * i (l i , t i ) is the belief computed in the last iteration, and (l * i , t * i ) is the solution for the maximum score in Eq.(4). Special Case: Tree-Structured Model: For tree structures, exact inference can be performed efficiently by one pass of dynamic programming, which is a special case of max-sum algorithm by passing messages from leaves to a chosen root node. By keeping track of indexes of arg max l * k ,t * k for each pass, the maximum score assignment can be obtained by backtracking from the root node to the leaves. This procedure is also known as Verberti decoding, and has been widely used in previous pose estimation works with tree-structured models <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b2">3]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">The Message Passing Layer in Neural Networks</head><p>In literature, there are mainly two possible ways to organize the message passing schedule. The flooding schedule <ref type="bibr">Figure 3</ref>. From left to right, we show the estimated poses generated by the first, the second, and the third message passing layer, respectively. Intuitively, a part could receive messages from further parts as the number of message passing layers increases, which may result in better results. simultaneously passes messages across every link in both directions at each time step, while the serial schedule passes one message at each time. By following the flooding schedule, we integrate the procedure introduced in Eq.(5-6) into the network by designing a novel message passing layer.</p><p>As shown in <ref type="figure" target="#fig_1">Figure 2</ref> (c), each node sends a message to each of its neighbors simultaneously (solid lines in <ref type="figure" target="#fig_1">Figure 2 (c)</ref>), and the belief of each part is updated by summing its unary potential φ(l i , t i ) (dashed lines) and the incoming messages. The belief u i ( * , t i ) corresponds to a feature map with mixture type t i for the ith part in the message passing layer. After convergence, the optimum pose estimation is obtained by selecting the location and type with maximum belief for each part, as in Eq. <ref type="formula" target="#formula_8">(7)</ref>.</p><p>Although message passing may need several iterations to converge, we observe that a cascade of three message passing layers is enough to produce satisfactory results in practice. Examples of estimated poses from different message passing layers are visualized in <ref type="figure">Figure 3</ref>. Intuitively, more message passing layers (i.e., more iterations in the max-sum algorithm) lead to better results. We take the tree-structured model shown in <ref type="figure" target="#fig_1">Figure 2</ref> (a) as an example: In the first round, neck only receives messages from its neighbors head and shoulders. In the second round, however, neck could receive messages from parts a step further such as elbows and hips. Computation: The computational complexity of message passing is O(L 2 T 2 ) for L possible part locations and T mixture types. Since our pairwise terms are quadratic functions of location l i and l j , we can accelerate the maximization over l i by employing the generalized distance transforms <ref type="bibr" target="#b10">[11]</ref>, and the computational complexity of updating one message is reduced to O(LT 2 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Learning</head><p>Several recent works produced heat-maps of body parts by using fully convolutional networks <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b2">3]</ref>. Some approaches train the fully convolutional networks with full images <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b39">40]</ref>. Others first train a DCNN from local image patches, then the learned DCNNs are fixed <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b2">3]</ref>. We initialize the DCNN by pretraining from local image patches with mixture type labels, and then jointly learn the DCNN and the deformable model by finetuning from full images. Pretraining with Part Mixture Types: Pretraining utilizes part mixture types as supervision to train the front-end DCNN that serves as part detectors. Existing human pose datasets are annotated with body part locations l, but without part mixture type labels t. We define the part types as the different relative locations clusters of a part with respect to its neighboring parts. Let r ij be the relative position from part i to its neighboring part j, we cluster the relative position r ij over the training set into T i clusters. Each cluster corresponds to a set of part instances that share with similar relative locations. The type label for each part can be derived by cluster membership, and serves as an extra supervision for pretraining the front-end DCNN. The mixture types obtained from locations are strongly correlated to appearance of parts. For example, horizontal arm is one part type and vertical arm is another type -they are different in pose configuration and appearance. We tried to remove pretraining, but the net failed to converge to satisfactory training loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Finetuning of the Full Model:</head><p>We finetune the unified model by the hinge loss function. Suppose there are N message passing layers, the final heat-map for each part is obtained as follows:</p><formula xml:id="formula_9">si(li) = max t i (u N i (li, ti) + bi),<label>(8)</label></formula><p>where b i is the bias. Denote the ground-truth location of part i byl i . The ground-truth heat-map for part i is</p><formula xml:id="formula_10">si(li) = +1, if ||li −li||∞ ≤ δ; −1, otherwise.<label>(9)</label></formula><p>where δ is a constant threshold. This produces the groundtruth heat-map with a box centered at locationl i : the ground-truth heat-map has value 1 inside the box, and value −1 outside the box. Ideally, we not only hope the predicted part locations to be close to ground-truth locations, but also hope the maximum response of each part in Eq.(8) to be higher than a threshold. This motivates us to train our model in a maxmargin manner.</p><p>Given the ground-truth heat-maps i (l i ) and the predicted heat-map s i (l i ) of part i, the loss function is</p><formula xml:id="formula_11">J(l, t) = 1 KL K i=1 L l i =1 max(0, 1 −si(li) · si(li)),<label>(10)</label></formula><p>where max(0, 1 −s i (l i ) · s i (l i )) is the hinge loss at location l i and J is the overall loss for all parts. We apply stochastic gradient descent to learn the parameters. First, we compute the subgradients w.r.t. the final heat-map for each part as,</p><formula xml:id="formula_12">∂J ∂si(li) = −si(li), ifsi(li) · si(li) &lt; 1; 0, otherwise.<label>(11)</label></formula><p>Then the partial derivatives w.r.t. each layer can be computed by using the standard backpropagation algorithm. For example, the partial derivative of the deformation weights w are computed as,</p><formula xml:id="formula_13">∂J ∂w t k ,t i k,i ∝ N n=1 l i ,t i ∂J ∂u n i (li, ti) d(l k − li),<label>(12)</label></formula><p>where k ∈ N(i). Recall that d(l k − l i ) is the standard quadratic deformation features defined in Eq.(3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experiments</head><p>In this section, we present experimental settings, experimental results, and diagnostic analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Experimental Settings</head><p>Datasets: We evaluate the proposed methods on three well known public pose estimation benchmarks: The Leeds Sports Poses (LSP) <ref type="bibr" target="#b17">[18]</ref> dataset, the Frames Labeled in Cinema (FLIC) <ref type="bibr" target="#b34">[35]</ref> dataset, and the Image Parse (PARSE) <ref type="bibr" target="#b32">[33]</ref> dataset. (i) LSP contains 1000 training and 1000 testing images from sports activities with challenging articulations. Each person is roughly 150 pixels in height with 14 joints full-body annotations. (ii) FLIC consists of 3987 training and 1016 testing images collected from popular Hollywood movies with diverse appearances and poses. Each person has 10 upper-body joints annotated. (iii) PARSE contains 305 images of highly articulated human poses with full body annotations. The PARSE dataset is only used for the evaluation of cross-dataset generalization: we directly apply the model trained on the LSP dataset to the 205 test images of the PARSE dataset. To compare with previous methods, we use Observer-Centric annotations on both the LSP dataset and the FLIC dataset, and Person-Centric annotations on the PARSE dataset. Data Augmentation: To reduce overfitting, we augment the training data by rotating through 360 degrees for every 9 degrees. Then we mirror the images horizontally. Note that this also increases the training patches of body parts with different mixture types. The negative samples are randomly cropped from the negative images of the INRIA Person dataset <ref type="bibr" target="#b7">[8]</ref>. We randomly select 5% of the training data as validation set when we pretrain the front-end DCNN, and these data are further used to finetune the full model.</p><p>Previous works <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b3">4]</ref> observed that adding midway parts between neighboring annotated parts helps to reduce foreshortening and improves overall performance. Hence we interpolate midway parts on both the LSP and the FLIC datasets, which results in K = 26 and 18 parts respectively.  limb endpoints and groundtruth limb endpoints are within half of the limb length. However, different interpretations of PCP lead to different results. Hence we adopt the strict PCP as discussed in <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b3">4]</ref> for fair comparison <ref type="bibr" target="#b0">1</ref> .</p><p>PDJ is introduced in <ref type="bibr" target="#b34">[35]</ref> as a complementary evaluation metric of PCP, as PCP penalizes short limbs. PDJ measures the detection rate of joints, where a joint is considered as detected if the distance between the predicted joint and the ground-truth joint is less than a fraction of torso diameter. The torso diameter is defined as the distance between the left shoulder and the right hip of each ground-truth pose. Front-End DCNN Architecture: We investigate two DCNN architectures in this paper. The first one (ChenNet) is based on <ref type="bibr" target="#b3">[4]</ref>, which consists of five convolution layers, two max-pooling layers and three fully-connected layers, and is trained from random initialization. The second one is the 16-layer VGG architecture pretrained on the ImageNet dataset <ref type="bibr" target="#b36">[37]</ref>. To reduce computation, we resize the original input of VGG from 224 × 224 to 112 × 112, and remove the last pooling layer. This also improves the spatial localization accuracy with fewer pooling operations. The number of mixture types is set as T i = 13 for all parts i ∈ {1, · · · , K}. Both architectures produce K i=1 T i +1 heat-maps as the input of the message passing layers, which include one background heat-map. If the mixture number T i is reduced to 11, PCP drops by 1%. The stride size is 4 for ChenNet and is 16 for VGG, hence the heat-maps size is 1/4 of the input image size for ChenNet and 1/16 for VGG. Connections Among parts: The tree-structured models are visualized in <ref type="figure">Figure 7</ref>. Based on the tree structure, the loopy structured models add edges between knees on the LSP dataset, the structure of which is visualized in the second row of <ref type="figure" target="#fig_3">Figure 4</ref>. On the FLIC dataset, we only conduct experiments with tree-structured model. We perform exact inference with tree-structured models. If not specified, three message passing layers are used for loopy models.</p><p>Parameter Settings: During the pretraining stage, each image is normalized to 150 pixels in body height. Patch size is set to 36 × 36, which is able to cover sufficient context. By changing the patch size to 0.8 and 1.2 times of the original scale, PCP is reduced by 6.2% and 0.5% on the LSP dataset with VGG architecture. We keep the batch size as 512, and the learning rates are initialized as 0.005 and 0.001 for ChenNet and VGG, respectively. The dropout rate is set as 0.5. We drop the learning rate by a factor of 10 for every 5 epochs, and the front-end DCNN is trained for 15 epochs. δ in Eq. (9) is set as 1/5 of the patch size. The change of δ from 1/5 to 1/3 results in less than 1% strict PCP variation.</p><p>During the joint finetuning stage, the batch size is 5, and the learning rate is relatively low at 0.0001 for both Chen-Net and VGG. The dropout rate is increased to 0.6 to avoid overfitting. Since the parameters of the DCNN are well initialized during pretrianing, and the deformation weights are shared across different message passing layers and are relatively few, finetuning the model for 1 epoch already provides satisfactory results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Torso Head U. L. U. L. Mean arms arms legs legs Yang&amp;Ramanan <ref type="bibr" target="#b50">[51]</ref> 84.  <ref type="bibr" target="#b29">[30]</ref>, and Kiefel and Gehler <ref type="bibr" target="#b20">[21]</ref>. We report the PDJ rate at the threshold of 0.2 in the legend. <ref type="table">Table 1</ref> and <ref type="table">Table 2</ref> report strict PCP results on the LSP dataset and the FLIC dataset respectively. Our best performance on the LSP is achieved by using VGG together with loopy model (Ours-VGG-LG), which improves the mean strict PCP by 6.1% when compared with <ref type="bibr" target="#b3">[4]</ref>. The best performance on FLIC is achieved by using VGG with treestructured model (Ours-VGG-T), and improves the mean strict PCP by 1.9% when compared with <ref type="bibr" target="#b3">[4]</ref>. On the LSP dataset with many challenging articulations, our method has significant improvements on limbs, i.e. arms and legs, which are the most difficult body parts to locate. <ref type="figure">Figure 5</ref> shows PDJ results on the LSP dataset. By comparing the PDJ value at the threshold 0.2, our method outperforms state-of-the-art methods by a significant margin on all body parts except ankles.</p><p>PDJ results on the FLIC dataset is reported in <ref type="figure" target="#fig_4">Figure 6</ref>. Our method achieves the best performance on both elbows and wrists compared with state-of-the-art methods.  We compare our method (VGG with tree-structured models) with Chen and Yuille <ref type="bibr" target="#b3">[4]</ref>, Fan et al. <ref type="bibr" target="#b9">[10]</ref>, Tompson et al. <ref type="bibr" target="#b39">[40]</ref>, DeepPose <ref type="bibr" target="#b41">[42]</ref>, and MODEC <ref type="bibr" target="#b34">[35]</ref>. We report the PDJ rate at the threhold of 0.2 in the legend. <ref type="figure">Figure 7</ref>. Qualitative results on the LSP dataset (the 1st row), the FLIC dataset (the 2nd row), and the PARSE dataset (the 3rd row). We visualize the joint locations together with the connections among parts used in this paper (for simplicity, we only show tree-structure), and the same limb across different images has the same color. Some failure cases are showed in the last row. Our method may lead to wrong estimations due to significant occlusions, ambiguous background, or heavily overlapping persons.</p><p>dataset. As shown in <ref type="table">Table 3</ref>, our method outperforms the state-of-the-art methods with a large margin, which implies that our method has good generalization ability. Joint Training vs. Independent Training: To investigate the efficiency of joint training of part detectors and deformable mixture of parts, we train a model whose architecture is the same as Ours-ChenNet-LG on the LSP dataset. However, we first train the part detectors, then we fix the part detectors to train the message passing layers. In this scenario, the mean PCP is 74.2%, as reported in <ref type="table">Table 1</ref> (Ours-ChenNet-LG-Ind). In comparison, our proposed joint learning (Ours-ChenNet-LG) has 4.4% gain. Number of the Message Passing Layers: The mean PCPs obtained by the first, the second and the third message passing layer of Ours-VGG-LT are 80.7% (Ours-VGG-LT-MP1), 80.9% (Ours-VGG-LT-MP2), and 81.1% (Ours-VGG-LT), respectively. As discussed in Section 4.2, we observe that a cascade of three message passing layers is enough to produce satisfactory results in practice, as shown in <ref type="figure">Figure 3</ref>.</p><p>Components Investigation: We first evaluate the perfor-mance of individual part detectors. Without spatial constraints, our method obtains 40.6% and 60.1% strict PCPs on the LSP dataset with ChenNet (Ours-ChenNet-Unary) and VGG (Ours-VGG-Unary) respectively, as reported in <ref type="table">Table 1</ref>.</p><p>We conduct four experiments to analyze the influence of different components on the LSP dataset, and report the results in <ref type="table">Table 1</ref>. First, we use ChenNet with tree-structured model (Ours-ChenNet-T), which outperforms the best previously published result <ref type="bibr" target="#b3">[4]</ref> by 3.1% on average. This proves the effectiveness of jointly training DCNNs and deformable mixture of parts. Part detector and message passing are jointly learned in Ours-ChenNet-T but separately learned in <ref type="bibr" target="#b3">[4]</ref>. Second, we build a loopy model based on tree-structured model by adding an edge between knees (Ours-ChenNet-LG), and get 0.5% improvement. We observe that this improvement is mainly due to the reduction of double-counting problem, as shown in <ref type="figure" target="#fig_3">Figure 4</ref>. Next, we evaluate the ImageNet pretrained VGG with tree-structured model (Ours-VGG-T). This gives an improvement over the ChenNet (Ours-ChenNet-T in <ref type="table">Table 1</ref> ) by 2.6%, which shows the expressive power of deeper DCNN and the robustness of the ImageNet pretrained feature representation. Finally, by combining VGG with loopy model, the Ours-ChenNet-LG in <ref type="table">Table 1</ref> achieve the best result on the LSP dataset.</p><p>Qualitative Evaluation: <ref type="figure">Figure 7</ref> shows some pose estimation results on all the three datasets. Our method is robust to highly articulated poses with variant orientation, foreshortening, cluttered background, occlusion, and overlapping people. Some failure cases are also showed in the last row of <ref type="figure">Figure 7</ref>. Our method may lead to wrong estimations due to significant occlusions, ambiguous background, or heavily overlapping persons. Please refer to the captions for detailed discussion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>This paper has proposed to incorporate the DCNN and the deformable mixture of parts model into an end-to-end framework. Our framework is able to mine hard negatives by considering the spatial and appearance consistency among body parts. Therefore, the DCNN can be trained more effectively. The joint learning of DCNN and deformable mixture of parts improves the performance on several widely used benchmarks, which demonstrates the effectiveness of our method. In the future work, we plan to investigate learning graph structures with deep models.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Motivation. Left: Ground-truth locations of head (a) and ankle (d). Middle: The noisy heat-maps predicted by conventional DCNN during the training stage. Right: With body joint consistency considered by the proposed framework, the heat-maps are better predicted.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Illustration of the proposed framework. (a) visualizes a loopy model, where nodes (red circles) specify the positions and mixture types of body parts, and edges (white lines) indicate the relationships between parts. During inference, a node sends a message to each of its neighbors and receives messages from each neighbor (indicated by arrows). The proposed framework can be viewed as two components: (b) a front-end DCNN for learning feature representations of body parts and (c) message passing layers for conducting inference and learning on mixture of parts with deformation constraints between parts. Specifically, each message passing layer performs one iteration of message passing in a forward pass. (d) are predicted heat-maps for parts. Please refer to the text for the notations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Evaluation Measure: Two widely used evaluation metrics, i.e., Percentage of Correct Parts (PCP) and Percentage of Detected Joints (PDJ), are used for comparison. PCP measures the rate of correctly detected limbs: a limb is considered as correctly detected if the distances between detected</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>The double-counting problem in tree-structured models (the 1st row), could be reduced by introducing additional pairwise constraints (indicated by white dashed lines in the 2nd row).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>PDJ comparison of elbows and wrists on the FLIC dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>Figure 5. PDJ results for elbows, wrists, knees and ankles on the LSP dataset. We compare our method (VGG using loopy model) with Chen and Yuille [4], Ouyang et al. [26], Ramakrishna et al. [32], Pishchulin et al.</figDesc><table>1 77.1 52.5 35.9 69.5 65.6 60.8 
Pishchulin et al. [29] 
87.4 77.4 54.4 33.7 75.7 68.0 62.8 
Eichner&amp;Ferrari [9] 
86.2 80.1 56.5 37.4 74.3 69.3 64.3 
Kiefel&amp;Gehler [21] 
84.3 78.3 54.1 28.3 74.5 67.6 61.2 
Pose Machines [32] 
88.1 80.4 62.8 39.5 79.0 73.6 67.8 
Ouyang et al. [26] 
88.6 84.3 61.9 45.4 77.8 71.9 68.7 
Pishchulin et al. [30] 
88.7 85.1 61.8 45.0 78.9 73.2 69.2 
DeepPose [42] 
-
-
56 
38 
77 
71 
-
Chen&amp;Yuille [4] 
92.7 87.8 69.2 55.4 82.9 77.0 75.0 
Ours-ChenNet-Unary 
62.1 62.3 35.8 18.2 48.5 38.2 40.6 
Ours-ChenNet-T 
94.8 82.4 75.0 62.4 85.3 79.2 78.1 
Ours-ChenNet-LG-Ind 
93.0 82.1 70.6 55.4 82.1 75.3 74.2 
Ours-ChenNet-LG 
95.0 83.5 75.0 61.9 86.9 79.8 78.6 
Ours-VGG-Unary 
83.4 69.0 53.5 34.9 72.2 63.5 60.1 
Ours-VGG-T 
96.2 83.4 78.7 65.8 87.9 81.1 80.7 
Ours-VGG-LG-MP1 
96.3 84.3 78.4 66.3 87.9 80.7 80.7 
Ours-VGG-LG-MP2 
96.7 83.6 78.2 66.3 88.3 81.2 80.9 
Ours-VGG-LG 
96.5 83.1 78.8 66.7 88.7 81.7 81.1 

Table 1. Comparison of strict PCP on the LSP dataset. We investigate our 

method with different network architectures (ChenNet and VGG), as well 
as different graph structures (tree-structured model (T) and loopy graph 
(LG)). We also investigate the performance of invividual part detectors 
(Unary), joint training vs. independent training (Ind), and different number 
of message passing layers (MP1, MP2). Note that DeepPose [42] uses 
Person-Centric annotations. 

Method 
U.arms 
L.arms 
Mean 
MODEC [35] 
84.4 
52.1 
68.3 
Tompson et al. [41] 
93.7 
80.9 
87.3 
Chen&amp;Yuille [4] 
97.0 
86.8 
91.9 
Ours-ChenNet-T 
97.9 
88.3 
93.1 
Ours-VGG-T 
98.1 
89.5 
93.8 

Table 2. Strict PCP results on the FLIC dataset. We investigate our 

method with different network architectures (ChenNet and VGG) with 
tree-structured model (T). 

Method 
Torso Head U. 
L. 
U. 
L. 
Mean 
arms arms legs legs 
Yang&amp;Ramanan [51] 
82.9 77.6 55.1 35.4 69.0 63.9 60.7 
Johnson&amp;Everingham [19] 87.6 76.8 67.3 45.8 74.7 67.1 67.4 
Pishchulin et al. [31] 
88.8 73.7 53.7 36.1 77.3 67.1 63.1 
Pishchulin et al. [29] 
92.2 70.7 54.9 39.8 74.6 63.7 62.9 
Pishchulin et al. [30] 
93.2 86.3 63.4 48.8 77.1 68.0 69.4 
Yang&amp;Ramanan [52] 
85.9 86.8 63.4 42.7 74.9 68.3 67.1 
Ouyang et al. [26] 
89.3 89.3 67.8 47.8 78.0 72.0 71.0 
Ours-ChenNet-LG 
96.6 87.3 80.0 65.9 83.7 74.1 79.1 
Ours-VGG-LG 
97.1 86.8 80.2 69.3 84.9 78.5 81.0 

Table 3. Strict PCP results on PARSE dataset. Note that our model is 

trained on the LSP dataset to demonstrate its generalization ability. 

Normalized Precision Threshold 

0 
0.1 
0.2 
0.3 
0.4 
0.5 

Detection Rate 

0 

0.1 

0.2 

0.3 

0.4 

0.5 

0.6 

0.7 

0.8 

0.9 

1 

Elbows 

Ours: 80.3 
Chen&amp;Yuille: 70.3 
Ouyang et al. : 61.7 
Ramakrishna et al. : 61.4 
Pishchulin et al. : 61.4 
Kiefel&amp;Gehler: 55.9 

Normalized Precision Threshold 

0 
0.1 
0.2 
0.3 
0.4 
0.5 

Detection Rate 

0 

0.1 

0.2 

0.3 

0.4 

0.5 

0.6 

0.7 

0.8 

0.9 

1 

Wrists 

Ours: 73.5 
Chen&amp;Yuille: 63.2 
Ouyang et al. : 49.3 
Ramakrishna et al. : 47.2 
Pishchulin et al. : 47.7 
Kiefel&amp;Gehler: 36.2 

Normalized Precision Threshold 

0 
0.1 
0.2 
0.3 
0.4 
0.5 

Detection Rate 

0 

0.1 

0.2 

0.3 

0.4 

0.5 

0.6 

0.7 

0.8 

0.9 

1 

Knees 

Ours: 82.8 
Chen&amp;Yuille: 78.0 
Ouyang et al. : 70.0 
Ramakrishna et al. : 69.1 
Pishchulin et al. : 75.2 
Kiefel&amp;Gehler: 70.5 

Normalized Precision Threshold 

0 
0.1 
0.2 
0.3 
0.4 
0.5 

Detection Rate 

0 

0.1 

0.2 

0.3 

0.4 

0.5 

0.6 

0.7 

0.8 

0.9 

1 

Ankles 

Ours: 68.8 
Chen&amp;Yuille: 72.0 
Ouyang et al. : 67.6 
Ramakrishna et al. : 68.8 
Pishchulin et al. : 68.4 
Kiefel&amp;Gehler: 66.9 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>Generalization Evaluation: To investigate the generalization ability of our method, we apply the model trained on the LSP dataset directly to the official test set of the PARSE</figDesc><table>Normalized Precision Threshold 

0 
0.1 
0.2 
0.3 
0.4 
0.5 

Detection Rate 

0 

0.1 

0.2 

0.3 

0.4 

0.5 

0.6 

0.7 

0.8 

0.9 

1 

Elbows 

Ours: 96.6% 
Chen&amp;Yuille: 94.9% 
Fan et al. : 94.2% 
Tompson et al. : 91.3% 
DeepPose: 91.0% 
MODEC: 75.5% 

Normalized Precision Threshold 

0 
0.1 
0.2 
0.3 
0.4 
0.5 

Detection Rate 

0 

0.1 

0.2 

0.3 

0.4 

0.5 

0.6 

0.7 

0.8 

0.9 

1 

Wrists 

Ours: 94.5% 
Chen&amp;Yuille: 92.0% 
Fan et al. : 89.7% 
Tompson et al. : 87.8% 
DeepPose: 80.9% 
MODEC: 57.9% 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We use a widely used implementation of strict PCP available at http://human-pose.mpi-inf.mpg.de/ to evaluate our results.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Poselets: Body part detectors trained using 3d human pose annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Semantic image segmentation with deep convolutional nets and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Parsing occluded people by flexible compositions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Articulated pose estimation by a graphical model with image dependent pairwise relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Mixing body-part sequences for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cherian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Adaptive occlusion state estimation for human pose tracking under selfocclusions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N.-G</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-W</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="649" to="661" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Structured feature learning for pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Appearance sharing for collective human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Eichner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Combining local appearance and holistic view: Dual-source deep neural networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Pictorial structures for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Huttenlocher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">2d human pose estimation in tv shows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marín-Jiménez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Statistical and Geometrical Approaches to Visual Motion Analysis</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="128" to="147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The representation and matching of pictorial structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Fischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Elschlager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computers</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="67" to="92" />
			<date type="published" when="1973" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deformable part models are convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning human pose estimation features with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Modeep: A deep learning framework using motion features for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Global pose estimation using non-tree models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>Martin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Clustered pose and nonlinear appearance models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning effective human pose estimation from inaccurate annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Object detection from video tubelets with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Human pose estimation with fields of parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kiefel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deeply learning the messages in message passing inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V D</forename><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep structured scene parsing by learning with image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep convolutional neural fields for depth estimation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Estimating human body configurations using shape context matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Multi-source deep learning for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Joint deep learning for pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Factors in finetuning deep model for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Poselet conditioned pictorial structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Strong appearance and expressive spatial models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Articulated people detection and pose estimation: Reshaping the future</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Thormaehlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Pose machines: Articulated pose estimation via inference machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Munoz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Bagnell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning to parse images of articulated objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Recovering human body configurations using pairwise constraints between parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Modec: Multimodal decomposable models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sapp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Taskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Measure locally, reason globally: Occlusion-sensitive articulated pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Fast globally optimal 2d human detection with loopy graph models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-P</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Exploring the spatial hierarchy of mixture models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">G</forename><surname>Narasimhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Efficient object localization using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goroshin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Joint training of a convolutional network and a graphical model for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deeppose: Human pose estimation via deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Improved human parsing with a full relational model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">End-to-end integration of a convolutional network, deformable parts model and nonmaximum suppression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">An approach to posebased action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Beyond physical connections: Tree models in human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Deep networks for saliency detection via local estimation and global search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Learning deep feature representations with domain guided dropout for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Parsing clothing in fashion photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yamaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Kiapour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">E</forename><surname>Ortiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Clothing co-parsing by joint image segmentation and labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Articulated pose estimation with flexible mixtures-of-parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Articulated human detection with flexible mixtures of parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
