<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">WELDON: Weakly Supervised Learning of Deep Convolutional Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thibaut</forename><surname>Durand</surname></persName>
							<email>thibaut.durand@lip6.fr</email>
							<affiliation key="aff0">
								<orgName type="laboratory">UMR 7606</orgName>
								<orgName type="institution" key="instit1">Sorbonne Universités</orgName>
								<orgName type="institution" key="instit2">UPMC Univ Paris 06</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<address>
									<addrLine>4 place Jussieu</addrLine>
									<postCode>LIP6, 75005</postCode>
									<settlement>Paris</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Thome</surname></persName>
							<email>nicolas.thome@lip6.fr</email>
							<affiliation key="aff0">
								<orgName type="laboratory">UMR 7606</orgName>
								<orgName type="institution" key="instit1">Sorbonne Universités</orgName>
								<orgName type="institution" key="instit2">UPMC Univ Paris 06</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<address>
									<addrLine>4 place Jussieu</addrLine>
									<postCode>LIP6, 75005</postCode>
									<settlement>Paris</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
							<email>matthieu.cord@lip6.fr</email>
							<affiliation key="aff0">
								<orgName type="laboratory">UMR 7606</orgName>
								<orgName type="institution" key="instit1">Sorbonne Universités</orgName>
								<orgName type="institution" key="instit2">UPMC Univ Paris 06</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<address>
									<addrLine>4 place Jussieu</addrLine>
									<postCode>LIP6, 75005</postCode>
									<settlement>Paris</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">WELDON: Weakly Supervised Learning of Deep Convolutional Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:48+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we introduce a novel framework for WEakly supervised Learning of Deep cOnvolutional neural Networks (WELDON). Our method is dedicated to automatically selecting relevant image regions from weak annotations, e.g. global image labels, and encompasses the following contributions. Firstly, WELDON leverages recent improvements on the Multiple Instance Learning paradigm, i.e. negative evidence scoring and top instance selection. Secondly, the deep CNN is trained to optimize Average Precision, and fine-tuned on the target dataset with efficient computations due to convolutional feature sharing. A thorough experimental validation shows that WELDON outperforms state-of-the-art results on six different datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Over the last few years, deep learning and Convolutional Neural Networks (CNN) <ref type="bibr" target="#b21">[22]</ref> have become state-of-the-art methods for various visual recognition tasks, e.g. image classification or object detection. To overcome the limited invariance capacity of CNN, bounding box annotations are often used <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b15">16]</ref>. However, these rich annotations rapidly become costly to obtain <ref type="bibr" target="#b5">[6]</ref>, making the development of Weakly Supervised Learning (WSL) models appealing.</p><p>Recently, there have been some attempts for WSL training of deep CNNs <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b35">36]</ref>. In this context, image annotations consist in global labels, and the training objective is to localize image regions which are the most relevant for classification. In computer vision, the dominant approach for WSL is the Multiple Instance Learning (MIL) paradigm <ref type="bibr" target="#b8">[9]</ref>: an image is considered as a bag of regions, and the model seeks the max scoring instance in each bag <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b10">11]</ref>. Recently, relaxations of standard MIL assumptions have been introduced in the context of Latent SVM models and shallow architectures <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b9">10]</ref>, showing improved recognition performances on various object and scene datasets. In this paper, we propose a new model for WEakly supervised Learning of Deep cOnvolutional neural Networks (WELDON), which is illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>. WELDON is trained to automatically select relevant regions from images annotated with a global label, and to perform end-toend learning of a deep CNN from the selected regions. The ultimate goal is image classification (or ranking). We call this setting weakly-supervised, because the localization step only exploits global labels.</p><p>Regarding WSL, WELDON is dedicated to selecting two types of regions, adapted from <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b37">38]</ref> to deep networks: green regions in <ref type="figure" target="#fig_0">Figure 1</ref> correspond to areas with top scores, i.e. regions which best support the presence of the global label. On the contrary, red regions incorporate negative evidence for the class, i.e. are the lowest scoring areas. Our deep WSL model is detailed in section 3.</p><p>Regarding training, the model parameters are optimized using back-propagation with standard classification losses, but we also adapt the learning to structured output ranking. We design a network architecture which enables fast region feature computation by convolutional sharing. The network is initialized from deep features trained on ImageNet, and the parameters are fine-tuned on the target dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works &amp; Contributions</head><p>The computer vision community is currently witnessing a revolutionary change, essentially caused by Convolutional Neural Networks (CNN) and deep learning. Beyond the outstanding success reached in the context of large scale classification (ImageNet) <ref type="bibr" target="#b21">[22]</ref>, deep features also prove to be very effective for transfer learning: state-of-the-art results on standard benchmarks are nowadays obtained with deep features as input. Recent studies reveal that performances can further be improved by collecting large datasets that are semantically closer to the target domain <ref type="bibr" target="#b53">[54]</ref>, or by fine-tuning the network with data augmentation <ref type="bibr" target="#b6">[7]</ref>.</p><p>Despite their excellent performances, current CNN architectures only carry limited invariance properties: although a small amount of shift invariance is built into the models through subsampling (pooling) layers, strong invariance is generally not dealt with <ref type="bibr" target="#b52">[53]</ref>. Recently, attempts have been made to overcome this limitation. Some methods revisit the BoW model with deep features as local region activations <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b17">18]</ref> or designed BoW layers <ref type="bibr" target="#b1">[2]</ref>. The drawback of these models is that background regions are encoded into the final representation, decreasing its discriminative power. Another option to gain strong invariance is to explicitly align image regions, e.g. by using Weakly Supervised Learning (WSL) models.</p><p>In the computer vision community, WSL has been predominantly addressed through the Multiple Instance Learning (MIL) paradigm <ref type="bibr" target="#b8">[9]</ref>. In standard MIL modeling, an image is regarded as a bag of instances (regions), and there is an asymmetric relationship between bag and instance labels: a bag is positive if it contains at least one positive instance, and negative if all its instances are negativei.e. the Negative instances in Negative bags (NiN) hypothesis. MIL models thus perform image prediction through its max scoring region. The Deformable Part Model (DPM) <ref type="bibr" target="#b13">[14]</ref> is an instantiation of the MI-SVM model <ref type="bibr" target="#b0">[1]</ref> for MIL, which is extremely popular for WSL due to its excellent performances for object detection. Extensive works have therefore used DPM and its generalization to structured output prediction, LSSVM <ref type="bibr" target="#b49">[50]</ref>, for weakly supervised scene recognition and object localization <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b46">47]</ref>. Contrarily to these methods built upon handcrafted features, e.g. BoW models <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b16">17]</ref> or biologically-inspired models <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b47">48]</ref>, recent approaches tackle the problem of WSL training of deep CNNs, e.g. <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b35">36]</ref>, incorporating a max CNN layer accounting for the MIL hypothesis.</p><p>Recently, interesting MIL extensions have been introduced in <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b9">10]</ref>. All these methods use a bag prediction strategy which departs from the standard max scoring function in MIL, especially due to the relaxation of the common Negative instances in Negative bags (NiN) MIL assumption. In the Learning with Label Proportion (LLP) framework <ref type="bibr" target="#b50">[51]</ref>, only label ratios between ⊕/⊖ instances in bags are provided during training. In <ref type="bibr" target="#b23">[24]</ref>, the LLP method of <ref type="bibr" target="#b50">[51]</ref> is explicitly applied to MIL problems, in the context of video event detection. LLP is shown to outperform baseline methods (mi/MI-SVM <ref type="bibr" target="#b0">[1]</ref>), especially by its capacity to relax the NiN assumption. In <ref type="bibr" target="#b26">[27]</ref>, the authors question the NiN assumption by claiming that it is often violated in practice during image annotation: human rather label images based on their dominant concept than on the actual presence of the concept in each subregion. To support the dominant concept annotation, the authors in <ref type="bibr" target="#b26">[27]</ref> introduce a prediction function selecting the top scoring instances in each bag. Other approaches departs from the NiN assumption by tracking negative evidence of a class with regions <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b9">10]</ref>: for example, a cow detector should strongly penalize the prediction of the bedroom class. In <ref type="bibr" target="#b37">[38]</ref>, the authors introduce a WSL learning formulation specific to multi-class classification, where negative evidence is explicitly encoded by augmenting model parameters to represent the positive/negative contribution of a part to a class. In <ref type="bibr" target="#b9">[10]</ref>, the idea of negative evidence is formalized by the introduction of a generic structured output latent variable, where the prediction function is extended from max to max+min region scores. The min scoring region accounts for the concept of negative evidence, and is capitalized on for learning a more robust model.</p><p>Many computer vision tasks are evaluated with ranking metrics, e.g. Average Precision (AP). In the WSL setting, this is, however, a very challenging problem: for example, no algorithm exists for solving the loss-augmented inference problem with Latent Structural SVM <ref type="bibr" target="#b49">[50]</ref>. In <ref type="bibr" target="#b3">[4]</ref>, LAPSVM is introduced, enabling a tractable optimization by defining an ad-hoc prediction rule dedicated to ranking. In <ref type="bibr" target="#b9">[10]</ref>, the proposed ranking model offers the ability to solve loss-augmented inference with an elegant symmetrization due to the max+min prediction function.</p><p>In this paper, we introduce a new model for WSL training of deep CNNs, which takes advantage of recent MIL extensions. The approach the most closely connected to ours is <ref type="bibr" target="#b33">[34]</ref>, which we extend at several levels. Our submission therefore encompasses the following contributions:</p><p>• We improve the deep WSL modeling in <ref type="bibr" target="#b33">[34]</ref> by incorporating top instance <ref type="bibr" target="#b26">[27]</ref> and negative evidence <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b9">10]</ref> insights into our deep prediction function. Contrarily to <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b26">27]</ref>, we propose an end-to-end training of deep CNNs.</p><p>• We improve deep WSL training in <ref type="bibr" target="#b33">[34]</ref> by introducing a specific architecture design which enable an easy and effective transfer learning and fine-tuning. In addition, we adapt our training scheme to explicitly optimize over ranking metrics, e.g. AP.</p><p>• We report excellent performances, outperforming state-of-the-art results on six challenging datasets. A systematic evaluation of our modeling and training contributions highlights their importance for training deep CNN models from weak annotations. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">WELDON Model</head><p>The proposed WELDON model is decomposed into two sub-networks: a deep feature extraction net and a prediction net, as illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>. The feature extraction net purpose is to extract a fixed-size deep descriptor for each region in the image, while the prediction net outputs a structured output for the whole image. We firstly detail the prediction network, since the main paper contributions are incorporated at this level, mainly by the introduction of novel methods for weakly supervised learning of deep CNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Prediction network design</head><p>The prediction net acts on the L5 layer, which is a set of d(= 512) feature maps with n×n (n ≥ 7) spatial neurons. L5 is computed by the feature extraction net (Section 3.2). a) Transfer layer The first layer of the prediction network transforms the L5 layer into a layer L6 of size n ′ ×n ′ ×d ′1 (d ′ = 4096), as illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>. This convolutional layer is composed of filters W 6 , each of size 7×7×d. Note that each 7×7 area in L5 in thus mapped to a fixed-size d ′dimensional vector, so that this transfer layer is equivalent to applying the whole CNN on each of the 7 × 7 region. This architecture design serves two purposes: fast feature computation in regions (see Section 3.2), and transferring W 6 weights from large scale datasets (see Section 4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>b) Weakly-Supervised Prediction (WSP) module</head><p>This is the heart of the proposed method, and is dedicated to selecting relevant regions for properly predicting the global (structured) label associated to each training image.</p><p>The WSP module consists in a succession of two layers. The first layer is a linear prediction model W 7 , which is 1 n ′ = n − 6 because of the W 6 filter padding. dedicated to providing a (structured output) prediction for each of the n ′ × n ′ spatial cell in L6. This corresponds to a fully connected layer applied to each spatial cell in L6, which we implement using 1 × 1 convolutions, as in <ref type="bibr" target="#b28">[29]</ref>. The L7 layer is thus of size n ′ × n ′ × C, where C is the size of the structured prediction map, e.g. C is the number of classes for multi-class classification (we detail our structured output instantiations in Section 4).</p><p>The second layer of the WSP module is a spatial pooling layer s, which aggregates, for each output c ∈ {1; C}, the score over the n ′ ×n ′ regions into a single scalar value. This give the final prediction layer L8. As mentioned in Section 2, the standard approach for WSL inherited from MIL is to select the max scoring region. We propose to improve this strategy in two complementary directions.</p><p>i) Top instances Based on recent MIL insights on learning with top instances <ref type="bibr" target="#b26">[27]</ref>, we propose to extend the selection of a single region to multiple high scoring regions.</p><p>Formally, let us denote as h i ∈ {0, 1} the binary variable denoting the selection of the i th region from layer L7, and l 7 i,c the value of the i th region score for output (e.g. class) c.We propose the following aggregation strategy s top , which selects the k highest scoring regions as follows:</p><formula xml:id="formula_0">s top (L7) = max h n ′2 i=1 h i · l 7 i , s.t. n ′2 i=1 h i = k<label>(1)</label></formula><p>where h = {h i }, i ∈ 1; n ′2 , and l 7 i = l 7 i,c , c ∈ {1; C}. Beyond the relaxation of the NiN assumption, which is sometimes inappropriate (see Section 2), the intuition behind s top is to provide a more robust region selection strategy. Indeed, using a single area for training the model necessarily increases the risk of selecting outliers, guiding the training of the deep CNN towards bad local minima.</p><p>ii) MinMax layer When using top instances in Eq (1) for classifying images, we make use of the most informative regions. Recent studies show that this information can be effectively combined with negative evidence for a class, e.g. using regions which best support the absence of the class <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b9">10]</ref>. In this submission, we propose to incorporate this negative evidence in our prediction layer using multiple instances, in the same way as for top instances. Therefore, we augment our aggregation strategy with the term s low , which selects the m lowest-scoring regions in an image:</p><formula xml:id="formula_1">s low (L7) = min h n ′2 i=1 h i · l 7 i , s.t. n ′2 i=1 h i = m<label>(2)</label></formula><p>The final prediction of the network, that we denote as L8, simply consists in summing s top and s low . If we denote as t * c (resp. l * c ) the k top (resp. m lowest) instances selected for output c, the c th output feature L8(c) is:</p><formula xml:id="formula_2">L8(c) = s top (L7(c)) + s low (L7(c)) = k t * c =1 l 7 t * c + m l * c =1 l 7 l * c (3)</formula><p>The proposed WSP aggregation scheme in Eq. (3) thus generalizes the max+min prediction function in <ref type="bibr" target="#b9">[10]</ref> in the case of multiple top positive/negative instances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Feature extraction network design</head><p>The feature extraction network is dedicated to computing a fixed-size representation for any region of the input image. When using CNNs as feature extractors, the most naive option is to process input regions independently, i.e. to resize each region to match the size of a full image for CNN architectures trained on large scale databases such as ImageNet (e.g. 224×224). This is the approach followed in R-CNN <ref type="bibr" target="#b15">[16]</ref>, or in MANTRA <ref type="bibr" target="#b9">[10]</ref>. This is, however, highly inefficient since feature computation in (close) neighbor regions is not shared. Recent improvements in SPP nets <ref type="bibr" target="#b18">[19]</ref> or fast R-CNN <ref type="bibr" target="#b14">[15]</ref> process images of any size by using only convolutional/pooling layers of CNNs trained on Ima-geNet, subsequently applying max pooling to map each region into a fixed-size vector. Fully-convolutional networks are also used for semantic segmentation <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b30">31]</ref>.</p><p>We propose here a different strategy, which is based on a multi-scale sliding window scheme. In the proposed architecture, input images at a given scale are rescaled to a constant size IxI, with I ≥ 224. For all I, we consider regions of size 224 × 224 pixels, so that the region scale is α = 224/I (see details in <ref type="table">Table 1</ref> of supplementary 1). Input images are processed with the fully convolutional/pooling layers of CNNs trained on ImageNet, leading to L5 layers of different sizes.</p><p>Our multi-scale strategy is close to that of <ref type="bibr" target="#b33">[34]</ref>, but the region size is designed to fit a 224×224 pixel area (i.e. 7×7 in L5 layer), which is not the case in <ref type="bibr" target="#b33">[34]</ref>. This is a crucial difference, which enables the weights W6 to the first prediction layer L6 in <ref type="figure" target="#fig_1">Figure 2</ref> to be transferred from Ima-geNet, which is capitalized on for defining a training strategy robust to over-fitting, see Section 4.2. We now detail the training of our deep WSL architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Training the WELDON Model</head><p>As shown in <ref type="figure" target="#fig_1">Figure 2</ref>, the WELDON model outputs L8 ∈ R C . This vector represents a structured output, which can be used in a multi-class or multi-label classification framework, but also in a ranking problem formulation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Training formulation</head><p>In this paper, we consider three different structured prediction for WELDON, and their associated loss functions during training.</p><p>Multi-class classification In this simple case, C is the number of classes. We use the usual soft-max activation function on top of L8: P (L8(c)) = e L8(c) / c ′ e L8(c ′ ) , with its corresponding log loss during training.</p><p>Multi-label classification In the case of multiple labels, we use a one-against-all strategy, as <ref type="bibr" target="#b33">[34]</ref>. For C different classes, we train the C binary classifiers jointly, using logistic regression for prediction P (L8(c)) = 1 + e −L8(c) −1 , with its associated log loss 2 .</p><p>Ranking: Average Precision We also tackle the problem of optimizing ranking metrics, and especially Average Precision (AP) with our WELDON model. We use a latent structured output ranking formulation, following <ref type="bibr" target="#b51">[52]</ref>: our input is a set of N training images x = {x i }, i ∈ {1; N }, with their binary labels y i , and our goal is to predict a ranking matrix c ∈ C of size N × N providing an ordering of the training examples (our ranking feature map is detailed supplementary 2.1, Eq (1)). Here, we explicitly denote the output L8(x, c) to highlight the dependence on x.</p><p>During training, we aim at minimizing the following loss: ∆ ap (c * , c) = 1 − AP (c * , c) , where c * is the groundtruth ranking. Since AP is non-smooth, we define the following surrogate (upper-bound) loss:</p><formula xml:id="formula_3">ℓ W (x, c * ) = max c∈C [∆ ap (c * , c)+L8(x, c)−L8(x, c * )] (4)</formula><p>The maximization in Eq (4) is generally referred to as Loss-Augmented Inference (LAI), while inference consists computingĉ(x) = arg max c∈C L8(x, c). Exhaustive maximization is intractable due to the huge size of the structured 2 Experimentally, hinge loss with linear prediction performs similarly. output space. The problem is even exacerbated in the WSL setting, see <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b9">10]</ref>. We exhibit here the following result for WELDON (proof in supplementary 2.2):</p><p>Proposition 1 For each training example, let us denote s(i) = s top (W 7 L6 i ) + s low (W 7 L6 i ) in Eq (3). Inference and LAI for the WELDON ranking model can be solved exactly by sorting examples in descending order of score s(i).</p><p>Proposition 1 shows that the optimization over regions, i.e. score s(i), decouples from the maximization over output variables c. This reduces inference and LAI optimization to fully supervised problems. Inference solution directly corresponds to s(i) sorting. For solving LAI with AP loss ∆ ap in Eq (4), we use the exact greedy algorithm of [52] 3 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Optimization</head><p>Given the loss functions given in Section 4.1, WELDON parameters are adjusted using gradient-based methods.</p><p>For multi-class and multi-label predictions, error gradients in L8 are well-known. For the ranking instantiation, we have (details in supplementary 3):</p><formula xml:id="formula_4">∂ℓ ∂W 7 = ∂L8(x,c) ∂W 7 − ∂L8(x, c * ) ∂W 7</formula><p>wherec is the LAI solution. In all cases, error gradient is back-propagated in the deep CNN through chain rule.</p><p>Transfer learning &amp; fine-tuning Similarly to other deep WSL models, our whole CNN contains a lot of parameters. The vast majority of weights is located in W 6 ( <ref type="figure" target="#fig_1">Figure 2</ref>), which contains ∼ 10 8 parameters. Training such huge models on medium-size datasets as those studied in this paper (with [10 3 -10 5 ] examples) is highly prone to over-fitting. With a network even bigger than ours, the authors in <ref type="bibr" target="#b33">[34]</ref> address this issue by extensively using regularization during training with dropout and data-augmentation. We propose here to couple these regularization strategies with a two-step learning procedure to limit over-fitting.</p><p>In a first training phase, all parameters except those of the WSP prediction module, i.e. W 7 , are frozen. All other parameters, i.e. convolutional layers and W 6 are transferred from CNNs trained on large-scale datasets (Ima-geNet). Note that the transfer for W 6 is fully effective thanks to the carefully designed architecture of our feature extraction network (Section 3.2) and the transfer layer (Section 3.1a)). It is, for example, not possible as it with the architecture in <ref type="bibr" target="#b33">[34]</ref>. Note that W 7 only contains ∼ 10 4 parameters, and can therefore robustly be optimized in the considered medium-size datasets. <ref type="bibr" target="#b2">3</ref> Faster (approximate) methods, e.g. <ref type="bibr" target="#b31">[32]</ref>, could also be used.</p><p>In a second training phase, starting with W 7 initialized from the first phase, a fine-tuning of all other CNN parameters is achieved. We use dropmap as regularization strategy, consisting in randomly freezing maps in L 6 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>Our deep CNN architecture is based on VGG16 <ref type="bibr" target="#b44">[45]</ref>. We implement our model using Torch7 (http://torch.ch/) <ref type="bibr" target="#b3">4</ref> .</p><p>We evaluate our WELDON strategy on several Computer Vision benchmarks corresponding to various visual recognition tasks. While some choose pre-trained deep features according to the target task (like Places features for Scene recognition <ref type="bibr" target="#b53">[54]</ref>), we knowingly decide with WEL-DON to use only deep features pre-trained on ImageNet whatever the visual recognition task. This is to put to the proof our claim about genericity of our deep architecture.</p><p>Absolute comparison with state-of-the-art methods is provided in Section 5.1, while Section 5.2 analyzes the impact of the different improvements introduced in Section 3 and 4 for training deep WSL CNNs.</p><p>Experimental Setup In order to get results in very different recognition contexts, 6 datasets are used: object recognition (Pascal VOC 2007 <ref type="bibr" target="#b11">[12]</ref>, Pascal VOC 2012 <ref type="bibr" target="#b12">[13]</ref>), scene categorization (MIT67 <ref type="bibr" target="#b39">[40]</ref> and 15 Scene <ref type="bibr" target="#b25">[26]</ref>), and visual recognition, where context plays an important role (COCO <ref type="bibr" target="#b29">[30]</ref>, Pascal VOC 2012 Action <ref type="bibr" target="#b12">[13]</ref>).</p><p>For MIT67, 15 Scene and VOC 2007, performances are evaluated following the standard protocol. For VOC 2012, evaluation is carried out on the val set (which does not require server evaluation). On COCO dataset, we follow the protocol in <ref type="bibr" target="#b33">[34]</ref>, and perform classification experiments. On Pascal VOC 2012 Action, we use the same weakly supervised protocol as in <ref type="bibr" target="#b9">[10]</ref>, with evaluation on the val set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Overall comparison</head><p>Firstly, we compare the proposed WELDON model to state-of-the-art methods. We use the multi-scale WSL model described in Section 3.2, and scale combination is performed using an Object-Bank <ref type="bibr" target="#b27">[28]</ref> strategy. For the selection of top/low instances, we use here the default setting of k = m = 3 (Eq (1) and Eq (2) in Section 3.1), for scale α ≤ 70% <ref type="table">(Table 1 of</ref>  <ref type="figure" target="#fig_0">supplementary 1)</ref>. This parameter is analyzed in Section 5.2, showing further improvements by careful tuning. Results for object (resp. scene and context) datasets are gathered in <ref type="table" target="#tab_0">Table 1 (resp. Table 2 and Table 3)</ref>.</p><p>For object datasets, we can show in <ref type="table">Table 1</ref> that WEL-DON outperforms all recent methods based on deep features by a large margin. More specifically, the improvement compared to deep features computed on the whole image <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b44">45]</ref> is significant: there is an improvement over VOC 2007 VOC 2012 Return Devil <ref type="bibr" target="#b6">[7]</ref> 82.4 VGG16 (online code) <ref type="bibr" target="#b44">[45]</ref> 84.5 82.8 SPP net <ref type="bibr" target="#b18">[19]</ref> 82.4 Deep WSL MIL <ref type="bibr" target="#b33">[34]</ref> 81.8 MANTRA <ref type="bibr" target="#b9">[10]</ref> 85.8 WELDON 90.2 88.5 <ref type="table">Table 1</ref>. mAP results on object recognition datasets. WELDON and state-of-the-art methods results are reported.</p><p>the best method <ref type="bibr" target="#b44">[45]</ref> of ∼ 6 pt on both datasets. Note that since we use deep features VGG16 from <ref type="bibr" target="#b44">[45]</ref>, the performance gain directly measures the relevance of using a WSL method, which selects localized evidence for performing prediction, rather than relying on the whole image information. Compared to SPP net <ref type="bibr" target="#b18">[19]</ref>, the improvement of ∼ 8 pt on VOC 2007 highlights the superiority of region selection based on supervised information, rather than using handcrafted aggregation with spatial-pooling BoW models. The most important comparison is the improvement over other recent WSL methods on deep features <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b9">10]</ref>. Compared to <ref type="bibr" target="#b9">[10]</ref>, the improvement of 4.4 pt on VOC 2007 essentially shows the importance of using multiple instances, and the relevance of an end-to-end training of a deep CNN in the target dataset. We also outperform the deep WSL CNN in <ref type="bibr" target="#b33">[34]</ref>, the approach which is the most closely connected to ours, by 6.7 pt on VOC 2012. This big improvement illustrates the positive impact of incorporating MIL relaxations for WSL training of deep CNNs, i.e. negative evidence scoring and top-instance selection. Finally, we can point out the outstanding score reached by WELDON on VOC 2007, exceeding the nominal score of 90%.</p><p>15 Scene MIT67 CaffeNet ImageNet <ref type="bibr" target="#b19">[20]</ref> 84.2 56.8 CaffeNet Places <ref type="bibr" target="#b53">[54]</ref> 90.2 68.2 VGG16 (online code) <ref type="bibr" target="#b44">[45]</ref> 91.2 69.9 MOP CNN <ref type="bibr" target="#b17">[18]</ref> 68.9 MANTRA <ref type="bibr" target="#b9">[10]</ref> 93.3 76.6 Negative parts <ref type="bibr" target="#b37">[38]</ref> 77.1 WELDON (OB) 94.3 78.0 The results shown in <ref type="table" target="#tab_0">Table 2</ref> for scene recognition also illustrate the big improvement of WELDON compared to deep features computed on the whole image <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b44">45]</ref> and MOP CNN <ref type="bibr" target="#b17">[18]</ref>, a BoW method pooling deep features with VLAD. It is worth noticing that WELDON also outperforms recent part-based methods including negative evidence during training <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b37">38]</ref>. This shows the improvement brought out by the end-to-end deep WSL CNN training with WELDON. Note that in these scene datasets, deep features trained on Places <ref type="bibr" target="#b53">[54]</ref> reach much better results than those trained on ImageNet. Therefore, we can expect further performance improvement with WELDON by using stronger feature as input for transfer, before fine-tuning the network to the target dataset.</p><p>In <ref type="table">Table 3</ref>, we show the results in datasets where contextual information is important for performing prediction. On VOC 2012 action and COCO, selecting the regions corresponding to objects or parts directly related to the class is important, but contextual features are also strongly related to the decision. WELDON outperforms VGG16 <ref type="bibr" target="#b44">[45]</ref> by ∼ 8 pt on both datasets, again validating our WSL deep method in this context. On COCO, the improvement is from 62.8% <ref type="bibr" target="#b33">[34]</ref> to 68.8% for WELDON. This shows the importance of the negative evidence and top-instance scoring in our WSP module, which better help to capture contextual information than the standard MIL max function used in <ref type="bibr" target="#b33">[34]</ref>. Finally, note that the very good results in COCO also illustrate the efficiency of the proposed WSL training of deep CNN with WELDON, which is able to deal with this large datasets (80 classes and ∼ 80000 training examples).</p><p>VOC 2012 action COCO VGG16 (online code) <ref type="bibr" target="#b44">[45]</ref> 67.1 59.7 Deep WSL MIL <ref type="bibr" target="#b33">[34]</ref> 62.8 WELDON 75.0 68.8 <ref type="table">Table 3</ref>. WELDON results and comparison to state-of-the-art methods on context datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">WELDON Analysis</head><p>In this section, we analyze the impact on prediction performances of the different contributions of WELDON given in Section 3 and 4. Our baseline model a) is the WSL CNN model using an aggregation function s=max at the WSP module stage <ref type="figure" target="#fig_1">(Figure 2</ref>), evaluated at scale α = 30%. It gives a network similar to <ref type="bibr" target="#b33">[34]</ref>, trained at a single scale. To measure the importance of the difference between WEL-DON and a), we perform a systematic evaluation on the performance when the following variations are incorporated: b) Use of k top instances instead of the max. We use k = 3. The results are reported in <ref type="table">Table 4</ref> for object and context datasets with AP evaluation (VOC 2007 and VOC 2012 action), and in <ref type="table">Table 5</ref>  From this systematic evaluation, we can draw the following conclusions:</p><p>• Both b) and c) improvements result in a very large performance gain on all datasets, with a comparable impact on performances: ∼ +30 pt on MIT67, ∼ +15 pt on 15-Scene, ∼ +15 pt on VOC 2012 Action and∼ +4 pt on VOC 2007. When looking more accurately, we can notice that max+min leads always to a larger improvement, e.g. is 4 pt above on 15-Scene or VOC 2012 Action and 3 pt on MIT67.</p><p>• Combining b) and c) improvements further boost performances: +3 pt on MIT67 and VOC 2012 Action, +2 pt on 15-Scene, +1pt on VOC 2007. This shows the complementarity of these two extensions at the aggregation level. We perform an additional experiment for comparing b)+c) and c), by setting the same number of regions (e.g. 6 for k-max and 3-3 for k-m max+min). It turns out that k-m max+min is the best method for various k/m values, showing that negative evidence contains significant information for visual prediction.</p><p>• Minimizing an AP loss enables to further improve performances. Interestingly, the same level of improvement is observed when AP optimizing is added to the c) configuration than to the more powerful b)+c) configuration: +3pt on VOC 2012 Action, +1 pt on VOC 2007. This shows that b) and c) are conditionally independent from the AP optimization.</p><p>• Fine-tuning favorably impacts performances, with +0.6 pt gain on MIT67 and 15-Scene. Note that the performance level is already high at the b)+c) configuration, making further improvements challenging. These results are obtained with the two-step finetuning proposed in section 4.2. We compare this strategy to a parallel optimization, consisting in jointly updating all network parameters. Performances drop with this parallel procedure, e.g. 73.5% on MIT67.</p><p>To further evaluate the impact of the number k top and m low instances, we show in <ref type="figure" target="#fig_3">Figure 3</ref> the performance variation (k = m) on MIT67 and 15 Scene. We can see that performances can still be significantly improved on these datasets when k and m increase, although performances decrease for k ≥ 8 on MIT67 (see results in other datasets on supplementary 4). Finally, we show in <ref type="figure">Figure 4</ref> the performance in different configurations, corresponding to sequentially adding the previous improvements in the following order: a), a)+b), b)+c), and b)+c)+d) for VOC 2007 / VOC 2012 / VOC 2012 Action and c)+c)+e) for MIT67 and 15 Scene. On all dataset, we can see the very large improvement from configuration a) to configuration b)+c)+d)/e). The behavior can, however, be different among datasets: for example, the performance boost is sharp from a) to a)+b) on MIT67 (the following improvements being less pronounced), whereas there is a linear increase from a) b)+c)+d) on VOC 2007 and VOC 2012. instances. For each image, the first column represents WELDON prediction for the ground truth classifier (with its corresponding score), and the second column shows prediction and score for an incorrect classifier.</p><p>Qualititative analysis of region selection To illustrate the region selection policy performed by WELDON, we show in <ref type="figure">Figure 5</ref> the top 3 positive (resp. top 3 negative) regions selected by the model in green (resp. red), on the VOC 2007 dataset. We show the results for the ground truth classification model in the first column, with its associated prediction score. We can notice that top positive green regions detect several discriminant parts related to the object class, potentially capturing several instances or modalities (e.g. wheels or airfoil for the car model), whereas negative evidence on red regions, which should remain small, encode contextual information (e.g. road or sky for airplane, or trees for horse). The region selection results are shown for incorrect classification models in the second column, again with the prediction score. We can notice that red regions correspond to multiple negative evidence for the class, e.g. parts of coach strongly penalizes the prediction of the class horse, or seat or handlebar negatively supports the prediction of the sofa category.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we introduce WELDON, a new method for training deep CNNs in a weakly supervised manner. Our method exploits to the full extend deep CNN strategy in multiple instance learning framework to efficiently deal with weak supervision. The whole architecture is carefully designed for fast processing by sharing region feature computations, and robust training.</p><p>We show the excellent performances of WELDON for WSL prediction on very different visual recognition tasks: object class recognition, scene classification, and images with a strong context, outperforming state-of-the-art results on six challenging datasets. Future works include adapting WELDON for other structured visual applications, e.g. metric learning <ref type="bibr" target="#b24">[25]</ref>, semantic segmentation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>The WELDON model is a deep CNN trained in a weakly supervised manner. To perform image prediction, e.g. classification or ranking, WELDON automatically selects multiple positive (green) + negative (red) evidences on several regions in the image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>WELDON deep architecture: our model is composed of 2 sub-networks. The feature extraction net outputs a fixed-size vector for any region in the image, using a multi-scale sliding window mechanism. The prediction net is composed of a transfer layer with weights W6, which enables using networks pre-trained on large-scale datasets for model initialization, and a Weakly-Supervised Prediction (WSP) module, which is the main point studied in this submission. In the proposed WSP module, the spatial aggregation function s combines improvements on the MIL modeling, i.e. top-instance scoring and negative evidence into the training of the full deep CNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>c) Incorporation of negative evidence through max+min aggregation function. When b)+c) are combined, we use m lowest-instances instead of the min, with m = 3. d) Learning the deep WSL with ranking loss, e.g. AP, in the concerned datasets (PASCAL VOC). e) Fine-tuning the network on the target dataset, i.e. using the second training phase in Section 4.2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Multi-class accuracy with respect to the number of top/low instances for MIT67 and 15 Scene at scale α = 30%.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .Figure 5 .</head><label>45</label><figDesc>Performance variations when the different improvements are incorporated: from the baseline model a) to b), a)+b), b)+c), and b)+c)+d)/e). Visual results of WELDON on VOC 2007 with k = m = 3 instances. The green (resp. red) boxes are the 3 top (resp. 3 low)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 2 .</head><label>2</label><figDesc></figDesc><table>Multiclass accuracy results on scene categorization 
datasets. WELDON and state-of-the-art methods results are re-
ported. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>for scene datasets. Scene databases with multi-class classification evaluation. FT: fine-tuning.</figDesc><table>a) max b) +top c) +min d) +AP VOC07 VOC act 

83.6 
53.5 
86.3 
62.6 
87.5 
68.4 
88.4 
71.7 
87.8 
69.8 
88.9 
72.6 

Table 4. Systematic evaluation of our WSL deep CNN contribu-
tions. Object and Context databases with AP evaluation. 

a) max b) +top c) +min d) +FT MIT67 15-Scene 
42.3 
72.0 
69.5 
85.9 
72.1 
89.7 
74.5 
90.9 
75.1 
91.5 

Table 5. Systematic evaluation of our WSL deep CNN contribu-
tions. </table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">We will make our code publicly available if accepted.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments This research was supported by a DGA-MRIS scholarship.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Support vector machines for multiple-instance learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Andrews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Tsochantaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">NetVLAD: CNN architecture for weakly supervised place recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arandjelović</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gronat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Pooling in image representation: the visual codeword point of view. Computer Vision and Image Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Avila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Valle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Araujo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Optimizing average precision using weakly supervised data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Behl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">V</forename><surname>Jawahar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Object classification with latent window parameters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Namboodiri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Tutorial: Visual learning with weak supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blaschko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Taskar</surname></persName>
		</author>
		<idno>CVPR 2013. 1</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Return of the devil in the details: Delving deep into convolutional nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chatfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Semantic image segmentation with deep convolutional nets and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Solving the multiple instance problem with axis-parallel rectangles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">G</forename><surname>Dietterich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">H</forename><surname>Lathrop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lozano-Pérez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artif. Intell</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">MANTRA: Minimum Maximum Latent Structural SVM for Image Classification and Ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Incremental learning of latent structural svm for weakly supervised image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Picard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">The PASCAL Visual Object Classes Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<ptr target="http://www.pascal-network.org/challenges/VOC/voc2007/workshop/index.html.5" />
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">The PASCAL Visual Object Classes Challenge 2012 (VOC2012) Results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<ptr target="http://www.pascal-network.org/challenges/VOC/voc2012/workshop/index.html.5" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained part based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning Deep Hierarchical Visual Feature Coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multi-scale orderless pooling of deep convolutional activation features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimedia</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Blocks that shout: Distinctive parts for scene classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Juneja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">V</forename><surname>Jawahar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS. 2012</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Self-paced learning for latent variable models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Packer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Video event detection by inferring temporal instance labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-T</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Fantope regularization in metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Multiple instance learning for soft bags via top instances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Object bank: A high-level image representation for scene classification &amp; semantic feature sparsification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Network in network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<meeting><address><addrLine>Zürich</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Efficient optimization for average precision svm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mohapatra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jawahar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning and transferring mid-level image representations using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Is object localization for free? weakly-supervised learning with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Scene recognition and weakly supervised object localization with deformable part-based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Modeling local and global deformations in deep learning: Epitomic convolution, multiple instance learning, and sliding window detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Savalle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Reconfigurable models for scene recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">N</forename><surname>Parizi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G</forename><surname>Oberlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Automatic discovery and optimization of parts for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">N</forename><surname>Parizi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Fisher kernels on visual vocabularies for image categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Dance</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Recognizing indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Quattoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Objectcentric spatial pooling for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Latent pyramidal regions for recognizing scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sadeghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Tappen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Robust object recognition with cortex-like mechanisms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bileschi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riesenhuber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Discriminative spatial saliency for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jurie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Video google: A text retrieval approach to object matching in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Learning discriminative part detectors for image classification and cosegmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Dynamic scene classification: Learning motion descriptors with slow features analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Thériault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Extended Coding and Pooling in the HMAX Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Thériault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing (TIP)</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Learning structural svms with latent variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-N</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">∝svm for learning with label proportions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jebara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">A support vector method for optimizing average precision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Finley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Radlinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">PANDA: Pose Aligned Networks for Deep Attribute Modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Learning Deep Features for Scene Recognition using Places Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
