<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Real-Time Depth Refinement for Specular Objects</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Or -El</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Israel Institute of Technology</orgName>
								<address>
									<settlement>Technion</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rom</forename><surname>Hershkovitz</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Israel Institute of Technology</orgName>
								<address>
									<settlement>Technion</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Wetzler</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Israel Institute of Technology</orgName>
								<address>
									<settlement>Technion</settlement>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guy</forename><surname>Rosman</surname></persName>
							<email>rosman@csail.mit.edu</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Computer Science and Artificial Intelligence Lab</orgName>
								<address>
									<region>MIT</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alfred</forename><forename type="middle">M</forename><surname>Bruckstein</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Israel Institute of Technology</orgName>
								<address>
									<settlement>Technion</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Kimmel</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Israel Institute of Technology</orgName>
								<address>
									<settlement>Technion</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Real-Time Depth Refinement for Specular Objects</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:44+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The introduction of consumer RGB-D scanners set off a major boost in 3D computer vision research. Yet, the precision of existing depth scanners is not accurate enough to recover fine details of a scanned object. While modern shading based depth refinement methods have been proven to work well with Lambertian objects, they break down in the presence of specularities. We present a novel shape from shading framework that addresses this issue and enhances both diffuse and specular objects' depth profiles. We take advantage of the built-in monochromatic IR projector and IR images of the RGB-D scanners and present a lighting model that accounts for the specular regions in the input image. Using this model, we reconstruct the depth map in real-time. Both quantitative tests and visual evaluations prove that the proposed method produces state of the art depth reconstruction results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The introduction of commodity RGB-D scanners marked the beginning of a new age for computer vision and computer graphics. Despite their popularity, such scanners can obtain only the rough geometry of scanned surfaces due to limited depth sensing accuracy. One way to mitigate this limitation is to refine the depth output of these scanners using the available RGB and IR images.</p><p>A popular approach to surface reconstruction from image shading cues is the Shape from Shading (SfS). Shape reconstruction from a single image is an ill-posed problem since beside the surface geometry, the observed image also depends on properties like the surface reflectance, the lighting conditions and the viewing direction. Incorporating data from depth sensors has proved to be successful in eliminating some of these ambiguities <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b11">12]</ref>. However, many of these efforts are based on the assumption that the scanned surfaces are fully Lambertian, which limits the variety of objects they can be applied to. Directly applying such meth-ods to specular objects introduces artifacts to the surface in highly specular regions due to the model's inability to account for sudden changes in image intensity.</p><p>Here, we propose a novel real-time framework for depth enhancement of non-diffuse surfaces. To that end, we use the IR image supplied by the depth scanners. The narrowband nature of the IR projector and IR camera provides a controlled lighting environment. Unlike previous approaches, we exploit this friendly environment to introduce a new lighting model for depth refinement that accounts for specular reflections as well as multiple albedos. To enable our real-time method we directly enhance the depth map by using an efficient optimization scheme which avoids the traditional normals refinemet step.</p><p>The paper outline is as follows: Section 2 reviews previous efforts in the field. An overview of the problem is presented in Section 3. The new method is introduced in Section 4, with results in Section 5. Section 6 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Efforts</head><p>The classical SfS framework assumes a Lambertian object with constant albedo and a single, distant, lighting source with known direction. There are several notable methods which solve the classical SfS problem. These can be divided into two groups: propagation methods and variational ones. Both frameworks were extensively researched during the last four decades. Representative papers from each school of thought are covered in <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b5">6]</ref>.</p><p>The main practical drawback about classical shape from shading, is that although a diffusive single albedo setup can be easily designed in a laboratory, it can be rarely found in more realistic environments. As such, modern SfS approaches attempt to reconstruct the surface without any assumptions about the scene lighting and/or the object albedos. In order to account for the unknown scene conditions, these algorithms either use learning techniques to construct priors for the shape and scene parameters, or acquire a rough depth map from a 3D scanner to initialize the surface.</p><p>Learning based methods. Barron and Malik <ref type="bibr" target="#b0">[1]</ref> constructed priors from statistical data of multiple images to recover the shape, albedo and illumination of a given input image. Kar et al. <ref type="bibr" target="#b9">[10]</ref> learn 3D deformable models from 2D annotations in order to recover detailed shapes. Richter and Roth <ref type="bibr" target="#b14">[15]</ref> extract color, textons and silhouette features from a test image to estimate a reflectance map from which patches of objects from a database are rendered and used in a learning framework for regression of surface normals. Although these methods produce excellent results, they depend on the quality and size of their training data, whereas the proposed axiomatic approach does not require a training stage and is therefore applicable in more general settings.</p><p>Depth map based methods. Bohme et al. <ref type="bibr" target="#b2">[3]</ref> find a MAP estimate of an enhanced range map by imposing a shading constraint on a probalistic image formation model. Yu et al. <ref type="bibr" target="#b22">[23]</ref> use mean shift clustering and second order spherical harmonics to estimate the fdepth map scene albedos and lighting from a color image. These estimations are then combined together to improve the given depth map accuracy. Han et al. <ref type="bibr" target="#b6">[7]</ref> propose a quadratic global lighting model along with a spatially varying local lighting model to enhance the quality of the depth profile. Kadambi et al. <ref type="bibr" target="#b8">[9]</ref> fuse normals obtained from polarization cues with rough depth maps to obtain accurate reconstructions. Even though this method can handle specular surfaces, it requires at least three photos to reconstruct the normals and it does not run in real-time. Several IR based methods were introduced in <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b18">19]</ref>. The authors of <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b3">4]</ref> suggest a multi-shot photometric stereo approach to reconstruct the object normals. Choe et al. <ref type="bibr" target="#b4">[5]</ref> refine 3D meshes from Kinect Fusion <ref type="bibr" target="#b10">[11]</ref> using IR images captured during the fusion pipeline. Although this method can handle uncalibrated lighting, it is niether one-shot nor real-time since a mesh must first be acquired before the refinement process begins. Ti et al. <ref type="bibr" target="#b18">[19]</ref> propose a simultaneous time-of flight and photometric stereo algorithm that utilizes several light sources to produce accurate surface and surface normals. Although this method can be implemented in real time, it requires four shots per frame for reconstruction as opposed to our single shot approach. More inline with our approach, Wu et al. <ref type="bibr" target="#b21">[22]</ref> use second order spherical harmonics to estimate the global scene lighting, which is then followed by efficient scheme to reconstruct the object. In <ref type="bibr" target="#b11">[12]</ref> Or -El et al. introduced a real-time framework for direct depth refinement that handles natural lighting and multiple albedo objects. Both algorithms rely on shading cues from an RGB image taken under uncalibrated illumination with possibly multiple light sources. Correctly modeling image specularities under such conditions is difficult. We propose to overcome the light source ambiguity issue by using the avail-ability of a single IR source with known configuration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Overview</head><p>Shape from Shading (SfS) tries to relate an object's geometry to its image irradiance. Like many other inverse problems, SfS is also an ill-posed one because the per-pixel image intensity is determined by several elements: the surface geometry, its albedo, scene lighting, the camera parameters and the viewing direction.</p><p>When using depth maps from RGB-D scanners one could recover the camera parameters and viewing direction, yet, in order to obtain the correct surface, we first need to account for the scene lighting and the surface albedos. Failing to do so would cause the algorithm to change the surface geometry and introduce undesired deformations. Using cues from an RGB image under uncalibrated illumination like <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b11">12]</ref> requires an estimation of global lighting parameters. Although such estimations work well for diffuse objects, they usually fail when dealing with specular ones and result in a distorted geometry. The reason is that specularities are sparse outliers that are not accounted for by classical lighting models. Furthermore, trying to use estimated lighting directions to model specularities is prone to fail when there are multiple light sources in the scene.</p><p>In our scenario, the main lighting in the IR image comes from the scanner's projector, which can be treated as a point light source. Observe that in this setting, we do not need to estimate a global lighting direction, instead, we use a near light field model to describe the per-pixel lighting direction. Subsequently, we can also account for specularities and non-uniform albedo map.</p><p>In our setting, an initial depth estimation is given by the scanner. We avoid the process of computing a refined normal field and then fusing depth with normal estimates, which is common to SfS methods, and solve directly for the depth. This eliminates the need to enforce integrability and reduces the problem size by half. We deal with the nonlinear part by calculating a first order approximation of the cost functional and thereby achieve real-time performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Proposed Framework</head><p>A novel IR based real-time framework for depth enhancement is proposed. The suggested algorithm requires a depth map and an IR image as inputs. We assume that the IR camera and the depth camera have the same intrinsic parameters, as is usually the case with common depth scanners. In addition, we also assume that the whole system is calibrated and that the translation vector between the scanner's IR projector and IR camera is known.</p><p>Unfortunately, the raw depth map is usually quantized and the surface geometry is highly distorted. Therefore, we first smooth the raw depth map and estimate the surface nor- We then move on to recover the scene lighting using a near-field lighting model which explicitly accounts for object albedos and specularities.</p><p>After we find the scene lighting along with albedo and specular maps, we can directly update the surface geometry by designing a cost functional that relates the depth and IR intensity values at each pixel. We also show how the reconstruction process can be accelerated in order to obtain real-time performance. <ref type="figure" target="#fig_0">Figure 1</ref> shows a flowchart of the proposed algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Near Field Lighting Model</head><p>Using an IR image as an input provides several advantages to the reconstruction process. Unlike other methods which require alignment between RGB and depth images, in our case, the depth map and IR image are already aligned as they were captured by the same camera. Moreover, the narrowband nature of the IR camera means that the main light source in the image is the scanner's own IR projector whose location relative to the camera is known. Therefore, we can model the IR projector as a point light source and use a near field lighting model to describe the given IR image intensity at each pixel,</p><formula xml:id="formula_0">I = aρ d d 2 p S diff + ρ d S amb + aρ s d 2 p S spec .<label>(1)</label></formula><p>Here, a is the projector intensity which is assumed to be constant throughout the image. d p is the distance of the surface point from the projector. ρ d and ρ s are the diffuse and specular albedos. S amb is the ambient lighting in the scene, which is also assumed to be constant over the image. S diff is the diffuse shading function of the image which is given by the Lambertian reflectance model The specular shading function S spec is set according to the Phong reflectance model</p><formula xml:id="formula_1">S diff = N · l p .<label>(2)</label></formula><formula xml:id="formula_2">Depth Scanner Surface N { l c , d c } { l p , d p } Projector IR Camera</formula><formula xml:id="formula_3">S spec = 2( l p · N ) N − l p · l c α ,<label>(3)</label></formula><p>where N is the surface normal, l p , l c are the directions from the surface point to the projector and camera respectively and α is the shininess constant which we set to α = 2. <ref type="figure" target="#fig_1">Figure 2</ref> describes the scene lighting model. For ease of notation, we definẽ</p><formula xml:id="formula_4">S diff = a d 2 p S diff ,S spec = a d 2 p S spec .<label>(4)</label></formula><p>The intrinsic camera matrix and the relative location of the projector with respect to camera are known. In addition, the initial surface normals can be easily calculated from the given rough surface. Therefore, l c , l p , d p , S diff and S spec can be found directly whereas a, S amb , ρ d and ρ s need to be recovered. Although we are using a rough depth normal field to compute l c , l p , d p , S diff and S spec we still get accurate shading maps since the lighting is not sensitive to minor changes in the depth or normal field as shown in <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b13">14]</ref>. Decomposing the IR image into its Lambertian and Specular lighting components along with their respective albedo maps has no unique solution. To achieve accurate results while maintaining real-time performance we choose a greedy approach which first assumes Lambertian lighting and gradually accounts for the lighting model from Eq. 1. Every pixel in the IR image which has an assigned normal can be used to recover a and S amb . Generally, most of the light reflected back to the camera is related to the diffuse component of the object whereas highly specular areas usually have a more sparse nature. Thus, the specular areas can be treated as outliers in a parameter fitting scheme as they have minimal effect on the outcome. This allows us to assume that the object is fully Lambertian (i.e ρ d = 1, ρ s = 0), which in turn, gives us the following overdetermined linear system for n valid pixels (n ≫ 2), </p><formula xml:id="formula_5">    S 1 diff (d 1 p ) 2 1 . . . . . . S n diff (d n p ) 2 1      a S amb =    I 1 . . . I n    .<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Specular Albedo Map</head><p>The specular shading map is important since it reveals the object areas which are likely to produce specular reflections in the IR image. Without it, bright diffuse objects can be mistaken for specularities. Yet, sinceS spec was calculated as if the object is purely specular, using it by itself will fail to correctly represent the specular irradiance, as it would falsely brighten non-specular areas. In order to obtain an accurate representation of the specularities it is essential to find the specular albedo map to attenuate the non-specular areas ofS spec .</p><p>We now show how we can take advantage of the sparse nature of the specularities to recover ρ s and get the correct specular scene lighting. We will define a residual image I s res as being a difference between the original image I and our current diffuse approximation together with the ambient lighting. Formally, we write this as I s res = I − (S diff + S amb ).</p><p>As can be seen in <ref type="figure" target="#fig_2">Figure 3</ref> (c), the sparse bright areas of I s res are attributable to the true specularities in I. Specular areas have finite local support, therefore we choose to model the residual image I s res as ρ sSspec such that ρ s will be a sparse specular albedo map. This will yield an image that contains just the bright areas of I s res . In addition, in order to preserve the smooth nature of specularities we add a smoothness term that minimizes the L1 Total-Variation of ρ s . To summarize, the energy minimization problem to estimate ρ s can be written as min ρs λ s 1 ρ sSspec − I s</p><formula xml:id="formula_7">res 2 2 + λ s 2 ρ s 1 + λ s 3 ∇ρ s 1 ,<label>(7)</label></formula><p>where λ s 1 , λ s 2 , λ s 3 are weighting terms for the fidelity, sparsity and smoothness terms, respectively. To minimize the cost functional, we use a variation of the Augmented Lagrangian method suggested in <ref type="bibr" target="#b20">[21]</ref> where we substitute the frequency domain solution with a Gauss-Seidel scheme on the GPU. We refer the reader to the above paper for additional details on the optimization procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Recovering the Diffuse Albedo</head><p>As was the case with specular shading, the diffuse shading map alone does not sufficiently explain the diffuse lighting. This is due to the fact that the diffuse shading is calculated as if there was only a single object with uniform albedo. In reality however, most objects are composed of multiple different materials with different reflectance properties that need to be accounted for.</p><p>Using the estimated specular lighting from section 4.1.1 we can now compute a residual image between the original image I and the specular scene lighting which we write as I d res = I − ρ sSspec . (8) I d res should now contain only the diffuse and ambient irradiance of the original image I. This can be used in a data fidelity term for a cost functional designed to find the diffuse albedo map ρ d .</p><p>We also wish to preserve the piecewise-smoothness of the diffuse albedo map. Otherwise, geometry distortions will be mistaken for albedos and we will not be able to recover the correct surface. The IR image and the rough depth map provide us several cues that will help us to enforce piecewise smoothness. Sharp changes in the intensity of the IR image imply a change in the material reflectance. Moreover, depth discontinuities can also signal possible changes in the albedo.</p><p>We now wish to fuse the cues from the initial depth profile and the IR image together with the piecewise-smooth albedo requirement. Past papers <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b11">12]</ref> have used bilateral smoothing. Here, instead, we base our scheme on the geomtric Beltrami framework such as in <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b19">20]</ref> which has the advantage of promoting alignment of the embedding space channels. Let, M(x, y) = {x, y, β I I d res (x, y), β z z(x, y), β ρ ρ d (x, y)} (9) be a two dimensional manifold embedded in a 5D space with the metric</p><formula xml:id="formula_8">G = M x , M x M x , M y M x , M y M y , M y .<label>(10)</label></formula><p>The gradient of ρ d with respect to the 5D manifold is</p><formula xml:id="formula_9">∇ G ρ d = G −1 · ∇ρ d ,<label>(11)</label></formula><p>By choosing large enough values of β I , β z and β ρ and minimizing the L1 Total-Variation of ρ d with respect to the manifold metric, we basically perform selective smoothing according to the "feature" space (I d res , z, ρ d ). For instance, if β I ≫ β z , β ρ , 1, the manifold gradient would get small values when sharp edges are present in I d res since G −1 would decrease the weight of the gradient at such locations.</p><p>To conclude, the minimization problem we should solve in order to find the diffuse albedo map is</p><formula xml:id="formula_10">min ρ d λ d 1 ρ d S diff + S amb − I d res 2 2 + λ d 2 ∇ G ρ d 1 . (12)</formula><p>Here, λ d 1 , λ d 2 are weighting terms for the fidelity and piecewise-smooth penalties. We can minimize this functional using the Augmented Lagrangian method proposed in <ref type="bibr" target="#b15">[16]</ref>. The metric is calculated separately for each pixel, therefore, it can be implemented very efficiently on a GPU with limited effect on the algorithm's runtime.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Surface Reconstruction</head><p>Once we account for the scene lighting, any differences between the IR image and the image rendered with our lighting model are attributed to geometry errors of the depth profile. Usually, shading based reconstruction algorithms opt to use the dual stage process of finding the correct surface normals and then integrating them in order to obtain the refined depth. Although this approach is widely used, it has some significant shortcomings. Calculating the normal field is an ill-posed problem with 2n unknowns if n is the number of pixels. The abundance of variables can result in distorted surfaces that are tilted away from the camera. In addition, since the normal field is an implicit surface representation, further regularization such as the integrability constraint is needed to ensure that the resulting normals would represent a valid surface. This additional energy minimization functional can impact the performance of the algorithm.</p><p>Instead, we use the strategy suggested in <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b21">22]</ref> and take advantage of the rough depth profile acquired by the scanner. Using the explicit depth values forces the surface to move only in the direction of the camera rays, avoids unwanted distortions, eliminates the need to use an integrability constraint and saves computation time and memory by reducing the number of variables.</p><p>In order to directly refine the surface, we relate the depth values to the image intensity through the surface normals. Assuming that the perspective camera intrinsic parameters are known, the 3D position P (i, j) of each pixel is given by</p><formula xml:id="formula_11">P (z(i, j)) = j − c x f x z(i, j), i − c y f y z(i, j), z(i, j) T ,<label>(13)</label></formula><p>where f x , f y are the focal lengths of the camera and (c x , c y ) is the camera's principal point. The surface normal N at each 3D point is then calculated by</p><formula xml:id="formula_12">N (z(i, j)) = P x × P y P x × P y .<label>(14)</label></formula><p>We can use Eqs. (1), <ref type="bibr" target="#b1">(2)</ref> and <ref type="formula" target="#formula_0">(14)</ref> to write down a depth based shading term written directly in terms of z,</p><formula xml:id="formula_13">E sh (z) = aρ d d 2 p ( N (z) · l p ) + ρ d S amb + ρ sSspec − I 2 2 .</formula><p>(15) This allows us to refine z by penalizing shading mismatch with the original image I. We also use a fidelity term that penalizes the distance from the initial 3D points</p><formula xml:id="formula_14">E f (z) = w(z − z 0 ) 2 2 , w = 1 + j − c x f x 2 + i − c y f y 2 ,<label>(16)</label></formula><p>and a smoothness term that minimizes the second order TV-L1 of the surface</p><formula xml:id="formula_15">E sm (z) = Hz 1 , H = D xx D yy .<label>(17)</label></formula><p>Here, D xx , D yy are the second derivatives of the surface. Combining Eqs. <ref type="bibr" target="#b14">(15)</ref>, <ref type="bibr" target="#b15">(16)</ref> and <ref type="formula" target="#formula_0">(17)</ref>  results in a non-linear optimization problem min</p><formula xml:id="formula_16">z λ z 1 E sh (z) + λ z 2 E f (z) + λ z 3 E sm (z),<label>(18)</label></formula><p>where λ z 1 , λ z 2 , λ z 3 are the weights for the shading, fidelity and smoothness terms, respectively. Although there are several possible methods to solve this problem, a fast scheme is required for real-time performance. To accurately and efficiently refine the surface we base our approach on the iterative scheme suggested in <ref type="bibr" target="#b12">[13]</ref>. Rewriting Eq. (15) as a function of the discrete depth map z, and using forward derivatives we have</p><formula xml:id="formula_17">I i,j − ρ d S amb − ρ sSspec = aρ d d 2 p ( N (z) · l p ) = f (z i,j , z i+1,j , z i,j+1 ).<label>(19)</label></formula><p>At each iteration k we can approximate f using the first order Taylor expansion about (z k−1 i,j , z k−1 i+1,j , z k−1 i,j+1 ), such that</p><formula xml:id="formula_18">I i,j − ρ d S amb − ρ sSspec = f (z k i,j , z k i+1,j , z k i,j+1 ) ≈ f (z k−1 i,j , z k−1 i+1,j , z k−1 i,j+1 ) + ∂f ∂z k−1 i,j (z k i,j − z k−1 i,j ) + ∂f ∂z k−1 i+1,j (z k i+1,j − z k−1 i+1,j ) + ∂f ∂z k−1 i,j+1 (z k i,j+1 − z k−1 i,j+1 ).</formula><p>(20) Rearranging terms to isolate terms including z from the current iteration, we can define</p><formula xml:id="formula_19">I z k res = I i,j − ρ d S amb − ρ sSspec − f (z k−1 i,j , z k−1 i+1,j , z k−1 i,j+1 ) + ∂f ∂z k−1 i,j z k−1 i,j + ∂f ∂z k−1 i+1,j z k−1 i+1,j + ∂f ∂z k−1 i,j+1 z k−1 i,j+1 ,<label>(21)</label></formula><p>and therefore minimize min</p><formula xml:id="formula_20">z k λ z 1 Az k −I z k res 2 2 +λ z 2 w(z k −z 0 ) 2 2 +λ z 3 Hz k 1<label>(22)</label></formula><p>at each iteration with the Augmented Lagrangian method of <ref type="bibr" target="#b20">[21]</ref>. Here, A is a matrix that represents the linear operations performed on the vector z k . Finally, we note that this pipeline was implemented on an Intel i7 3.4GHz proces- </p><formula xml:id="formula_21">(a) (b) (c) (d) (e) (f) (g)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results</head><p>We preformed several tests in order to evaluate the quality and accuracy of the proposed algorithm. We show the algorithm's accuracy in recovering the specular lighting of the scene and why it is vital to use an IR image instead of an RGB image. In addition, we demonstrate that the proposed framework is state of the art, both visually and qualitatively.</p><p>In order to test the specular lighting framework, we took 3D objects from the Stanford 3D 1 , 123D Gallery 2 and Blendswap 3 repositories. For each model we assigned a mix of diffuse and specular shaders and rendered them under an IR lighting scenario described in Section 4.1 (single light source) and natural lighting scenarios (multiple light sources) using the Cycles renderer in Blender. To get a ground truth specularity map for each lighting scenario, we also captured each model without its specular shaders and subtracted the resulting images.</p><p>We tested the accuracy of our model in recovering specularities for each lighting setup. We used Eqs. <ref type="bibr" target="#b1">(2)</ref> and <ref type="formula" target="#formula_5">(5)</ref>   <ref type="table">Table 2</ref>: Quantitative comparison of depth accuracy in specular areas. All values are in millimeters. get the diffuse and ambient shading maps under IR lighting. For natural lighting, the diffuse and ambient shading were recovered using first and second order spherical harmonics in order to have two models for comparison. In both lighting scenarios the surface normals were calculated from the ground truth depth map. The specular lighting is recovered using Eqs. (3) and <ref type="formula" target="#formula_7">(7)</ref>, where the IR lighting direction l p is calculated using the camera-projector calibration parameters. In the natural lighting scene we use the relevant normalized coefficients of the first and second order spherical harmonics in order to compute the general lighting di- rection. From the results in <ref type="table">Table 1</ref> we can infer that the specular irradiance can be accurately estimated in our proposed lighting model as opposed to the natural lighting (NL SH1/2) where estimation errors are much larger. The reason for large differences is that, as opposed to our lighting model, under natural illumination there are usually multiple light sources that cause specularities whose directions cannot be recovered accurately. An example of this can be seen in <ref type="figure" target="#fig_3">Figure 4</ref>.</p><formula xml:id="formula_22">(a) (b) (c) (d) (e) (f) (g) (h) (i)</formula><formula xml:id="formula_23">(a) (b) (c) (d) (e) (f) (g) (h) (i)</formula><p>To measure the depth reconstruction accuracy of the proposed method we performed experiments using both synthetic and real data. In the first experiment, we used the 3D models with mixed diffuse and specular shaders and rendered their IR image and ground truth depth maps in Blender. We then quantized the ground truth depth map to 1.5mm units in order to simulate the noise of a depth sensor. We applied our method to the data and defined the reconstruction error as the absolute difference between the result and the ground truth depth maps. We compared our method's performance with the methods proposed in <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b21">22]</ref>. The comparisons were performed in the specular regions of the objects according to the ground truth specularity maps. The results are shown in <ref type="table">Table.</ref> 2. A qualitative evaluation of the accuracy when the method is applied to the synthetic data can be seen in <ref type="figure" target="#fig_4">Figures. 5 and 6</ref>.</p><formula xml:id="formula_24">(a) (b) (c) (d) (e) (f) (g) (h) (i)</formula><p>In the second experiment we tested our method under laboratory conditions using a structured-light 3D scanner to capture the depth of several objects. The camera-projector system was calibrated according to the method suggested in <ref type="bibr" target="#b24">[25]</ref>. We reduced the number of projected patterns in order to obtain a noisy depth profile. To approximate an IR lighting scenario, we used a monochromatic projector and camera with dim ambient illumination.</p><p>We also tested the algorithm with an Intel Real-Sense depth scanner, using the IR image and depth map as inputs. The camera-projector calibration parameters were acquired from the Real-Sense SDK platform. Although no accurate ground-truth data was available for these experiments, we note that while all methods exhibit sufficient accuracy in diffuse areas, the proposed method is the only one that performs qualitatively well in highly specular areas as can be seen in Figures 7 and 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>We presented a new framework for depth refinement of specular objects based on shading cues from an IR image. To the best of our knowledge, the proposed method is the first depth refinement framework to explicitly account for specular lighting. An efficient optimization scheme enables our system to produce state of the art results at real-time rates.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Algorithm's flowchart mals.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Scene lighting model</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>(a) Simulated IR image of the Armadillo mesh. (b) Recovered image of the diffuse and ambient shadingS diff +S amb . (c) Residual image for specular albedo estimation I s res . (d) Ground Truth specularity map of (a). Note that specularities in (d) are basically the sparse representation of the residual image (c).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Greek Statue: (a) Single light source IR image. (b) Ground truth specular irradiance map for (a). (c) Specular irradiance estimation error map. This is the absolute difference map between our predicted specular irradiance and the ground truth. (d) Multiple light source natural lighting (NL) image. (e) Specular lighting ground truth of (d). (f,g) Specular irradiance error maps of (d) as estimated using first (SH1) and second (SH2) order spherical harmonics respectively. Note the reduced errors when using a single known light source (c) as opposed to estimating multiple unknown light sources using spherical harmonics lighting models (f,g). sor with 16GB of RAM and an NVIDIA GeForce GTX650 GPU. The runtime for a 640 × 480 image is approximately 80 milliseconds.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Results for the simulated Armadillo scene, (a) Input IR image. (b) Ground truth model. (c) Initial Depth. (d)-(f) Reconstructions of Wu et al., Or -El et al. and our proposed method respectively. (g)-(i) Magnifications of a specular area. Note how our surface is free from distortions in specular areas unlike the other methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Results for the simulated Pumpkin scene, (a) Input IR image. (b) Ground truth model. (c) Initial Depth. (d)-(f) Reconstructions of Wu et al., Or -El et al. and our proposed method respectively. (g)-(i) Magnifications of a specular area. Note the lack of hallucinated features in our method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Results for the lab conditions experiment, (a) Input IR image. (b) Initial Depth. (c) Result after bilateral smoothing. (d)-(f) Reconstructions of Wu et al., Or -El et al. and the proposed method, respectively. (g)-(i) Magnifications of a specular region.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Results from Intel's Real-Sense depth scanner, (a) Input IR image. (b) Initial Depth. (c) Result after bilateral smoothing. (d)-(f) Reconstructions of Wu et al., Or -El et al. and the proposed method, respectively. (g)-(i) Magnifications of a specular region.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>Wu et al. Or-El et al. Proposed Wu et al. Or-El et al.</figDesc><table>to 
Model 

Median Error (mm) 
90 th % (mm) 
Proposed 
Armadillo 
0.335 
0.318 
0.294 
1.005 
0.821 
0.655 
Dragon 
0.337 
0.344 
0.324 
0.971 
0.917 
0.870 
Greek Statue 
0.306 
0.281 
0.265 
0.988 
0.806 
0.737 
Stone Lion 
0.375 
0.376 
0.355 
0.874 
0.966 
0.949 
Cheeseburger 
0.191 
0.186 
0.168 
0.894 
0.756 
0.783 
Pumpkin 
0.299 
0.272 
0.242 
0.942 
0.700 
0.671 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">http://graphics.stanford.edu/data/3Dscanrep/ 2 http://www.123dapp.com/Gallery/content/all 3 http://www.blendswap.com/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We wish to thank Alon Zvirin for his help with the experiments. This research was supported by European Communitys FP7-ERC program grant agreement no. 267414. G.R is partially funded by VITALITE Army Research Office Multidisciplinary Research Initiative program, award W911NF-11-1-0391.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Shape, illumination, and reflectance from shading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1670" to="1687" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Lambertian reflectance and linear subspaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Basri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="218" to="233" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Shading constraint improves accuracy of time-of-flight measurements. Computer vision and image understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Böhme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Haker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Martinetz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Barth</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="page" from="1329" to="1335" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Photometric refinement of depth maps for multi-albedo objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chatterjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Govindu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="933" to="941" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Exploiting shading cues in kinect IR images for geometry refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>So Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3922" to="3929" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Numerical methods for shape-from-shading: A new survey with benchmarks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-D</forename><surname>Durou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Falcone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sagona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="22" to="43" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">High quality shape from a single RGB-D image under uncalibrated natural illumination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">High quality photometric reconstruction using a depth camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Haque</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chatterjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Govindu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2283" to="2290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Polarized 3D: High-quality depth sensing with polarization cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kadambi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Taamazyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Raskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3370" to="3378" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Categoryspecific object reconstruction from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tulsiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1966" to="1974" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">KinectFusion: Real-time dense surface mapping and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Newcombe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Izadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Hilliges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Molyneaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hodges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fitzgibbon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE international symposium on Mixed and augmented reality</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="127" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">RGBD-Fusion: Real-time high precision depth recovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>El</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rosman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wetzler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kimmel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Bruckstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="5407" to="5416" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Shape from shading using linear approximation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ping-Sing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="487" to="498" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">An efficient representation for irradiance environment maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hanrahan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th annual conference on Computer graphics and interactive techniques</title>
		<meeting>the 28th annual conference on Computer graphics and interactive techniques</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="497" to="500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Discriminative shape from shading in uncalibrated illumination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Richter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1128" to="1136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Group-valued regularization for analysis of articulated motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rosman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-C</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kimmel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NORDIA workshop, European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="52" to="62" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Tensor-based image diffusions derived from generalizations of the total variation and beltrami functionals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roussos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Maragos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="4141" to="4144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A general framework for low level vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sochen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kimmel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Malladi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="310" to="318" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Simultaneous time-offlight sensing and photometric stereo with a single tof sensor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4334" to="4342" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Efficient beltrami flow in patchspace</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wetzler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kimmel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Scale Space and Variational Methods in Computer Vision (SSVM)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="134" to="143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Augmented lagrangian method, dual methods, and split bregman iteration for ROF, vectorial TV, and high order models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-C</forename><surname>Tai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Img. Sci</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2010-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Real-time shading-based refinement for consumer depth cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zollhöfer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nießner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stamminger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Izadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Transactions on Graphics (Proceedings of SIGGRAPH Asia</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Shading-based shape refinement of RGB-D images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Shape-fromshading: a survey. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-S</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Cryer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="690" to="706" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Novel method for structured light system calibration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Optical Engineering</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="83601" to="83602" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
