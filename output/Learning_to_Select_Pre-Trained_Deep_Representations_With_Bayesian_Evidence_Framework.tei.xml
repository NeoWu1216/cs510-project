<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning to Select Pre-trained Deep Representations with Bayesian Evidence Framework</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong-Deok</forename><surname>Kim</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Software R&amp;D Center</orgName>
								<orgName type="department" key="dep2">Device Solutions</orgName>
								<orgName type="institution">Samsung Electronics</orgName>
								<address>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taewoong</forename><surname>Jang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Stradvision Inc</orgName>
								<address>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Han</surname></persName>
							<email>bhhan@postech.ac.kr</email>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<address>
									<postBox>POSTECH</postBox>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungjin</forename><surname>Choi</surname></persName>
							<email>seungjin@postech.ac.kr</email>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<address>
									<postBox>POSTECH</postBox>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning to Select Pre-trained Deep Representations with Bayesian Evidence Framework</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:42+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a Bayesian evidence framework to facilitate transfer learning from pre-trained deep convolutional neural networks (CNNs). Our framework is formulated on top of a least squares SVM (LS-SVM) classifier, which is simple and fast in both training and testing, and achieves competitive performance in practice. The regularization parameters in LS-SVM is estimated automatically without grid search and cross-validation by maximizing evidence, which is a useful measure to select the best performing CNN out of multiple candidates for transfer learning; the evidence is optimized efficiently by employing Aitken's delta-squared process, which accelerates convergence of fixed point update. The proposed Bayesian evidence framework also provides a good solution to identify the best ensemble of heterogeneous CNNs through a greedy algorithm. Our Bayesian evidence framework for transfer learning is tested on 12 visual recognition datasets and illustrates the state-of-the-art performance consistently in terms of prediction accuracy and modeling efficiency.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Image representations from deep CNN models trained for specific image classification tasks turn out to be powerful even for general purposes <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b22">23]</ref> and useful for transfer learning or domain adaptation. Therefore, CNNs trained on specific problems or datasets are often fine-tuned to facilitate training for new tasks or domains <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b35">36]</ref>, and an even simpler approachapplication of off-the-shelf classification algorithms such as SVM to the representations from deep CNNs <ref type="bibr" target="#b6">[7]</ref>-is getting more attractive in many computer vision problems. However, fine-tuning of an entire deep network still requires a lot of efforts and resources, and SVM-based methods also * This work was done when Y. Kim and T. Jang were with POSTECH. <ref type="figure">Figure 1</ref>. We address a problem to select the best CNN out of multiple candidates as shown in this figure. Additionally, our algorithm is capable of identifying the best ensemble of multiple CNNs to further improve performance.</p><p>involve time consuming grid search and cross validation to identify good regularization parameters. In addition, when multiple pre-trained deep CNN models are available, it is unclear which pre-trained models are appropriate for target tasks and which classifiers would maximize accuracy and efficiency. Unfortunately, most existing techniques for transfer learning or domain adaptation are limited to empirical analysis or ad-hoc application specific approaches.</p><p>We propose a simple but effective algorithm for transfer learning from pre-trained deep CNNs based on Bayesian least squares SVM (LS-SVM), which is formulated with Bayesian evidence framework <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b28">29]</ref> and LS-SVM <ref type="bibr" target="#b25">[26]</ref>. This approach automatically determines regularization parameters in a principled way, and shows comparable performance to the standard SVMs based on hinge loss or squared hinge loss. More importantly, Bayesian LS-SVM provides an effective solution to select the best CNN out of multiple candidates and identify a good ensemble of heterogeneous CNNs for performance improvement. <ref type="figure">Figure 1</ref> illustrates our approach. We also propose a fast Bayesian LS-SVM, which maximizes the evidence more efficiently based on Aitken's delta-squared process <ref type="bibr" target="#b0">[1]</ref>.</p><p>One may argue against the use of LS-SVM for classification because the least squares loss function in LS-SVM tends to penalize well-classified examples. However, least squares loss is often used for training multilayer perceptron <ref type="bibr" target="#b3">[4]</ref> and shows comparable performance to SVMs <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b36">37]</ref>. In addition, Bayesian LS-SVM provides a technically sound formulation with outstanding performance in terms of speed and accuracy for transfer learning with deep representations. We also propose a fast Bayesian LS-SVM, which maximizes the evidence more efficiently based on Aitkens delta-squared process <ref type="bibr" target="#b0">[1]</ref>. Considering simplicity and accuracy, we claim that our fast Bayesian LS-SVM is a reasonable choice for transfer learning with deep learning representation in visual recognition problems. Based on this approach, we achieved promising results compared to the state-of-the-art techniques on 12 visual recognition tasks.</p><p>The rest of this paper is organized as follows. Section 2 describes examples of transfer learning or domain adaptation based on pre-trained CNNs for visual recognition problems. Then, we discuss Bayesian evidence framework applicable to the same problem in Section 3 and its acceleration technique using Aitken's delta-squared process in Section 4. The performance of our algorithm in various applications is demonstrated in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Since AlexNet <ref type="bibr" target="#b14">[15]</ref> demonstrated impressive performance in the ImageNet large scale visual recognition challenge (LSVRC) 2012, a few deep CNNs with different architectures, e.g., VGG <ref type="bibr" target="#b24">[25]</ref> and GoogLeNet <ref type="bibr" target="#b26">[27]</ref>, have been proposed in the subsequent events. Instead of training deep CNNs from scratch, some people have attempted to refine pre-trained networks for new tasks or datasets by updating the weights of all neurons or have adopted the intermediate outputs of existing deep networks as generic visual feature descriptors. These strategies can be interpreted as transfer learning or domain adaptation.</p><p>Refining a pre-trained CNN is called fine-tuning, where the architecture of the network may be preserved while weights are updated based on new training data. Finetuning is generally useful to improve performance <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b35">36]</ref> but requires careful implementation to avoid overfitting. The second approach regards the pre-trained CNNs as feature extraction machines and combines the deep representations with the off-the-shelf classifiers such as linear SVM <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b33">34]</ref>, logistic regression <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b33">34]</ref>, and multi-layer neural network <ref type="bibr" target="#b20">[21]</ref>. The techniques in this category have been successful in many visual recognition tasks <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24]</ref>.</p><p>When combining a classification algorithm with image representations from pre-trained deep CNNs, we often face a critical issue. Although several deep CNN models trained on large scale image repositories are publicly available, there is no principled way to select a CNN out of multiple candidates and find the best ensemble of multiple CNNs for performance optimization. Existing algorithms typically rely on ad-hoc methods for model selection and fail to provide clear evidence for superior performance <ref type="bibr" target="#b1">[2]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Bayesian LS-SVM for Model Selection</head><p>This section discusses a Bayesian evidence framework to select the best CNN model(s) in the presence of transferable multiple candidates and identify a reasonable regularization parameter for LS-SVM classifier automatically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Problem Definition and Formulation</head><p>Suppose that we have a set of pre-trained deep CNN models denoted by {CNN m |m = 1 . . . M }. Our goal is to identify the best performing deep CNN model among the M networks for transfer learning. A naïve approach is to perform fine tuning of network for target task, which requires substantial efforts for training. Another option is to replace some of fully connected layers in a CNN with an off-the-shelf classifier such as SVM and check the performance of target task through parameter tuning for each network, which would also be computationally expensive.</p><p>We adopt a Bayesian evidence framework based on LS-SVM to achieve the goal in a principled way, where the evidence of each network is maximized iteratively and the maximum evidences are used to select a reasonable model. During the evidence maximization procedure, the regularization parameter of LS-SVM is identified automatically without time consuming grid search and cross-validation. In addition, the Bayesian evidence framework is also applied to the construction of an ensemble of multiple CNNs to accomplish further performance improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">LS-SVM</head><p>We deal with multi-label or multi-class classification problem, where the number of categories is K.</p><formula xml:id="formula_0">Let D = {(x n , y (k) n ), k = 1 . . . K} n=1.</formula><p>..N be a training set, where x n ∈ R D is a feature vector and y (k) n is a binary variable that is set to 1 if label k is given to x n and 0 otherwise. Then, for each class k, we minimize a least squares loss with L 2 regularization penalty as follows:</p><formula xml:id="formula_1">min w (k) ∈R D y (k) − X ⊤ w (k) 2 + λ (k) w (k) 2 ,<label>(1)</label></formula><p>where X = [x 1 , . . . , x n ] ∈ R D×N and y (k) = [y (k) 1 , . . . , y</p><p>N ] ⊤ ∈ R N . The optimal solution of the prob-lem in (1) is given by</p><formula xml:id="formula_3">w (k) = (XX ⊤ + λ (k) I) −1 Xy (k) , = U (S + λ (k) I) −1 U ⊤ Xy (k) ,<label>(2)</label></formula><p>where U SU ⊤ is the eigen-decomposition of XX ⊤ and I is an identity matrix. This regularized least squares approach has clear benefit that it requires only one eigendecomposition of XX ⊤ to obtain the solution in <ref type="formula" target="#formula_3">(2)</ref> for all combinations of λ (k) and y (k) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Bayesian Evidence Framework</head><p>The optimization of the regularized least squares formulation presented in (1) is equivalent to the maximization of the posterior with fixed hyperparamters α and β denoted by p(w|y, X, α, β), where λ = α/β. The posterior can be decomposed into two terms by Bayesian theorem as</p><formula xml:id="formula_4">p(w|y, X, α, β) ∝ p(y|X, w, β)p(w|α),<label>(3)</label></formula><p>where p(y|X, w, β) corresponds to Gaussian observation noise model given by</p><formula xml:id="formula_5">p(y|X, w, β) = N n=1 N (y n |x ⊤ n w, β −1 )<label>(4)</label></formula><p>and p(w|α) denotes a zero-mean isotropic Gaussian prior as</p><formula xml:id="formula_6">p(w|α) = N (w|0, α −1 I).<label>(5)</label></formula><p>Note that we dropped superscript (k) for notational simplicity from the equations in this subsection.</p><p>In the Bayesian evidence framework <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b28">29]</ref>, the evidence, also known as marginal likelihood, is a function of hyperparameters α and β as p(y|X, α, β) = p(y|X, w, β)p(w|α)dw.</p><p>Under the probabilistic model assumptions corresponding to <ref type="bibr" target="#b3">(4)</ref> and <ref type="formula" target="#formula_6">(5)</ref>, the log evidence L(α, β) is given by</p><formula xml:id="formula_8">L(α, β) ≡ log p(y|X, α, β) (7) = D 2 log α + N 2 log β − 1 2 log |A| − β 2 y − X ⊤ m 2 − α 2 m ⊤ m − N 2 log 2π,</formula><p>where the precision matrix and mean vector of the posterior p(w|y, X, α, β) = N (w|m, A −1 ) are given respectively by</p><formula xml:id="formula_9">A = αI + βXX ⊤ and m = βA −1 Xy.</formula><p>The log evidence L(α, β) is maximized by repeatedly alternating the following fixed point update rules</p><formula xml:id="formula_10">α = γ m ⊤ m and β = N − γ y − X ⊤ m 2 ,<label>(8)</label></formula><p>which involves the derivation of γ as</p><formula xml:id="formula_11">γ = D d=1 βs d α + βs d = D d=1 s d λ + s d ,<label>(9)</label></formula><p>where {s d } D d=1 are eigenvalues of XX ⊤ . Note that m and γ should be re-estimated after each update of α and β.</p><p>Another pair of update rules of α and β are derived by an expectation-maximization (EM) technique as</p><formula xml:id="formula_12">α = D m ⊤ m + Tr(A −1 )</formula><p>and</p><formula xml:id="formula_13">(10) β = N y − X ⊤ m 2 + Tr(A −1 XX ⊤ ) ,<label>(11)</label></formula><p>but these procedures are substantially slower than the fixed point update rules in <ref type="bibr" target="#b7">(8)</ref>. Through the optimization procedures described above, we determine the regularization parameter λ = α/β. Although the estimated parameters are not optimal, they may still be reasonable solutions since they are obtained by maximizing marginal likelihood in (6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Model Selection using Evidence</head><p>The evidence computed in the previous subsection is for a single class, and the overall evidence for entire classes, denoted by L * , is obtained by the summation of the evidences from individual classes, which is given by</p><formula xml:id="formula_14">L * = K k=1 L(α (k) , β (k) ).<label>(12)</label></formula><p>We compute the overall evidence corresponding to each deep CNN model, and choose the model with the maximum evidence for transfer learning. We expect that the selected model performs best among all candidates, which will be verified in our experiment.</p><p>In addition, when an ensemble of deep CNNs needs to be constructed for a target task, our approach selects a subset of good pre-trained CNNs in a greedy manner. Specifically, we add a network with the largest evidence in each stage and test whether the augmented network improves the evidence or not. The network is accepted if the evidence increases, or rejected otherwise. After the last candidate is tested, we obtain the final network combination and its associated model learned with the concatenated feature descriptors from accepted networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Fast Bayesian LS-SVM</head><p>Bayesian evidence framework discussed in Section 3 is useful to identify a good CNN for transfer learning and a reasonable regularization parameter. To make this framework even more practical, we present a faster algorithm to accomplish the same goal and a new theory that guarantees the converges of the algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Reformulation of Evidence</head><p>We are going to reduce L(α, β) to a function with only one parameter that directly corresponds to the regularization parameter λ = α/β. To this end, we re-write L(α, β) by using the eigen-decomposition XX ⊤ = U SU ⊤ as</p><formula xml:id="formula_15">L(α, β) = D 2 log α + N 2 log β − 1 2 D d=1 log(α + βs d ) − β 2 y ⊤ y + β 2 2 D d=1 h 2 d α + βs d − N 2 log 2π, (13) where s d is the d-th diagonal element in S and h d denotes the d-th element in h = U ⊤ Xy. Then, we re-parameterize L(α, β) into F(λ, β) as F(λ, β) = D 2 log λ + N 2 log β − 1 2 D d=1 log(λ + s d ) − β 2 y ⊤ y − D d=1 h 2 d λ + s d − N 2 log 2π. (14)</formula><p>The derivative of F(λ, β) with respect to β is given by</p><formula xml:id="formula_16">∂F ∂β = N 2β − 1 2 y ⊤ y − D d=1 h 2 d λ + s d ,</formula><p>and we obtain the following equation by setting this derivative to zero,</p><formula xml:id="formula_17">β = N y ⊤ y − D d=1 h 2 d λ+s d .<label>(15)</label></formula><p>Finally, we obtain a one-dimensional function of the log evidence by plugging <ref type="bibr" target="#b14">(15)</ref> into <ref type="bibr" target="#b13">(14)</ref>, which is given by <ref type="figure" target="#fig_0">Figure 2</ref> illustrates the curvature of this log evidence function with respect to log λ. </p><formula xml:id="formula_18">F(λ) = 1 2 D d=1 log λ λ + s d + N 2 log N − N 2 − N 2 log 2π − N 2 log y ⊤ y − D d=1 h 2 d λ + s d .<label>(16)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">New Fixed-point Update Rule</head><p>We now derive a new fixed point update rule and present the sufficient condition for the existence of a fixed point. The stationary points in <ref type="bibr" target="#b15">(16)</ref> with respect to λ satisfy</p><formula xml:id="formula_19">1 2 D d=1 s d λ(λ + s d ) − N 2 D d=1 h 2 d (λ+s d ) 2 y ⊤ y − D d=1 h 2 d λ+s d = 0,<label>(17)</label></formula><p>and we update the fixed-point by maximizing <ref type="formula" target="#formula_1">(16)</ref> as</p><formula xml:id="formula_20">λ = D d=1 s d λ+s d N y ⊤ y− D d=1 h 2 d /(λ+s d ) D d=1 h 2 d (λ+s d ) 2 .<label>(18)</label></formula><p>As illustrated in <ref type="figure" target="#fig_0">Figure 2</ref>, F(λ) in <ref type="formula" target="#formula_1">(16)</ref> is neither convex nor concave as illustrated in the supplementary file. However, we can show the sufficient condition of the existence of the fixed point using the following theorem.</p><p>Theorem 1. Denote the update rule in (18) by f (λ). If y is a binary variable and x n is an L 2 normalized nonnegative vector, then f (λ) has a fixed point.</p><p>Proof. We first show that f (λ) is asymptotically linear in λ as</p><formula xml:id="formula_21">lim λ→∞ f (λ) λ = lim λ→∞ y ⊤ y − d=1 h 2 d λ+s d D d=1 s d λ+s d λN D d=1 h 2 d (λ+s d ) 2 = y ⊤ y D d=1 s d N D d=1 h 2 d = y 2 X 2 F N Xy 2 .</formula><p>Since y is binary and x n is L 2 normalized and nonnegative, we can derive the following two relations,</p><formula xml:id="formula_22">y 2 X 2 F = P N and<label>(19)</label></formula><formula xml:id="formula_23">Xy 2 = n:yn=1</formula><p>x n 2 &gt; n:yn=1 <ref type="figure">Figure 3</ref>. Aitken's delta-squared process. The fixed point update function f (λ) is approximated by green dashed line and its intersection with y = λ becomes the next update point.</p><formula xml:id="formula_24">x 2 n = P,<label>(20)</label></formula><formula xml:id="formula_25">y=f(λ) y=λ λ 0 λ 1 λ 2 λ f(λ 0 ) f(λ 1 )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Fast Bayesian Least Squares</head><p>Input: X ∈ R N ×D and y ∈ R N . Output: Optimal solutions (w, λ). <ref type="bibr" target="#b18">(19)</ref> and <ref type="formula" target="#formula_3">(20)</ref>, it is shown that y 2 X 2 F &lt; N Xy 2 . Obviously, f (0) &gt; 0 and there exists a λ + such that f (λ + ) &lt; λ + . The intermediate value theorem implies the existence of λ * such that f (λ * ) = λ * , where 0 &lt; λ * &lt; λ + as illustrated in <ref type="figure">Figure 3</ref>.</p><formula xml:id="formula_26">Initialize λ // e.g., λ = 1 (U , S) ← eigen-decomposition(XX ⊤ ) s ← diag(S), h ← U ⊤ Xy repeat λ 0 ← λ λ 1 ← UPDATE (λ 0 , s, h, N, y ⊤ y) λ 2 ← UPDATE (λ 1 , s, h, N, y ⊤ y) λ ← λ 0 − (λ1−λ0) 2 (λ2−λ1)−(λ1−λ0) if λ &lt; 0 or λ = ±∞ then λ ← λ 2 end if until |λ − λ 0 | &lt; ǫ // e.g., ǫ = 10 −5 w ← U (S + λI) −1 h where P = N n=1 y n . From</formula><p>The fixed point is unique if f (λ) is concave. Although it is always concave according to our observation, we have no proof yet and leave it as a future work</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Speed Up Algorithm</head><p>We accelerate the fixed point update rule in (18) by using Aitken's delta-squared process <ref type="bibr" target="#b0">[1]</ref>. <ref type="figure">Figure 3</ref> illustrates the Aitken's delta-squared process. Let's focus on the two points (λ 0 , f (λ 0 )) and (λ 1 , f (λ 1 )), and line going through  these two points. The equation of this line is</p><formula xml:id="formula_27">Algorithm 2 λ = UPDATE(λ, s, h, N, y ⊤ y) γ ← D d=1 s d λ+s d β ← N/(y ⊤ y − D d=1 h 2 d λ+s d ) m ⊤ m ← D d=1 h 2 d (λ+s d ) 2 λ ← γ β m ⊤ m return λ</formula><formula xml:id="formula_28">y = λ 1 + (λ − λ 0 ) λ 2 − λ 1 λ 1 − λ 0 ,<label>(21)</label></formula><p>where f (λ 0 ) and f (λ 1 ) are replaced by λ 1 and λ 2 , respectively. The idea behind Aitken's method is to approximate fixed point λ * using the intersection of the line in <ref type="formula" target="#formula_1">(21)</ref> with line y = λ, which is given by</p><formula xml:id="formula_29">λ = λ 0 − (λ 1 − λ 0 ) 2 (λ 2 − λ 1 ) − (λ 1 − λ 0 ) .<label>(22)</label></formula><p>Our fast Bayesian learning algorithm for the regularized least squares problem in (1) is summarized in Algorithm 1. In our algorithm, we first compute the eigen-decomposition of XX ⊤ . This is the most time consuming part but needs to be performed only once since the result can be reused for every label in y. After that, we obtain the regularization parameter λ through an iterative procedure.</p><p>When we apply the Aitken's delta-squared process, we have two potential failure cases as in <ref type="figure" target="#fig_2">Figure 4</ref>(a) and 4(b). The first case often arises if the initial λ 0 is far from the fixed point λ * , and the second case occurs when the approximating line in (21) is parallel to y = λ. Fortunately, these failures rarely happen in practice and can be handled easily by skipping the procedure in <ref type="bibr" target="#b21">(22)</ref> and updating λ with λ 2 . <ref type="figure" target="#fig_3">Figure 5</ref> demonstrates the relative convergence rates of three different techniques-Aitken's delta-squared process in Algorithm 1, fixed point update rules in <ref type="bibr" target="#b7">(8)</ref>, and EM update method, where the Aitken's delta-squared process is significantly faster than others for convergence. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>We present the details of our experiment setting and the performance of our algorithm compared to the state-of-theart techniques in 12 visual recognition benchmark datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Datasets and Image Representation</head><p>The benchmark datasets involve various visual recognition tasks such as object recognition, photo annotation, scene recognition, fine grained recognition, visual attribute detection, and action recognition. <ref type="table" target="#tab_0">Table 1</ref> presents the characteristics of the datasets. In our experiment, we followed the given train and test split and evaluation measure of each dataset. For the datasets with bounding box annotations such as CUB200-2011, UIUC object attribute, Human attribute, and Stanford 40 actions, we enlarged the bounding boxes by 150% to consider neighborhood context as suggested in <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b1">2]</ref>.</p><p>For deep learning representations, we selected 4 pretrained CNNs from the Caffe Model Zoo: GoogLeNet <ref type="bibr" target="#b30">[31]</ref>, VGG19 <ref type="bibr" target="#b24">[25]</ref>, and AlexNet [7] trained on ImageNet, and GoogLeNet trained on Places <ref type="bibr" target="#b30">[31]</ref>. As generic image representations, we used the 4096 dimensional activations of the first fully connected layer in VGG19 and AlexNet and the 1024 dimensional vector obtained from the global average pooling layer located right before the final softmax layer in GoogLeNet.</p><p>Our implementation is in Matlab2011a, and all experiments were conducted on a quad-core Intel(R) core(TM) i7-3820 @ 3.60GHz processor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Bayesian LS-SVM vs. SVM</head><p>We first compare the performance of our Bayesian LS-SVM with the standard SVM when they are applied to deep CNN features for visual recognition problems. We used only a single image scale 256 × 256 in this experiment. LI-BLINEAR <ref type="bibr" target="#b9">[10]</ref> package is used for SVM training and the regularization parameters are selected by grid search with cross validations. <ref type="table" target="#tab_1">Table 2</ref> presents the complete results of our experiment. Bayesian LS-SVM is competitive to SVM in terms of prediction accuracy even with significantly reduced training time. Training SVM is getting slower than Bayesian LS-SVM as the number of classes increases so it is particularly slow in Caltech 256 and SUN 397 datasets.</p><p>Another notable observation in <ref type="table" target="#tab_1">Table 2</ref> is that the order of prediction accuracy is highly correlated to the evidence. This means that the selected model by Bayesian LS-SVM produces reliable testing accuracy and a proper deep learning image representation is obtained without time consuming grid search and cross validation. Note that cross validations in LS-SVM and SVM play the same role, but are less reliable and slower than our Bayesian evidence framework. The capability to select the appropriate CNN model and the corresponding regularization parameter is one of the most important properties of our algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Comparison with Other Methods</head><p>We now show that our Bayesian LS-SVM identifies a combination of multiple CNNs to improve accuracy without grid search and cross validation. For each task, we select a subset of 4 pre-trained CNNs in a greedy manner; we add CNNs to our selection, one by one, until the evidence does not increase. Our algorithm is compared with DeCAF <ref type="bibr" target="#b6">[7]</ref>, Zeiler <ref type="bibr" target="#b33">[34]</ref>, INRIA <ref type="bibr" target="#b20">[21]</ref>, KTH-S <ref type="bibr" target="#b22">[23]</ref>, KTH-FT <ref type="bibr" target="#b1">[2]</ref>, VGG <ref type="bibr" target="#b24">[25]</ref>, Zhang <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b35">36]</ref>, and TUBFI <ref type="bibr" target="#b2">[3]</ref>. In addition, our ensembles identified by greedy evidence maximization are compared with the oracle combinations-the ones with the highest accuracy in test set found by exhaustive search-and the best combinations found by exhaustive evidence maximization. <ref type="table">Table 3</ref> presents that our ensembles approach achieves the best performance in most of the 12 tasks. The identified ensembles by the greedy approach are consistent with the selections by exhaustive evidence maximization and even oracle selections 1 made by testing accuracy maximization. Note that our network selections are natural and reasonable; GoogLeNet-ImageNet and VGG19 are selected frequently while GoogLeNet-Place is preferred to GoogLeNet-ImageNet in MIT Indoor and SUN-397 since the datasets are constructed for scene recognition. It turns out that the proposed algorithm tends to choose the networks with higher accuracies in the target task even though it makes selections based only on the evidence in a greedy manner. An interesting observation is that our result is less  <ref type="bibr" target="#b8">[9]</ref> object recognition 5717 5823 20 1.5 mean AP Caltech 101 <ref type="bibr" target="#b11">[12]</ref> object recognition 3060 6086 102 1 mean Acc. Caltech 256 <ref type="bibr" target="#b13">[14]</ref> object recognition 15420 15187 257 1 mean Acc. ImageCLEF 2011 <ref type="bibr" target="#b19">[20]</ref> photo annotation 8000 10000 99 11.9 mean AP MIT Indoor Scene <ref type="bibr" target="#b21">[22]</ref> scene recognition 5360 1340 67 1 mean Acc. SUN 397 Scene <ref type="bibr" target="#b31">[32]</ref> scene recognition 19850 19850 397 1 mean Acc. CUB 200-2011 <ref type="bibr" target="#b29">[30]</ref> fine-grained recognition 5994 5794 200 1 √ mean Acc. Oxford Flowers <ref type="bibr" target="#b17">[18]</ref> fine-grained recognition 2040 6149 200 1 mean Acc. UIUC object attributes <ref type="bibr" target="#b10">[11]</ref> attribute detection 6340 8999 64 7.1 √ mean AUC Human attributes <ref type="bibr" target="#b4">[5]</ref> attribute detection 4013 4022 9</p><p>1.8 √ mean AP Stanford 40 actions <ref type="bibr" target="#b32">[33]</ref> action recognition 4000 5532 40 1 √ mean AP consistent with the selections by oracle and exhaustive evidence maximization in Stanford 40 Actions dataset, where GoogLeNet-Place seems to provide complementary infor-mation even with its low accuracy and is helpful to improve recognition performance. It is probably because actions are frequently performed at typical places, e.g., a fair portion of <ref type="table">Table 3</ref>. Comparison to existing methods in the 12 benchmark datasets. The best ensembles identified by maximizing evidence through exhaustive search mostly coincide with the oracle combinations-the ones with the highest accuracy in test set, which is also found by exhaustive search. The ensembles identified by our greedy search are very similar to the ones by these exhaustive search methods, and our algorithm consequently performs best in many tested datasets. We used three scales {256, 384, 512} as done in <ref type="bibr" target="#b24">[25]</ref>, where we simply averaged the prediction scores from three scales. images in brushing teeth class are taken from bathrooms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We described a simple and efficient technique to transfer deep CNN models pre-trained on specific image classification tasks to another tasks. Our approach is based on Bayesian LS-SVM, which combines Bayesian evidence framework and SVM with a least squares loss. In addition, we presented a faster fixed point update rule for evidence maximization through Aitken's delta-squared process. Our fast Bayesian LS-SVM demonstrated competitive results compared to the standard SVM by selecting a deep CNN model in 12 popular visual recognition problems. We also achieved the state-of-the-art performance by identifying a good ensemble of the candidate models through our Bayesian LS-SVM framework.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Plot of the log evidence F (λ) with respect to log λ. Note that F (λ) is neither convex nor concave.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Two failure cases of Aitken's delta-squared process. (left) The first case arises if initial λ0 is far from the fixed point λ ⋆ , which results in λ &lt; 0. (right) The second case occurs when approximating line (dashed green) is parallel to y = λ, where λ = ±∞.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Comparison between Aitken's delta-squared process, fixed point update rules, and EM update rules on PASCAL VOC 2012 dataset (class = aeroplane). Aitken's delta-squared process significantly faster than other methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 .</head><label>1</label><figDesc>Characteristics of the 12 datasets. N1: number of training data, N2: number of test data, K: number of classes, L: average number of labels per image, AP: average precision, Acc.: accuracy, AUC: area under the ROC curve.</figDesc><table>Dataset 
Task 
N1 
N2 
K 
L Box Measure 

PASCAL VOC 2007 [8] 
object recognition 
5011 
4952 
20 
1.5 
mean AP 
PASCAL VOC 2012 </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 .</head><label>2</label><figDesc></figDesc><table>Comparisons between Bayesian LS-SVM and SVM. Without time consuming cross validation procedure, Bayesian LS-SVM 
achieves prediction accuracy competitive to SVM. In addition, Bayesian LS-SVM selects the proper CNN for each task by using the 
evidence, which is denoted by bold-faced numbers. Best accuracy in LS-SVM and SVM denotes the maximum achievable accuracy in test 
dataset using all available learned models. Note that the selected model by Bayesian evidence framework or cross validation may not be 
the best one in testing. The following sets of regularization parameters are tested for cross validation in LS-SVM and SVM, respectively: 
{2 −10 , 2 −9 , . . . , 1, . . . , 2 9 , 2 10 } and {0.01, 0.05, 0.1, 0.5, 1, 2, 5, 10}. (G I : GoogLeNet-ImageNet, Gp: GoogLeNet-Place, V: VGG19, and 
A: AlexNet) 

LS-SVM 
SVM 
LS-SVM 
SVM 

Bayesian 
CV (5 folds) 
CV (5 folds) 
Bayesian 
CV (5 folds) 
CV (5 folds) 
CNN Best Acc. 
Evidence 
Time Acc. Time Best Acc. 
Time 
Best Acc. 
Evidence 
Time Acc. Time Best Acc. 
Time 
PASCAL VOC 2007 [8] 
SUN-397 [32] 
G I 
85.3 85.2 
46.9 ×10 3 
1.1 85.2 
8.4 85.0 84.7 
122.4 
48.1 47.0 
12.8 ×10 6 
3.1 48.1 
36.5 54.2 54.2 
8739.6 
G P 
74.1 73.8 
38.6 ×10 3 
1.0 74.0 
8.1 74.1 73.9 
144.3 
61.1 60.1 
13.2 ×10 6 
2.9 61.1 
34.4 63.3 63.3 
8589.4 
V 
85.9 85.8 48.0 ×10 3 
41.9 85.8 172.2 85.9 85.8 
257.5 
55.0 53.7 
12.9 ×10 6 
57.4 54.9 419.8 57.1 57.1 20254.0 
A 
75.2 75.0 
32.5 ×10 3 
41.7 75.0 160.4 75.3 75.2 
211.1 
45.4 44.9 
12.7 ×10 6 
50.8 45.4 419.0 48.6 48.6 10781.8 
PASCAL VOC 2012 [9] 
CUB-200 [30] 
G I 
84.4 84.3 
51.3 ×10 3 
1.2 84.3 
8.6 83.9 83.7 
140.8 
65.2 64.3 
15.6 ×10 5 
1.3 64.1 
11.0 67.6 56.5 
1201.9 
G P 
73.2 72.9 
40.6 ×10 3 
1.1 73.1 
8.4 73.2 73.1 
170.7 
16.4 13.6 
14.9 ×10 5 
1.5 15.0 
11.1 16.8 11.1 
1664.6 
V 
85.2 85.1 52.9 ×10 3 
42.7 85.2 161.5 85.6 85.4 
295.9 
69.2 68.6 
15.8 ×10 5 
44.1 61.5 259.2 71.1 59.4 
2776.2 
A 
74.1 73.9 
34.3 ×10 3 
42.7 74.0 161.8 74.4 74.3 
160.7 
59.0 58.5 
15.5 ×10 5 
45.3 46.6 257.9 61.4 51.6 
1645.5 
Caltech 101 [12] 
Oxford Flowers [18] 
G I 
90.6 90.0 
37.8 ×10 4 
1.0 89.6 
6.0 91.4 85.1 
325.0 
85.5 84.7 
21.8 ×10 4 
0.9 82.0 
5.5 87.4 72.0 
198.8 
G P 
57.0 54.3 
30.6 ×10 4 
0.9 55.1 
5.9 57.2 41.8 
390.3 
55.6 51.7 
19.4 ×10 4 
0.9 51.8 
5.5 57.1 32.8 
234.7 
V 
92.2 92.1 40.9 ×10 4 
31.5 88.8 142.7 92.2 86.8 
729.4 
87.5 87.1 
22.5 ×10 4 
26.9 82.1 142.2 87.6 73.4 
520.9 
A 
89.3 89.2 
37.3 ×10 4 
32.0 83.4 146.9 90.0 83.5 
595.3 
87.6 87.6 
22.9 ×10 4 
27.3 81.8 146.7 88.3 77.1 
271.3 
Caltech 256 [14] 
UIUC Attributes [11] 
G I 
77.8 77.2 
59.9 ×10 5 
2.3 77.8 
21.8 81.2 81.2 4060.4 
91.5 90.3 
13.5 ×10 4 
1.4 90.9 
8.0 91.3 90.6 
605.5 
G P 
44.9 42.6 
55.9 ×10 5 
2.2 44.9 
21.2 48.6 48.6 4991.8 
87.8 86.6 
10.5 ×10 4 
1.3 87.1 
7.4 88.0 87.6 
726.0 
V 
82.0 81.1 62.3 ×10 5 
52.5 81.7 339.7 82.7 82.7 9653.1 
92.5 91.1 
14.4 ×10 4 
43.8 92.0 186.3 92.2 91.7 
1285.4 
A 
69.7 68.9 
58.6 ×10 5 
52.9 69.7 336.9 72.3 72.3 5348.6 
91.4 89.9 
12.9 ×10 4 
44.1 91.0 191.2 90.8 90.5 
683.7 
ImageCLEF [20] 
Human Attributes [5] 
G I 
49.1 48.9 
20.5 ×10 4 
1.5 48.8 
37.0 47.7 47.4 1218.6 
76.0 75.8 
-74.8 ×10 2 
1.0 75.8 
5.0 74.2 74.1 
70.6 
G P 
47.5 47.1 
20.8 ×10 4 
1.4 47.1 
36.9 47.1 46.7 1410.5 
58.7 58.4 -103.1 ×10 2 
1.0 58.0 
4.8 56.9 56.5 
85.5 
V 
50.7 50.3 21.3 ×10 4 
45.9 50.4 248.5 50.4 50.1 2531.2 
75.4 75.1 
-76.0 ×10 2 
40.3 75.2 124.2 73.1 72.8 
131.9 
A 
44.8 44.6 
18.7 ×10 4 
46.1 44.6 245.9 44.4 44.1 2140.0 
71.9 71.3 
-84.4 ×10 2 
40.7 71.7 121.2 70.0 69.9 
63.3 
MIT Indoor [22] 
Stanford 40 Action [33] 
G I 
66.7 66.0 
30.1 ×10 4 
1.2 66.7 
5.8 69.4 69.2 
400.9 
70.2 69.8 
100.4 ×10 3 
1.0 69.6 
11.6 69.8 69.6 
211.7 
G P 
80.0 79.9 35.2 ×10 4 
1.1 80.0 
5.8 81.1 80.4 
402.5 
48.3 47.6 
86.5 ×10 3 
1.1 47.9 
11.4 48.2 47.7 
246.2 
V 
73.2 73.1 
31.1 ×10 4 
42.6 73.2 186.8 74.7 74.7 
895.5 
75.4 75.2 109.3 ×10 3 
41.1 75.1 142.9 75.8 75.3 
418.7 
A 
62.0 61.1 
28.6 ×10 4 
42.2 60.5 187.4 63.1 63.1 
460.9 
58.0 57.7 
89.6 ×10 3 
41.5 57.5 156.5 57.4 57.1 
206.8 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">This option is practically impossible since it requires evaluation with test dataset using all available models for model selection.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">On Bernoulli&apos;s numerical solution of algebraic equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Aitken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the Royal Society of Edinburgh</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="1927" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">From generic to specific deep representations for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Azizpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Razavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sulivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Maki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Carlsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">The joint submission of the TU Berlin and Fraunhofer FIRST (TUBFI) to the ImageCLEF2011 photo annotation task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Samek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kloft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-R</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kawanabe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<title level="m">Neural Networks for Pattern Recognition</title>
		<imprint>
			<publisher>Clarendon press Oxford</publisher>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Describing people: A poselet-based approach to attribute classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Return of the devil in the details: Delving deep into convolutional nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chatfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">DeCAF: a deep convolutional activation feature for generic visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">The PASCAL Visual Object Classes Challenge 2007 (VOC 2007) Results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">The PASCAL Visual Object Classes Challenge 2012 (VOC 2012) Results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">LIBLINEAR: A library for large linear classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1871" to="1874" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Describing objects by their attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Endres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="page" from="59" to="70" />
		</imprint>
		<respStmt>
			<orgName>CVIU</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Caltech-256 object category dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Griffin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Holub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">ImageNet classification wit deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">25</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Bayesian interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J C</forename><surname>Mackay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning multi-domain convolutional neural networks for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Automated flower classification over a large number of classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-E</forename><surname>Nilsback</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Indian Conference on Computer Vision, Graphics and Image Processing</title>
		<meeting>the Indian Conference on Computer Vision, Graphics and Image Processing</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning deconvolution network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The CLEF 2011 photo annotation and concept-based retrieval tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nowak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nagel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liebetrau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CLEF Workshop Notebook Paper</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning and transferring mid-level image representations using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Recognizing indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Quattoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torrabla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">CNN features off-the-shelf: An astounding baseline for recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Razavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Azizpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Carlsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">OverFeat: Integrated recognition, localization and detection using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Least squares support vector machine classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A K</forename><surname>Suykens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vandewalle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Processing Letters</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="293" to="300" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Benchmarking least squares support vector machines classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Van Gestel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A K S B</forename><surname>Baesems</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Viaene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vanthienen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dedene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">De</forename><surname>Moor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vandewalle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="5" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Bayesian framework for least-squares support vector machine classifiers, gaussian processes, and kernel fisher discriminant analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Van Gestel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A K</forename><surname>Suykens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lanckrie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lambrechts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">D</forename><surname>Moor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vandewalle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">The Caltech-UCSD Birds-200-2011 dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">A GPU implementation of GoogLeNet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<pubPlace>Princeton University</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">SUN database: Large-scale scene recognition from abbey to zoo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">A</forename><surname>Ehinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torrabla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Action recognition by learning bases of action attributes and parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">PANDA: Pose aligned networks for deep attribute modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Partbased R-CNNs for fine-grained category detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">SVM vs regularized least squares classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
