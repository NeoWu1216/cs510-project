<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Fast Algorithms for Linear and Kernel SVM+</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Computer Vision Laboratory</orgName>
								<orgName type="institution">ETH Zürich</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengxin</forename><surname>Dai</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Computer Vision Laboratory</orgName>
								<orgName type="institution">ETH Zürich</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">University of Adelaide</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Xu</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">School of Electrical and Information Engineering</orgName>
								<orgName type="institution">University of Sydney</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Computer Vision Laboratory</orgName>
								<orgName type="institution">ETH Zürich</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">ESAT/PSI</orgName>
								<orgName type="institution">VISICS</orgName>
								<address>
									<settlement>Leuven</settlement>
									<region>KU</region>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Fast Algorithms for Linear and Kernel SVM+</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:40+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The SVM+ approach has shown excellent performance in visual recognition tasks for exploiting privileged information in the training data. In this paper, we propose two efficient algorithms for solving the linear and kernel SVM+, respectively. For linear SVM+, we absorb the bias term into the weight vector, and formulate a new optimization problem with simpler constraints in the dual form. Then, we develop an efficient dual coordinate descent algorithm to solve the new optimization problem. For kernel SVM+, we further apply the ℓ 2 -loss, which leads to a simpler optimization problem in the dual form with only half of dual variables when compared with the dual form of the original SVM+ method. More interestingly, we show that our new dual problem can be efficiently solved by using the SMO algorithm of the one-class SVM problem. Comprehensive experiments on three datasets clearly demonstrate that our proposed algorithms achieve significant speed-up than the state-of-the-art solvers for linear and kernel SVM+.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Many computer vision tasks contain privileged information that only exists in the training data, and not available during the test stage. For example, the training images of many datasets for image recognition are annotated with privileged information such as attributes, object bounding boxes, textual descriptions, depth information. Although the raw test images in the real-world applications are not associated with such information, it has been demonstrated that such information is useful for learning classifiers with better recognition performance <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b34">35]</ref>.</p><p>This problem is known as the Learning Using Privileged Information (LUPI) problem <ref type="bibr" target="#b31">[32]</ref>. Different from the traditional learning paradigm, where the training data and the test data have the same representation, LUPI leverages training data containing additional information that is only available during the training process, not in the testing pro-cess. Such additional information in the training data is referred to as privileged information or hidden information.</p><p>Following the LUPI paradigm, Vapnik and Vashist <ref type="bibr" target="#b31">[32]</ref> proposed an SVM-like algorithm called SVM+, in which they replace the slack variables in the standard SVM with a slack function defined in the privileged feature space. Through the slack function, the additional privileged information is used to model the loss function, which guides the hyperplane learning in the main feature space. In contrast, the slack variables in the standard SVM are only constrained to non-negative values, which is often less effective than the slack function in SVM+.</p><p>Although SVM+ can be formulated as a quadratic programming problem in the dual form similarly as the standard SVM, it is still non-trivial to efficiently solve it, because the introduction of the slack function leads to more constraints, and makes the number of dual variables doubled. While an SMO-style algorithm was developed in <ref type="bibr" target="#b25">[26]</ref>, the working set selection method is complicated, and the algorithm is also slow in practice. Moreover, it is unclear how to apply it to linear SVM+ without calculating the kernel matrix, which is becoming more crucial, due to rapidly increasing data in real-world applications.</p><p>In this paper, we propose two efficient algorithms for solving linear SVM+ and kernel SVM+, respectively. In particular, inspired by linear SVM <ref type="bibr" target="#b15">[16]</ref>, we augment the feature vector with an additional constant element, and absorb the bias term in the decision function into the weight vector, which leads to a dual form with simpler constraints. The new linear SVM+ formulation can be efficiently solved by using the dual coordinate descent method, in a similar way to linear SVM. For kernel SVM+, we further propose to apply the ℓ 2 -loss, which leads to a simpler optimization problem in the dual form with only half of dual variables when compared with the original SVM+. More interestingly, we show that the resultant dual form is in analogy to one-class SVM, which can thus be efficiently solved using the efficient sequential minimal optimization (SMO) algorithm <ref type="bibr" target="#b27">[28]</ref> with the existing state-of-the-art SVM solvers such as LIBSVM <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b0">1]</ref>.</p><p>We implement our algorithms for linear and kernel SVM+ based on LIBLINEAR <ref type="bibr" target="#b8">[9]</ref> and LIBSVM <ref type="bibr" target="#b0">[1]</ref>, respectively. We conduct extensive experiments on three tasks: digit recognition on the MNIST+ dataset <ref type="bibr" target="#b31">[32]</ref>, scene recognition on the Scene-15 dataset <ref type="bibr" target="#b21">[22]</ref>, and web image retrieval on the NUS-WIDE dataset <ref type="bibr" target="#b3">[4]</ref>. The results demonstrate that our proposed algorithms achieve significant speed-up than the state-of-the-art solvers for solving the linear and kernel SVM+ problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Learning Using Privileged Information (LUPI), as a new learning paradigm, was first proposed by Vapnik and Vashist <ref type="bibr" target="#b31">[32]</ref>, therein with the SVM+ algorithm. After that, many variants of SVM+ have been proposed for solving different tasks <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b22">23]</ref>. In <ref type="bibr" target="#b23">[24]</ref>, Liang and Cherkassky developed a multi-task learning approach based on SVM+. In <ref type="bibr" target="#b16">[17]</ref>, a multi-task multi-class extension of SVM+ was proposed. Fouad et al. <ref type="bibr" target="#b11">[12]</ref> designed a two-step approach for metric learning, and Xu et al. <ref type="bibr" target="#b33">[34]</ref> formulated a convex formulation for metric learning using privileged information based on the information theory metric learning (ITML) method. Sharmanska et al. <ref type="bibr" target="#b28">[29]</ref> proposed the Rank Transfer method for utilizing privileged information, and demonstrated the effectiveness of privileged information in various computer vision tasks. In <ref type="bibr" target="#b22">[23]</ref>, Li et al. extended SVM+ to the multi-instance learning scenario for image retrieval and object recognition by learning using web data. In <ref type="bibr" target="#b32">[33]</ref>, Wang et al. proposed a classifier learning algorithm for utilizing privileged information. In <ref type="bibr" target="#b10">[11]</ref>, Feyereisl et al. extended structure SVM to exploiting privileged information for object localization.</p><p>Vapnik and Vashist also showed that SVM+ has a faster convergence rate than the standard SVM method under certain conditions <ref type="bibr" target="#b31">[32]</ref>. A more thorough theoretic study of SVM+ can be found in <ref type="bibr" target="#b26">[27]</ref>. In <ref type="bibr" target="#b30">[31]</ref>, two mechanisms for LUPI are further explained. Lapin et al. <ref type="bibr" target="#b20">[21]</ref> also discovered the relationship between SVM+ and weighted SVM, while Li et al. <ref type="bibr" target="#b22">[23]</ref> discussed the connection of the unsupervised domain adaptation method and SVM+.</p><p>One closely related work is <ref type="bibr" target="#b25">[26]</ref>, in which an SMO-style algorithm was developed for the SVM+ problem. However, as the two sets of dual variables introduced for the constraints related to the main and privileged features in the primal form are tangled together in the constraints of dual problem, it is non-trivial to design the working set selection method for the SMO algorithm, and leads to a complicated algorithm that is less efficient in practice. Moreover, it is also unclear that how to apply the the SMO-style algorithm for solving the linear SVM+ problem without calculating the kernel matrix. In contrast, in this paper, by absorbing the bias term into the weight vector of the SVM+ classifier, we arrive at an optimization problem with simpler constraints in the dual form. Thus, the conventional dual coordinate descent algorithm can be applied to efficiently solve linear SVM+. Moreover, for the SVM+ form, we further apply the ℓ 2 -loss and obtain a smaller dual problem with only a half of dual variables of the original SVM+ formulation. The new dual form shares a similar formulation with one-class SVM, and thus can be solved by using the SMO algorithm implemented in the existing SVM solvers such as LIBSVM <ref type="bibr" target="#b0">[1]</ref> without developing any new specific SMO algorithm. We demonstrate that our proposed two algorithms are more efficient than the SMO-style algorithm in <ref type="bibr" target="#b25">[26]</ref>.</p><p>We are also aware that there are some works proposed for solving the tasks in which training and test data contains different information, which are less related to SVM+ <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b34">35]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Learning Using Privileged Information</head><p>In the following, we denote a vector (resp., matrix) with a lower (resp., upper) case letter in boldface. For example, a represents a vector, and A a matrix. The transpose of a vector or matrix is denoted by using the superscript ′ . We use 0 n , 1 n ∈ R n to represent the column vector with n zeros and ones, respectively. We also simply use 0 and 1 instead of 0 n and 1 n , when the dimensionality is obvious. Moreover, a • b (resp., A • B) denotes the element-wise product between two vectors (resp., matrices).</p><p>In the Learning Using Privileged Information (LUPI) paradigm <ref type="bibr" target="#b31">[32]</ref>, the training samples contain additional information that is not available in the test stage. Formally, we represent the training data as {(x i ,x i , y i )|i = 1, . . . , n}, where n is the total number of training samples, x i ∈ R D is the main feature vector of the i-th training sample with D being the feature dimensionality,x ∈ RD is the corresponding privileged feature vector withD being its dimension. The goal of LUPI is to learn a decision function f (x) for classifying any test sample x ∈ R D in the main feature space.</p><p>In <ref type="bibr" target="#b31">[32]</ref>, an SVM based approach called SVM+ was proposed. Similar to SVM, the decision function in SVM+ is represented as f (x) = w ′ ϕ(x) + b, where ϕ(·) is a feature mapping induced by the kernel on training data, w is the weight vector, and b is the bias term. The objective of SVM+ can be written as,</p><formula xml:id="formula_0">miñ w,b,w,b 1 2 ( ∥w∥ 2 + γ∥w∥ 2 ) + C n ∑ i=1 ξ(w,b, ψ(x i )) (1) s.t. y i (w ′ ϕ(x i ) + b) ≥ 1 − ξ(w,b, ψ(x i )), (2) ξ(w,b, ψ(x i )) ≥ 0,<label>(3)</label></formula><p>where ξ(w,b, ψ(x i )) =w ′ ψ(x) +b is the slack function defined in the privileged feature space, ψ(·) is the feature mapping function for the privileged features, andw andb are respectively the weight vector and bias term for the slack function. It can be observed that SVM+ replaces the slack variables in the standard SVM formulation with the slack function ξ(w,b, ψ(x i )). As a result, the loss occurred by the training samples x i can be regularized by the privileged informationx i , therefore the hyperplane of SVM+ classifier can be tuned by using the privileged information during the training process. Let us introduce two sets of dual valuables {α i | n i=1 } and {ζ i | n i=1 } for the constraints in <ref type="formula">(2)</ref> and <ref type="formula" target="#formula_0">(3)</ref>, respectively, and also denote two vectors α = [α 1 , . . . , α n ] ′ ∈ R n and ζ = [ζ 1 , . . . , ζ n ] ′ ∈ R n . We arrive at the dual form of SVM+,</p><formula xml:id="formula_1">min (ζ,α)∈A 1 2 (α • y) ′ K(α • y) − 1 ′ α<label>(4)</label></formula><formula xml:id="formula_2">+ 1 2γ (α + ζ − C1) ′K (α + ζ − C1), where A = {(ζ, α)|y ′ α = 0, 1 ′ (α + ζ − C1) = 0, α ≥ 0, ζ ≥ 0}</formula><p>is the feasible set of (α, ζ), K ∈ R n×n is the kernel matrix based on the main training features with each element being</p><formula xml:id="formula_3">K ij = ϕ(x i ) ′ ϕ(x j ), andK ∈ R n×n is the kernel matrix based on the privileged features with each el- ement beingK ij = ψ(x i ) ′ ψ(x j )</formula><p>. After solving the above problem, the weight vectors w andw can be reconstructed based on the Karush-Kuhn-Tucker (KKT) conditions as,</p><formula xml:id="formula_4">w = n ∑ i=1 α i y i ϕ(x i ),<label>(5)</label></formula><formula xml:id="formula_5">w = 1 γ n ∑ i=1 (α i + ζ i − C)ψ(x i ).<label>(6)</label></formula><p>Although the dual problem in (4) is a quadratic programming problem, solving it with the existing optimization toolboxes is certainly undesirable for real-world visual recognition tasks. However, it is non-trivial to develop efficient algorithm like the SMO algorithm for solving the standard SVM. The main difficulty comes from the constraint 1 ′ (α + ζ − C1) = 0, in which two sets of dual variables are tangled together, which makes the working set selection method become complicated <ref type="bibr" target="#b25">[26]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Dual Coordinate Descent Algorithm for Solving Linear SVM+</head><p>In this section, we present the dual coordinate descent method for solving the linear SVM+. In particular, we absorb the bias term into the weight vector w by appending a constant entry of 1 to each feature vector, i.e.,</p><formula xml:id="formula_6">x ← [x ′ , 1] ′ , and w ← [w ′ , b] ′ for the main features, andx ← [x ′ , 1] ′ , andw ← [w ′ ,b] ′ for privileged information.</formula><p>For ease of presentation, we still use w and x (resp.,w,x) to represent the weight vector and feature vector for the main (resp., privileged) features. Now, the objective of linear SVM+ can be reformulated as follows,</p><formula xml:id="formula_7">miñ w,w 1 2 ( ∥w∥ 2 + γ∥w∥ 2 ) + C n ∑ i=1w ′x (7) s.t. y i w ′ x i ≥ 1 −w ′x , w ′x i ≥ 0,</formula><p>and its dual form can be written as,</p><formula xml:id="formula_8">min (ζ,α)∈A 1 2 (α • y) ′ K(α • y) − 1 ′ α (8) + 1 2γ (α + ζ − C1) ′K (α + ζ − C1), where the feasible set becomes A = {(ζ, α)|α ≥ 0, ζ ≥ 0}, K ∈ R n×n is the linear kernel matrix of main features defined as K ij = x ′ i x j , andK ∈ R n×n is the linear ker- nel matrix of privileged features defined asK ij =x ′ ix ′ j .</formula><p>Compared to the dual form of the original SVM+ formulation (4), the objective function remains the same, but we do not have the constraints <ref type="formula">(8)</ref>, after absorbing of the bias terms in <ref type="bibr" target="#b6">(7)</ref>. The weight vectors w andw can be represented using the dual variables in the same way as in <ref type="formula" target="#formula_4">(5)</ref> and <ref type="formula" target="#formula_5">(6)</ref> without using the feature mapping functions (i.e., we can set ϕ( <ref type="formula">(8)</ref> can be rewritten in a more concise form as,</p><formula xml:id="formula_9">y ′ α = 0, 1 ′ (α + ζ − C1) = 0 in</formula><formula xml:id="formula_10">x i ) = x i and ψ(x i ) =x i . Let us define Q = [ K • (yy ′ ) + 1 γK 1 γK 1 γK 1 γK ] ∈ R 2n×2n , β = [α ′ , ζ ′ ] ′ ∈ R 2n , e = [(1 + C γK 1) ′ , ( C γK 1) ′ ] ′ ∈ R 2n , the problem in</formula><formula xml:id="formula_11">min β 1 2 β ′ Qβ − e ′ β<label>(9)</label></formula><formula xml:id="formula_12">s.t. β ≥ 0.</formula><p>Now we develop a coordinate descent algorithm for the problem in (9) similarly as that for linear SVM <ref type="bibr" target="#b15">[16]</ref>. In the dual coordinate descent algorithm, we iteratively update one entry of β each time. This process is repeated until the stopping criterion is reached.</p><p>In particular, let us denote f (β) = 1 2 β ′ Qβ−e ′ β, and we propose to update the i-th entry as β i ← β i + d, where d is a scalar variable that needs to be solved. By defining δ (i) = [0, . . . , 0, 1, 0, . . . , 0] as the vector with all zeros except the i-th entry being one, the subproblem for solving d can be formulated as</p><formula xml:id="formula_13">min d f (β + dδ (i) ) s.t. β + dδ (i) ≥ 0,<label>(10)</label></formula><p>which has an analytic solution as follows,</p><formula xml:id="formula_14">d = max(−β i , − ∇ i f (β) Q ii )<label>(11)</label></formula><p>where Q ii is the (i, i)-th entry of the matrix Q in <ref type="formula" target="#formula_11">(9)</ref>, and</p><formula xml:id="formula_15">∇ i f (β) is the gradient of f (β) w.r.t. β i .</formula><p>Now we discuss how to efficiently calculate the righthand side of <ref type="bibr" target="#b10">(11)</ref>. The scalar Q ii can be pre-computed efficiently, so the major problem is the calculation of ∇ i f (β). In particular, the gradient of f (β) can be written as ∇f (β) = Qβ − e. Then, for any given index i, the gradient of f (β) w.r.t. β i can be calculate as,</p><formula xml:id="formula_16">∇ i f (β) = (Qβ) i − e i = 2n ∑ j=1 Q ij β j − e i ,<label>(12)</label></formula><p>where Q ij is the (i, j)-th element of the matrix Q, and e i is the i-th element of the vector e.</p><p>We discuss the detailed calculations in two cases. Let us denote a matrix H = K • yy ′ ∈ R n×n . For ∀i = 1, . . . , n, we have <ref type="formula" target="#formula_4">(5)</ref> and <ref type="formula" target="#formula_5">(6)</ref>, the gradient of f w.r.t. β i in <ref type="bibr" target="#b11">(12)</ref> can be written as,</p><formula xml:id="formula_17">2n ∑ j=1 Q ij β j = n ∑ j=1 H ij α j + 1 γ n ∑ j=1K ij α j + 1 γ n ∑ j=1K ij ζ j where H ij is the (i, j)-th element of the matrix H. We also have e i = 1 + C γ ∑ n j=1K ij . By applying the KKT condi- tions w.r.t. w andw in</formula><formula xml:id="formula_18">∇ i f (β) i = y i w ′ x i − 1 +w ′x i , ∀1 ≤ i ≤ n<label>(13)</label></formula><p>which takes O(D +D) time complexity, and it can be further speed-up if the feature vectors x andx are sparse. Similarly, for ∀i = n + 1, . . . , 2n, we have</p><formula xml:id="formula_19">2n ∑ j=1 Q ij β j = 1 γ n ∑ j=1K ij α j + 1 γ n ∑ j=1K ij ζ j ,</formula><p>and e i = C γ ∑ n j=1K ij . By using the KKT condition forw in <ref type="bibr" target="#b5">(6)</ref>, the gradient in <ref type="bibr" target="#b11">(12)</ref> can be written as,</p><formula xml:id="formula_20">∇ i f (β) i =w ′x i , ∀n + 1 ≤ i ≤ 2n,<label>(14)</label></formula><p>which takes O(D) time complexity only, and can also be further speed-up if the feature vectorx is sparse. This completes the calculation of (11). After solving d, we update the i-th dual variable β i by using β i ← β i + d. Considering only one dual variable is modified each time, the weight vectors can be updated efficiently at each iteration. Note that we have α i = β i when 1 ≤ i ≤ n, and ζ i−n = β i , when n + 1 ≤ i ≤ n. Based on (5) and <ref type="formula" target="#formula_5">(6)</ref>, the updating rules for w andw can be written as</p><formula xml:id="formula_21">w ← w + dy i x i , if 1 ≤ i ≤ n<label>(15)</label></formula><formula xml:id="formula_22">w ←w + 1 γ dx i , if 1 ≤ i ≤ 2n<label>(16)</label></formula><p>Algorithm 1 Dual coordinate descent algorithm for solving the linear SVM+ problem in <ref type="formula">(7)</ref> Input:</p><formula xml:id="formula_23">{(x i ,x i , y i )| n i=1 }, C, and γ. 1: Initialize w = 0, andw = − C γ ∑ n i=1x i . 2: Set Q ii = x ′ i x i for 1 ≤ i ≤ n, and Q ii =x ′ ix i for n + 1 ≤ i ≤ 2n. 3: repeat 4:</formula><p>Randomly pick an index i.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:</head><p>if 1 ≤ i ≤ n then <ref type="bibr">6:</ref> Calculate ∇ i f (β) using (13). Calculate ∇ i f (β) using <ref type="bibr" target="#b13">(14)</ref>. <ref type="bibr">9:</ref> end if 10:</p><p>Calculate d using (11) based on Q ii and ∇ i f (β).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>11:</head><p>if 1 ≤ i ≤ n then <ref type="bibr">12:</ref> Update w using (15). <ref type="bibr">13:</ref> end if <ref type="bibr">14:</ref> Updatew using <ref type="bibr" target="#b15">(16)</ref>. <ref type="bibr">15:</ref> until The convergence criterion is reached. Output: Weight vectors w andw.</p><p>We summarize our dual coordinate descent algorithm for solving linear SVM+ in Algorithm 1. We first initialize the weight vectors as w = 0 D , andw = − C γ ∑ n i=1x i (i.e., the solution of w andw when α = 0 and ζ = 0). Then, each time, we randomly pick up an index i, and solve d (i.e., the change of β i ) using <ref type="bibr" target="#b10">(11)</ref>. In particular, when 1 ≤ i ≤ n, we calculate the gradient ∇ i f (β) using <ref type="bibr" target="#b12">(13)</ref>, and update the weight vectors w andw using <ref type="bibr" target="#b14">(15)</ref> and <ref type="formula" target="#formula_5">(16)</ref>, respectively; when n + 1 ≤ i ≤ 2n, we calculate the gradient ∇ i f (β) using <ref type="bibr" target="#b13">(14)</ref> and update the weight vectorw only using <ref type="bibr" target="#b15">(16)</ref>. The above process is repeated until the objective converges. Actually, the problem in (9) can be treated as a special form of the linear SVM problem discussed in <ref type="bibr" target="#b15">[16]</ref>, so the convergence of our algorithm follows that of the dual coordinate descent algorithm for linear SVM. We implement our algorithm based on the LIBLINEAR software <ref type="bibr" target="#b8">[9]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">SMO Algorithm for Solving Kernel SVM+</head><p>In this section, we develop an efficient algorithm for solving kernel SVM+ based on the ρ-SVM formulation. Similar to linear SVM+, we also augment the feature vector in the nonlinear feature space, so we can absorb the bias term into the weight vector, i.e., ϕ(</p><formula xml:id="formula_24">x i ) ← [ϕ(x i ) ′ , 1] ′ and w ← [w ′ , b] ′ for the main features (resp., ψ(x i ) ← [ψ(x i ) ′ , 1] ′ andw ← [w ′ ,b] ′ for the privileged features) 1 .</formula><p>For ease of presentation, we still use ϕ(x i ) and w to de-1 Usually we do not know the explicit form of φ(·), but the inner product of the augmented features can be simply calculated by adding one, i.e., φ(x i ) ′ φ(x j ) ← φ(x i ) ′ φ(x j ) + 1, so the kernel matrix based on the augmented features can be calculated by K ← K + 11 ′ . The same approach can be applied to the privileged features. note the nonlinear feature and weight vector for the main features (resp., ψ(x i ) andw for privileged features) below.</p><p>Specifically, after using the augmented features, the decision function can be represented as f (x) = w ′ ϕ(x). We use the squared hinge loss for ρ-SVM, which leads to the ℓ 2 loss ρ-SVM formulation as follows,</p><formula xml:id="formula_25">min w,b,ρ 1 2 ∥w∥ 2 + 1 2 C n ∑ i=1 ξ 2 i − ρ<label>(17)</label></formula><formula xml:id="formula_26">s.t. y i (w ′ ϕ(x i )) ≥ ρ − ξ i .</formula><p>By replacing each slack variable ξ i with the slack function ξ(x i ) =w ′ ψ(x i ), we arrive at the primal form of ℓ 2 -SVM+ as follows,</p><formula xml:id="formula_27">miñ w,w,ρ 1 2 ( ∥w∥ 2 + γ∥w∥ 2 ) + 1 2 C n ∑ i=1 (w ′ ψ(x i )) 2 − ρ s.t. y i (w ′ ϕ(x i )) ≥ ρ −w ′ ψ(x i ).<label>(18)</label></formula><p>Now we derive the dual form of our ℓ 2 -SVM+ formulation in <ref type="bibr" target="#b17">(18)</ref>. By introducing dual variables α 1 , . . . , α n for the constraints in <ref type="formula" target="#formula_27">(18)</ref>, we write its Lagrangian as follows,</p><formula xml:id="formula_28">L = 1 2 ( ∥w∥ 2 + γ∥w∥ 2 ) + 1 2 C n ∑ i=1 (w ′ ψ(x i )) 2 − ρ (19) − n ∑ i=1 α i (y i (w ′ ϕ(x i )) − ρ +w ′ ψ(x i )) ,</formula><p>Let us denote a vector α = [α 1 , . . . , α n ] ′ . By setting the derivatives of (19) w.r.t. to the primal variables w,w, ρ to zeros, we obtain the constraint α ′ 1 = 1 as well as two KKT conditions for w andw as,</p><formula xml:id="formula_29">w = n ∑ i=1 α i y i ϕ(x i ),<label>(20)</label></formula><formula xml:id="formula_30">w = n ∑ i=1 α i (γI + CPP ′ ) −1 ψ(x i ),<label>(21)</label></formula><p>where P = [ψ(x 1 ), . . . , ψ(x n )] is the data matrix of privileged features in the nonlinear feature space. Substituting the two equations in <ref type="bibr" target="#b19">(20)</ref> and <ref type="formula" target="#formula_30">(21)</ref> into the Lagrangian in <ref type="bibr" target="#b18">(19)</ref>, we arrive at the dual form of (18),</p><formula xml:id="formula_31">min α 1 2 α ′ (H + G)α (22) s.t. 1 ′ α = 1, α ≥ 0, where G = P ′ (γI + CPP ′ ) −1 P, H = K • (yy ′ ), and</formula><p>K is the kernel matrix of augmented main features with K ij = ϕ(x i ) ′ ϕ(x j ) being its (i, j)-th element. Based on the equation (γI + CPP ′ ) −1 P = P (γI + CP ′ P) −1 , we</p><formula xml:id="formula_32">have G = P ′ P (γI + CP ′ P) −1 =K ( γI + CK ) −1 ,</formula><p>Algorithm 2 Algorithm for solving the ℓ 2 -SVM+ problem in <ref type="bibr" target="#b17">(18)</ref> Input: K,K ∈ R n×n , C, and γ.</p><formula xml:id="formula_33">1: Calculate G = 1 C I − 1 C ( I + C γK ) −1 .</formula><p>2: Set Q = H + G, and ν = 1/n. whereK is the kernel matrix of augmented privileged features withK ij = ψ(x i ) ′ ψ(x j ) being its (i, j)-th element. By further applying the Woodbury Identity, we obtain G = <ref type="formula">(22)</ref> is a quadratic programming problem with the number of dual variables being n, which is only half number of variables in the dual form of the original SVM+ method in <ref type="bibr" target="#b3">(4)</ref>. Moreover, the constraints in <ref type="bibr" target="#b21">(22)</ref> are also simpler, so it can be solved by using the efficient SMO algorithm. Actually, the problem in <ref type="formula">(22)</ref> shares a similar form with one-class SVM in the LIBSVM software <ref type="bibr" target="#b0">[1]</ref>. In particular, we write the dual form of one-class SVM as follows (Eqn. <ref type="bibr" target="#b7">(8)</ref> </p><formula xml:id="formula_34">1 C I − 1 C ( I + C γK ) −1 . Note the problem in</formula><formula xml:id="formula_35">in [1]), min α 1 2 α ′ Qα (23) s.t. 1 ′ α = νn, 0 ≤ α ≤ 1,</formula><p>where Q is the kernel matrix in one-class SVM, ν is a predefined parameter, n is total the number of training samples. Note the constraints α ≥ 0 and 1 ′ α = 1 in <ref type="formula">(22)</ref> imply 0 ≤ α ≤ 1. By setting Q = (H + G) and ν = 1 n , the dual problem of ℓ 2 -SVM+ in <ref type="bibr" target="#b21">(22)</ref> can be converted to the optimization problem in <ref type="bibr" target="#b22">(23)</ref>. Therefore, we ignore the details of the SMO algorithm here, and employ the SMO implementation in LIBSVM to solve our ℓ 2 -SVM+ problem.</p><p>The detailed algorithm for solving ℓ 2 -SVM+ is described in Algorithm 2. We first calculate the matrix G using the kernel matrix of privileged features. Then we call the one-SVM solver in LIBSVM to obtain the dual variable vector α. Finally, the decision function can be written as</p><formula xml:id="formula_36">f (x) = ∑ n i=1 α i y i ϕ(x i ) ′ ϕ(x)</formula><p>, where x is the test sample, and ϕ(·) is the augmented nonlinear feature vector.</p><p>Although in our algorithm a matrix inverse operator is involved when calculating the matrix G, the size of G is only n × n, and the subsequent QP problem is also only with n dual variables. With the excellent implementation of matrix inverse function such as that in MATLAB, and the SMO implementation in LIBSVM, our algorithm is much faster than the existing SVM+ solver <ref type="bibr" target="#b25">[26]</ref> in practice.</p><p>Moreover, in some scenarios such as multi-instance learning using privileged information <ref type="bibr" target="#b24">[25]</ref>, it often needs to iteratively solve the SVM+ problem. In this case, after calculating the matrix inverse, we only need to iteratively solve the one-class SVM, which is much more efficient than the traditional method which needs to iteratively optimize a QP problem for solving the SVM+ problem (see Section 6.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experiments</head><p>In this section, we evaluate the efficiency of our proposed two algorithms for linear and kernel SVM+, and compare them with the existing SVM+ solvers for the image classification and web image retrieval tasks. Considering we solve two variants of SVM+ instead of the original SVM+ formulation, we also report the classification/retrieval performance for comparisons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Image Classification</head><p>We conduct the experiments for two image recognition tasks: digit recognition and scene recognition. Datasets: In the digit classification task, Vapnik and Vashist <ref type="bibr" target="#b31">[32]</ref> have shown that the additional textual descriptions in training data are helpful for learning a better classifier. We employ the benchmark MNIST+ dataset used in <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b25">26]</ref> for classifying the images as two digits "5" and "8". The dataset is constructed by using a subset of the MNIST dataset containing the images of the digits "5" and "8". It is split into a training set of 100 images (50 images with digit "5", and 50 images with digit "8"), a validation set of 4, 002 images, and a test set of 1, 866 images. All the images are resized into 10 × 10 pixels, and the 100-d vector of raw pixels is used as the main feature vector for each image. Each training image is additionally supplied with a poetic description (see <ref type="bibr" target="#b31">[32]</ref> for the examples), which is converted into a 21-d textual feature vector, and used as the privileged information in SVM+.</p><p>For the scene recognition task, we employ the Scene-15 dataset <ref type="bibr" target="#b21">[22]</ref>, which contains 4, 485 images of 15 different scenes. We split the data into three sets: 300 images as the training set, 40% images as the test set, and the rest as the validation set. We downsample all the images by factor 3, because small images are preferred in real-world applications for saving the storage of visual data at test time. The CNN features <ref type="bibr" target="#b18">[19]</ref> are extracted with the CAFFE framework <ref type="bibr" target="#b17">[18]</ref>, which has shown good performance in various visual recognition tasks. During training, the CNN features extracted from the original images are treated as privileged information, and the CNN features extracted from the downsampled images as the main features. The experimental setting is inspired by <ref type="bibr" target="#b5">[6]</ref>, where high-resolution images yield superior performance than low-resolution images for standard vision tasks. PCA is applied on two types of features to obtain 100-d compact representations. Baselines: In our experiments, we consider two settings, the linear case and the nonlinear case. In the linear case, there is no solver specifically designed for linear SVM+. So we mainly compare our method with the following two baselines,</p><p>• LIBLINEAR: The standard linear SVM without using privileged information implemented in LIBLIN-EAR <ref type="bibr" target="#b8">[9]</ref>. We include it as a baseline for investigating the effectiveness of our linear SVM+ formulation. • gSMO: The SMO-style algorithm for solving SVM+ in the dual form, which is proposed in <ref type="bibr" target="#b25">[26]</ref>. We use the released C++ implementation from the authors.</p><p>Note that the gSMO method was designed for solving kernel SVM+, but it can still take the feature vectors as the input. So we also include it as a baseline for the linear case. For the nonlinear case, the Gaussian kernel is used for all the methods, in which the bandwidth parameter is set as the mean of distances between all training samples by default. Besides the aforementioned gSMO method, we also compare our ℓ 2 -SVM+ method with the existing state-of-theart solvers of SVM+, and also include the standard SVM solved by LIBSVM as a baseline for investigating the effectiveness of SVM+.</p><p>• SVM: The standard SVM without using privileged information, whihc is solved by using LIBSVM <ref type="bibr" target="#b8">[9]</ref>. We include it as a baseline for investigating the effectiveness of our ℓ 2 -loss SVM+ formulation. • CVX-SVM+: An implementation based on the CVX optimization toolbox provided in <ref type="bibr" target="#b23">[24]</ref>. It directly solves the quadratic programming problem in (4) using the QP solver in CVX. • MAT-SVM+: We additionally include the QP solver implemented in MATLAB R2014b as a baseline, and employ it to solve the QP problem in <ref type="bibr" target="#b3">(4)</ref>.</p><p>For all methods, the tradeoff parameters are determined based on the validation set. One-vs-all strategy is used on the Scene-15 dataset, which contains 15 classes. The classification accuracy on the test set is reported for performance evaluation. The training time of all methods is measured on a workstation with Intel i7-3770K CPU@3.50GHz.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.1">Experiments on Linear SVM+</head><p>Experimental results: The classification accuracies and training time of all methods on two datasets are summarized in <ref type="table" target="#tab_0">Table 1</ref>. In terms of the classification accuracy, we observe that both our linear SVM+ algorithm and gSMO achieve better results than the baseline linear SVM method, which demonstrates the effectiveness of exploiting the poetic description as privileged information for improving the image based digit recognition task. Our method achieves slight better results than the gSMO method, possibly because we use a new variant of SVM+, and our dual coordinate descent algorithm guarantees the optimal solution.  In terms of the training time, both SVM+ algorithms are slower than the baseline linear SVM, because that more complicated objective functions are used to incorporate privileged information. Our new dual coordinate descent algorithm for linear SVM+ is much faster than gSMO, which generally takes only a bit more training time than the linear SVM as shown in <ref type="table" target="#tab_0">Table 1</ref>. Results using different number of training samples: We further take the Scene-15 dataset to investigate the accuracies and efficiency of different methods w.r.t. different number of training samples. Similarly as in <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b25">26]</ref>, we randomly sample 40%, 50%, 60%, 70%, 80% and 90% training samples for learning SVM and SVM+ classifiers. The experiments are repeated for 10 times, and we plot the average accuracies and training time of different methods in <ref type="figure" target="#fig_2">Figure 1</ref>. While the accuracies of all methods become higher when the number of training data increases, our linear SVM+ algorithm consistently outperforms the linear SVM method, and generally achieves slight better performance than the gSMO method. In term of the training time, it can be observed that the training time of our method increases linearly w.r.t. the number of training samples, and is several times faster than the baseline gSMO method. Convergence: As mentioned in Section 4, our algorithm can be treated as a special form of the linear SVM discussed in <ref type="bibr" target="#b15">[16]</ref>. So it shares the similar convergence property as the dual coordinate descent algorithm for linear SVM. To verify the convergence of our algorithm, in <ref type="figure" target="#fig_3">Figure 2</ref>, we take the MNIST+ dataset as an example to plot the objective values of our algorithm when the number of iterations 2 increases. It can be observed that our algorithm converges well. The <ref type="bibr" target="#b1">2</ref> Similarly as in LIBLINEAR, one iteration refers to that we pass all training samples once. objective value decreases very fast within the first ten iterations, and continues to decrease as the number of iterations increases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.2">Experiments on Kernel SVM+</head><p>Experimental results: We report the classification accuracies and training time of all methods on two datasets when using the Gaussian kernel in <ref type="table" target="#tab_1">Table 2</ref>. On the MNIST+ dataset, we observe that the SVM+ algorithms again achieve better results than the baseline SVM algorithm, due to the utilizing of poetic descriptions as privileged information. However, on the Scene-15 dataset, gSMO and CVX-SVM+ are worse than the standard SVM method. Our ℓ 2 -SVM+ algorithm achieves better result than the baseline algorithms, showing our new formulation for kernel SVM+ is effective for the scene classification problem on this dataset. In terms of the training time, we observer that our newly proposed algorithm is the most efficient one among all SVM+ algorithms, and generally achieves order-ofmagnitude speedup over the second fastest algorithm (MAT-SVM+ on the MNIST+ dataset, and gSMO on the Scene-15 dataset). The results demonstrate the efficiency of our new reformulation of kernel SVM+. With the reformulation, we convert it as a one-class SVM problem, and take advantage of the existing state-of-the-art SMO implementation in LIB-SVM to solve it. Results using different number of training samples: Similarly as for the linear case, in <ref type="figure" target="#fig_1">Figure 3</ref>, we take the Scene-15 dataset to plot the classification accuracies and training time of different methods by using different percentages of training data. The CVX-SVM+ method is not included when reporting the training time for better visualization. We observe that our ℓ 2 -SVM+ algorithm outperforms other SVM+ algorithms in terms of both classification accuracy and efficiency when varying the number of training samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Web Image Retrieval</head><p>In this subsection, we demonstrate the advantage of our ℓ 2 -SVM+ algorithm for solving the multiple instance  learning using privileged information problem. We employ the mi-SVM-PI algorithm proposed in <ref type="bibr" target="#b24">[25]</ref> to evaluate different SVM+ solvers. The mi-SVM-PI algorithm needs to iteratively solves the SVM+ problem, and simultaneously infer the labels for training samples under MIL constraints. We compare the mi-SVM-PI method based on our ℓ 2 -SVM+ algorithm, with their original implementation based on MATALB. Experimental setting: It has been shown that the textual descriptions associated with the web images are effective for learning better classifiers using multiple instance learning approaches <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b24">25]</ref>. Following <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b24">25]</ref>, we employ the NUS-WIDE web image dataset, which contains 269, 648 web images crawled from the image sharing website Flickr. It is officially split into a training set of 60% images, and test set of 40% images. All the images are accompanied with textual tags provided by Flickr users. The test images are annotated for 81 concepts. Similarly as in <ref type="bibr" target="#b22">[23]</ref>, we extract the DeCAF 6 features <ref type="bibr" target="#b7">[8]</ref>, which leads to a 4096-dim feature vector for each web image. For the training data, we also extract a 200-dim term frequency feature from the associated textual tags of each image, and use it as the privileged information. 25 positive bags (resp., negative bags) are constructed with each bag containing 15 relevant images (resp., irrelevant images) as the training data for each concept. The Gaussian kernel is used for the visual features, and linear kernel is used for the textual features. The image retrieval performance is evaluated on the test set, in which only the visual features are extracted. The average precision based on the top-ranked 100 test images is used for performance evaluation, and the mean average precision (MAP)  <ref type="table" target="#tab_2">Table 3</ref>. From the table, we observe that the SVM+ methods using both solvers achieve better results than the standard SVM method, because of using additional textual information in the training process. Moreover, by incorporating the multi-instance learning approach, the MAPs of mi-SVM-PI method based on both solvers are further improved. In both cases, the methods based on our ℓ 2 -SVM+ achieve slight better results than the ones based on MAT-SVM+. In terms of the efficiency, we observe that our ℓ 2 -SVM+ again achieves order-of-magnitude speed-up when compared with MAT-SVM+. When incorporating the multiinstance learning approach, the mi-SVM-PI based on our ℓ 2 -SVM+ uses only a bit more time than ℓ 2 -SVM+, because we only need to calculate the matrix inverse once for each concept, and iteratively solve the one-class SVM problem. In contrast, the mi-SVM-PI based on MAT-SVM+ needs to iteratively solve the QP problem introduced by the SVM+ problem. Therefore, our method is more than 30 times faster than MAT-SVM+ on the NUS-WIDE dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>In this paper, we have proposed two new algorithms for solving the linear and kernel SVM+. By reformulating the original SVM+ method, we obtain the dual problem with simpler constraints. Then we develop an efficient dual coordinate descent algorithm to solve the linear SVM+ problem. We also show that the kernel SVM+ using the ℓ 2 -loss can be converted to the one-class SVM problem, and thus can be efficiently solved by using the SMO algorithm implemented in the existing SVM solvers such as LIBSVM. Comprehensive experiments on three tasks have demonstrated the efficiency of our proposed algorithms for linear and kernel SVM+.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>3 :</head><label>3</label><figDesc>Obtain α by solving the problem in (23) with the oneclass SVM solver in LIBSVM. Output: Dual variable vector α.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 .</head><label>1</label><figDesc>The accuracies (Figure (a)) and the training time (Figure (b)) of different methods for solving linear SVM+ when using different number of training samples on the Scene-15 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 .</head><label>2</label><figDesc>The objective of our coordinate descent algorithm for linear SVM+ on MNIST+ dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 .</head><label>3</label><figDesc>The accuracies (Figure (a)) and training time (Figure (b)) of different methods for solving kernel SVM+ when using different number of training samples on the Scene-15 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><head>Table 1 .</head><label>1</label><figDesc>Accuracies (%) and training time of all methods on the MNIST+ and Scene-15 datasets in linear case. Our results are highlighted in boldface.</figDesc><table>MNIST+ 
Scene-15 
Accuracy Time (ms) Accuracy Time (s) 
SVM 
81.73 
0.6 
77.56 
0.76 

SVM+ 
gSMO 84.35 
72.1 
78.05 
2.52 
Ours 
84.62 
9.5 
78.10 
0.87 

40 
50 
60 
70 
80 
90 

70 

72 

74 

76 

78 

Percentage of Training Data (%) 

Accuracy (%) 

SVM 
gSMO 
Ours 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 2 .</head><label>2</label><figDesc>Accuracies (%) and training time of all methods on the MNIST+ and Scene-15 dataset using the Gaussian kernel. Our results are highlighted in boldface.</figDesc><table>MNIST+ 
Scene-15 
Acc. Time (ms) Acc. Time (s) 
SVM 
92.34 
0.7 
78.79 0.80 

SVM+ 

gSMO 
92.77 
78.8 
77.91 9.23 
CVX-SVM+ 93.14 
767.7 
78.32 47.16 
MAT-SVM+ 93.14 
54.9 
79.38 12.07 
Ours 
93.15 
1.3 
80.57 1.08 

40 
60 
80 
100 
70 

72 

74 

76 

78 

80 

82 

Percentage of Training Data (%) 

Accuracy (%) 
SVM 
gSMO 
MAT−SVM+ 
CVX−SVM+ 
Ours 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 .</head><label>3</label><figDesc>MAP (%) and training time (s) of different methods on the NUS-WIDE dataset. Our results are highlighted in boldface. over 81 concepts is reported. Moreover, the training time of all methods over 81 concepts is reported for efficiency evaluation. Experimental results: We report the MAPs and training time of different methods in</figDesc><table>MAP Time (s) 
SVM 
54.41 
9.80 

SVM+ 
MAT-SVM+ 55.63 
204.10 
Ours 
55.68 
19.44 

mi-SVM-PI 
MAT-SVM+ 59.11 
765.47 
Ours 
59.43 
24.26 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>The work is supported by the ERC Advanced Grant Varcity (#273940).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">LIBSVM: A library for support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM TIST</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Boosting with side information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lyu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Recognizing RGB images by learning from RGB-D data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1418" to="1425" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">NUS-WIDE: a real-world web image database from National University of Singapore</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIVR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Metric imitation by manifold transfer for efficient vision applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kroeger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Is image superresolution helpful for other vision tasks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Latent low-rank transfer subspace learning for missing modality recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">DeCAF: A deep convolutional activation feature for generic visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">LIBLINEAR: A library for large linear classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R.-E</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Working set selection using second order information for training SVM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R.-E</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1889" to="1918" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Object localization based on structural SVM using privileged information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feyereisl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fouad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Raychaudhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Schneider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Incorporating privileged information through metric learning. T-NNLS</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1086" to="1098" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Cross modal distillation for supervision transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.00448</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Deep Learning and Representation Learning Workshop</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">LSDA: Large scale detection through adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A dual coordinate descent method for largescale linear SVM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Keerthi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sundararajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multitask multiclass privileged information support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Attributebased classification for zero-shot visual object categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nickisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harmeling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>T-PAMI</publisher>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning using privileged information: SVM+ and weighted SVM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lapin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="95" to="108" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Exploiting privileged information from web data for image categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="437" to="452" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Connection between SVM+ and multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Cherkassky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCNN</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Exploiting privileged information from web data for action and event recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">SMOstyle algorithms for learning using privileged information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pechyony</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Izmailov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vashist</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">DMIN</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">On the theory of learning with privileged information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pechyony</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Sequential minimal optimization: A fast algorithm for training support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Platt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Kernel Methods -Support Vector Learning</title>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning to rank using privileged information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sharmanska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Quadrianto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Multimodal learning with deep boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning using privileged information: Similarity control and knowledge transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Izmailov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">20232049</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A new learning paradigm: Learning using privileged information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vashist</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">56</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Classifier learning with hidden information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Distance metric learning using privileged information for face verification and person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">T-NNLS</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="3150" to="3162" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Can visual recognition benefi from auxiliary information in training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
