<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">RIFD-CNN: Rotation-Invariant and Fisher Discriminative Convolutional Neural Networks for Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gong</forename><surname>Cheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Automation</orgName>
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peicheng</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Automation</orgName>
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Han</surname></persName>
							<email>jhan@nwpu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Automation</orgName>
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">RIFD-CNN: Rotation-Invariant and Fisher Discriminative Convolutional Neural Networks for Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:45+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1</head><p>Thanks to the powerful feature representations obtained through deep convolutional neural network (CNN), the performance of object detection has recently been substantially boosted. Despite the remarkable success, the problems of object rotation, within-class variability, and between-class similarity remain several major challenges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>To address these problems, this paper proposes a novel and effective method to learn a rotation-invariant and Fisher discriminative CNN (RIFD-CNN) model. This is achieved by introducing and learning a rotation-invariant layer and a Fisher discriminative layer, respectively, on the basis of the existing high-capacity CNN architectures. Specifically, the rotation-invariant layer is trained by imposing an explicit regularization constraint on the objective function that enforces invariance on the CNN features before and after rotating. The Fisher discriminative layer is trained by</head><p>imposing the Fisher discrimination criterion on the CNN features so that they have small within-class scatter but large between-class separation. In the experiments, we comprehensively evaluate the proposed method for object detection task on a public available aerial image dataset and the PASCAL VOC 2007 dataset. State-of-the-art results are achieved compared with the existing baseline methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Object detection is one of the most fundamental yet challenging problems in computer vision community. Since the groundbreaking success of deep convolutional neural networks (CNN) <ref type="bibr" target="#b0">[1]</ref> in image classification task <ref type="bibr" target="#b1">[2]</ref> on the ImageNet large scale visual recognition challenge (ILSVRC) <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>, CNN-based object detection methods have recently attracted a great deal of research interest and have achieved state-of-the-art performance .</p><p>Among various CNN-based methods for object detection, one of the most notable work is made by Girshick et al. <ref type="bibr" target="#b5">[5]</ref> with the framework of region-CNN (R-CNN), which is ac-* Corresponding author. <ref type="figure" target="#fig_5">Figure 1</ref>. While the CNN features have shown impressive success for object detection, the problems of object rotation, within-class variability, and between-class similarity still remain several major challenges. Here we show some example patches for each challenge obtained from PASCAL VOC 2007 dataset <ref type="bibr" target="#b28">[28]</ref>. All example patches are warped into a fixed 224×224 pixel size. In this situation, how to learn a more powerful feature representation that is rotation insensitive and meanwhile has small within-class scatter and big between-class separation is highly desirable. The proposed rotation-invariant and Fisher discriminative CNN model provides a possible solution to address these problems.</p><p>tually a chain of conceptually simple steps: generating candidate object proposals, classifying them as foreground or background, and post-processing them to improve their fit to objects. Briefly, R-CNN framework proceeds as follows. First it extracts a few hundreds or thousands candidate object proposals which probably contain an object via the selective search algorithm <ref type="bibr" target="#b29">[29]</ref> to reduce the computational cost. Then, R-CNN uses AlexNet model <ref type="bibr" target="#b1">[2]</ref> to extract CNN features from object proposals and classifies them as objects or non-objects by using classspecific linear support vector machines (SVMs), where the AlexNet CNN model, with more than 60 million parameters, was first pre-trained on an auxiliary task of image classification in the ImageNet ILSVRC challenge <ref type="bibr" target="#b2">[3]</ref> and then transferred and fine-tuned on a small set of images annotated for the detection task. Finally, the candidate object proposals are refitted to detected objects by using a bounding box regressor <ref type="bibr" target="#b30">[30]</ref> to correct miss-localizations. This simple pipeline has achieved state-of-the-art detection performance on standard detection benchmarks (e.g., PASCAL VOC <ref type="bibr" target="#b28">[28]</ref>) with a large margin over all the previously published methods, which are mostly based on deformable part model (DPM) <ref type="bibr" target="#b30">[30]</ref>.</p><p>The success of R-CNN method <ref type="bibr" target="#b5">[5]</ref> is largely attributed to the ability of CNN model to extract more richer high-level object representation features as opposed to handengineered low-level features such as SIFT <ref type="bibr" target="#b31">[31]</ref> and HOG <ref type="bibr" target="#b32">[32]</ref>. However, while the CNN features have shown impressive success for object detection tasks, they are still difficult to effectively deal with the challenges (as illustrated in <ref type="figure" target="#fig_5">Figure 1</ref>) of object rotation, within-class variability, and between-class similarity, which are some important sources of detection error. In this situation, how to learn a more powerful feature representation that is rotation insensitive and meanwhile has small within-class scatter and big between-class separation is highly desirable. To address these problems and to further improve the state-of-the-arts, in this paper we propose a novel and effective method to learn a rotation-invariant and Fisher discriminative CNN (RIFD-CNN) model.</p><p>Our main contributions are summarized as follows: First, we build on the existing high-capacity CNN architectures <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b9">9]</ref> to train a rotation-invariant CNN (RI-CNN) model by adding and learning a new rotation-invariant layer. This newly added rotation-invariant layer is trained through incorporating a regularization constraint term on the objective function of our RI-CNN model, which enforces the training samples before and after rotating to share the similar feature representations and hence achieving rotation-invariance. Evaluations on a public aerial image dataset <ref type="bibr" target="#b33">[33]</ref> and comparisons with state-of-the-art methods demonstrate the effectiveness of the proposed RI-CNN model. Second, we propose a new method to train a Fisher discriminative CNN (FD-CNN) model by introducing and learning a Fisher discriminative layer. The Fisher discriminative layer is trained by imposing the Fisher discrimination criterion on the CNN features so that they have small within-class scatter but large between-class separation. Third, our RI-CNN model and FD-CNN model are complementary. By combining them together, we obtain a more powerful RIFD-CNN model. We have confirmed through comprehensive experiments that the proposed RIFD-CNN model can significantly improve the baseline methods on PASCAL VOC dataset <ref type="bibr" target="#b28">[28]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Object detection has been actively studied for the last few decades. The DPM <ref type="bibr" target="#b30">[30]</ref> and its variants <ref type="bibr" target="#b20">[20,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b34">[34]</ref><ref type="bibr" target="#b35">[35]</ref><ref type="bibr" target="#b36">[36]</ref><ref type="bibr" target="#b37">[37]</ref><ref type="bibr" target="#b38">[38]</ref><ref type="bibr" target="#b39">[39]</ref> have been the leading methods for object detection tasks for years owing to the carefully crafted features like HOG <ref type="bibr" target="#b32">[32]</ref>. In recent years, thanks to the availability of large scale training data, such as ImageNet <ref type="bibr" target="#b2">[3]</ref>, and the raise of high-performance computing systems, such as GPUs, various CNN-based methods  have been substantially improving upon the performance of object detection. Among them, the most related works and therefore also the baseline methods in our experiments are R-CNN method <ref type="bibr" target="#b5">[5]</ref> and its improvement method <ref type="bibr" target="#b17">[17]</ref>.</p><p>The introduction of the R-CNN framework <ref type="bibr" target="#b5">[5]</ref> opens the door to extract rich features through deep CNN models to improve object detection performance. In the work of <ref type="bibr" target="#b5">[5]</ref>, AlexNet CNN <ref type="bibr" target="#b1">[2]</ref> was used to extract a set of deep features from category-independent region proposals provided by selective search <ref type="bibr" target="#b29">[29]</ref> and then class-specific linear SVMs were adopted to classify them. By adopting the R-CNN framework <ref type="bibr" target="#b5">[5]</ref> with a deeper 16-layers VGGNet CNN model <ref type="bibr" target="#b9">[9]</ref>, the performance was further boosted.</p><p>Some variants focusing on different aspects are also developed based on the successful R-CNN framework <ref type="bibr" target="#b5">[5]</ref>. For instance, Zhang et al. <ref type="bibr" target="#b17">[17]</ref> addressed the inaccurate localization problem by using a search algorithm based on Bayesian optimization that sequentially proposes candidate regions for an object bounding box and training the CNN with a structured loss that penalizes the localization inaccuracy explicitly. Zhu et al. <ref type="bibr" target="#b21">[21]</ref> proposed an approach to improve the accuracy of object detection by exploiting a small number of accurate object segment proposals. They framed the problem as inference in a Markov random field, in which each detection hypothesis scores object appearance as well as contextual information using CNNs. Our method is also built upon the remarkable R-CNN framework <ref type="bibr" target="#b5">[5]</ref> with the existing CNN architecture such as AlexNet <ref type="bibr" target="#b1">[2]</ref> and VGGNet <ref type="bibr" target="#b9">[9]</ref>. However, different from previous work, this paper mainly focuses on enriching the power of the CNN feature representations via imposing rotation invariance and fisher discriminative criterion on the objective function of our new RIFD-CNN model. Consequently, our work is also partly related with the ideas of learning invariant features such as <ref type="bibr" target="#b40">[40]</ref><ref type="bibr" target="#b41">[41]</ref><ref type="bibr" target="#b42">[42]</ref><ref type="bibr" target="#b43">[43]</ref><ref type="bibr" target="#b44">[44]</ref><ref type="bibr" target="#b45">[45]</ref><ref type="bibr" target="#b46">[46]</ref> and the approaches incorporating discriminative terms into model training such as <ref type="bibr" target="#b47">[47,</ref><ref type="bibr" target="#b48">48]</ref>. In addition, other related works will be cited throughout the paper. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed method</head><p>The goal of our method is to learn a rotation-invariant and Fisher discriminative CNN model in order to advance the performance of object detection. This is achieved by introducing and learning a rotation-invariant layer ( <ref type="figure" target="#fig_0">Figure  2</ref>) and a Fisher discriminative layer <ref type="figure" target="#fig_2">(Figure 3</ref>), respectively, on the basis of the existing high-capacity CNN architectures. To be specific, the rotation-invariant layer is trained by incorporating a regularization constraint term on the objective function of the RI-CNN model, which explicitly enforces the feature representations of the training samples before and after rotating to be mapped close to each other, and hence achieving rotation-invariance. The Fisher discriminative layer is trained by imposing the Fisher discrimination criterion on the CNN features so that they have small within-class scatter but large between-class separation. In the remainder of this section we first describe how to learn rotation-invariant CNN model and next detail the training of Fisher discriminative CNN model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Learning rotation-invariant CNN model</head><p>The framework of the proposed rotation-invariant CNN (RI-CNN) model training is illustrated in <ref type="figure" target="#fig_0">Figure 2</ref>. It consists of two steps: data augmentation and model training. The first step mainly generates a set of positive and negative training samples by using a generic object proposal detection method <ref type="bibr" target="#b29">[29]</ref> and a simple rotating operation. In the second step, we build on the existing popular CNN architectures, such as AlexNet <ref type="bibr" target="#b1">[2]</ref> and VGGNet <ref type="bibr" target="#b9">[9]</ref>, to train our rotation-invariant CNN model by adding and learning a new rotation-invariant layer. 1, , kK =  . In our implementation, we treat all region proposals with ≥0.5 intersection over union (IoU) overlap with a ground-truth box as initial positives for that box's class and the rest as initial negatives.</p><p>Model training. As shown in <ref type="figure" target="#fig_0">Figure 2</ref>, in order to achieve rotation-invariance, we add a new rotation invariant fully-connected layer FCa that uses the output of layer FCm ( m is the number of network layers except for classifier layer, e.g., in AlexNet <ref type="bibr" target="#b1">[2]</ref> m=7 and in VGGNet <ref type="bibr" target="#b9">[9]</ref> m=15) as input. Different from the training of traditional CNN models that only optimizes the multinomial logistic regression objective, our RI-CNN model is now trained by optimizing a new objective function via imposing a regularization constraint term to enforce the training samples before and after rotating to share the similar features. The pseudo-code of RI-CNN training is given in Algorithm 1.</p><p>To avoid over-fitting and to reduce the training cost, the parameters (weights and biases) of layers C1, C2, … , and FCm, denoted by <ref type="bibr" target="#b12">12</ref>   <ref type="formula">(3)</ref> is the softmax classification loss function, which is defined by a (1 ) C + -class multinomial negative log-likelihood criterion. It seeks to minimize the misclassification error for the given training samples and is computed by</p><formula xml:id="formula_0">( ) ( ) () RI RI RI 1 ,, l o g 1 i i xb i x M x NK ∈ =− +   </formula><p>yO <ref type="bibr" target="#b3">(4)</ref> where , ab is the inner-product of a and b , N is the total number of initial training samples in X , and K is the total number of rotation transformations for each i x X ∈ . The second term (, ) RXTX φ in Eq. <ref type="formula">(3)</ref> is a rotationinvariance regularization constraint, which is imposed on the training samples before and after rotating, namely X and TX φ , to enforce them to share the similar features. We define the regularization constraint term as </p><formula xml:id="formula_1">( ) () ( ) 2 2 1 , 2 i ai a i xX RXTX x Tx N φ φ ∈ =−  OO<label>(5)</label></formula><p>We can easily see that the objective function defined by (7) not only minimizes the classification loss, but also imposes a regularization constraint to achieve rotation invariance. In practice, we solve this optimization problem by using stochastic gradient descent (SGD) method <ref type="bibr" target="#b49">[49]</ref>, which has been widely used in complicated optimization problems such as neural networks training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Learning Fisher discriminative CNN model</head><p>As illustrated in <ref type="figure" target="#fig_2">Figure 3</ref>, our Fisher discriminative CNN (FD-CNN) model is designed by adding a new Fisher discriminative fully-connected layer FCc that uses the output of layer FCm or FCa (in this situation by combining RI-CNN and FD-CNN together we can obtain a more powerful RIFD-CNN model) as input. This newly added F- isher discriminative layer is trained via imposing the Fisher discrimination criterion on the CNN features to enforce the learned FD-CNN features have small within-class scatter and large between-class separation.  <ref type="bibr" target="#b9">(9)</ref> In all our experiments, FCc has a size of 4096, and FCd has a size equal to the number of object classes. Here, different from the (C+1)-way softmax classifier layer FCb of RI-CNN model, FCd is now a C-way softmax classifier layer (without background class), so the training samples for FD-CNN learning now become all ground-truth bounding boxes for each object class denoted by  </p><formula xml:id="formula_3">( ) ( ) FD w FD B FD )( ) (( ) tr S tr S F =−  <label>(15)</label></formula><p>Thus, by incorporating Eqs. <ref type="bibr" target="#b15">(15)</ref> and <ref type="formula">(11)</ref>   <ref type="bibr" target="#b16">(16)</ref> As can be seen from Eq. (16), the new discriminative objective function not only minimizes the classification loss, but also imposes a regularization constraint to make the learned CNN features be more discriminative.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we first demonstrate that the use of our RI-CNN features can outperform the state-of-the-arts <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b46">46,</ref><ref type="bibr" target="#b51">51,</ref><ref type="bibr" target="#b52">52]</ref> for rotation-invariant object detection, specifically for finding aerial cars that appear at arbitrary orientations on a publicly available satellite image dataset <ref type="bibr" target="#b33">[33]</ref>. We next focus on comprehensively evaluating the proposed RI-CNN model, FD-CNN model, and especially their combination (RIFD-CNN model) for standard object detection tasks on PASCAL VOC 2007 dataset <ref type="bibr" target="#b28">[28]</ref>. The experimental results show that our method significantly improves the existing baseline methods such as <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b17">17]</ref>. The performance of object detection is measured according to the PASCAL criterion <ref type="bibr" target="#b28">[28]</ref>, i.e., the average precision (AP) and the mean AP over all object classes. Without explicit statement, we adopt the standard IoU criteria of 0.5 for all experimental evaluation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Aerial car detection</head><p>In this experiment, we use a public dataset introduced by <ref type="bibr" target="#b33">[33]</ref>, which has been widely used by some published work such as <ref type="bibr" target="#b33">[33,</ref><ref type="bibr" target="#b46">46,</ref><ref type="bibr" target="#b51">51,</ref><ref type="bibr" target="#b52">52]</ref>. This dataset consists of 30 aerial images with a total number of 1319 manually labeled cars that appear at arbitrary orientations. The task is challenging due to the low resolution and the varying illumination conditions caused by the shadows of buildings. Like the compared work <ref type="bibr" target="#b33">[33,</ref><ref type="bibr" target="#b46">46,</ref><ref type="bibr" target="#b51">51,</ref><ref type="bibr" target="#b52">52]</ref>, we perform 5-fold cross validation and report average results across all folds.</p><p>Implementation. We adopt the most popular AlexNet CNN <ref type="bibr" target="#b1">[2]</ref> pre-trained on ImageNet <ref type="bibr" target="#b2">[3]</ref> as our building block. To adapt it to the new aerial car detection task, we first perform SGD fine-tuning of the whole CNN parameters with a 2-way softmax classification layer (one for car and the other for background) using augmented training samples obtained from the aerial dataset <ref type="bibr" target="#b33">[33]</ref>. In this step, we sample 32 positives and 96 negatives for each SGD iteration to form a mini-batch of size 128. The SGD learning rate is set to 0.0005 to allow fine-tuning to make progress while not clobbering the initialization. We set the momentum to 0.9, and the weight decay to 0.0005 for all the layers. After that, we further train a RI-CNN model by adding and learning a new rotation-invariant layer as described in section 3.  Results and comparison with state-of-the-arts. <ref type="figure" target="#fig_6">Figure  4</ref> shows some example detections (true positive in green, false negative in red) by using our RI-CNN features with a linear SVM. As can be seen from <ref type="figure" target="#fig_6">Figure 4</ref>, despite the large variations in the orientations, the proposed method has successfully detected and located most of the cars. Besides, we also compare our results with some state-of-the-arts in Table1. As can be seen from <ref type="table">Table 1</ref>, using a simple linear SVM classifier, our RI-CNN model can 1) significantly improve the performance of the traditional CNN model with AlexNet architecture <ref type="bibr" target="#b1">[2]</ref> fine-tuned on the aerial car dataset, which is also the baseline of our method, and 2) outperform all other recent publications <ref type="bibr" target="#b46">[46,</ref><ref type="bibr" target="#b51">51,</ref><ref type="bibr" target="#b52">52]</ref> which address the rotation problem in different ways, where <ref type="bibr" target="#b52">[52]</ref> uses slot kernel structured SVM and the standard HOG feature, <ref type="bibr" target="#b46">[46]</ref> focuses on learning rotation-invariant feature and descriptor called RC-RBM and IHOF, respectively, and <ref type="bibr" target="#b51">[51]</ref> presents rotation-invariant HOG descriptors using Fourier analysis in polar and spherical coordinates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Object detection on PASCAL VOC 2007</head><p>In this experiment, we focus on the PASCAL VOC 2007 dataset <ref type="bibr" target="#b28">[28]</ref>, which is the most common benchmark to evaluate object detection algorithms. This dataset consists of 9963 complex scene images with 5011 training images and 4952 testing images, in which bounding boxes of 20 diverse object classes were manually labeled. The task is to predict bounding boxes of the objects of interest if they are present in the images.</p><p>Implementation. We build on the high-performance VGGNet CNN model <ref type="bibr" target="#b9">[9]</ref>, that was pre-trained on Image-Net <ref type="bibr" target="#b2">[3]</ref> and then fine-tuned on PASCAL VOC 2007 dataset to train our RI-CNN model, FD-CNN model, and RIFD-CNN model, respectively. To augment the training data, we adopt a family of 6 rotation transformations <ref type="bibr" target="#b12">12</ref> {, ,, }</p><formula xml:id="formula_4">K TT T T φφ φ φ =  with {1 0 ,2 0 ,3 0 } φ =± ± ± </formula><p>to carry out the rotation operation on each training sample to obtain 7x training samples. As <ref type="bibr" target="#b5">[5]</ref>, we map each object proposal (obtained via selective search method <ref type="bibr" target="#b29">[29]</ref>) to the ground- Following the R-CNN framework <ref type="bibr" target="#b5">[5]</ref>, we use our trained models to extract new CNN features from object proposals provided by selective search method <ref type="bibr" target="#b29">[29]</ref>, classify them with class-specific linear SVMs (trained using ground-truth positive samples and negative samples obtained via hard negative mining <ref type="bibr" target="#b30">[30]</ref>), and then perform non-maximum suppression and bounding box regression <ref type="bibr" target="#b30">[30]</ref>.</p><p>Results and comparison with state-of-the-arts. <ref type="table" target="#tab_6">Table  2</ref> reports the detection performance of our improved CNN models including RI-CNN model, FD-CNN model, and their combination (RIFD-CNN model). These results are obtained based on the building block of R-CNN <ref type="bibr" target="#b5">[5]</ref> with VGGNet <ref type="bibr" target="#b9">[9]</ref> and bounding box regression (BB), which is therefore also our baseline method. Compared with the baseline method, from <ref type="table" target="#tab_6">Table 2</ref> we can observe 1) RI-CNN model only improves the performance slightly for mAP (+0.7%) averaged over 20 categories. This can be easily explained: different from aerial images (as illustrated in <ref type="figure" target="#fig_6">Figure 4</ref>) in which objects appear at arbitrary orientations, the objects in nature scene images are typically in an upright orientation due to the Earth's gravity and so the orientation variations across images are generally small. For the improved object classes, rotations are mainly caused by aeroplane/bird flying and animal jumping. 2) FD-CNN model improves the performance for all 20 object    <ref type="bibr" target="#b5">[5]</ref> with two different networks (AlexNet <ref type="bibr" target="#b1">[2]</ref> and VGGNet <ref type="bibr" target="#b9">[9]</ref>) and its improvement work <ref type="bibr" target="#b17">[17]</ref> as a strong baseline. The last two rows report the results of our method and its combination with FGS. The entries with the best APs for each object category are bold-faced.</p><p>categories. Especially for those classes with big withinclass variability or large between-class similarity such as bird, boat, chair, plant, etc., we obtain significant performance improvement. 3) Further improvement has been achieved by combing RI-CNN model and FD-CNN model together (RIFD-CNN model), boosting the baseline model <ref type="bibr" target="#b5">[5]</ref> by 3.8% in mAP. The results demonstrate that our method is effective for addressing the problems of object rotation, within-class variability, and between-class similarity.</p><p>In <ref type="table" target="#tab_7">Table 3</ref>, we compare our method with other published work <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b12">12,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b38">38]</ref> on PASCAL VOC 2007 test set. Rows 1-7 show sliding window detectors that employ different features, where the first <ref type="bibr" target="#b30">[30]</ref> uses only HOG, the next two <ref type="bibr" target="#b34">[34,</ref><ref type="bibr" target="#b38">38]</ref> use different feature learning approaches to augment or replace HOG, and the last four <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b23">23]</ref> employ CNN features. Rows 8-14 show the results for R-CNN method <ref type="bibr" target="#b5">[5]</ref> with two different networks (AlexNet <ref type="bibr" target="#b1">[2]</ref> and VGGNet <ref type="bibr" target="#b9">[9]</ref>) and its improvement work <ref type="bibr" target="#b17">[17]</ref> as a strong baseline. The last two rows report the results of our method and its combination with fine-grained search (FGS) method <ref type="bibr" target="#b17">[17]</ref>. For all methods, the fine-tuning/ training of the networks (if applicable) as well as the training of the detection SVMs were performed on VOC 2007 train+val dataset. As shown in <ref type="table" target="#tab_7">Table 3</ref>, we achieve state-of-the-art results for 17 out of 20 categories compared with the existing baseline methods <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b17">17]</ref>. To be specific, our best result (RIFD-CNN + FGS) improves upon the best results of the baseline methods of <ref type="bibr" target="#b5">[5]</ref> and <ref type="bibr" target="#b17">[17]</ref> by 5.0% and 2.5%, respectively, in terms of mAP, which demonstrates the effectiveness and superiority of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this paper, we proposed an effective method to boost the performance of object detection in R-CNN framework <ref type="bibr" target="#b5">[5]</ref> by learning a rotation-invariant and Fisher discriminative CNN model. The proposed method could effectively address the challenges of object rotation, within-class variability, and between-class similarity. In the experiments, we have comprehensively evaluated the proposed method for object detection tasks on a public available aerial image dataset <ref type="bibr" target="#b33">[33]</ref> and the PASCAL VOC 2007 dataset <ref type="bibr" target="#b28">[28]</ref>. On both two datasets, we have achieved state-of-the-art performance compared with the existing baseline methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>The framework of the proposed RI-CNN training. It consists of two steps: data augmentation and model training. The first step mainly generates a set of augmented training samples by using a simple rotating operation. In the second step, we build on the existing high-capacity CNN architectures to train our rotation-invariant CNN model by adding and learning a new rotation-invariant layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Data augmentation. Given a set of initial training samples {,} X XX +− = , we generate a set of new training samples RI {, } X TX φ =  by rotating transformations, where X + denotes the initial positive examples, X − denotes the initial negative examples, of K rotation transformations with k T φ denoting the rotation operation of a training sample with the angle of k φφ ∈ ,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Architecture of the proposed FD-CNN model. It is achieved by adding a new Fisher discriminative layer on the existing CNNs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>is the ground-truth bounding boxes for the i-th object class. Given the training samplesFD  {} k x = and their ground-=∈ y  , our objective is now to train a FD-CNN model with the input-target pairs FD FD (,)  . Except for requiring that FD-CNN model should minimize the misclassification error on the training dataset, we also require that FD-CNN model should have powerful discriminative capability. For this purpose, we propose the following discriminative objective function to learn the are two trade-off parameters that control the relative importance of the three terms.The first term FD FD (,) M  in Eq. (10) is a classification error function that seeks to minimize the classification error for the given training samples and is computed by Eq. (10) is a discrimination regularization constraint imposed on the CNN features. Based on the Fisher discrimination criterion<ref type="bibr" target="#b50">[50]</ref>, this can be achieved by minimizing the within-class scatter of FD  , denoted by wF D () S  , and maximizing the between-class scatter of FD  , denoted by BF D n is the number of samples in the i-th object class, i m and m are the mean feature representations of FD</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>into Eq. (10), we can form the following discriminative objective function of FD-CNN model</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>1 ..</head><label>1</label><figDesc>In this step, we randomly sample 2 positive examples and 2 negative examples together with their corresponding 4×35=140 rotated examples to form a mini-batch of size 144. The SGD learning rate is set to 0.01 for the last two layers training and 0.0001 for the whole network fine-tuning, and decreases by 0.5 every 10000 iterations. The parameters of Eq. (7)are set to<ref type="bibr" target="#b0">1</ref> To augment the training data, we treat all region proposals (provided by<ref type="bibr" target="#b33">[33]</ref>) with ≥0.5 IoU overlap with a ground truth box as initial positives and the rest as initial negatives. Then, we use 35 rotation operation on each initial training sample to obtain 36x training samples. The augmented data are used for both fine-tuning and RI-CNN model training. Finally, we train a simple and efficient linear SVM classifier to classify all region proposals as cars or background.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 .</head><label>4</label><figDesc>Some example detections (true positive in green, false negative in red) by using our RI-CNN features with a linear SVM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>truth instance with which it has maximum IoU overlap (if any) and label it as a positive for the matched ground-truth class if the IoU is at least 0.5. All other proposals are labeled as negative examples for all classes. For both RI-CNN model and FD-CNN model training, the learning rate is set to 0.01 for the last two layers and 0.0001 for the whole network fine-tuning, and decreases by 0.5 every 10000 iterations. The parameters of Eqs. (7) and (16) each SGD iteration of RI-CNN training, we randomly sample 10 positive examples over all classes and 10 negative examples together with their corresponding 20×6=120 rotated examples to construct a mini-batch of size 140.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>y denotes the ground-truth label vector of sample i x with only one element being 1 and the others being 0, our objective is to train a RI-CNN model with the input-target pairs RI RI (,) . Apart from requiring that RI-CNN model should minimize the classification error on the training dataset, we also require that RI-CNN model should have the rotation invariance capability for any set of training samples { , } ii x Tx φ . To this end, we propose a new objective function to learn the parameters RI are two trade-off parameters that control the relative importance of the three terms.The first term RI RI (,) M  in Eq.</figDesc><table>{, ,, 
} 

m 

WW 
W 
 
and 12 
{, ,, } 

m 

B BB  
, 
are pre-trained on ImageNet dataset [3], domain specifica-
lly fine-tuned to adapt to the detection task (e.g., PASCAL 
VOC), and then transferred to our RI-CNN model. For a 
training sample 

RI 
i 

x ∈  , let 
( ) 
mi x 
O 
be the output of layer 
FCm, 
( ) 
ai x 
O 
be the output of layer FCa, 
( ) 
bi x 
O 
be the 
output of softmax classifier layer FCb, and ( , ) 

aa 

W B and 
(,) bb 
W B be the new parameters of layers FCa and FCb. 
Thus, 
( ) 
ai x 
O 
and 
( ) 
bi x 
O 
can be computed by 

() 
() 

( 
) 

ai 
a mi 
a 

xx κ 

=+ W 
OO 
B 
(1) 
() 

() 

( 
) 

bi 
b ai 
b 

xx ϕ 

=+ W 
OO 
B 
(2) 

where ( ) max( , ) 

κ 

= 
0 
xx and 

1 

( ) exp( )/ || exp( ) || 

ϕ 

= 
xxx 
are the ReLU and softmax non-linear activation functions. 
In all our experiments, FCa has a size of 4096, and FCb has 
a size equal to ( 
1) 
C + ( C object classes plus background). 
Given the training samples RI {| 
} 

ii 

x xXT X 

φ 

=∈ ∪ 
 
and their corresponding labels RI 

RI 

{} 

i 

i 

x |x 
=∈ y 
 , where 

i 

x 

1 
2 

{, ,, 
, , } 

mab 

= 
WW WW W W 
 

and RI 

1 
2 

{, ,, , , } 

mab 

B=BB 
BBB 
 

by the following formula 

( 
) 

( 
) 

( 
) 

RI 
RI 
RI 

2 
2 
1R 
RI 
R 
I 2 
I 

, 

min 
, 
, 
2 

J 

MR X T X 

φ 

λ 
λ 

= 

 
++ 
 
 

W 

W 

B 

 
(3) 

where 1 

λ and 2 
λ </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>Algorithm 1 Learning RI-CNN modelInput: a set of initial training samples and their corresponding ground-truth labels and a family of K rotation transformations denoting the rotation operation of a sample with the angle of k Output: the parameters of our RI-CNN model, denoted by</figDesc><table>where 
( ) 
ai x 
O 
serves as the RI-CNN feature of the training 
sample i 
x ; 
() 
ai Tx φ 
O 
denotes the average RI-CNN feature 
representation of rotated versions of the training sample i 
x 
and so it is formulated as 

( ) 

( ) 

1 

1 

j 

K 

ai 
a 
i 
j 

Tx 
T x 
K 

φ 
φ 

= 

=  

OO 
(6) 

{,} 
XX X 

+− 

= 

12 

{, , , } 

K 

TT T 
T 

φφ 
φ 
φ 

= 
 

with 

k 

T φ 

φ 

RI 
1 
2 

{, ,, 
, , } 

mab 

= 
WW WW W W 
 

and RI 

1 
2 

{, , , , , } 

mab 

B =BB 
B B B 
 

1: begin 
2: Obtain the augmented input-target pairs RI RI 

(,)  

3: Initialize (,) aa 
W B and (,) bb 
W B randomly 
4: while stopping criterion has not been met do 
5: compute classification error using Eq. (4) 
6: compute rotation-invariance constraint term using Eq. (5) 
7: compute objective function 

( 
) 

RI 
RI 
RI 

, 
J W B 

using Eq. (7) 
8: update RI 
W and RI 

B 

9: end while 
10: return RI 
W and RI 

B 

11: end begin 

As can be seen from Eq. (5), this term enforces the feature 
of each training sample to be close to the average feature 
representation of its rotated versions. If this term outputs a 
small value, the feature representation is sought to be 
approximately invariant to the rotation transformations. 
The third term 

2 
RI 2 

|| 
|| 
W 
in Eq. (3) is a weight decay term 
that tends to decrease the magnitude of the weights of RI 
W , 
and helps preventing over-fitting. 
By incorporating Eqs. (5) and (4) into Eq. (3), we have 
the following objective function 

( 
) 

( 
) 
() 

() 

( ) 

RI 

RI 
RI 
RI 

2 
2 
12 
RI 2 
2 

, 

1 
,log 
1 
min 

22 

i 

i 

i 

xb 
i 
x 

ai 
a 
i 
xX 

J 

x 
NK 

xT x 
N 

φ 

λ 
λ 

∈ 

∈ 

= 

 
−+ 
 + 
 
 
−+ 
  
 

 

 

W 

W 

B 

yO 

OO 

 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head></head><label></label><figDesc>Algorithm 2 gives the pseudo-code of FD-CNN training.Similar to RI-CNN training, to reduce the training cost, the parameters (weights and biases) of layers C1, C2, … , FCm, and FCa, denoted by ], domain-specifically fine-tuned, and then transferred to our FD-CNN model. For a training sample</figDesc><table>12 

{, ,, 
, } 

ma 

WW 
W W 
 
and 

12 

{, ,, , } 

ma 

B BB B 
 
, are pre-trained on ImageNet dataset 
[3FD 
k 

x ∈  , let 
() 
ak x 
O 
be the output of layer FCa, 
( ) 
ck x 
O 
be the output 
of layer FCc, 
( ) 
dk x 
O 
be the output of softmax classifier 
layer FCd, and ( , ) 

cc 

W B and ( , ) 

dd 

W B 
be the new 
parameters of layers FCc and FCd. Thus, 
( ) 
ck x 
O 
and 
() 
dk x 
O 
can be computed by 

() 
() 

( 
) 

ck 
c ak 
c 

xx κ 

=+ W 
OO 
B 
(8) 

() 
() 

( 
) 

dk 
d ck 
d 

xx ϕ 

=+ W 
OO 
B 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head></head><label></label><figDesc>The detection result comparison for different methods on the aerial car detection dataset.</figDesc><table>Model 
AP (%) 
RC-RBM IHOF [46] + linear SVM 
72.7 
Gradients IHOF [46] + linear SVM 
74.7 
RC-RBM [46] + Gradients IHOF [46] + linear SVM 
77.6 
Standard HOG [32] + slot kernel structured SVM [52] 75.7 
Rotation-invariant HOG [51] + linear SVM 
82.6 
Rotation-invariant HOG [51] + Random Forest 
84.2 
Fine-tuned CNN with AlexNet [2] + linear SVM 
90.2 
Our RI-CNN with AlexNet [2] + linear SVM 
94.6 
Table 1 </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head></head><label></label><figDesc>Model aero bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tv mAP R-CNN with VGGNet &amp; BB 73.4 77.0 63.4 45.4 44.6 75.1 78.1 79.8 40.5 73.7 62.2 79.4 78.1 73.1 64.2 35.6 66.8 67.2 70.4 71.1 66.0</figDesc><table>+ RI-CNN 
75.6 77.1 65.6 45.1 45.2 74.9 78.1 81.2 40.7 73.5 62.8 81.2 79.6 74.5 66.1 35.1 67.0 66.9 70.1 73.3 66.7 

+ FD-CNN 
76.3 80.8 68.5 50.1 46.1 77.2 79.6 81.9 47.7 75.9 66.2 81.6 79.9 74.3 69.8 41.1 68.9 70.3 73.3 73.8 69.2 

+ RIFD-CNN 
77.4 80.8 70.7 49.9 47.2 77.6 79.9 82.7 48.6 76.1 67.1 81.9 80.2 74.6 71.9 40.9 69.5 70.7 73.5 74.2 69.8 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 2 .</head><label>2</label><figDesc>Detection performance on PASCAL VOC 2007 test set using our improved CNN models including RI-CNN model, FD-CNN model, RIFD-CNN model, and the baseline method of R-CNN [5] with VGGNet [9] and bounding box regression (BB). The entries with the best APs for each object category are bold-faced. aero bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tv mAP + StructObj + FGS [17] 74.1 83.2 67.0 50.8 51.6 76.2 81.4 77.2 48.1 78.9 65.6 77.3 78.4 75.1 70.1 41.4 69.6 60.8 70.2 73.7 68.5 + RIFD-CNN (ours) 77.4 80.8 70.7 49.9 47.2 77.6 79.9 82.7 48.6 76.1 67.1 81.9 80.2 74.6 71.9 40.9 69.5 70.7 73.5 74.2 69.8 + RIFD-CNN + FGS (ours) 78.9 82.5 69.6 54.2 49.7 78.3 82.0 83.4 51.1 76.0 69.0 82.2 80.7 77.2 73.1 42.6 70.3 70.4 74.2 74.1 71.0</figDesc><table>Model 
DPM v5 [30] 
33.2 60.3 10.2 16.1 27.3 54.3 58.2 23.0 20.0 24.1 26.7 12.7 58.1 48.2 43.2 12.0 21.1 36.1 46.0 43.5 33.7 

DPM ST [34] 
23.8 58.2 10.5 8.5 27.1 50.4 52.0 7.3 19.2 22.8 18.1 8.0 55.9 44.8 32.4 13.3 15.9 22.8 46.2 44.9 29.1 

DPM HSC [38] 
32.2 58.3 11.5 16.3 30.6 49.9 54.8 23.5 21.5 27.7 34.0 13.7 58.1 51.6 39.9 12.4 23.5 34.4 47.4 45.2 34.3 

CNN-DPM-BB [22] 
50.9 64.4 43.4 29.8 40.3 56.9 58.6 46.3 33.3 40.5 47.3 43.4 65.2 60.5 42.2 31.4 35.2 54.5 61.6 58.6 48.2 

E2E-DPM [23] 
49.3 69.5 31.9 28.7 40.4 61.5 61.5 41.5 25.5 44.5 47.8 32.0 67.5 61.8 46.7 25.9 40.5 46.0 57.1 58.2 46.9 

DP-DPM [20] 
44.6 65.3 32.7 24.7 35.1 54.3 56.5 40.4 26.3 49.4 43.2 41.0 61.0 55.7 53.7 25.5 47.0 39.8 47.9 59.2 45.2 

Sliding-window CNN [12] 
64.1 72.3 62.8 44.0 44.2 66.4 72.5 67.7 35.2 68.9 35.9 62.7 69.0 65.7 65.8 36.2 60.1 50.3 63.2 66.0 58.6 

R-CNN with AlexNet 
64.2 69.7 50.0 41.9 32.0 62.6 71.0 60.7 32.7 58.5 46.5 56.1 60.6 66.8 54.2 31.5 52.8 48.9 57.9 64.7 54.2 
R-CNN with VGGNet 
71.6 73.5 58.1 42.2 39.4 70.7 76.0 74.5 38.7 71.0 56.9 74.5 67.9 69.6 59.3 35.7 62.1 64.0 66.5 71.2 62.2 

R-CNN with AlexNet &amp; BB 
68.1 72.8 56.8 43.0 36.8 66.3 74.2 67.6 34.4 63.5 54.5 61.2 69.1 68.6 58.7 33.4 62.9 51.1 62.5 64.8 58.5 

R-CNN with VGGNet &amp; BB 73.4 77.0 63.4 45.4 44.6 75.1 78.1 79.8 40.5 73.7 62.2 79.4 78.1 73.1 64.2 35.6 66.8 67.2 70.4 71.1 66.0 

+ FGS [17] 
74.2 78.9 67.8 51.6 52.3 75.7 78.7 76.6 45.4 72.4 63.1 76.6 79.3 70.7 68.0 40.3 67.8 61.8 70.2 71.6 67.2 

+ StructObj [17] 
73.1 77.5 69.2 47.6 47.6 74.5 78.2 75.4 44.5 76.3 64.9 76.7 76.3 69.9 68.1 39.4 67.0 65.6 68.7 70.9 66.6 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 3 .</head><label>3</label><figDesc>Performance comparison of our method against other methods on PASCAL VOC 2007 test set. Rows 1-7 show sliding window detectors that employ different features. Rows 8-14 show results for R-CNN framework</figDesc><table></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was supported in part by the National Science Foundation of China under Grants 61401357, 61522207, and 61473231.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="541" to="551" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Scalable object detection using deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Overfeat: Integrated recognition, localization and detection using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep neural networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deepid-net: Deformable deep convolutional neural networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Modeling Local and Global Deformations in Deep Learning: Epitomic Convolution, Multiple Instance Learning, and Sliding Window Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Savalle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Foroosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tappen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pensky</surname></persName>
		</author>
		<title level="m">Sparse Convolutional Neural Networks. In CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">AttentionNet: Aggregating Weak Directions for Accurate Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Improving object detection with deep convolutional networks via bayesian optimization and structured prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning and transferring mid-level image representations using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deformable part models are convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">segDeepM: Exploiting Segmentation and Context in Deep Neural Networks for Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deformable Part Models with CNN Features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Savalle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tsogkas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Parts and Attributes Workshop, ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">End-to-End Integration of a Convolutional Network, Deformable Parts Model and Non-Maximum Suppression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Simultaneous detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Part-based R-CNNs for fine-grained category detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">LSDA: Large scale detection through adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">S</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Convolutional Feature Masking for Joint Object and Stuff Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The PASCAL Visual Object Classes (VOC) Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Selective search for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">E</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="154" to="171" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained part-based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1627" to="1645" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning spatial context: Using stuff to find things</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Sketch tokens: A learned mid-level representation for contour and object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning coarse-to-fine sparselets for efficient object detection and scene classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Bottom-up segmentation for top-down detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Multi-class geospatial object detection and geographic image classification based on collection of part detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS J. Photogramm. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="page" from="119" to="132" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Histograms of sparse codes for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Effective and efficient midlevel visual elements-oriented land-use classification using VHR remote sensing images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="4238" to="4249" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning Invariant Representations with Local Transformations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Spatial Transformer Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Transformation pursuit for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Understanding image representations by measuring their equivariance and equivalence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lenc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Rotation, scaling and deformation invariant scattering for texture discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mallat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Discriminative unsupervised feature learning with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Learning rotation-aware features: From invariant priors to equivariant descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Fisher discrimination dictionary learning for sparse representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">DeepShape: Deep Learned Shape Descriptor for 3D Shape Matching and Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Learning representations by back-propagating errors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">323</biblScope>
			<biblScope unit="page" from="533" to="536" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Pattern classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Duda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Stork</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<publisher>Wiley-Interscience</publisher>
		</imprint>
	</monogr>
	<note>2nd Ed</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Rotation-invariant HOG descriptors using fourier analysis in polar and spherical coordinates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Skibbe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Blein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Palme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="342" to="364" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Learning equivariant structured output SVM regressors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blaschko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
