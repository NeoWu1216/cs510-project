<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Detection and Accurate Localization of Circular Fiducials under Highly Challenging Conditions</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lilian</forename><surname>Calvet</surname></persName>
							<email>lcalvet@simula.no</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Simula Research Laboratory</orgName>
								<address>
									<settlement>Oslo</settlement>
									<country>Norway ☞</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Gurdjos</surname></persName>
							<email>pgurdjos@enseeiht.fr</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Toulouse</orgName>
								<address>
									<country>France ☞</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Griwodz</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Simula Research Laboratory</orgName>
								<address>
									<settlement>Oslo</settlement>
									<country>Norway ☞</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><surname>Gasparini</surname></persName>
							<email>simone.gasparini@enseeiht.fr</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Toulouse</orgName>
								<address>
									<country>France ☞</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Detection and Accurate Localization of Circular Fiducials under Highly Challenging Conditions</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:39+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Using fiducial markers ensures reliable detection and identification of planar features in images. Fiducials are used in a wide range of applications, especially when a reliable visual reference is needed, e.g., to track the camera in cluttered or textureless environments. A marker designed for such applications must be robust to partial occlusions, varying distances and angles of view, and fast camera motions. In this paper, we present a robust, highly accurate fiducial system, whose markers consist of concentric rings, along with its theoretical foundations. Relying on projective properties, it allows to robustly localize the imaged marker and to accurately detect the position of the image of the (common) circle center. We demonstrate that our system can detect and accurately localize these circular fiducials under very challenging conditions and the experimental results reveal that it outperforms other recent fiducial systems.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The term fiducial marker, or simply fiducial, refers to a set of (coplanar) points encoded in a planar pattern allowing a reliable detection and identification across views. A fiducial marker system is a (set of) fiducial marker(s) coupled with dedicated computer vision algorithms solving the detection and identification problems. This is used in a variety of applications, both in computer vision and robotics, ranging from camera calibration to augmented reality or visual SLAM. The choice of fiducials is of crucial importance within this framework as markers must provide reliable visual references in the scene that can be used to estimate, e.g., the camera position or its motion. Such framework requires that the fiducial marker system be robustly and accurately detectable even under very challenging conditions, such as, e.g., when the markers are partially or largely occluded, or seen under highly skewed angles or from long (a) (b) (c) (d) <ref type="figure">Figure 1</ref>. (a,c) Synthetic images of circular fiducials under very challenging shooting conditions i.e., perturbed, in particular, by a (unidirectional) motion blur of magnitude 15px. (b,d) Using the proposed fiducial system, markers are correctly detected and identified with an accuracy of 0.54px and 0.36px resp. in (a) and (c) for the estimated imaged center of the outer ellipse whose semi-major axis (in green) is equal to 31.9px and 34.5px resp. distances, when the illumination is very poor or irregular, or when the camera undergoes very fast motions generating blur.</p><p>In this paper, we present a robust, highly accurate and theoretically-founded fiducial system, which is highly tolerant to all of the mentioned challenges, as shown in <ref type="figure">Figure  1</ref>. Its markers are based on concentric black rings on a white background, extending the one-ring markers introduced by Gatrell et al. in <ref type="bibr" target="#b9">[10]</ref>. The geometric properties of the concentric circles delivered by their edges are exploited to accurately detect the image of the circle common center, thus providing a highly reliable feature point that can be used for tracking and motion estimation. The thickness of the rings can be used to encode the information of the marker, typically a unique ID, thus providing a simple and reliable method for recognizing the different markers placed in the scene.</p><p>The detection method proposed in this work relies on the flow conservation property: the ingoing amount of gradient magnitudes through an arc of the outermost ellipse must be equal to the outgoing amount of gradient magnitudes through an arc of the innermost ellipse. This property also holds in presence of motion blur since, as described in <ref type="bibr" target="#b15">[16]</ref>, the image of a circular fiducial is not affected by the blur along the line perpendicular to the direction of the motion blur. Consequently, the proposed circular fiducial can be detected and identified without explicitly un-blurring the image before the detection stage. Furthermore, since any concentric circle-pair encodes -through the circular pointsthe Euclidean structure of its supporting plane, the perspective distortion can be removed by estimating its rectifying homography. While reaching the performance of other recent fiducial systems under favorable conditions, we show that the proposed fiducial system clearly outperforms these systems under highly challenging (i.e., more realistic) conditions.</p><p>The paper presents the system theoretical foundations and describes both the circular fiducial detection step and its subsequent validation, the latter simultaneously delivering the position of the imaged center along with the marker ID. A comparison of its performance with a variety of recent marker system under conditions of varying difficulties is finally provided.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>The most widely used fiducial systems use bitonal patterns, usually made of two main components: low frequency elements for marker detection, e.g. a square black border <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b17">18]</ref>, and high frequency elements for information encodage, e.g. an internal region filled with 6×6 grid of black and white cells <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b17">18]</ref>.</p><p>Thanks to the relatively simple detection algorithms allowing fast detection at high frame rates even on mobile phones, these systems have gained popularity in many augmented reality applications, supported by many freely available libraries, such as ARToolkit <ref type="bibr" target="#b11">[12]</ref> and ARToolkit-Plus <ref type="bibr" target="#b17">[18]</ref>.</p><p>However, this conventional approach suffers in presence of motion blur. Whereas low frequency components remain localizable to some extent, high frequency components are not preserved, which prevents the extraction of the marker identity. A trivial solution for this problem is to increase the size of the markers in the scene. However, for many applications, this can be invasive and therefore unacceptable. When processing video streams, temporal continuity can be exploited to improve the detection performance and robustness: the markers can be tracked across the frames, e.g. as implemented in ARToolkit <ref type="bibr" target="#b11">[12]</ref>. However, such tracking al-ARTKPlus <ref type="bibr" target="#b17">[18]</ref> RuneTag <ref type="bibr" target="#b1">[2]</ref> PRASAD <ref type="bibr" target="#b15">[16]</ref> Proposed <ref type="figure">Figure 2</ref>. Prior fiducials and our proposal. From left to right, the last two are designed to be detected under motion blur conditions. Only the proposed detection algorithm is able to exactly match a feature point (its center) across a collection of images, even in presence of motion blur.</p><p>gorithms fail when the camera moves rapidly, and require re-initialization whenever the imaged marker is no longer detected.</p><p>Lately, fiducial systems that are robust to motion blur have been proposed. The mono-spectrum marker <ref type="bibr" target="#b16">[17]</ref> consists of low frequency components in the form of coloured-"degraded" dots lying on a black square background. Regions of the markers are distinguished from other regions by means of a frequency spectrum specific to the marker. At the end of the localization process, the four marker corners are extracted as the centers of the dots located at each corner. A drawback of such an approach is that it is assumed that the image of a dot center corresponds to the center of the imaged dot. However, this underlying assumption is not valid under a projective camera model, thus preventing feature matching across a collection images. In practice, this can significantly affect the accuracy of the retrieved pose, because the error due to the center assumption increases with the size of the circle, and consequently the error of the computed pose grows as the camera gets closer to such a marker. This remark also holds for the methods proposed in <ref type="bibr" target="#b6">[7]</ref> and <ref type="bibr" target="#b15">[16]</ref>. In <ref type="bibr" target="#b6">[7]</ref>, a marker consists of four black dots, forming a square, located on a white background. In this approach, the marker detection is performed through machine learning techniques and it is to some extent capable of handling the presence of motion blur.</p><p>The fiducial markers proposed in <ref type="bibr" target="#b15">[16]</ref> and <ref type="bibr" target="#b13">[14]</ref> are the most similar to the one presented in this work. In <ref type="bibr" target="#b15">[16]</ref>, the authors propose a marker made up of concentric white rings on black background. However, even in this case, the image of the marker's center is incorrectly assumed to be at the geometrical center of the marker's image. Another drawback of this method is a very low information-coding capacity, which leads to a library size of only four different markers, thus dramatically limiting the number of possible applications. In <ref type="bibr" target="#b13">[14]</ref>, a fiducial system robust to motion blur is proposed. It relies on self-similar templates defined by a 2D rotationally invariant, bitonal intensity function. However, the self-similarity property only holds in theory for fronto-parallel acquisitions by calibrated cameras, even if it is empirically observed robustness to perspective distortions. While not discussed in <ref type="bibr" target="#b13">[14]</ref>, any identification task should run the detection algorithm as many times as the number of IDs, drastically reducing the identification power and/or increasing the computational time.</p><p>The problem of strong occlusion is addressed in Rune-Tag <ref type="bibr" target="#b1">[2]</ref>, where a marker is composed of rings of circular dots. The authors have shown that such a marker allows an accurate estimation of the camera pose while being robust to severe occlusion. The PiTag fiducial markers <ref type="bibr" target="#b2">[3]</ref>, which are also made up of circular dots but arranged in rectangles, have shown a similar robustness to occlusion, although with an even smaller number of circular dots.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The Proposed Circular Fiducial System</head><p>The proposed circular fiducial is a planar pattern consisting of a set of concentric black circular rings on a white support. Every circular fiducial naturally encodes three points two of which being the circular points of its supporting plane, which are complex conjugate points at infinity <ref type="bibr" target="#b14">[15]</ref>. The circular point-pair encodes the 2D Euclidean structure of this plane naturally, allowing to determine from its image a metric rectification that removes the projective distortion on the imaged marker. In the case of a calibrated camera, it also allows to compute the camera pose with respect to the supporting plane, up to an unknown planar rotation. Another benefit of circular fiducials is their known resiliance to severe occlusions <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b12">13]</ref> as well as linear motion blur <ref type="bibr" target="#b15">[16]</ref>. In terms of their information-coding capacity, a circular fiducial can be seen as a circular bar-code which encodes data by varying the widths and spacings of circular rings. Consequently, one of its (relative) shortcomings is the limited amount of information that can be encoded in this bar-code compared to some other fiducials. This shortcoming can be overcome, for example, by reusing IDs but combining them in unique patterns. Overall, despite such an high potential for being the 'ideal' fiducial, the circular ring pattern has received relatively little attention in the literature. An explanation of this lack of popularity may be the absence of code released under a public license.</p><p>We have recently reported <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref> how to integrate circular fiducials within a unified Structure-from-Motion paradigm, where we describe how images of circular point-pairs can be combined with images of natural points.</p><p>We assume that all views capture a cluttered scene in which M circular fiducials with N rings are visible. The gray-scale intensity of one image is represented by a differentiable function I :</p><formula xml:id="formula_0">[1, m] × [1, n] ⊂ R 2 → [0, 1], whose discretized representation is the sampling of I on the do- main [1, m] × [1, n] ⊂ N 2 .</formula><p>The image gradient field is denote by ∇ I : I → R 2 . We assume to be given the set of edge points that correspond to maxima of the gradient magnitude (e.g., using Canny's edge detector <ref type="bibr" target="#b5">[6]</ref>).</p><p>We use the Matlab-like notation M (1:r,1:c) for denoting the r × c-submatrix of M selected by the row range [1, r] and the column range <ref type="bibr">[1, c]</ref>. Similarly M (:,1:c) (resp. M (1:r,:) ), selects the first c (resp. r) columns (resp. rows) of M.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Circular fiducial detection</head><p>The problem of detection is formulated as that of seeking regions in the image that can potentially support a portion of the external contour of the imaged circular fiducial, socalled outer elliptical arc. To be candidates, the regions must obey some loose geometric constraints, as convexity and smoothing, as well as more specific photometric constraints derived from differential properties of the gray-scale intensity, related to the gradient field ∇ I within an imaged circular fiducial, as reported in the next paragraphs.</p><p>There are three different selection steps which can summarized as follows.</p><p>Pixel selection ( §3.1.2): We create paths in the image in the form of sequences of 2N linked edge pixels (when a fiducial has N rings) such that the direction of the path segment starting at pixel p is given by the image gradient at p. Then, we select pixels which appear as path ending points the greatest number of times (through a vote procedure).</p><p>Region-pair grouping: We group into 'inner' regions the selected ending points to form polygonal approximations of convex arcs and we group into 'outer' regions the associated starting points.</p><p>Region-pair selection ( §3.1.3): We select only regionpairs which satisfy the 'conservative constraint' which basically ensures that the ingoing amount of gradient magnitudes through the outer region must be equal to the outgoing amount of gradient magnitudes through the inner region.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Theoretical foundations of detection</head><p>At this step, in order to provide the theoretical foundations of our approach in a simple form, we will only consider circular fiducials with one ring, delimited by its outer and inner circles. Both circles are centered at the origin with radii 1 and r &lt; 1, and associated with closed and open disks,</p><formula xml:id="formula_1">B 1 [0] and B r (0) in R 2 . Thus , Ω = B 1 [0]\B r (0)</formula><p>, defines the surface of the ring between the two circles, in the following referred to as the interior of the ring. Exceptionally, the ring is not painted black but (again for pedagogic purposes) with a continuous gradation of gray hues (black is hue 0 and white is hue 1) as defined by the function α :</p><formula xml:id="formula_2">[−1, 1] 2 → [0, 1] we have α(x) = 1 − x 2 for all x ∈ Ω (interior of the ring), α(x) = 1 for all x /</formula><p>∈ Ω. This is seen in <ref type="figure" target="#fig_0">figure 3(b)</ref>. Consider a view of the scene under an imaging process H : [−1, 1] 2 → [1, m] × [1, n] ⊂ R 2 through some homographic mapping that restricts the central projection to the ring's supporting plane <ref type="bibr" target="#b10">[11]</ref>. Under the assumptions that (i) the scene is illuminated by a uniform parallel light beam, (ii) the surface supporting the fiducial is Lambertian, (iii) the chirality constraints are ensured, the intensity value for </p><formula xml:id="formula_3">(a) (b) (c) (d) (e)</formula><formula xml:id="formula_4">I(u) = α H −1 (u)<label>(1)</label></formula><formula xml:id="formula_5">For u / ∈ H([−1, 1] 2 ), I(u)</formula><p>is simply the luminance related to the corresponding projected scene point. Note that since the ring can be seen as an "infinite" set of concentric circles associated with the equipotentials of α, their images under H are the equipotentials of (1), the outer and inner equipotentials coinciding with the images of the outer and inner circles (see figure 3(c)) .</p><p>Assuming there are no edge points in H(Ω) except a few outliers, we now shift our first problem of determining the points of the outer ellipse into the one of linking each point of the outer ellipse to some point of the inner ellipse. Our idea for achieving this is to follow -if possiblefor each edge point u e the field line of the gradient field ∇ I through u e and to stop as soon as another edge point is encountered (see figures 3(d-e)). This field line is the planar curve passing through u e and whose tangent at each of its points u is collinear to the vector ∇ I (u), or more formally, the curve has parameterization φ :</p><formula xml:id="formula_6">U ⊂ R → R 2 , solution of the differential system φ ′ (t) = ∇ I (φ(t)) φ(t 0 ) = u 0 where (t 0 , u 0 ) ∈ Ω × R 2 defines an initial condition.</formula><p>Two key results are now given (cf. figure 3(e)):</p><formula xml:id="formula_7">Proposition 1.</formula><p>In the continuous image,</p><p>• Through any point in H(Ω), except the image of the circle centre, passes one and only one field line of ∇ I ;</p><p>• Any field line necessarily converges to one point on the inner ellipse.</p><p>The proofs are omitted here due to lack of space (the proof of the latter result relies on the fact that, when the inner radius of the ring is 0, any field line converges to the image of the circle center).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Pixel selection</head><p>Here, circular fiducials can have N rings, that is 2N concentric circles. The idea is to chose an adequate number N of rings in order to guarantee a good approximation of the field lines in the discrete image.</p><p>We call linked sequence (or path) a polygonal line with 2N edge points {u i1 , .., u i P } as vertices, when all pairs (u ij , u ij+1 ), j ∈ [1, 2N − 1] are pairs such that u ij is an edge point and u ij+1 is the closest edge point to u ij , outside the neighbourhood of u ij , that lies on the line passing through u ij with (−1) j ∇ I (u ij ) as direction. Assuming K linked sequences, for any k ∈ K, let S be the K × 2N -matrix where the row S (k,:) concatenates the indices of edge points for the sequence number k and V i = l ∈ K | S (l,2N ) = i is the set of indices of all linked sequences in which u i is the 2N -th (i.e., last) point. Note that card(V i ) can be seen as the number of 'votes' for u i as last point in a sequence.</p><p>If E denotes the set of indices of all edge points, then we can state the following pixel selection rule. The set F ⊂ E with card(F ) = T , containing the indices of the T edge points with the T highest votes i.e., of the u i with i satisfying min i∈F card(V i ) ≥ max j∈E\F card(V j ), is the set of indices of points selected as point candidates for the inner ellipses while S(∪ i∈F V i , 1) yields the set of indices of points selected as point candidates for the outer ellipses.</p><p>In practice, we have implemented this algorithm in the discrete image as follows. An edge pixel is first (arbitrarily) selected. We can see it as having a voting intention. It yields a valid vote only if it is possible to build from it a polygonal line {u i } i∈S (k,:) , whose vertices are 2N edge points satisfying some conditions (see below). If it is the case, the vote is confirmed. The purpose is to create polygonal lines that connect edge points of the outer ellipse to edge points of the inner ellipse. The two conditions for a polygonal line to approximate a field line arc are the following. For all j ∈ {1, ..., 2N − 1}: (i) u S (k,j+1) = u S (k,j) +a S (k,j) ∇ I (u S (k,j) ), where a S (k,j) &lt; 0 if j is uneven and a S (k,j) &gt; 0 otherwise; (ii) any segment with endpoints u S (k,j) , u S (k,j+1) does not include edge points other than vertices. It is worth of mentioning that we empirically verified that the underlying assumption holds in practice: defining a pixel as a square whose sides have length 1, through the pixel u S (k,1) and </p><formula xml:id="formula_8">(a) (b) (c)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Region-pair selection</head><p>At this step, a candidate is any pair of regions (R 1 , R 2 ) in the discrete image such that R 2 is a convex chain of pixels q with a number of votes card(V (q)) ≥ 2, while R 1 is the set of pixels p such that p is the first pixel in the linked sequence of which q is the last pixel. The issue here is to validate R 2 as a support of an arc of the inner ellipse by checking the average number of received votes.</p><p>It is assumed to have fitted all the pixels in R i to the parameters of some elliptical arcs E i using some robust technique 1 , and, in particular, to have estimated L(E i ), the length of E i . Note that if any of the two regions does not 'fit well' with a portion of an ellipse, then the pair is eliminated. In the next paragraph, we show that the total number card(V (R 2 )) of received votes by R 2 , i.e., in the region candidate to support an inner arc E 2 , must satisfy, for some threshold ǫ &gt; 0,</p><formula xml:id="formula_9">|card(V (R 2 )) − L(E 1 )| &lt; ǫ<label>(2)</label></formula><p>Principle of flow conservation for validation. For the moment, we deal again with a theoretical circular fiducial with one ring painted with a continuous gradation of gray hues, as detailed in previous paragraphs. Let T 1 and T 2 be two line fields of the gradient map ∇ I defined on H(Ω) i.e., inside the imaged ring. Let A 1 and A 2 be the arcs of the outer and inner ellipses respectively, such that the curve delimited by A 1 , T 1 , A 2 and T 2 <ref type="bibr" target="#b0">1</ref> The outer ellipse is estimated using the 'ellipse growing' technique of <ref type="bibr" target="#b12">[13]</ref>, where multiple arcs can be assembled. is closed, as shown in figure 5. We denote by A i , with i ∈ {1, 2}, the arclength-parameterized curve modeling the arc A i with L(A i ) denoting the length of A i .</p><p>The flux (or total flow) through the arc A i is defined as</p><formula xml:id="formula_10">Ai (∇ I · n i ) ds = Ai ∇ I ds<label>(3)</label></formula><p>since ∇ I and n i , the unit normal to A i , both evaluated at A i (s), are collinear vectors.</p><p>The proposed conservative constraint in our validation step relies on the principle of conservation.</p><formula xml:id="formula_11">A1 ∇ I ds = A2 ∇ I ds<label>(4)</label></formula><p>which says that the total ingoing flow though arc A 1 is exactly the same than the total outgoing flow through arc A 2 . This follows from the fact that the gradient field is, by definition, a conservative vector field. In terms of vocabulary, we will say that (A 1 , A 2 ) is a conservative arc-pair. In the discrete image, under the hypothesis that R 1 is a region (set of pixels) that supports an arc E 1 of the inner ellipse, one aims at determining whether the associated region R 2 supports an arc E 2 of the outer ellipse such that the arc-pair (E 1 , E 2 ) is a conservative arc-pair i.e., a pair like (A 1 , A 2 ) which satisfies (4). This is achieved by checking the total number of received votes by R 2 .</p><p>Let us deal now again with bitonal circular fiducials with multiple rings while assuming, that the principle of flow conservation holds for its image. The voting procedure creates paths (in the form of the so-called linked sequences) in the image gradient field whose endpoints are assumed to lie on a single field line. An important property of conservative vector fields is that the line integral is path-independent. Thus, the flows carried between the endpoints are the same for either any path or the line field. On the other hand, one can say that the paths created by linked sequences have the property that the incoming flow for the starting pixel of the path is 1 (a starting pixel initiates a vote). Hence, we can conclude that the outgoing flow of the inner arc is equal to card(V (R 2 )), the total number of received votes by the pixels of R 2 , cf. <ref type="figure" target="#fig_2">figure 5(b)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Circular fiducial validation</head><p>In the discrete image, let (R 1 , R 2 ) be two regions supporting two elliptical arcs (A 1 , A 2 ), respectively outer and inner, which satisfy the necessary condition (2) of the conservative constraint. We seek the image c of the circle centre (detected at sub-pixel accuracy) such that the image region delimited by the elliptical sector formed by c and R 2 can be validated as the image of a sector of a circular fiducial or not. For this purpose, our idea is to check the appearance similarity of 'metric rectifications' of different cross sections through c.</p><p>We first describe the geometric component of our validation process. We call any homography G of the image onto itself a canonical rectifying homography such that, by applying G to the two image ellipses corresponding the projections of two concentric circles, we obtain two concentric circles centred at the origin with radius 1. In other words, we recover the original Euclidean geometry of the plane supporting the circles up to a 2D rotation and scaling. It is well-known <ref type="bibr" target="#b14">[15]</ref> that the two image ellipses corresponding to the projections of two concentric circles (or, equivalently, one of these ellipses plus the image of the centre) are sufficient to determine of an exact solution for a canonic rectifying homography. In this work, we describe in proposition 2 a minimal parameterization of a canonical rectifying homography with only two degrees of freedom which are the coordinates of the image of the circle centre (the proof is omitted). </p><formula xml:id="formula_12">G(u) = −1 Q 22 uv −u 0 −Q 11 u 2 + 1 −v −Q 11 u Q 22 v 1 r 0 0 0 −1 0 0 0 s −1 with r = − Q 22 Q 11 (Q 11 u 2 + Q 22 v 2 + Q 33 ) 1/2 ,<label>(5)</label></formula><formula xml:id="formula_13">s = −Q 22 (1 − Q 11 u 2 ) 1/2</formula><p>defines an homography of the image plane onto itself such that G(c) is a canonical rectifying homography, providing c = (u c , v c ) ⊤ represents the image of the circle centre.</p><p>To check the appearance similarity of the candidate, we proceed as follows. Let x be a point located inside the inner ellipse and u j a pixel in R 1 , the region supporting the outer ellipse. Let us designate by cross section at u j the line segment in the image joining u j and x. Let {α t m j | α t = t−1 T −1 , t = 1..T } be the discretization into T equally spaced points of the canonical metric rectification of the cross section at u j , i.e., where m j = g x (u j ) is the image of u j by G(x).</p><p>The optimization of the image of the circle centre consists in seeking a solution of :</p><formula xml:id="formula_14">min c (j1,j2)∈J j1 =j2 T t=1 I g −1 c (α t m j1 ) − I g −1 c (α t m j2 ) 2<label>(6)</label></formula><p>where J indexes a set of sections and I g −1 c (α t m j k ) , k ∈ {1, 2}, is the intensity of the image of α t m j k which is a point of a discretized rectified cross section.</p><p>This non-linear least-square optimization uses as initial solution for c the centre of the outer ellipse (which is usually very close to the image of the centre). A solution is found using (few) quasi-Newton iterations. This is illustrated in <ref type="figure">figure 6</ref>. The outer ellipse is validated if the residual of optimization is below a given threshold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We conducted a large number of experiments with synthetic images to quantify the performance of the proposed fiducial system. We use a combination of several metrics. These are the (1) false positive rate, (2) misidentification rate and (3) false negative rate. The false positive rate reports the rate at which a marker is detected where none is present. The misidentifiation rate expresses the rate at which a marker is successfully detected, but misidentified as another marker with another ID. The false negative rate measures the rate at which no marker is detected although one is present. Reducing the false negative rate increases the risk of false positives and misidentifications. Metrics (1) and (2) related to the proposed fiducial system are reported in <ref type="table">Table 1</ref> whereas the metric (3) is reported in <ref type="figure">Figure 8</ref>, from (a) to (c). All three metrics are influenced by the conditions under which the fiducial systems achieves a reported performance, and here in particular, the systems (i) scaling tolerance, (ii) occlusion tolerance, (iii) motion blur tolerance and (iv) depth of field blur.</p><p>Scaling tolerance is the ability to detect and identify markers at a very small or very large size in pixels. A wide scaling tolerance allows a larger usable range of cameras as well as a more opportunities for placing markers in a scene. <ref type="figure">Figure 8</ref> shows results for the tolerance from (i) to (iii) <ref type="figure">(Figure 8 from (a)</ref> to (c)) while <ref type="figure">Figure 8.(d)</ref> shows the obtained imaged center accuracy. The distance from the camera to the marker is used to evaluate the scaling tolerance (i) <ref type="figure">(Figure 8(a)</ref>). The percentage of occlusion represents the percentage of area of the fiducial which is not visible (i.e. occluded) over the total aera of the fiducial (reported in <ref type="figure">Figure 8.(b)</ref>). Finally, the amount of blur can be expressed by two different quantities representing respectively the amount of blur due to depth of field and the amount of motion blur (supposed unidirectional in accordance with our assumptions)</p><p>The results delivered by the proposed solution has been compared to the ones delivered by the three following fiducial marker systems: i) ARTKPlus <ref type="bibr" target="#b11">[12]</ref>: this system remains, yet not recent, a reference in the publicly available solution; ii) PRASAD <ref type="bibr" target="#b15">[16]</ref>: this system is the closest to our proposal as using concentric rings in order the propose a fiducial detection resiliant to motion blur; iii) RuneTag <ref type="bibr" target="#b1">[2]</ref>: as we want to demonstrate the robustness of our system to severe occlusions, we also decided to conduct a comparison against this recent solution.</p><p>Scene. The following evaluation is conducted on a scene consisting of a single marker. The size of each "category" of marker in the scene is defined so that they all cover the same area of the supporting plane and that a circular marker is of unit radius. The supporting plane belongs to the xyplane while the marker center corresponds to the scene origin. The marker center is located at (x m , y m , D) w.r.t. the camera coordinate system, where (x m , y m ) is randomly distributed in [−0.5, 0.5] 2 and D is the distance from the marker center to the camera optical center whose unit is the circular marker radius. If not specified, the angle between the optical axis and the supporting plane normal is randomly distributed in [0, 75] degrees.</p><p>Camera. The synthesized cameras obey a pinhole camera model with square pixels; the principal point is at the center of the image and the focal length is of 800 pixels while the image resolution is 640 × 360 pixels.</p><p>Image signal corruption.</p><p>As we are dealing with binary patterns, pixels take their values in {0, 255}, resp. black or white. In order to vary both the contrast and the signal to noise ratio, once the original image is rendered, all its pixel values are divided by the scalar value c randomly distributed in <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">6]</ref>, thus simulating different lighting conditions. A first blur component related to the depth of field is simulated by applying a gaussian filtering of standard deviation σ randomly distributed in [0, 2]. Then a unidirectional motion blur of length l (whose magnitude is 0 pixel if not specified) is applied in a random direction. Finally, a noise of n gray pixels randomly distributed in [0, 5] is added on every pixel of the obtained image.</p><p>Results. All the results in <ref type="figure">Figure 8</ref> are expressed in terms of detection rate, i.e. τ d = 1 − τ n where τ n is the false negative rate. We can see that even without any occlusion and motion blur, the proposed system already outperforms the other ones. This can be mainly justified by the following: extensive experiments have shown that our system perform quite well even in the case where the marker is strongly inclined relatively to the pixel plane. This comes mainly from the fact that there are always image cuts along which the concentric circles are "minimally distorted", and identical information content is equally readable along any single direction. Markers that require the extraction of data from one or more dedicated directions to extract encoded information (including a classical barcode as well as 2D patterns) pay their much higher information-coding capacity with an increase in misidentifications with distortion increases. Tolerance to distortion must then be countered by mechanisms that avoid ID collisions, but these increase the Hamming distance and reduce their information-coding capacity. It is also important to mention that the RuneTag solution presents very poor performance as the solution relies on the performance of an ellipse detection algorithm used to detect the imaged circular dots whose performance drastically drop down while the size of the imaged marker is decreasing. Accuracy under very challenging shooting conditions. In the last experiment, whose results are illustrated in <ref type="figure">Figure  8</ref>(d), the tests have been run in very challenging conditions in term of lighting, noise, distance and motion blur. In such conditions, no fiducial system used for comparison has been able to detect any marker out of 800 images apart from the proposed one, which has been able to detect 506 of them, (i.e. 63 %). This experiment has been conducted with c = 5, i.e. pixels values ranking between 0 and 51 pixels, a noise randomly distributed in [0, 10] pixels and with a camera to marker distance D = 30.</p><p>We want to emphasize two important points with these experiments. The first one is that, even in such very challenging conditions, we obtain a high detection rate, i.e. 94%, 80%, 57% and 22% for a magnitude of motion blur of 0, 5, 10 and 15 pixels respectively. The second point is Motion blur length l (pixels) Imaged center error (pixels) <ref type="figure">Figure 8</ref>. Detection rate (a) vs. the distance D (b) vs. the percentage of occlusion (c) vs. motion blur magnitude (d) Accuracy of the imaged center estimation vs. the motion blur magnitude. See text for details.</p><formula xml:id="formula_15">(a) (b) (c) (d)</formula><p>to emphasize that, even under these challenging conditions, with an increasing motion blur magnitude, the overall accuracy of the imaged center estimation is less than one pixel, while its median is less than 0.4 pixels for a motion blur magnitude less than l = 10 pixels. False negative and confusion. Lastly, <ref type="table">Table.</ref> 1 shows the results of the evaluation of the false positive and the interconfusion marker rate. In the current implementation, a ring (black or white) holds a 1-bit information, its width being 0.10 or 0.15 (w.r.t. the outer circle of unit radius). In such a way, a library of fiducial markers composed of N rings has 2 2N −1 unique markers. We use N = 3 rings markers in our experiments, e.g. a total number of 32 IDs. Once a fiducial marker has been detected and its 1D signals along the cross sections have been rectified via the homoraphy (5) obtained through the optimization (6), the latter are read in a robust manner via the distance proposed in <ref type="bibr" target="#b8">[9]</ref> then delivering the marker ID associated to a marker profile. The false positive rate evaluation has been performed on five video sequences whose the image resolution is 640 × 360 pixels. All the systems but PRASAD present a false positive rate of 0%. However, our system presents a misidentification rate of 2.8% which is more than the ones delivered by ARTK-Plus but still negligible for a large number of applications such as automatic 3D reconstruction via Structure-from-Motion techniques or also camera localization for which constraints brought by the geometry can be used for ID disambiguation (e.g. through guided matching techniques) as long as a subset of imaged fiducial are correctly detected and identified in the image. Time performance. The current CPU implementation delivers a frame rate of 4fps on one CPU (i5-4590, 3Ghz) core on a 1280 × 720 image (showing up to 5 markers), which has been raised to 11 fps using a preliminary GPU (NVidia GTX 980 Ti, CUDA 7.0) implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work we presented a new fiducial system based on concentric circles and we showed that our system  <ref type="figure">Figure 9</ref>. Detection and matching of fiducial across two images under challenging shooting conditions: the left image is correctly focused contrary to the right image which is acquired by a fast moving camera which generates motion blur. The ARToolkitPlus system correctly detects and identifies its markers (surrounded in magenta) only in the sharp (left) image whereas no one is detected in the blurred view. The proposed system correctly detects and identifies the imaged markers in the two images.</p><p>provides an high detection accuracy as well as a good recognition rate in many challenging conditions, ranging from severe occlusions to motion blur to illumination changes. The code will be released in open source, available at http://github.com/lcalvet/CCTag.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 .</head><label>3</label><figDesc>(a) A circular fiducial with four black rings. (b) A circular fiducial with one single ring painted with a continuous gradation of gray hues (the inner circle has a near infinitesimal radius). Note that the circles of the fiducial in (a) are equipotentials in (b). (c) An imaged pattern. (d) The gradient map of the imaged pattern. (e) The field lines of the gradient map. an image point u ∈ H([−1, 1] 2 ) is given by (see figure 3(c))</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>(a) Three images of circular fiducial. (b) Binary edge images returned by the edge detection on which has been plotted a set of linked sequences. (c) Images of resulting votes. u S (k,2N ) it passes at least one field line in the continuous image. The voting procedure applied on real images is illustrated in figure 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 .</head><label>5</label><figDesc>(a) Flow conservation in a continuous image.. (b) Flow conservation in a discrete image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 .Figure 7 .</head><label>67</label><figDesc>(a) Estimated image of the centre at each iteration (b) The signal collected on the cross sections before the optimization (c) Rectified signal after optimization. (a) Selected edge points with associated gradient directions. (b) Cross sections. Proposition 2. Consider an adequate 2D affine representation of the image plane such that the (elliptical) image of a circle has simplified equation Q 11 u 2 + Q 22 v 2 = 1. Then, for any variable point u = (u, v) ⊤ , the matrix function 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>Table 1. False positive and misidentification evaluation. See text for details.</figDesc><table>Prop. PRASAD 
RuneTag 
ARTKPlus 
Nb. 
frames 
Garden 
1 
51 
0 
0 
1268 
False pos. 
0% 
4% 
0% 
0% 
Indoor 
0 
21 
0 
0 
1685 
False pos. 
0% 
1.3% 
0% 
0% 
Street 
0 
80 
0 
0 
1153 
False pos. 
0% 
6.9% 
0% 
0% 
Stripes 
0 
172 
0 
0 
762 
False pos. 
0% 
22.6 
0% 
0% 
Text 
0 
23 
0 
0 
525 
False pos. 
0% 
4.4% 
0% 
0% 
Total 
1 
347 
0 
0 
5393 
False pos. 
0% 
6.4% 
0% 
0% 
Nb. detect. 
744 
221 
72 
351 
800 
Fig. 8 (a) 
93% 
27.6% 
9% 
44% 
Nb. conf. 
21 
71 
0 
0 
800 
Fig. 8 (a) 
2.8% 
32% 
0% 
0% 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We write it as the inverse of some matrix form because we actually use (G(u)) −1 in the optimization step described in the sequel..</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement.</head><p>This work has been supported by the POPART EU-H2020 project (number 644874) <ref type="bibr" target="#b0">[1]</ref>.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<ptr target="http://www.popartproject.eu/" />
		<title level="m">POPART project</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Rune-tag: A high accuracy fiducial marker with strong occlusion resilience</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bergamasco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Albarelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rodolà</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torsello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="113" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Pi-tag: a fast image-space marker design based on projective invariants</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bergamasco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Albarelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torsello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Vis. Appl</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1295" to="1310" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">An enhanced structure-frommotion paradigm based on the absolute dual quadric and images of circular points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Calvet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gurdjos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="985" to="992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Camera tracking based on circular point factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Calvet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gurdjos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Charvillat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Pattern Recognition (ICPR)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2128" to="2131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A computational approach to edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Canny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="679" to="698" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Reliable Fiducial Detection in Natural Scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Claus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Fitzgibbon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<editor>T. Pajdla and J. Matas</editor>
		<meeting><address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">3024</biblScope>
			<biblScope unit="page" from="469" to="480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">ARTag, a Fiducial Marker System Using Digital Techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fiala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="590" to="596" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Reading 1d barcodes with mobile phones using deformable templates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Gallo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Manduchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1834" to="1843" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Robust image features: Concentric contrasting circles and their image extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">B</forename><surname>Gatrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">A</forename><surname>Hoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">W</forename><surname>Sklair</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics-DL tentative</title>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="page" from="235" to="244" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Multiple View Geometry in Computer Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
	<note>second edition</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Marker tracking and HMD calibration for a video-based augmented reality conferencing system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Billinghurst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE and ACM International Workshop on Augmented Reality (IWAR)</title>
		<imprint>
			<publisher>IEEE Comput. Soc</publisher>
			<date type="published" when="1999" />
			<biblScope unit="page" from="85" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Automatic detection of circular objects by ellipse growing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ken-Ichi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Naoya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Image Graphics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="35" to="50" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Robust visionbased underwater homing using self-similar landmarks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nègre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pradalier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dunbabin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Field Robotics</title>
		<imprint>
			<biblScope unit="page" from="360" to="377" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Euclidean structure from n 2 parallel circles: Theory and algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W P</forename><surname>Gurdjos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sturm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A motion blur resilient fiducial for quadcopter imaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<meeting><address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="254" to="261" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Monospectrum marker: an AR marker robust to image blur and defocus. The Visual Computer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Toyoura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Aruga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Turk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013-12" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="1035" to="1044" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">ARToolKitPlus for Pose Tracking on Mobile Devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Schmalstieg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision Winter Workshop (CVWW)</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="139" to="146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Efficient detection of projected concentric circles using four intersection points on a secant line</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Pattern Recognition (ICPR)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
