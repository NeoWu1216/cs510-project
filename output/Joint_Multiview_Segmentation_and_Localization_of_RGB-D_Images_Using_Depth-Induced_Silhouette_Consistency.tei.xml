<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Joint Multiview Segmentation and Localization of RGB-D Images using Depth-Induced Silhouette Consistency</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sen University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">SYSU-CMU Shunde International Joint Research Institute</orgName>
								<address>
									<country key="CN">P.R. China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwei</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Cai</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyang</forename><surname>Chao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sen University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">SYSU-CMU Shunde International Joint Research Institute</orgName>
								<address>
									<country key="CN">P.R. China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Rui</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Joint Multiview Segmentation and Localization of RGB-D Images using Depth-Induced Silhouette Consistency</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:41+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we propose an RGB-D camera localization approach which takes an effective geometry constraint, i.e. silhouette consistency, into consideration. Unlike existing approaches which usually assume the silhouettes are provided, we consider more practical scenarios and generate the silhouettes for multiple views on the fly. To obtain a set of accurate silhouettes, precise camera poses are required to propagate segmentation cues across views. To perform better localization, accurate silhouettes are needed to constrain camera poses. Therefore the two problems are intertwined with each other and require a joint treatment. Facilitated by the available depth, we introduce a simple but effective silhouette consistency energy term that binds traditional appearance-based multiview segmentation cost and RGB-D frame-to-frame matching cost together. Optimization of the problem w.r.t. binary segmentation masks and camera poses naturally fits in the graph cut minimization framework and the Gauss-Newton non-linear least-squares method respectively. Experiments show that the proposed approach achieves state-of-the-arts performance on both tasks of image segmentation and camera localization.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Object scanning in 3D is an important topic in computer vision with many applications. With the popularity of consumer-level depth cameras, even untrained users are able to scan objects at home. However, obtaining accurate camera poses is a major challenge for existing scanning systems. Typical RGB-D camera tracking systems leverage on either frame-to-frame matching <ref type="bibr" target="#b13">[14]</ref> or frame-to-model matching <ref type="bibr" target="#b18">[19]</ref> to localize cameras. In both cases drift is a common problem. For frame-to model tracking system such as KinectFusion <ref type="bibr" target="#b18">[19]</ref> where online depth images are constantly integrated into a truncated signed distance function (TSDF) based volumetric representation <ref type="bibr" target="#b5">[6]</ref>, even small errors of camera poses will make the TSDF model blurry and consequently fine details are lost.</p><p>Loop closure detection and pose graph optimization are effective tools to address the above problem. Additional features, such as colors <ref type="bibr" target="#b13">[14]</ref>, local features <ref type="bibr" target="#b22">[23]</ref>, and occluding contours <ref type="bibr" target="#b27">[28]</ref> have been considered in literature. However, another important type of constraints, i.e. geometric constraints, is overlooked by these approaches. Examples of the geometric constraints include epipolar tangency criterion <ref type="bibr" target="#b25">[26]</ref> and silhouette consistency <ref type="bibr" target="#b11">[12]</ref>, which has been proved to be very helpful in camera calibration. In this work, we propose a method to incorporate geometric constraints in camera localization. Specifically, we jointly optimize the set of silhouettes and the camera poses, requiring that the silhouettes and the camera poses are consistent.</p><p>Existing related approaches <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b25">26]</ref> usually assume that a set of accurate silhouettes are provided. However in practice segmenting object in all viewpoints requires tedious user interactions. On the other hand, automatic multiview segmentation methods may fail in general cases, or the outputted silhouettes are not accurate enough to provide useful constraints for localization. To cope with this issue, our proposed method jointly perform a silhouette-consistent multiview segmentation on the fly while optimizing camera localization.</p><p>In multiview segmentation, accurate camera poses are required to propagate segmentation cues across views. In localization, accurate and silhouettes are needed to provide useful constraints to camera poses. Therefore the two problems are intertwined with each other and a joint treatment is preferred. Although both multiview segmentation and RGB-D camera localization have been intensively studied in literature, few approaches have modeled the two problems jointly.</p><p>To this aim, we describe an RGB-D object scanning pipeline consisting of two steps, i.e. an online keyframe collecting step and an offline joint optimization step. At the online step, a user walks around an object with a depth camera in hand. Meanwhile a realtime tracker, e.g. Kinect-Fusion <ref type="bibr" target="#b18">[19]</ref>, evenly captures a set of keyframes covering the object. At the offline step, the proposed approach is adopted <ref type="figure">Figure 1</ref>. System Overview to jointly estimate the segmentation masks and the poses of keyframes. For simplicity and without loss of generality, we assumed the color and depth frames are already aligned. <ref type="figure">Fig. 1</ref> provides an overview of the system.</p><p>To model the two problem jointly, we introduce a novel silhouette consistency term to constrain both the segmentation masks and the camera poses. The silhouette consistency term has following merits:</p><p>1. It effectively penalizes inconsistent labeling between views, facilitated by the available depth.</p><p>2. It is sub-modular, enabling an efficient optimization.</p><p>Due to the silhouette consistency term, the joint segmentation and localization problem can be decoupled into two sub-problems and efficiently solved using off-the-shelf approaches, i.e. color model-based segmentation methods <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8]</ref> and graph-based RGB-D SLAM model <ref type="bibr" target="#b13">[14]</ref>. Optimization is iterated between estimating the binary foreground and backgorund labeling, whose objective is submodular and naturally fits in the graph cut framework <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b19">20]</ref>, and refining the camera poses, which naturally fits in the Gauss-Newton non-linear least-squares method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Multiview segmentation</head><p>Object segmentation from images has been studied intensively in literature and has been extended from monocular case <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b6">7]</ref> to object co-segmentation <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref> and to multiview configuration <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9]</ref>. Existing multiview segmentation methods can be broadly classified in two streams. The first stream focuses on a volumetric 3D reconstruction of the object, and then computes the segmentation as a byproduct by reprojecting the 3D model back to each view <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b3">4]</ref>. These approaches suffer from the volume size limit and are not pixel-accurate for high-resolution inputs. The second stream works on image domain. Some solve a binary MRF for each view using the unary term to carry information propagated by other views <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b8">9]</ref>. Some optimize an MRF containing all views and randomly generated 3D samples <ref type="bibr" target="#b7">[8]</ref>. Our approach falls into this stream, and solves an MRF simultaneously for all views. But unlike existing methods, we explicitly model silhouette consistency w.r.t. camera poses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Camera localization</head><p>RGB-D SLAM is also an intensively studied problem, in which typical systems usually consist of two steps, i.e. online tracking and offline optimization. For online tracking, many approaches have been developed, e.g. frameto-model matching <ref type="bibr" target="#b18">[19]</ref> and frame-to-frame matching <ref type="bibr" target="#b13">[14]</ref>. Beyond depth, additional features such as colors <ref type="bibr" target="#b13">[14]</ref>, local features <ref type="bibr" target="#b22">[23]</ref>, undistorted depth <ref type="bibr" target="#b26">[27]</ref> and contour information <ref type="bibr" target="#b27">[28]</ref>, have been explored. The work of Zhou et al. <ref type="bibr" target="#b27">[28]</ref> is the most related one to ours, which explicitly takes contour information into consideration when matching a frame to a model in the KinectFusion <ref type="bibr" target="#b18">[19]</ref> framework. However, this work is an online tracking method which does not consider all frames globally as in an offline optimization process. A major problem of online tracking is pose drift. Loop closure detection and pose graph optimization are effective tools to address the problem in an offline optimization process <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b17">18]</ref>. Our approach is a kind of offline optimization method. Different from existing techniques, we exploit another type of constraint, i.e. silhouette consistency, and enforce it in a new way by a proposed silhouette consistency energy term facilitated by the available depth. <ref type="figure">Figure 1</ref> shows the pipeline of the proposed approach which consists of an online data capture component and an offline optimization component. The offline optimization component consists of two modules, i.e. multiview segmentation and RGB-D image alignment. We first present the two modules independently, and then introduce a new silhouette consistency term which enables a joint optimization between them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Formulation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Notation</head><p>The input is a multiview sequence of keyframes con-</p><formula xml:id="formula_0">taining N RGB-D images {I i } N i=1 , {D i } N i=1 , and initial poses {T i } N i=1</formula><p>, which are usually obtained by a continuous tracker, e.g. KinectFusion <ref type="bibr" target="#b18">[19]</ref>. The variables we want to optimize are</p><formula xml:id="formula_1">• {S i } N i=1</formula><p>, the set of binary-valued silhouettes, where S i (p) = 1 denotes p is a foreground pixel, and S i (p) = 0 denotes background;</p><formula xml:id="formula_2">• {θ fgd i } N i=1 , {θ bgd i } N i=1</formula><p>, the set of foreground and background color models;</p><formula xml:id="formula_3">• {T i } N i=1</formula><p>, the set of camera poses that map local coordinates to world coordinates.</p><p>For convenience we denote them collectively as S, θ, T respectively. In the following, we will use i, j to index images, and p, q to index pixels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Multiview Segmentation w.r.t. S, θ</head><p>As a common objective in object segmentation, we want the binary labeling to agree with the foreground/background color models <ref type="bibr" target="#b19">[20]</ref>, which is enforced by the appearance energy</p><formula xml:id="formula_4">E Appearence (S, θ) = i p∈Ωi −Prob(I i (p) | S i (p), θ bgd i , θ fgd i )<label>(1)</label></formula><p>where Ω i denotes the set of pixels in the i-th image, and</p><formula xml:id="formula_5">Prob(I i (p) | S i (p), θ bgd i , θ fgd i ) denotes the probability that color I i (p) belongs to the foreground color model θ fgd i if S i (p) = 1, or the probability that I i (p) belongs to θ bgd i if S i (p) = 0.</formula><p>For each view, we train a Gaussian Mixture Model (GMM) for foreground and background respectively. Each GMM has five components in all experiments. Our experiments showed that in most indoor environment where RGB-D images are usually captured, the number components are good enough to model the color distributions.</p><p>The color models are efficiently learned from the initial set of silhouettes, which is obtained by projecting the visual hull induced by all image rectangles back to 2D. A user can additionally place a bounding box or draw scribbles to further constrain the problem. It is noted that the user only need to provide guidance in a few views, since our silhouette consistency term introduced in section 3.4 is able to effectively propagate these information across views. During the segmentation process, more guidance can be given in each iteration if the user is not satisfied with the results.</p><p>We also encourage the labeling to be smooth and aligned with image edges</p><formula xml:id="formula_6">E Smooth (S) = i p,q∈N4 w pq ||S i (p) − S i (q)|| 2 (2) where N 4 denotes a 4-neighborhood on image grid, w pq = exp(−||I i (p) − I i (q)||/γ 1 − ||D i (p) − D i (q)||/γ 2 )</formula><p>is a weight to encourage discontinuity on edges. For pixels without depth, w pq only considers color.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">RGB-D Image Alignment w.r.t. T</head><p>We adopt the frame-to-frame matching approach proposed by Kerl et al. <ref type="bibr" target="#b13">[14]</ref> to model both color and depth alignment error</p><formula xml:id="formula_7">E ColorAlign (T) = i p∈Ωi j∈Ni ||I i (p) − I j (q)|| 2 (3) E DepthAlign (T) = i p∈Ωi j∈Ni ||D i (p) − D j (q)|| 2 (4)</formula><p>whereΩ i is the set of pixels with valid depths, N i is the set of neighboring cameras of the i-th keyframe in a pose graph, and</p><formula xml:id="formula_8">q = π j (T −1 j T i π −1 i (p, D i (p)))<label>(5)</label></formula><p>is pixel p's correspondence in image j, with π j , π −1 i being the corresponding projection and inverse-projection respectively. Note that the poses T are parametrized in se(3) during optimization <ref type="bibr" target="#b0">[1]</ref>.</p><p>Before the global optimization, we need to construct a graph of keyframes, in which an edge connecting two keyframes means that the two frames view a large overlap of the common surface. Due to the trajectory drift problem, we need to carefully establish graph edges containing necessary loop-closures. Specifically, for each keyframe we collect candidates of neighbor frames by checking the angles of principal axis and distances of camera positions. If the two cameras meet a condition (e.g. angle ≤ 60 • and distance ≤ 0.5m), we further validate it by doing a dense alignment, i.e. minimizing Eq. (3) and (4) over a twonode graph. After the alignment, we count the number of matched pixels (e.g. difference of depths ≤ 0.5cm and angle of normals ≤ 15 • ). And if the ratio of matched pixels to total pixels with valid depth is above a threshold (e.g. 0.6), we establish an edge to connect them in the pose graph. Since in object scanning, images are usually captured in an outside-in mode, the trajectory of cameras is not as complex as in large-scale scene modeling <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b26">27]</ref>, and seldom drifts significantly. The loop-closure detection in object scanning is less challenge than in large-scale scene modeling. The above simple strategy successfully detected necessary loop-closures in all our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Silhouette Consistency w.r.t. S, T</head><p>So far camera localization and multiview segmentation are modeled independently, but as we have argued a joint treatment would benefit them from each other. To this aim, we introduce the following silhouette consistency term</p><formula xml:id="formula_9">E SilConsistency (S, T) = i p∈Ωi j =i S i (p)·||S i (p)−S j (q)|| 2 (6)</formula><p>Eq. (6) looks very similar to Eq. <ref type="formula">(3)</ref> and <ref type="formula">(4)</ref>. Readers may understand as that a silhouette is only an additional channel beyond depth and color, which supplement object contour information to the optimization. However, it is significantly different from color and depth channels due to the following properties.</p><p>First, the silhouette consistency depends on both the segmentation S and the poses T, therefore it connects and regularize both S and T; Second, the subscript j in (6) ranges over all images instead of only the neighboring views as in (3)(4). This property is crucial and helps to prevent incremental pose drift during optimization. As shown in <ref type="figure" target="#fig_0">Fig. 2</ref>, even a back view of the object provides constraints (for both segmentation and localization) and hints (for segmentation) to a front view; Third, the penalty on a pixel p is active only when S i (p) = 1, i.e. when p is a foreground pixel. This is coherent with the mathematical definition of silhouette consistency, i.e. any 3D point that lies on the object's surface must project inside all other silhouettes in 2D, while a 3D point that lies outside the surface could project to either inside or outside of other silhouettes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Overall Energy</head><p>Putting all the pieces together, we obtain the overall objective function</p><formula xml:id="formula_10">E All (S, θ, T) = E Appearence (S, θ) + E Smooth (S) (7) + E DepthAlign (T) + E ColorAlign (T) + E SilConsitency (S, T)</formula><p>Without the silhouette consistency term (6), segmentation and localization would have become two independent problems as the binary masks and the camera poses would have nothing to interact on. The silhouette consistency term enables a joint formulation of the two problems, which provides constraints and hints for both tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Optimization</head><p>The objective function depends on both discrete and continuous sets of variables, whose minimization is challenging. Luckily the overall objective can be decomposed into two subproblems, namely segmentation and localization</p><formula xml:id="formula_11">E Segmentation = E Appearance + w 1 E Smooth + λ Sil E SilConsistency (8) E Localization = E DepthAlign + w 2 E ColorAlign + λ Sil E SilConsistency<label>(9)</label></formula><p>which can be optimized using off-the-shelf methods, i.e. Graph Cut <ref type="bibr" target="#b15">[16]</ref> and Gauss-Newton non-linear least-squares method <ref type="bibr" target="#b9">[10]</ref>. Minimization of the original problem is then reduced to solving the two subproblems iteratively. Alg. 1 provides an overview of the optimization. We set the coefficients w 1 = 50, w 2 = 0.1, and λ Sil = 0.1 in all experiments.  <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b8">9]</ref>. A single graph cut is run independently for each image. Possible multiview segmentation cues are preprocessed and encoded in unary terms.</p><p>Algorithm 1 Optimize E All Initialize S, θ by any monocular segmentation method. Initialize T by minimizing E DepthAlign + E ColorAlign using non-linear least squares <ref type="bibr" target="#b9">[10]</ref>. repeat repeat Fix T, S, min θ E Segmentation by GMM EM learning. Fix T, θ, min S E Segmentation by Graph Cut <ref type="bibr" target="#b15">[16]</ref>. until converged Fix S, θ, min T E Localization by non-linear least squares. until converged</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Optimize Segmentation</head><p>We optimize the segmentation subproblem in a block gradient descent fashion, as shown in Alg. 1. When keeping the foreground masks S fixed, the parameters of each image's GMM color model can be re-estimated by EM algorithm.</p><p>When optimizing E Segmentation w.r.t. segmentation masks S, it becomes a discrete optimization problem. Both E Appearance and E Smooth are widely used submodular energies. And it is easy to check that the E SilConsistency is also submodular</p><formula xml:id="formula_12">E SilConsistency (0, 1) + E SilConsistency (0, 1) = 1 (10) &gt; E SilConsistency (0, 0) + E SilConsistency (1, 1) = 0<label>(11)</label></formula><p>Therefore the subproblem can be efficiently solved by graph cut <ref type="bibr" target="#b15">[16]</ref>. <ref type="figure" target="#fig_1">Fig. 3</ref> compares the segmentation graph of our approach to some of the existing approaches <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b8">9]</ref>. Our silhouette consistency term acts as a kind of smoothness constraints to regularize labeling across images. In contrast to <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b8">9]</ref>, our approach segments all images simultaneously by one graph cut. Labeling cues in one view are effectively propagated to all other views via known depth and camera poses. Hard-to-segment regions in one view, e.g. regions close to silhouette boundaries where depth usu-ally misses, will get hints from other views, in which corresponding regions may have depth and be easy to segment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Optimize Localization</head><p>With S, θ fixed, the localization subobjective E Localization is the sum of all quadratic terms with T and therefore can be effectively solved by the Gauss-Newton non-linear least-squares method. Specifically, we parameterize T i by a 6-vector ξ i = (a i , b i , c i , α i , β i , γ i ) that represents an incremental transformation relative to the current T i . Here </p><p>where J is the Jacobian and r is the residual vector. Both J and r are linear combinations of three terms computed from term (3), (4) and <ref type="bibr" target="#b5">(6)</ref>. Solving the linear equation yields an improved camera transformation</p><formula xml:id="formula_14">T k+1 i = exp(ξ i )T k i<label>(13)</label></formula><p>Details of derivation on the Jacobian are provided in the supplementary material.</p><p>To prevent poses from trapping in bad local minimal and to improve optimization speed, we adopt a three level coarse-to-fine pyramid scheme. Blocks of the combined measurement Jacobian J i and residual r i can be computed in GPU, and reduce to a single 6N × 6N linear equation. Then we solve it on CPU by the Cholesky decomposition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiment</head><p>To evaluate the proposed approach, we collected ten RGB-D datasets with the ASUS Xtion sensor. <ref type="figure" target="#fig_3">Fig. 4</ref> shows some sample color frames. The KinFu tracker <ref type="bibr" target="#b18">[19]</ref> is used to continuously track the online stream, and a new keyframe is saved when its relative rotation or translation to the last keyframe is larger than 10 • or 10cm. Each sequences consists of about 60 keyframes of 640 × 480 depth and color images, with their corresponding initial poses. The proposed approach takes about 200s to run on a regular PC and outputs refined camera poses, silhouettes, and a high-quality 3D mesh model. All keyframes are manually segmented to generating ground truth silhouettes. We set γ 1 = 30 and γ 2 = 30. Results are not very sensitive to these two parameters. To balance any two energy terms λ 1 E 1 + λ 2 E 2 = λ 1 k e k 1 + λ 2 k e k 2 in Eq. <ref type="formula">(8)</ref> and <ref type="formula" target="#formula_11">(9)</ref>, we determine the medians e med 1 , e med 2 of {e k 1 }, {e k 2 } respectively, and set λ 1 /λ 2 = e med 2 /e med 1 . This strategy works well in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Segmentation</head><p>An important feature of our object scanning system is it is able to generate a set of silhouettes on the fly. Given the keyframes as input, initial silhouettes are generated by projecting the commonly visible part of 3D space back to each image, which is the intersection of all viewing cones induced by the image rectangles. Here, we assume the object completely appears in all views. Although our system enables a user to provide bounding boxes or scribbles to guide the segmentation, we did not make use of the user input in experiments. The GMM color models are initialized from these initial masks.</p><p>Tbl. 1, provides a quantitative evaluation of the segmentation results among Grabcut <ref type="bibr" target="#b19">[20]</ref>, Djelouah'13 <ref type="bibr" target="#b7">[8]</ref>, Diebold'15 <ref type="bibr" target="#b6">[7]</ref> and ours. Accuracy is measured by the percentage of mislabled pixels compared to hand-labeled ground truths. Grabcut performs inferior compared to all methods since multiview geometric cues are not explored. In some cases, masks in Djelouah'13 appear to be inflating since the 3D consistency enforced by its sparse 3D samples does not penalize background pixels being labeling foreground. To achieve accurate results, Diebold'15 needs about 5.3 scribbles for each image in average since it cannot leverage on results/guidance of other views. Our results outperform the others, since the silhouette consistent term makes substantial use of the available depth and enforce the consistency among multiple views explicitly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Localization</head><p>Directly evaluating the accuracy of camera poses is a challenging task since ground truth poses are difficult to obtain in general. Instead, we evaluate poses by two indirect measures, i.e. the calibration ratio <ref type="bibr" target="#b1">[2]</ref> and accuracy of the reconstructed 3D model. Calibration ratio is based on the observation that given a set of perfect silhouettes and perfect camera poses, the viewing ray of every foreground pixel should intersect with the silhouette-induced viewing cones of all the other views in a common intersection. And the ratio for image i is defined as</p><formula xml:id="formula_15">C i = 1 |M i | (N − 1) p∈Mi Φ(r p )<label>(14)</label></formula><p>where M i is the set of foreground pixels of image i, N is the number of cameras. r p is the induced viewing ray of pixel p, and Φ(r p ) is the maximum number of cameras whose viewing cones induced by their own silhouettes have at least one common interval along r p . Therefore, if both camera poses and silhouettes are perfect, calibration ratio is equal to one, otherwise, it will be less than one. Since camera poses and silhouettes are the only two reasons that affect the calibration ratio, if we fix the silhouettes to be the manually labeled ground truth, calibration ratio is a good measure of the accuracies of camera poses. Tbl. 2 shows the averaged calibration ratios over all cameras in each iteration. The Calibration ratios steadily increase along iteration, which indicates that the poses are becoming more and more accurate. Calibration ratios converges after four iterations in almost all cases we tested. <ref type="figure">Fig. 7</ref> provides a visual comparison of the reconstructed models. Without the silhouette consistent energy, our approach reduce to the RGB-D alignment approach presented in Section 3.3, which is itself a typical offline optimization method for improving camera poses <ref type="bibr" target="#b13">[14]</ref>. There for we use it as a baseline. As shown in the figure, models generated by our joint optimization preserve more fine details, such as the keyboard on the belly of Tomcat. Beyond the usage in calibration, silhouettes can help depth integration <ref type="bibr" target="#b4">[5]</ref> and mesh optimization <ref type="bibr" target="#b20">[21]</ref> to preserve fine structures. Further discussion on this direction is beyond scope of this work.</p><p>We scanned the Tomcat and MusicBox by a commercial high-quality 3D scanner 1 whose precision is about 1mm in general. <ref type="figure">Fig. 6</ref> shows model errors of Kinfu, RGB-D alignment only and the joint optimization. As shown in the figure, our model obtains significantly lower errors. Since Kinfu has no offline optimization, drift of camera poses significant hurts model qualities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We have presented an RGB-D camera localization approach that effectively exploits the silhouette constraints. Unlike existing silhouette-based calibration approaches which usually assume accurate silhouettes are provided, our system is able to generate object silhouettes on the fly during optimization, making its usage very practical. Experiments demonstrated large improvements on both tasks of object segmentation and camera localization.  (a) Color image (b) Grabcut <ref type="bibr" target="#b19">[20]</ref> (c) Djelouah'13 <ref type="bibr" target="#b7">[8]</ref> (d) Diebold'15 <ref type="bibr" target="#b6">[7]</ref> (e) Ours <ref type="figure">Figure 5</ref>. Examples of generated silhouettes. <ref type="figure">Figure 6</ref>. Quantitative evaluation of model error. Row 1-3 show results of Kinfu <ref type="bibr" target="#b18">[19]</ref>, RGB-D alignment only (i.e. our approach with silhouette consistency term disabled) and joint optimization, respectively.  <ref type="table">Table 2</ref>. Averaged calibration ratios increase with iterations. <ref type="figure">Figure 7</ref>. Visual comparison of generated meshes. Row 2-4 are meshes generated by Kinfu <ref type="bibr" target="#b18">[19]</ref>, RGB-D alignment only (i.e. our approach with silhouette consistency term disabled) and joint optimization. Row 5-7 show close-up views of the respective models.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Silhouette consistency term constrains both near and far views. A pixel inside the silhouette of a back view is projected to a near (side) view and a far (front) view via depth and camera poses. The left figure shows a consistent case, while the right figure shows an inconsistent case which introduces a cost λ sil .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>(Left) Segmentation Graph of our approach, labeling of all views are solved simultaneously. (Right) Segmentation graph of</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(a i , b i , c i ) is a translation vector, and (α i , β i , γ i )can be interpreted as angular velocity. Stacking all ξ i together, we get a 6N -dimensional variable ξ. To solve each iteration we calculate the linearized least-</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Sample keyframes of our collected datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>Tomcat Musicbox Donkey Dragon Chair Horse Plant Gundam Bag Bag2Table 1. Comparison of error rates of generated silhouettes.</figDesc><table>GrabCut [20] 
5.31 
4.78 
5.63 
4.24 
0.98 
5.20 
5.98 
9.31 
3.38 6.17 
Djelouah'13 [8] 
1.54 
1.46 
1.63 
1.27 
0.26 
1.50 
4.20 
3.00 
0.85 1.92 
Diebold'15 [7] 
0.35 
0.37 
0.31 
0.29 
0.14 
0.63 
2.73 
0.52 
0.20 0.39 
Ours 
0.35 
0.35 
0.28 
0.20 
0.13 
0.41 
2.03 
0.48 
0.23 0.31 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>Tomcat Musicbox Donkey Dragon Chair Horse Plant Gundam</figDesc><table>Bag 
Bag2 
Initial 
0.75 
0.79 
0.66 
0.78 
0.56 
0.63 
0.55 
0.51 
0.66 
0.77 
Iter 1 
0.83 
0.84 
0.82 
0.88 
0.74 
0.81 
0.69 
0.87 
0.89 
0.93 
Iter 2 
0.89 
0.91 
0.90 
0.90 
0.89 
0.94 
0.77 
0.90 
0.93 
0.94 
Iter 3 
0.97 
0.94 
0.92 
0.92 
0.95 
0.98 
0.88 
0.92 
0.95 
0.96 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Artec3D, http://www.artec3d.com</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">A tutorial on se (3) transformation parameterizations and on-manifold optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-L</forename><surname>Blanco</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">3</biblScope>
		</imprint>
		<respStmt>
			<orgName>University of Malaga, Tech. Rep</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">On using silhouettes for camera calibration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Boyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ACCV 2006</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Interactive graph cuts for optimal boundary &amp; region segmentation of objects in nd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Y</forename><surname>Boykov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-P</forename><surname>Jolly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision, 2001. ICCV 2001. Proceedings. Eighth IEEE International Conference on</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="105" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Automatic 3d object segmentation in multiple views using volumetric graph-cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Vogiatzis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hernández</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="14" to="25" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multiview stereo and silhouette consistency via convex functionals over convex domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kolev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1161" to="1174" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A volumetric method for building complex models from range images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Levoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH</title>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="page" from="303" to="312" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Interactive multi-label segmentation of rgb-d images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Diebold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Demmel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hazırbaş</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Moeller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Scale Space and Variational Methods in Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multi-view object segmentation in space and time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Djelouah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-S</forename><surname>Franco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Boyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">Le</forename><surname>Clerc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV), 2013 IEEE International Conference on</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Sparse multi-view consistency for object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Djelouah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-S</forename><surname>Franco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Boyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">Le</forename><surname>Clerc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perez</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Lsd-slam: Large-scale direct monocular slam</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schöps</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2014</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fusion of multiview silhouette cues using a space occupancy grid</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-S</forename><surname>Franco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Boyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision, 2005. ICCV 2005. Tenth IEEE International Conference on</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1747" to="1753" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Silhouette coherence for camera calibration under circular motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hernández</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schmitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="343" to="349" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Snakes: Active contour models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kass</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Witkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Terzopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="321" to="331" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Robust odometry estimation for RGB-D cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kerl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sturm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE International Conference on Robotics and Automation</title>
		<meeting><address><addrLine>Karlsruhe, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="3748" to="3754" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Fast joint estimation of silhouettes and dense 3d geometry from multiple images. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kolev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="493" to="505" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">What energy functions can be minimized via graph cuts? Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kolmogorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zabin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multiple view object cosegmentation using appearance and stereo cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kowdle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">N</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2012</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">g2o: A general framework for graph optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kuemmerle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Grisetti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Strasdat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Konolige</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Burgard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Robotics and Automation (ICRA)</title>
		<meeting>the IEEE International Conference on Robotics and Automation (ICRA)<address><addrLine>Shanghai, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="3607" to="3613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Kinectfusion: Real-time dense surface mapping and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Newcombe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Izadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Hilliges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Molyneaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hodges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Fitzgibbon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">10th IEEE International Symposium on Mixed and Augmented Reality</title>
		<meeting><address><addrLine>Basel, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="127" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Grabcut: Interactive foreground extraction using iterated graph cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kolmogorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Multi-view reconstruction using photo-consistency and exact silhouette constraints: A maximum-flow formulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">N</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<idno>17-20 Octo- ber 2005</idno>
	</analytic>
	<monogr>
		<title level="m">10th IEEE International Conference on Computer Vision (ICCV 2005</title>
		<meeting><address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="349" to="356" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Exact voxel occupancy with graph cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Snow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zabih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="345" to="352" />
		</imprint>
	</monogr>
	<note>Proceedings. IEEE Conference on</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A benchmark for the evaluation of rgb-d slam systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sturm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Engelhard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Endres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Burgard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Conference on Intelligent Robot Systems (IROS)</title>
		<meeting>of the International Conference on Intelligent Robot Systems (IROS)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Cosegmentation revisited: Models and optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vicente</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kolmogorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2010</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="465" to="479" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Object cosegmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vicente</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kolmogorov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2217" to="2224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Reconstruction of sculpture from its profiles with unknown camera positions. Image Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-Y</forename><forename type="middle">K</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="381" to="389" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Simultaneous localization and calibration: Self-calibration of consumer depth cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2014</title>
		<meeting><address><addrLine>Columbus, OH, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Depth camera tracking with contour cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2015</title>
		<meeting><address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="632" to="638" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
