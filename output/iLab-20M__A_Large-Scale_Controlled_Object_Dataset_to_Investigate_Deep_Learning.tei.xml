<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">iLab-20M: A large-scale controlled object dataset to investigate deep learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Borji</surname></persName>
							<email>aborji@crcv.ucf.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Center for Research in Computer Vision</orgName>
								<orgName type="institution">University of Central Florida</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saeed</forename><surname>Izadi</surname></persName>
							<email>sizadi@aut.ac.ir</email>
							<affiliation key="aff1">
								<orgName type="institution">Amirkabir University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Itti</surname></persName>
							<email>itti@usc.edu</email>
							<affiliation key="aff2">
								<orgName type="institution">University of Southern California</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">iLab-20M: A large-scale controlled object dataset to investigate deep learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:41+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Tolerance to image variations (e.g., translation, scale,  pose, illumination, background)  is an important desired property of any object recognition system, be it human or machine. Moving towards increasingly bigger datasets has been trending in computer vision especially with the emergence of highly popular deep learning models. While being very useful for learning invariance to object inter-and intra-class shape variability, these large-scale wild datasets are not very useful for learning invariance to other parameters urging researchers to resort to other tricks for training models. In this work, we introduce a large-scale synthetic dataset, which is freely and publicly available, and use it to answer several fundamental questions regarding selectivity and invariance properties of convolutional neural networks. Our dataset contains two parts: a) objects shot on a turntable: 15 categories, 8 rotation angles, 11 cameras on a semi-circular arch, 5 lighting conditions, 3 focus levels, variety of backgrounds (23.4 per instance) generating 1320 images per instance (about 22 million images in total), and b) scenes: in which a robotic arm takes pictures of objects on a 1:160 scale scene. We study: 1) invariance and selectivity of different CNN layers, 2) knowledge transfer from one object category to another, 3) systematic or random sampling of images to build a train set, 4) domain adaptation from synthetic to natural scenes, and 5) order of knowledge delivery to CNNs. We also discuss how our analyses can lead the field to develop more efficient deep learning methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Object and scene recognition is arguably the most important problem in computer vision and while humans do it quickly and almost effortlessly, machines still lag behind humans. In some cases, where variability is relatively low (e.g., frontal face recognition) machines outperform humans but they do not perform quite as well when variety is high. Hence, the crux of the object recognition problem is tolerance to intra-and inter-class variability, lighting, scale, in-plane and in-depth rotation, background clutter, etc <ref type="bibr" target="#b8">[9]</ref>.</p><p>Thanks to big data and deep neural networks, computer vision has recently enjoyed a rapid progress, witnessed by high accuracies over the ImageNet dataset (top-5 error rate between 3-10% over 1,000 object categories). Recent models (e.g., Alexnet <ref type="bibr" target="#b30">[31]</ref>, VGG <ref type="bibr" target="#b53">[54]</ref>, Overfeat <ref type="bibr" target="#b49">[50]</ref>, GoogLeNet <ref type="bibr" target="#b56">[57]</ref>, and ResNet <ref type="bibr" target="#b22">[23]</ref>) have surpassed previous scores in several benchmarks such as generic object and scene recognition <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b53">54]</ref>, object detection <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b19">20]</ref>, semantic scene segmentation <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b19">20]</ref>, face detection and recognition <ref type="bibr" target="#b65">[66]</ref>, texture recognition <ref type="bibr" target="#b6">[7]</ref>, fine-grained recognition <ref type="bibr" target="#b38">[39]</ref>, multi-view 3D shape recognition <ref type="bibr" target="#b55">[56]</ref>, activity recognition <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b27">28]</ref>, and saliency prediction <ref type="bibr" target="#b31">[32]</ref>.</p><p>One chief concern regarding the wild large-scale benchmarks and datasets, however, is the lack of control over data collection procedures and deep comprehension of stimulus variety. While existing large-scale datasets are very rich in terms of inter-and intra-class variability, they fail to probe the ability of a model to solve the general invariance problem. In order words, natural image datasets (e.g., Ima-geNet <ref type="bibr" target="#b7">[8]</ref>, SUN <ref type="bibr" target="#b63">[64]</ref>, PASCAL VOC <ref type="bibr" target="#b13">[14]</ref>, LabelMe <ref type="bibr" target="#b47">[48]</ref>, Tiny <ref type="bibr" target="#b60">[61]</ref>, and MS COCO <ref type="bibr" target="#b37">[38]</ref>) are inherently biased in the sense that they do not offer all object variations <ref type="bibr" target="#b59">[60]</ref>. To remedy this, some works (e.g., <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b40">41]</ref>) have resorted to synthetic datasets where several object parameters exist.</p><p>Ideally, we would like models to be tolerant to identitypreserving image variations (e.g., variation in position, scale, pose, illumination, occlusion). To probe this, some researchers have used synthetic home-brewed datasets either by taking pictures of objects on a turntable (e.g., NORB <ref type="bibr" target="#b34">[35]</ref>, COIL <ref type="bibr" target="#b40">[41]</ref>, SOIL-47 <ref type="bibr" target="#b28">[29]</ref>, ALOI <ref type="bibr" target="#b18">[19]</ref>, GRAZ <ref type="bibr" target="#b41">[42]</ref>, BigBIRD <ref type="bibr" target="#b54">[55]</ref>) or by constructing 3D graphic models and rendering textures to them (e.g., Pinto et al. <ref type="bibr" target="#b44">[45]</ref>, Peng et al. <ref type="bibr" target="#b42">[43]</ref>). While proven to be beneficial in the past, these datasets are very small for training deep neural networks with millions of parameters. Further, they usually have small number of classes, instances per class, background variability, in plane and in-depth rotation, illuminations, scale, and total number of images. Here, to remedy these shortcomings, we introduce a large-scale controlled object dataset with rich variety and a larger set of images. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Several controlled datasets have been introduced in the past which have dramatically helped progress in computer vision <ref type="table">(Table 1)</ref>. Two famous examples are FERET face <ref type="bibr" target="#b43">[44]</ref> and MNIST digit <ref type="bibr" target="#b33">[34]</ref> datasets. Nowadays, we have face and digit recognition systems that perform either at the level of humans (e.g., <ref type="bibr" target="#b57">[58]</ref>) or superior (perhaps not as robust due to variations and noise). Similar datasets are available for generic object recognition but lack characteristics of a large-scale representative dataset covering many sorts of invariance (e.g., background clutter, shape, occlusion, size). For example, the COIL dataset <ref type="bibr" target="#b40">[41]</ref>, which also used a turntable to film 100 objects under various lightings and poses, contains one object instance per category (e.g., one telephone, one mug). The larger ALOI dataset <ref type="bibr" target="#b18">[19]</ref> contains 1,000 objects but few instances per category. The NORB dataset <ref type="bibr" target="#b34">[35]</ref> has 50 small toy objects (10 instances in each of 5 categories). Almost all available turntable datasets are small scale and not very rich in terms of variations.</p><p>Previous research using controlled datasets, such as turntables images, has been mainly focused on inspecting models or brewing concepts and ideas. Some recent works have attempted to show that there is a real benefit of these datasets in transferring knowledge to large-scale natural scene datasets <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b66">67]</ref>. This has been studied under the names of domain adaptation, task transfer, or multi-task learning. The idea here is that knowledge gained from a controlled dataset (or task), via turntables or graphic models, can be transferred to real-world naturalistic datasets with even different statistics (e.g., texture). For example, Peng et al. <ref type="bibr" target="#b42">[43]</ref> trained models on an augment of synthetically generated images (using a 3D graphics object model) and natural scenes (from ImageNet and PASCAL) and reported an improvement in accuracy over the latter datasets. They, however, did not probe whether the improvement was due to learning better invariance or instance level variety and richness. Some other works have also advocated similar directions <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b15">16]</ref>.</p><p>Another motivation for utilizing controlled datasets comes from neuroscience and cognitive vision literature. CNNs were initially inspired by the hierarchical structure of the visual ventral stream <ref type="bibr" target="#b17">[18]</ref>. They were later used to explain some physiological and behavioral data of humans and monkeys (e.g., <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b50">51]</ref>). It has been asserted that humans learn invariance with few exemplars a.k.a. zero-or one-shot learning. This is the opposite of the way that CNNs currently learn recognition. These models need an enormous amount of labeled data. In this work, we explore the ways a large-scale controlled dataset, containing rich information regarding various object parameters, can be utilized to improve object recognition performance. It is important to be aware of human performance to gauge the progress <ref type="bibr" target="#b3">[4]</ref>. Just recently, He et. al. <ref type="bibr" target="#b23">[24]</ref> reported a top-5 error rate of 4.9% on ImageNet which is lower than 5.1% human error rate on this dataset <ref type="bibr" target="#b46">[47]</ref>. This raises some questions: Have models surpassed humans? If yes, in what aspects? Is it theoretically possible to achieve a better performance than humans on these problems? etc.</p><p>Another related area to our work, which naturally fits well to turntable setups, is the manifold embedding and di-mensionality reduction. These techniques try to preserve and leverage the underlying low dimensional manifold in data in supervised or unsupervised manners (e.g., <ref type="bibr" target="#b68">[69,</ref><ref type="bibr" target="#b58">59]</ref>). For instance, Weston et al. <ref type="bibr" target="#b62">[63]</ref> introduced an embeddingbased regularizer to impose the same labels for the neighboring training samples to benefit from the structure in the data. They used gradient descent to optimize the regularizer and adopted it for CNNs. Another classic example is Siamese Networks <ref type="bibr" target="#b4">[5]</ref> which are two identical copies of the same network, with the same weights, fed into a 'distance measuring' layer to compute whether the two examples are similar or not. Given the labeled data, the network encourages similar examples to be close, and dissimilar ones to have a certain minimum distance from each other. While these techniques have been applied to controlled datasets, their usefulness over large-scale controlled datasets still remains to be explored. Our proposed dataset can be helpful in this direction as it combines the best of the two worlds: instance-level variety of large-scale datasets and rich parametrization of controlled synthetic images which are precious to study probing the behavior of CNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The iLab-20M dataset</head><p>Many image datasets have been proposed to assist machine vision algorithm development and testing <ref type="table">(Table 1)</ref>. Those datasets which have provided large collections of training exemplars per well-defined object category have been useful in advancing the state of the art. Excellent examples include FERET for face recognition <ref type="bibr" target="#b43">[44]</ref>, with 14,051 images of 1,199 individuals in one class (human faces), or MNIST for handwritten digits <ref type="bibr" target="#b33">[34]</ref>, with 60,000 images in 10 classes from 500 writers. Today, recognizing faces or handwritten digits is considered a reasonably well solved problem, although of course improving tolerance to noise and other nuisance parameters is always possible.</p><p>In other domains, including recognition of objects from generic categories, most efforts have focused on providing very useful test sets and performance challenges (e.g., Ima-geNet <ref type="bibr" target="#b7">[8]</ref>), but these often lack in the sheer volume of training exemplars provided within each object category and for each object instance, lack pose information, and often contain occlusions. This limits their usefulness for training. For example, the 'calculator' category of Caltech-256 <ref type="bibr" target="#b21">[22]</ref> contains 100 images of what appears to be 100 different calculators with no pose data. While this is highly appropriate for testing, we hypothesize that training can be greatly improved by using many different views of different instances of objects in a number of categories, shot in many different environments, and with pose information explicitly known. Indeed, biological systems can rely on object persistence and active vision to obtain many different views of a new physical object. In humans and monkeys, this is believed to be exploited by the neural representations <ref type="bibr" target="#b36">[37]</ref>, though Recording parameters are: resolution 960 × 720, color mode YUYV, brightness 128, contrast 32, saturation 32, gain 30, auto white balance off, manual white balance temperature 3100K, sharpness 72, auto exposure off, auto focus off, focus base value 97-119. b) robotic-assisted arms, one holding the camera, the other taking wide-field pictures from random viewpoints and distances. c) a sample instance of a car from 5 consecutive rotations and 5 consecutive arch cameras. d) a sample instance from each object category (same lighting, rotation and focus; all set to zero) presented in the order shown in <ref type="table" target="#tab_2">Table 2</ref>. e) an instance of a boat under different illuminations.</p><p>the exact mechanisms remain poorly understood. Although adult humans can learn new object instances from a single view, one should not forget that this ability might only emerge at the culmination of a long evolutionary process plus 20-some years of individual training. Popular datasets fall short in at least one dimension, be it the number of classes, objects per class, number of backgrounds/environments, or views per object, as shown in Table 1. Particularly relevant to our effort are: 1) COIL <ref type="bibr" target="#b40">[41]</ref>, which also used a turntable to film 100 objects under various lighting and poses; however, COIL only contains one object instance per category and only black backgrounds (similar to the larger ALOI dataset with 1,000 objects and a few per category <ref type="bibr" target="#b18">[19]</ref>), and 2) NORB <ref type="bibr" target="#b34">[35]</ref> with 50 small toy objects similar to the ones we used (10 instances in each of 5 categories); however, all objects were painted uniformly and shot in grayscale on blank backgrounds (different backgrounds were later composited digitally).  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>❳ ❳ ❳ ❳ ❳ ❳ ❳ ❳</head><formula xml:id="formula_0">✓ ✓ - - - - ✓ ✓ ✓ ✓ - - - - ✓ -</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Turntable setup</head><p>The turntable consists of a 14"-diameter circular plate actuated by a robotic servo mechanism. A CNC-machined semi-circular arch (radius 8.5") holds eleven Logitech C910 USB webcams which capture color images of the objects placed on the turntable ( <ref type="figure" target="#fig_0">Fig. 1.a)</ref>. A micro-controller system actuates the rotation servo mechanism and switches on and off four LED lightbulbs (Ecosmart ECS 16 WW FL, 295 lumens each, color rendering index 87, correlated color temperature 3000K). Lights are controlled independently, in 5 conditions: all lights on, or one of the four lights on.</p><p>Cameras were connected to a Linux computer (6-core AMD Phenom CPU, 16GB RAM) with 11 independent USB controllers. Camera settings were as follows, using the Linux V4L2 driver: resolution 960 × 720, color mode YUYV, brightness 128 (default for these cameras), contrast 32 (default), saturation 32 (default), gain 30, auto white balance off, manual white balance temperature 3100K, sharpness 72 (default), auto exposure off, manual exposure 125 (all lights on) or 450 (one light on), autofocus off, focus base value 97 -119 depending on the camera. Objects were mainly Micro Machines toys (Galoob Corp.) and N-scale model train toys, as shown <ref type="figure" target="#fig_0">Fig. 1</ref>.d. These objects present the advantage of small scale, yet demonstrate a high level of detail and, most remarkably, a wide range of shapes (i.e., many different molds were used to create the objects, as opposed to just a few molds and many different painting schemes). Backgrounds were 125 color printouts of satellite imagery from the Internet, and 7 plain solid-color backgrounds (white, red, blue, yellow, etc). Every object was shot on all solid-color backgrounds, for possible later compositing of additional digital backgrounds, and for possible reconstruction of 3D models. Every object was shot on at least 14 backgrounds, in a relevant context (e.g., cars on roads, trains on railtracks, boats on water).</p><p>In total, 1,320 images were captured for each object and background combination: 11 azimuth angles (from the 11 cameras), 8 turntable rotation angles, 5 lighting conditions, and 3 focus values (-3, 0, and +3 from the default focus of each camera). Each image was saved with lossless PNG compression (∼1 MB per image). The complete dataset hence consists of 704 objects, each shot on 14 or more backgrounds, with 1,320 images per object/background combination, or almost 22M images (See <ref type="table" target="#tab_2">Table 2</ref>). The dataset is freely available and distributed on 3 8TB hard drives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Robotics-assisted model scenes</head><p>In addition, we created robotics-assisted model scenes to record broader scenes where objects were placed in variable contexts. The long-term motivation for this larger scenery is to collect many images which can be used to test algorithms both on their ability to first locate and then to recognize objects, and on their possible ability to exploit larger scene contexts to aid recognition (see, for example <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b24">25]</ref>).</p><p>The robotics-assisted scenes ( <ref type="figure" target="#fig_0">Fig. 1.b)</ref> consist of a 40"×29" table onto which 1:160 poster prints of satellite images (e.g., Google maps) are placed (corresponding to a real-world area of 195m×118m). One 8-axis robot arm holds a camera (Microsoft LifeCam Cinema, 1280×720, YUYV) which can be placed and oriented at any location and pose reachable by the arm. A second arm holds a light source (Jingsam LED 7W, 437 lumens, 3000K).</p><p>The robots are programmed in two ways: 1) pseudorandom motion, generating flybys, 2) point to specific locations on the table and shoot objects from different viewpoints and distances. An interactive user interface assists in configuring a scene for robotics-assisted filming.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments and results</head><p>To start exercising the dataset, we tested it on small subsets of the available data. To understand generalization across image variations (object shape, object viewpoint, lighting, etc), CNNs are evaluated by taking slices of the dataset. We utilize pre-trained Alexnet <ref type="bibr" target="#b30">[31]</ref> (on ImageNet) and fine-tune it on iLab-20M. The behavior of off-the-shelf features is investigated in our analyses as well. We use 7 object categories (out of 16) and avoid data augmentation as we have flipped versions of the objects from the turntable. The label layer contains several units depending on the task (2, 4 or 7 for object categorization; variable number of units for parameter prediction). We report average accuracies and standard deviations where there is randomness in the experimental procedure. Experiments are performed using the publicly available Caffe toolkit <ref type="bibr" target="#b26">[27]</ref> ran on a Nvidia Titan X GPU and Ubuntu 14.04 OS.</p><p>We aim to answer these questions: Can a pre-trained CNN model predict the setting parameters such as lighting source, degree of azimuthal rotation, degree of camera elevation, etc? Can it transfer the learned knowledge from one object category to another? Which parameters are more important in the transfer? How much knowledge can a model transfer from iLab-20M to the ImageNet? Which one is a better strategy to make an object dataset: random or systematic image harvesting? How the order of learning parameter invariance influences overall network parameter tolerance and accuracy? Some of these questions have been addressed in the past to some extent <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b67">68,</ref><ref type="bibr" target="#b69">70,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b39">40</ref>].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Selectivity and invariance</head><p>Humans are very good at predicting the category of an object and also telling about its parameters. Human visual system is selective to object category and invariant to parameters and variations. In this experiment, we aim to systematically investigate this competition for two layers of the Alexnet: pool5 and fc7. We probe the expressive power of these layers for object category and parameter prediction.</p><p>Four categories from iLab-20M (out of 16) were chosen for this analysis including boat, bus, tank and ufo. Images were lumped to train a SVM classifier. All features were normalized to have zero-mean before feeding to the classifier. The dimensionality was reduced to N-dimensions using SVD, where N refers to the number of instances in the training set. The reported results are average accuracy over random 5-fold cross validation test sets, each of size 2K. We trained two SVMs, one for category prediction and another for parameter prediction. Results are shown in <ref type="figure" target="#fig_1">Fig. 2</ref>.</p><p>As expected, we see that fc7 features result in a high classification accuracy, however, the surprising salient result is the shoulder-to-shoulder performance of pool5 and fc7 layers. Relying on this outcome, it seems that both fc7 and pool5 representations convey useful discriminative information for object recognition. Comparing the performance Lighting Camera Rotation  over parameter prediction, one can notice the superiority of pool5 layer over fc7. This is consistent with the work by Bakry et al. <ref type="bibr" target="#b1">[2]</ref> where they analytically found that fully connected layers make effort to collapse the low-dimensional intrinsic parameter manifolds to achieve invariant representations. However, only view manifold was taken into consideration in Bakry et al.'s work, while here we analyze the behavior of more common parameters.</p><p>In brief, our results suggest that the feature space spanned by pool5 layer contains more information than fc7 layer for parameter prediction. At the same time, the very representation forces different categories to be highly apart from each other (thus keeping the structure of manifolds as linearly-separable as possible for different categories). The representation by fc7 sensibly discards parameter information to become invariant while keeping the categories as separable as possible. We observe that the layer just before fully connected layers provides better compromise between categorization and parameter estimation.</p><p>Parameter prediction accuracies for lighting (5 classes), turntable rotation (4 classes), and camera view (6 classes) in order are 100%, 62%, and 77%. These figures suggest that camera view (considering the normalized-to-chance accuracy) has the most complex structure for parameter prediction whereas the lighting is simpler. This is somewhat sensible since changing camera view leads to geometric shape variations, and ports the prediction task into a much more difficult problem to address. In contrast, lighting variations do not alter the shape of the object, and are thus easier to capture. Note that this result is on our data and may not necessarily scale to natural scenes.</p><p>We use the t-SNE dimensionality reduction method in <ref type="bibr" target="#b61">[62]</ref> to visualize the learned representations over seven  categories of iLab-20M along with variation parameters (See <ref type="figure" target="#fig_2">Figure 3)</ref>. The fc7 representation works remarkably well at recognizing objects as they are mutually linearly separable after fine-tuning. Further, pool5 representation does not contain discriminative information compared to fc7. Please see also the supplement for more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Knowledge transfer</head><p>Humans are very efficient at estimating and transferring parameters of a seen object to another unseen object in complicated scenarios. For example, they can reliably estimate the lighting source direction of an object and tell whether another object has been subject to the same lighting exposure. Complementary to our previous analysis, in this experiment, we aim to assess the power of CNNs in transferring the learned parameter over one object category to another. We focus on pool5 layer here since as we discussed above, fc7 layer is invariant to parameters and is thus less useful for discriminating between different parameters.</p><p>All parameters were fixed except one (i.e., slicing the dataset along only one parameter). We included instances from four categories (boat, bus, tank, ufo) in the training set, and tested the learned knowledge on instances from an unseen category (f1car) as well as 4 seen categories (but different instances). We utilized the pool5 representation and reduced the dimensionality to N , where N refers to number of samples. The 5-fold cross validation average accuracy for parameter prediction is shown in <ref type="figure" target="#fig_4">Fig. 4</ref>.</p><p>Results show a decent degree of knowledge transfer. As <ref type="figure" target="#fig_4">Fig. 4</ref> exhibits, the lighting parameter is relatively easier to be transferred to unseen categories. It has a headto-head accuracy across seen and unseen categories. On the other hand, knowledge transfer for rotation and camera view parameters is accompanied with sensible degradation in performance. In summary, we see that the knowledge is promisingly transferable across seen and unseen categories. The degradation in rotation and camera prediction is intuitively justifiable as these parameters are highly dependent on the 3D properties of the object shape (See also <ref type="bibr" target="#b35">[36]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Systematic and random sampling</head><p>Large-scale datasets have been so far constructed by harvesting images randomly from the web. The major reason- ing for doing so is to include as much variability (mainly intra-and inter-class variation) as possible in the dataset. It has not yet been systematically studied whether this is a good strategy compared to controlled strategies conducted in turntable setups. In this analysis, we consider two strategies to find the answer: i) Random strategy where n samples (across all parameters and instances) are chosen randomly and are used to train an SVM to predict the object category, and ii) Systematic (or exhaustive) strategy, in which an object instance is chosen randomly and then other images from that object are added to our training set, by scanning all parameters, until n samples are reached. We assume availability of a fixed limited budget (time or cost) enough for processing only n samples. We addressed a 4 class problem (boat, bus, tank, ufo) by increasing n starting from 12 up to 10,000 samples. In each experiment, n/4 samples were chosen randomly from all 4 categories across all parameters, and were fed into the Alexnet to get the fc7 (or pool5) representation. Then, we trained a linear SVM classifier on this data. A fixed test set of size 500 was randomly selected from all categories with all parameters and was kept fixed during the analysis. We measured category prediction at fc7 and parameter prediction at pool5, reducing the dimensionality to 2,500 for all values of n in the latter. Results are shown in <ref type="figure" target="#fig_5">Fig. 5</ref>.</p><p>We observe that random sampling strategy performs better in category prediction. This makes sense since randomly choosing images offers more instance level variety (better than systematic) leading to better recognition. Interestingly, and counter-intuitively, we see that random strategy works better in parameter prediction as well. We believe that the parameter prediction is somewhat dependent on the 3D properties of object shape, and since in the systematic strategy, the learner is not faced with sufficient instances, it fails to predict parameters compared to random strategy. Overall, what we learn is that instance level variation is of P P P P P P   high importance for both category and parameter prediction and this is perhaps why the systematic sampling strategy is hindered. Thus, in dataset creation, it is vitally advantageous to have as much instance level variation as possible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Domain adaptation</head><p>Currently, there is a gap in relating results learned over synthetic datasets to results learned on large-scale datasets. We train models on iLab-20M and apply them to natural scenes (and vice versa) to see how much knowledge they can transfer from one dataset (source domain) to another (target domain). This way, we can also discover along which dimension(s) a dataset varies the most and whether it offers sufficient variability for learning invariance. In other words, we can somehow indirectly measure dataset bias <ref type="bibr" target="#b59">[60]</ref>. Ultimately, it is desirable to generalize what is learned from synthetic datasets to natural scene datasets.</p><p>We consider two scenarios: i) a binary classification problem boat vs. tank, and ii) a 4-class problem including boat, tank, bus and train. In each scenario, we train a SVM (using fc7 representation) from either natural scenes (selected from ImageNet) or iLab-20M and apply it to the other dataset. We also merge images from the two datasets and measure the accuracy on each individual dataset. We consider both off-the-shelf features of the Alexnet (pre-trained over ImageNet) and fine-tuned features over iLab-20M.</p><p>Augmenting data along all parameters: Here, we choose images along all parameters. Results in <ref type="table" target="#tab_4">Table 3</ref> show that training on each type of image, expectedly works the best on the same type of test image (95% from ImageNet to Ima-geNet and 97% from iLab-20M to iLab-20M). Cross application of models results in lower (but above 50% chance) accuracy. We observe that fine tuning the Alexnet on iLab-20M boosts the performance on iLab-20M to 100% while hindering the accuracy over ImageNet as CNN features are now tailored (and are hence selective) to our images. <ref type="table" target="#tab_5">Table 4</ref> shows domain adaptation results over 4 classes. Results align with accuracies over 2 classes, although accu-  racies are lower here. Here, again combining images from datasets hinders performance over each individual dataset due to contamination of features. The reason why performance is low when applying a model trained on iLab-20M to ImageNet is mainly because objects in these two datasets have different textures and statistics which demand more sophisticated ways of domain adaptation.</p><p>Accuracies over 2-class and 4-class problems are very high (&gt; 95%). To further investigate accuracy of Alexnet, we increased the number of classes to 7. As seen in the confusion matrices in <ref type="figure" target="#fig_7">Fig. 6</ref>, fine tuning the network increases the accuracy from 92.5% to 99.9% with only two mistakes 1 .</p><p>Augmenting data along a single parameter: Here, we investigate which parameter is more effective in domainadaptation (from synthetic to natural images.). Two categories, existing in both datasets, are considered: boat and tank. To form a training set, we vary only one parameter at a time while keeping all others fixed. Then, fc7 features are computed for the training set and a linear SVM is trained. The same features are computed for natural images and the learned model on synthetic samples is tested on them. For each parameter, we had 275 synthetic images for training and a fixed set of 3,000 images from ImageNet for testing.</p><p>In a complementary experiment, all parameters were allowed to vary except one (opposite of the above). A set of 2,000 samples were randomly selected (complying with the conditions) and a linear SVM was trained on them (using fc7). The parameter whose absence drops the accuracy more is considered to be more dominant. 5-fold cross validation accuracies are reported in <ref type="figure" target="#fig_8">Fig. 7</ref>.</p><p>As shown in the bar chart in <ref type="figure" target="#fig_8">Fig. 7</ref>, the camera-view is of the highest importance as it leads to the highest accuracy on the fixed natural test set. This is reasonable since real world objects are often viewed from angles at different degrees of elevation (in-depth rotation). We thus speculate that camera-view might be the dominant varying parameter in natural scenes. The (in-plane) rotation is the next important parameter as it gains the next top accuracy on natural images. Surprisingly, the lighting source is ranked as the least effective parameter in our analysis. The absence of camera-view drops the recognition accuracy more than the other two parameters (the right side bars in <ref type="figure" target="#fig_8">Fig. 7</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Analysis of parameter learning order</head><p>In this analysis, we study whether/how the order of knowledge delivery to CNNs matters. First, we prepare two datasets (training with 40K images, validation with 10K) from four categories (boat, bus, tank and train) and annotate them with rotation labels. Alexnet is fine tuned on the training set. We set the learning rate for all the layers to 0.001, except fc8 layer which is set to 0.01. All other parameters are set to their default values. Next, we prepare a new training set including 40K images from the same four object categories and annotate them with camera view labels. A validation set of size 10K is also constructed. Obtained weights from the first step are loaded to the network and are treated as a promising initialization point for another round of fine tuning over the new data.</p><p>We assess the performance of the network for camera view and rotation prediction using the pool5 representation. As fine tuning with low learning rate slightly changes weights within the network, we are interested to see which order of changes in weights (before fully connected layers) gives the superior performance in our desired task. To hunt what we are looking for, prepared datasets are delivered to the network in reverse order (i.e., camera first, rotation next). We denote the two orderings as follows: 1) rotationcamera, and 2) camera-rotation for simpler reference. In the evaluation phase, 2,000 samples are randomly selected from four categories, and pool5 features are extracted. After mean subtraction and dimensionality reduction, 5-fold cross validation accuracies of models are reported in <ref type="table">Table. 5</ref>.</p><p>Counter-intuitively, we find that order of data delivery is very important to the network such that when the network is fed with samples with rotation labels prior to camera labels, it ostensibly performs better in parameter prediction. We also find that when the network is firstly fine tuned on rotation, the second stage (i.e., fine tuning on camera labels) does not impair the weights for rotation prediction. In contrast, when the camera labels are seen first, rotation prediction accuracy is expectedly better than the previous ordering. This boost, however, causes dramatic degradation in camera prediction performance.  <ref type="table">Table 5</ref>. Influence of data delivery order on parameter prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>❳ ❳ ❳ ❳ ❳ ❳ ❳ ❳</head><p>As in the previous experiments, camera view variation is a more ill-structured parameter to predict. When the network sees the camera labels in the second stage, the adapted weights are more biased towards learning this parameter. This bias does also try to keep the pre-seen knowledge for rotation unchanged. We thus conclude that when there is the option for stage-wise training, it would be better to learn parameters following a simple to complex order. This way, the last steps are devoted to manage the difficulties in complex parameters, while imposing less damage to weights adapted for simpler parameters (thus maintaining the structure).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion</head><p>We challenged the solitary use of uncontrolled natural image datasets in guiding the object recognition progress and introduced a large-scale controlled object dataset of over 20M images with a rich parameter variety. By cutting slices through our dataset, we systematically studied the invariance and generalization properties of CNNs by independently varying the choice of object instances, viewpoints, lighting conditions, or backgrounds between training and test sets. Progressively extending these results on increasingly larger subsets of our dataset may help gain new insights on how the algorithms can be modified to show greater invariance and generalization capabilities.</p><p>In summary, we learn that: i) the representation learned in pool5 layer is selective to parameters while fc7 layer is not, ii) the knowledge obtained from some parameters is easier to be transferred to unseen object categories, iii) random sampling strategy leads to better generalization since more instance level variations can be captured, iv) simple cross application of one dataset to another results in above chance accuracy but does not improve performance, and v) it would be advantageous to feed the network with data that has been sorted according to complexities of different dimensions. This can lead to layer-wise training of CNNs for learning different invariances in different layers.</p><p>In the future, we will attempt to evaluate the accuracy of recent deep learning architectures on our dataset. In particular, we will consider techniques such as feature embedding and loss regularization <ref type="bibr" target="#b62">[63,</ref><ref type="bibr" target="#b4">5]</ref> and joint prediction of camera parameters and object categories <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b52">53]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Turn-table photo shooting setup. a) turntable with 8 rotation angles, 11 cameras on a semicircular arch, 4 lighting sources (generating 5 lighting conditions), 3 focus values and random backgrounds (overall 8 × 11 × 5 × 3 = 1320 images for each instance per background).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Selectivity and invariance. Expressive power of Alexnet pool5 and fc7 layers for category and parameter prediction on a 4 class problem.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3</head><label>3</label><figDesc>. t-SNE representation of the Alexnet layers. The fc7 representation works remarkably well at recognizing objects as they are mutually linearly separable after fine-tuning. Further, pool5 representation does not contain discriminative information compared to fc7. This figure also demonstrates the effect of fine-tuning. Distribution of samples for different categories tend to become very compact after fine-tuning. Fine-tuning does not seem to add more discriminative power to the pool5 representation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Knowledge transfer over object categories with one parameter changing. Alexnet is trained over four object classes and is tested on the same or different object classes (over different instances).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Analysis of two sampling strategies over a 4-class classification problem (boat, bus, tank, ufo). Left: category prediction accuracy using fc7 features. Right: Parameter prediction accuracy using pool5 features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 .</head><label>6</label><figDesc>Confusion matrices of Alexnet over seven categories of the iLab-20M dataset without (left) and with fine tuning (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 .</head><label>7</label><figDesc>Domain adaptation with a single parameter change.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>Table 1. Overview of some popular object recognition datasets. The last one proposed here avoids the dreaded entry of "1" in any column of the table. while a majority of them have a relatively modest number of instances (airplane: 179, floor lamp: 276, boat: 349).</figDesc><table>Dataset 

Ref 
Domain 
Object 
Objects 
Backgrd 
Views per 
Bounding 
Object 
Total 
Classes 
per Class 
per obj 
obj+bg 
Box? 
Contours? 
Images 

COIL 
[41] 
Handheld 
100 
1 
1 
72 
Implicit 
No 
7,200 
SOIL-47 
[29] 
Handheld 
-
47 
1 
42 
Implicit 
No 
1,974 
Pascal 
[14] 
Misc 
20 
790-10,129 
1 
1 
Yes 
Partial 
11,540 
Caltech-101 
[15] 
Google 
102 
31-800 (µ = 90) 
1 
1 
No 
No 
9,144 
Caltech-256 
[22] 
Google 
257 
80-827 (µ = 119) 
1 
1 
No 
No 
30,607 
LabelMe 
[48] 
Misc 
900 
? 
∼1 
∼1 
Partial 
Partial 
62,197 (a) 
NORB 
[35] 
Toys 
5 
10 
1 (b) 
1,944 
Implicit 
No 
48,600 (b) 
FERET 
[44] 
Faces 
1 
1,199 
1 
1-24 
Yes 
No 
14,051 
MNIST 
[34] 
Digits 
10 
6,000 
1 
1 
Implicit 
No 
60,000 
ETHZ 
[17] 
Natural 
5 
32-87 
1 
1 
Yes 
Yes 
255 (c) 
TINY 
[61] 
Web 
75,062 
? 
1 (?) 
1 
Implicit 
No 
79,302,017 (d) 
CIFAR-100 
[30] 
Web 
100 
600 
1 
1 
Implicit 
No 
60,000 (d) 
ALOI 
[19] 
Handheld 
1,000 (e) 
∼1 
1 
108 
Implicit 
No 
110,250 
GRAZ 
[42] 
Photographs 
4 
311-420 
1 
1 
No 
Partial 
1,476 
CoPhIR 
[3] 
Flickr 
? (f ) 
? 
1 (?) 
1 (?) 
No 
No (f ) 
106,000,000 
ImageNet 
[8] 
Misc 
21,841 
∼1 
∼1 
∼1 
Yes 
No 
14,197,122 
SUN 
[64] 
Misc 
3,819 
(g) 
1 
1 
Yes 
Yes 
131,067 
MS COCO 
[38] 
Misc 
91 
∼5,000 
1 
1 
Yes 
Yes 
328,000 (a) 
RGB-D 
[33] 
Household 
51 
∼6 
1 
250 
Yes 
No 
250,000 
Big-BIRD 
[55] 
Household 
100 
1 
1 
600 
Yes 
No 
250,000 
iLab-20M 
-
Toy vehicles 
15 
25-160 
14-40 
1,320 
Implicit 
No 
21,798,480 

Implicit bounding box means that it can be trivially computed (e.g., objects are centered within images). Notes: (a) Still growing. (b) Many additional 
images were created by digitally jittering objects and compositing various backgrounds. (c) 289 objects in 255 images. (d) Image resolution 32 × 32. Note 
that CIFAR is a subset of the TINY dataset. (e) 1,000 objects total, not grouped by categories. (f) MPEG-7 and Flickr user tags (e.g., summer, Paris, China) 
available. (g) The number of instances per object category shows the long tail phenomenon: a few categories have a large number of instances (window: 
16,080, chair: 7,971, wall: 20,213) </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 .</head><label>2</label><figDesc>Summary statistics of iLab-20M dataset. There are 21,798,480 images in total from 16 categories (one used for calibration purposes only) with 25 to 160 instances per category. Five parameters include: 11 cameras on an arch, 4 lighting sources on 4 corners (5 conditions), 8 horizontal rotations, 132 backgrounds (7 solid color) and 3 focus values. Average number of backgrounds per object instance is 23.39. There are 46 unique backgrounds per category (average backgrounds per object 145.76 with std = 162.62; min = 25, max = 731). Total size of the dataset with resolution 960 × 720 is 17.65TB. The cropped version of the images (256 × 256 pixels) is also available with 2.2TB in size. Total number of images per category is rounded to save space.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="true"><head>Table 3 .</head><label>3</label><figDesc>Domain adaptation on boat vs. tank classification (in percentage).</figDesc><table>P P P P P P 

train 

test 
Without fine tuning 
With fine tuning 

Natural 
iLab-20M Natural 
iLab-20M 

Natural [2000] 
96.48 (0.5) 55.6 (2.7) 95.56 (0.6) 68.06 (2.0) 

iLab-20M [2000] 66.92 (3.2) 96.90 (0.2) 65.22 (1.4) 99.72 (0.1) 

iLab-20M [1000] 94.42 (0.8) 93.94 (0.4) 92.52 (0.2) 98.70 (0.2) 

+ Natural [1000] 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 4 .</head><label>4</label><figDesc></figDesc><table>Domain adaptation over a 4-class problem (boat, tank, bus, and 

train). Numbers in parentheses are standard deviations. 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Please see the supplementary material for t-SNE visualization<ref type="bibr" target="#b61">[62]</ref> of without-and with fine-tuned fc7 and pool5 features.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments: This work was supported by the National Science Foundation (grant numbers CCF-1317433 and CNS-1545089), the Army Research Office (W911NF-12-1-0433), and the Office of Naval Research (N00014-13-1-0563). The authors affirm that the views expressed herein are solely their own, and do not represent the views of the United States government or any agency thereof. We also wish to thank NVIDIA for their generous donation of the GPU used in this study.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">From generic to specific deep representations for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Azizpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Razavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Maki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Carlsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="36" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Digging deep into the layers of cnns: In search of how cnns achieve view invariance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bakry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elhoseiny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>El-Gaaly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Elgammal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.01983</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Enabling content-based image retrieval in very large digital libraries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bolettieri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Esuli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Falchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lucchese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Perego</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Rabitti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Second Workshop on Very Large Digital Libraries</title>
		<meeting><address><addrLine>Corfu, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-10-02" />
			<biblScope unit="page" from="43" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Human vs. computer in scene and object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="113" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Signature verification using a siamese time delay neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bromley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Bentz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Säckinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Pattern Recognition and Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">04</biblScope>
			<biblScope unit="page" from="669" to="688" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Semantic image segmentation with deep convolutional nets and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.7062</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep filter banks for texture recognition and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cimpoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3828" to="3836" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2009</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">How does the brain solve visual object recognition?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Dicarlo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zoccolan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">C</forename><surname>Rust</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="415" to="434" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.02753</idno>
		<title level="m">Inverting visual representations with convolutional networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Adapting object recognition across domains: A demonstration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Draper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Ahlrichs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Paulus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision Systems</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="256" to="267" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Modeling search for people in 900 scenes: A combined source model of eye guidance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">A</forename><surname>Ehinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hidalgo-Sotelo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Visual Cognition</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="945" to="978" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Convolutional models for joint object categorization and pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elhoseiny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>El-Gaaly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bakry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Elgammal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05175</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning generative visual models from few training examples: an incremental bayesian approach tested on 101 object categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshop on Generative-Model Based Vision</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Unsupervised visual domain adaptation using subspace alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Habrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sebban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2960" to="2967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Object detection with contour segment networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fukushima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biological cybernetics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="193" to="202" />
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">The amsterdam library of object images. International Journal of Computer Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-M</forename><surname>Geusebroek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Burghouts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Smeulders</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Domain adaptation for object recognition: An unsupervised approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="999" to="1006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Caltech-256 object category dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Griffin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Holub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03385</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning spatial context: Using stuff to find things</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">5302</biblScope>
			<biblScope unit="page" from="30" to="43" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Deep learning for singleview instance recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Held</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.08286</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Conference on Multimedia</title>
		<meeting>the ACM International Conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="675" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Evaluating colour-based object recognition algorithms using the soil-47 database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koubaroulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cmp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">2002</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Kruthiventi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ayush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">V</forename><surname>Babu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deepfix</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1510.02927</idno>
		<title level="m">A fully convolutional neural network for predicting human eye fixations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A large-scale hierarchical multi-view rgb-d object dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Gradientbased learning applied to document recognition. Proceedings of the IEEE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning methods for generic object recognition with invariance to pose and lighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Why The Brain Separates Face Recognition From Object Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Leibo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mutch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<meeting><address><addrLine>Granada, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Unsupervised natural visual experience rapidly reshapes size-invariant object representation in inferior temporal cortex</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Dicarlo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Bilinear cnn models for fine-grained visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roychowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1449" to="1457" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Do convnets learn correspondence?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1601" to="1609" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Columbia object image library (coil 100) 1996. Columbia University</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nayar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Murase</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Generic object recognition with boosting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Opelt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fussenegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Auer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Emt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">U</forename><surname>Graz</surname></persName>
		</author>
		<idno>TR-EMT-2004-01</idno>
		<imprint>
			<date type="published" when="2004" />
			<pubPlace>Austria. 1, 2</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Exploring invariances in deep convolutional neural networks using synthetic images. CoRR, abs/1412</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">7122</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">The feret evaluation methodology for face-recognition algorithms. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Rizvi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Rauss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Comparing state-of-the-art visual features on invariant object recognition tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Barhomi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">D</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Dicarlo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Applications of computer vision (WACV), 2011 IEEE workshop on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="463" to="470" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Hierarchical models of object recognition in cortex</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riesenhuber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature neuroscience</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1019" to="1025" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Labelme: a database and web-based tool for image annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">P</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="157" to="173" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Adapting visual category models to new domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2010</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="213" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Overfeat: Integrated recognition, localization and detection using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6229</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">A feedforward architecture accounts for rapid categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">15</biblScope>
			<biblScope unit="page" from="6424" to="6429" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Robust object recognition with cortex-like mechanisms. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bileschi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riesenhuber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="411" to="426" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Bigbird: A large-scale 3d database of object instances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">S</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Achim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="509" to="516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Multiview convolutional neural networks for 3d shape recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="945" to="953" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Deepface: Closing the gap to human-level performance in face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1701" to="1708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Manifold regularized deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">S</forename><surname>Tomar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Rose</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="348" to="352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Unbiased look at dataset bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1521" to="1528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">80 million tiny images: A large data set for nonparametric object and scene recognition. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1958" to="1970" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Deep learning via semi-supervised embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ratle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mobahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks: Tricks of the Trade</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="639" to="655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Sun database: Large-scale scene recognition from abbey to zoo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">A</forename><surname>Ehinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer vision and pattern recognition (CVPR), 2010 IEEE conference on</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3485" to="3492" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Performance-optimized hierarchical models predict neural responses in higher visual cortex</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Yamins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">F</forename><surname>Cadieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A</forename><surname>Solomon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Seibert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Dicarlo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">23</biblScope>
			<biblScope unit="page" from="8619" to="8624" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">From facial parts responses to face detection: A deep learning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3676" to="3684" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">A unified perspective on multi-domain and multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.7489</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">How transferable are features in deep neural networks? In NIPS</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lipson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3320" to="3328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Scene recognition by manifold regularized deep learning architecture. Neural Networks and Learning Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2222" to="2233" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="818" to="833" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
