<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Semantic Object Parsing with Local-Global Long Short-Term Memory</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Sun Yat-sen University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Adobe Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donglai</forename><surname>Xiang</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Sun Yat-sen University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">360 AI Institute</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Semantic Object Parsing with Local-Global Long Short-Term Memory</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:45+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Semantic object parsing is a fundamental task for understanding objects in detail in computer vision community, where incorporating multi-level contextual information is critical for achieving such fine-grained pixel-level recognition. Prior methods often leverage the contextual information through post-processing predicted confidence maps. In this work, we propose a novel deep Local-Global Long Short-Term Memory (LG-LSTM) architecture to seamlessly incorporate short-distance and long-distance spatial dependencies into the feature learning over all pixel positions. In each LG-LSTM layer, local guidance from neighboring positions and global guidance from the whole image are imposed on each position to better exploit complex local and global contextual information. Individual LSTMs for distinct spatial dimensions are also utilized to intrinsically capture various spatial layouts of semantic parts in the images, yielding distinct hidden and memory cells of each position for each dimension. In our parsing approach, several LG-LSTM layers are stacked and appended to the intermediate convolutional layers to directly enhance visual features, allowing network parameters to be learned in an end-to-end way. The long chains of sequential computation by stacked LG-LSTM layers also enable each pixel to sense a much larger region for inference benefiting from the memorization of previous dependencies in all positions along all dimensions. Comprehensive evaluations on three public datasets well demonstrate the significant superiority of our LG-LSTM over other state-of-the-art methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Semantic object parsing, which refers to segmenting an image region of an object into several semantic parts, enables the computer to understand the contents of an image in detail, as illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>. It helps many higher-level computer vision applications, such as image-to-caption Corresponding author is Liang Lin (Email: linliang@ieee.org). This work was done when the first author worked as an intern at NUS. generation <ref type="bibr" target="#b4">[5]</ref>, clothes recognition and retrieval <ref type="bibr" target="#b35">[36]</ref>, and human behavior analysis <ref type="bibr" target="#b33">[34]</ref>.</p><p>Recently, many research works <ref type="bibr" target="#b14">[15]</ref>[33] <ref type="bibr" target="#b30">[31]</ref> <ref type="bibr" target="#b18">[19]</ref>[20] <ref type="bibr" target="#b14">[15]</ref> have been devoted to exploring various Convolutional Neural Networks (CNN) based models for semantic object parsing, due to their excellent performance in image classification <ref type="bibr" target="#b28">[29]</ref>, object segmentation <ref type="bibr" target="#b17">[18]</ref> <ref type="bibr" target="#b16">[17]</ref> and object detection <ref type="bibr" target="#b15">[16]</ref>. However, the classification of each pixel position by CNNs can only leverage very local information from limited neighboring context, depicted by small convolutional filters. Intuitively, larger local context and global perspective of the whole image are very critical cues to recognize each semantic part in the image. For instance, in terms of local context, visually similar regions can be predicted as "left-leg" or "right-leg" depending on their specific locations and neighboring semantic parts (e.g. "leftshoes" or "right-shoes"), especially for regions with two crossed legs. Similarly, the regions of "tail" and "leg" can be distinguished by the spatial layouts relative to the region of "body". In terms of global perspective, distinguishing "skirt" from "dress" or "pants" needs the guidance from the prediction on other semantic regions such as "upperclothes" or "legs". Previous works often resort to some post-processing techniques to separately address these complex contextual dependencies, such as, super-pixel smoothing <ref type="bibr" target="#b14">[15]</ref>, mean field approximation <ref type="bibr" target="#b21">[22]</ref> and conditional random field <ref type="bibr" target="#b2">[3]</ref>[38] <ref type="bibr" target="#b32">[33]</ref>. They improve accuracy through carefully designed processing on the predicted confidence maps instead of explicitly increasing the discriminative capability of visual features and networks. These separate steps often make feature learning inefficient and result in suboptimal prediction for pixel-wise object parsing.</p><p>Instead of employing separate processing steps, this work aims to explicitly increase the capabilities of features and networks in an end-to-end learning process. One key bottleneck to increase the network capability is the longchain problem in deep CNN structures, that is, information from previous computations rapidly attenuates as it progresses through the chain. Similar problem exists when recurrent neural networks are applied to long sequential data. LSTM recurrent neural networks <ref type="bibr" target="#b11">[12]</ref> were originally introduced to address this problem, which utilize the memory cells to process sequential data with complex and sequentialized interdependencies by independently reading, writing and forgetting some information. It could be similarly extended to image analysis. Particularly, the emergence of Grid LSTM <ref type="bibr" target="#b13">[14]</ref> allows for multi-dimensional spatial communications. Our work builds on Grid LSTM <ref type="bibr" target="#b13">[14]</ref> and proposes a novel Local-Global LSTM (LG-LSTM) for CNNbased semantic object parsing in order to simultaneously model global and local contextual information for improving the network capability. The proposed LG-LSTM layers are appended to the intermediate convolutional layers in a Fully Convolutional Neural Network <ref type="bibr" target="#b22">[23]</ref> to enhance visual features by seamlessly incorporating long-distance and short-distance dependencies. The hidden cells in LG-LSTM serve as the enhanced features, and the memory cells serve as the intrinsic states that recurrently remember all previous interactions of all positions in each layer.</p><p>To incorporate local guidance, in each LG-LSTM layer, the features at each position are influenced by the hidden cells at that position in the previous LG-LSTM layer (i.e. depth dimension) as well as the hidden cells from eight neighboring positions (i.e. spatial dimensions). The depth LSTM along the depth dimension is used to communicate information directly from one layer to the next while the spatial LSTMs along spatial dimensions allow for the complex spatial interactions and memorize previous contextual dependencies. Individual memory cells for each position are used for each of the dimensions to capture the diverse spatial layouts of semantic parts in different images.</p><p>Moreover, to further incorporate global guidance, the whole hidden cell maps obtained from the previous LG-LSTM layer are split into nine grids, with each grid covering one part of the whole image. Then the max-pooling over each grid selects discriminative features as global hidden cells, which are used to guide the prediction on each position. In this way, the global contextual information can thus be conveniently harnessed together with the local spatial dependencies from neighboring positions to improve the network capability.</p><p>By stacking multiple LG-LSTM layers and sequentially performing learning and inference, the prediction of each position is implicitly influenced by the continuously updated global contextual information about the image and the local contextual information of neighboring regions in an end-to-end way. During the training phase, to keep the net-work invariant to spatial transformations, all gate weights for the spatial LSTMs for local contextual interactions are shared across different positions in the same LG-LSTM layer, and the weights for the spatial LSTMs and the depth LSTM are also shared across different LG-LSTM layers.</p><p>Our main contributions can be summarized in four aspects. 1) The proposed LG-LSTM exploits local contextual information to guide the feature learning of each position by using eight spatial LSTMs and one depth LSTM. 2) The global hidden cells are posed as the input states of each position to leverage long-distance spatial dependencies of the whole image.</p><p>3) The stacked LG-LSTM layers allow longrange contextual interactions benefiting from the memorization of previous dependencies in all positions. 4) The proposed LG-LSTM layers are incorporated into fully convolutional networks to enable an end-to-end feature learning over all positions. We conduct comprehensive evaluations and comparisons on the Horse-Cow parsing dataset <ref type="bibr" target="#b30">[31]</ref>, and two human parsing datasets (i.e. ATR dataset <ref type="bibr" target="#b14">[15]</ref> and Fashionista dataset <ref type="bibr" target="#b36">[37]</ref>). Experimental results demonstrate that our architecture significantly outperforms previously published methods for object parsing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Semantic Object Parsing: There have been increasing research works in the semantic object parsing problem including the general object parsing <ref type="bibr">[</ref>  <ref type="bibr" target="#b20">[21]</ref>. Recent stateof-the-art approaches often rely on deep convolutional neural networks along with advanced network architectures <ref type="bibr" target="#b10">[11]</ref>[33] <ref type="bibr" target="#b18">[19]</ref>. Instead of learning features only from local convolutional kernels as in these previous methods, we solve this problem through incorporating the novel LG-LSTM layers into CNNs to capture both long-distance and short-distance spatial dependencies. In addition, the adoption of hidden and memory cells in LSTMs makes it possible to memorize the previous contextual interactions from local neighboring positions and the whole image in previous LG-LSTM layers. It should be noted that while <ref type="bibr" target="#b37">[38]</ref> models mean-field approximate inference as recurrent networks, it can only refine results based on predicted pixelwise confidence maps. Compared with <ref type="bibr" target="#b37">[38]</ref>, the pro-posed LG-LSTM layers can exploit the extra long-range de-pendencies on feature maps.</p><p>LSTM on Image Processing: LSTM networks have been successfully applied to many tasks such as handwriting recognition <ref type="bibr" target="#b9">[10]</ref>, machine translation <ref type="bibr" target="#b27">[28]</ref> and image-to-caption generation <ref type="bibr" target="#b34">[35]</ref>. They have been further extended to multi-dimensional learning and applied to image processing tasks <ref type="bibr" target="#b1">[2]</ref> [30] such as biomedical image segmentation <ref type="bibr" target="#b26">[27]</ref>, person detection <ref type="bibr" target="#b25">[26]</ref> and scene labeling <ref type="bibr" target="#b0">[1]</ref>. Most recently, Grid LSTM <ref type="bibr" target="#b13">[14]</ref> extended LSTM cells to allow the multi-dimensional communication LG-LSTM integrates several novel local-global LSTM layers into the CNN architecture for semantic object parsing. An input image goes through several convolutional layers to generate its feature maps. Then the transition layer and several stacked LG-LSTM layers are appended to continuously improve the feature capability. Based on these enhanced feature maps, the feed-forward convolutional layer attached to the last LG-LSTM layer produces the final object parsing result. The individual cross-entropy loss over all pixels is used to supervise the updating of each LG-LSTM layer.</p><p>across the LSTM cells, and the stacked LSTM and multidimensional LSTM <ref type="bibr" target="#b7">[8]</ref> can be regarded as special cases of Grid LSTM. The proposed LG-LSTM architecture in this work is extended from Grid LSTM and adapted to the complex semantic object parsing task. Instead of pure local factorized LSTMs in <ref type="bibr" target="#b13">[14]</ref> [8], the features of each position in the proposed LG-LSTM are influenced by the shortdistance dependencies as well as the long-distance global information from the whole image. Most of previous works verified the capability of Grid LSTM on very simple data (simple digital images, graphical or texture data), while we focus particularly on the higher-level object parsing task. The closest work to our method is the scene labeling approach proposed in <ref type="bibr" target="#b0">[1]</ref>, where 2D LSTM cells are performed on the non-overlapping image patches. However, our architecture differs from that work in that we employ eight spatial LSTMs and one depth LSTM on each pixel, and learn distinct gate weights for different LSTMs by considering different spatial interdependencies. In addition, global hidden cells are also incorporated as the inputs for different LSTMs in each LG-LSTM layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The Proposed</head><p>LG-LSTM Architecture</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Overview</head><p>The proposed LG-LSTM aims to generate the pixel-wise semantic labeling for each image. As illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>, the input image is first passed through a stack of convolutional layers to generate a set of convolutional feature maps. Then the transition layer adapts convolutional feature maps into the inputs of LG-LSTM layers, which are fed into the first LG-LSTM layer. The LG-LSTM layers is able to memorize long-period of the context information from local neighboring positions and global view of the image. More details about the LG-LSTM layers are presented in Section 3.2. After each LG-LSTM layer, one feed-forward convolutional layer with 1 × 1 filters generates the C confidence maps based on these improved features. The individual cross-entropy loss function over all pixels is used after each feed-forward convolutional layer in order to train each</p><p>LG-LSTM layer. Finally, after the last LG-LSTM layer, the C confidence maps for C labels (including background) are inferred by the last feed-forward layer to produce the final object parsing result.</p><p>Transition Layer. To make sure the number of the input states for the first LG-LSTM layer is consistent with that of the following LG-LSTM layers so that they can share all gate weights, the feature maps from convolutional layers are first adapted by the transition layer and then fed into the LG-LSTM layers. The transition layer uses the same number of LSTMs as in LG-LSTM layers, and passes the convolutional features of each position into these LSTMs to generate individual hidden cells. These resulting hidden cells are then used to construct input states for the first LG-LSTM layer. These weight matrices in the transition layer are not shared with those of the LG-LSTM layers because their dimensions of input states are not consistent. In the initialization, all memory cells are set as zeros for all positions, following the practical settings used in pedestrian detection <ref type="bibr" target="#b25">[26]</ref>. The updated memory cells from the transition layer can then be shared with LG-LSTM layers, which enables LG-LSTM layers to memorize feature representations obtained from convolutional layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Local-Global LSTM Layers</head><p>In this section, we describe the novel LG-LSTM layer tailored for semantic object parsing. To be self-contained, we first recall the standard LSTM recurrent neural network <ref type="bibr" target="#b11">[12]</ref> and then describe the proposed LG-LSTM layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">One-dimensional LSTM</head><p>The LSTM network <ref type="bibr" target="#b11">[12]</ref> easily memorizes the long-period interdependencies in sequential data. In image understanding, this temporal dependency learning can be conveniently converted to the spatial domain. The stacked layers of feature maps also enable the memorization of previous states at each pixel position (referred as depth dimension in this work). Each LSTM accepts the previous input x i and determines the current states that comprises the hidden cells h i+1 ∈ R d and the memory cells m i+1 ∈ R d , where d is the output number. Following <ref type="bibr" target="#b8">[9]</ref>, the LSTM network consists of four gates: the input gate g u , the forget gate g f , the memory gate g c and the output gate g o . The W u , W f , W c , W o are the corresponding recurrent gate weight matrices. Suppose, H i is the concatenation of the input x i and the previous states h i . The hidden and memory cells can be updated as</p><formula xml:id="formula_0">g u = σ(W u * H i ), g f = σ(W f * H i ), g o = σ(W o * H i ), g c = tanh(W c * H i ), m i+1 = g f ⊙ m i + g u ⊙ g c , h i+1 = tanh(g o ⊙ m i ),<label>(1)</label></formula><p>where δ is the logistic sigmoid function, and ⊙ indicates a pointwise product. Let W denote the concatenation of four weight matrices. Following <ref type="bibr" target="#b13">[14]</ref>, we use the function LSTM(·) to shorten Eqn. <ref type="formula" target="#formula_0">(1)</ref> as</p><formula xml:id="formula_1">(h i+1 , m i+1 ) = LSTM(H i , m i , W).<label>(2)</label></formula><p>The mechanism acts as a memory and implicit attention system, whereby the information from the previous inputs can be written to the memory cells and used to communicate with sequential inputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Local-Global LSTM Layers</head><p>Grid LSTM <ref type="bibr" target="#b13">[14]</ref> extended one-dimensional LSTM to cells that are arranged in a multi-dimension grid. Inspired by the design of Grid LSTM, we propose the multi-dimensional Local-Global LSTM to adapt to higher-level image processing. The features of each position depend on the local short-distance and global long-distance information. Local information propagated from neighboring pixels can help retain the short-distance contextual interactions (e.g. object boundaries) while global information obtained from the whole feature maps can provide the long-distance contextual guidance (e.g. global spatial layouts of semantic parts) to boost feature prediction of each position.</p><p>Local Hidden Cells. In terms of local interactions, as illustrated in <ref type="figure" target="#fig_2">Figure 3</ref>, the feature prediction of each position j takes in N = 8 hidden cells from N local neighboring pixels from N spatial LSTMs and the hidden cells from one depth LSTM. Intuitively, the depth LSTM can help track the previous information at each position using the memory cells benefited from the LSTM mechanism, like in the one-dimension LSTM. Each spatial LSTM computes the propagated hidden cells starting from each position to its corresponding spatial direction, as illustrated by the green arrows in <ref type="figure" target="#fig_2">Figure 3</ref>. Thus, each position can provide distinct guidance to each spatial direction by employing distinct spatial LSTMs, which take the spatial layouts and interactions into account for feature prediction. Let h s i,j,n ∈ R d , n ∈ {1, · · · , N } denote the hidden cells propagated from the corresponding neighboring position to a specific pixel j along the n-th spatial dimension by the n-th spatial LSTM, obtained from the i-th layer. The h e i,j ∈ R d indicates the hidden cell computed by the depth LSTM on the position j using the weights updated in the i-th layer.</p><p>Global Hidden Cells. In terms of global interaction, the feature prediction of each position j also takes the global hidden cells generated based on the whole hidden cell maps from the previous LG-LSTM layer as inputs. Specifically, the whole hidden cell maps are constructed by the hidden cells of the depth LSTM, i.e. h e i,j of all positions, which represents the enhanced features of each position. As shown in <ref type="figure" target="#fig_3">Figure 4</ref>, we partition the whole hidden cell maps into nine grids and then the global max-pooling within each grid is performed, resulting in 3 × 3 hidden cell maps. The hidden cells for each grid can thus capture the global information of each grid. Such partition and max-pooling is perform over all the d channels, forming 9 × d hidden cells. We denote the global hidden cells obtained from the i-th</p><p>LG-LSTM layer as f i ∈ R 9d . The global max pooling of hidden cells enables the model to seamlessly incorporate global and local information based on previous hidden cells in the previous LG-LSTM layer.</p><p>LG-LSTM Layer. Given the global hidden cells f i , the local hidden cells {h s i,j,n } N 1 from N spatial LSTMs and h e i,j from one depth LSTM for each position j, the input states H i,j ∈ R (9+N +1)×d fed into the (i+1)-th LG-LSTM layer at each position j can be computed as</p><formula xml:id="formula_2">H i,j = [f i h s i,j,1 h s i,j,2 . . . h s i,j,N h e i,j ] T .<label>(3)</label></formula><p>Denote memory cells of all N spatial dimensions for each position j as {m s i,j,n } N 1 ⊂ R d in the i-th LSTM layer and those of depth dimension as m e i,j ∈ R d . Extended from Grid LSTM <ref type="bibr" target="#b13">[14]</ref>, the new hidden cells and memory cells of each position j for all N + 1 dimensions are calculated as</p><formula xml:id="formula_3">(ĥ s i+1,j,1 , m s i+1,j,1 ) = LSTM(H i,j , m s i,j,1 , W s i ), (h s i+1,j,2 , m s i+1,j,2 ) = LSTM(H i,j , m s i,j,2 , W s i ), . . . (ĥ s i+1,j,N , m s i+1,j,N ) = LSTM(H i,j , m s i,j,N , W s i ), (h e i+1,j , m e i+1,j ) = LSTM(H i,j , m e i,j , W e i ),<label>(4)</label></formula><p>whereĥ s i+1,j,n represents the hidden cells propagated from the position j to the n-th spatial direction, which are used by its neighboring positions to generate their input hidden cells in the next layer. Note thatĥ s i+1,j,n can be distinguished from the h s i+1,j,n by the different starting points and directions for information propagation. Each position thus has N sides of incoming hidden cells and N sides of outgoing hidden cells for incorporating complex local interactions. Although the same H i,j for each position is applied for all LSTMs, the distinct memory cells for each position used in different LSTMs enable individual information propagation from N -sides. These LSTM functions are operated on all positions to produce the whole hidden cell maps. To keep invariance along different spatial dimensions, the weight matrices W s i of N spatial LSTMs are shared. By sequentially stacking several LSTM layers, the receptive field of each position can be considerably increased to sense a much larger contextual region. In addition, long-distance information can also be effectively captured by using the global hidden cells. The input features fed into feed-forward convolutional layers are computed as H i,j for each position.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Settings</head><p>Dataset: We evaluate the performance of LG-LSTM architecture for semantic object parsing on the Horse-Cow parsing dataset <ref type="bibr" target="#b30">[31]</ref> and two human parsing datasets, ATR dataset <ref type="bibr" target="#b18">[19]</ref> and Fashionista dataset <ref type="bibr" target="#b36">[37]</ref>.</p><p>Horse-Cow parsing dataset <ref type="bibr" target="#b30">[31]</ref>. The Horse-Cow parsing dataset is a part segmentation benchmark introduced in <ref type="bibr" target="#b30">[31]</ref>. For each class, most observable instances from PASCAL VOC 2010 benchmark <ref type="bibr" target="#b6">[7]</ref> are manually selected, including 294 training images and 227 testing images. Each image pixel is elaborately labeled as one of the four part classes, including head, leg, tail and body. Following the experiment protocols in <ref type="bibr" target="#b30">[31]</ref> and <ref type="bibr" target="#b32">[33]</ref>, we use all the training images to learn the model and test every image given the object class. The standard intersection over union (IOU) criterion and pixel-wise accuracy are adopted for evaluation. We compare the results of our LG-LSTM with three state-of-the-art methods, including the compositional-based method ("SPS") <ref type="bibr" target="#b30">[31]</ref>, the hypercolumn ("HC") <ref type="bibr" target="#b10">[11]</ref> and the most recent method ("Joint") <ref type="bibr" target="#b32">[33]</ref>.</p><p>ATR dataset <ref type="bibr" target="#b14">[15]</ref> and Fashionista dataset <ref type="bibr" target="#b36">[37]</ref>. Human parsing aims to predict every pixel of each image with 18 labels: face, sunglass, hat, scarf, hair, upper-clothes, left-arm, right-arm, belt, pants, left-leg, right-leg, skirt, leftshoe, right-shoe, bag, dress and null. Originally, 7,700 images are included in the ATR dataset <ref type="bibr" target="#b14">[15]</ref>, with 6,000 for training, 1,000 for testing and 700 for validation. 10,000 real-world human pictures are further collected by <ref type="bibr" target="#b18">[19]</ref> to cover images with more challenging poses, occlusion and clothes variations. We follow the training and testing settings used in <ref type="bibr" target="#b18">[19]</ref>. The Fashionista dataset contains 685 images, among which 229 images are used for testing and the rest for training. We use the same evaluation metrics as in <ref type="bibr" target="#b35">[36]</ref> [15] <ref type="bibr" target="#b18">[19]</ref>, including accuracy, average precision, average recall, and average F-1 score. We compare the results of our LG-LSTM with five recent state-of-the-art approaches <ref type="bibr" target="#b36">[37]</ref> [36] <ref type="bibr" target="#b20">[21]</ref> [15] <ref type="bibr" target="#b18">[19]</ref>.</p><p>Implementation Details: In our experiments, five LG-LSTM layers are appended to the convolutional layers right before the prediction layer of the basic CNN architecture. For fair comparison with <ref type="bibr" target="#b32">[33]</ref>, we fine-tune the network based on the publicly available pre-trained VGG-16 classi- <ref type="table">Table 1</ref>. Comparison of object parsing performance with three state-of-the-art methods over the Horse-Cow object parsing dataset <ref type="bibr" target="#b30">[31]</ref>. We report the IoU accuracies on background class, each part class and foreground class. fication network for the Horse-Cow parsing dataset. We utilize the slightly modified "DeepLab-CRF-LargeFOV" network structure presented in <ref type="bibr" target="#b2">[3]</ref> as the basic architecture due to its leading accuracy and competitive efficiency. This network architecture <ref type="bibr" target="#b2">[3]</ref> transforms VGG-16 ImageNet model to fully-convolutional network and changes the number of filters in the last two layers from 4096 to 1024. For evaluation on two human parsing datasets, the basic "Co-CNN" structure proposed in <ref type="bibr" target="#b18">[19]</ref> is utilized due to its leading accuracy. In terms of training based on "Co-CNN", our model is trained from the scratch following the same training and testing settings in <ref type="bibr" target="#b18">[19]</ref>. Our code is implemented based on the publicly available Caffe platform <ref type="bibr" target="#b12">[13]</ref> and all networks are trained on a single NVIDIA GeForce GTX TITAN X GPU with 12GB memory. We use the same settings for data augmentation techniques for the object part segmentation and human parsing as in <ref type="bibr" target="#b32">[33]</ref> and <ref type="bibr" target="#b18">[19]</ref>, respectively. The scale of input image is fixed as 321 × 321 for training models based on VGG-16 for object part segmentation. During training based on "Co-CNN", the input image is rescaled to 150 × 100 following the same settings in <ref type="bibr" target="#b18">[19]</ref>. During fine-tuning, the learning rate of the newly added layers, including transition layer, LG-LSTM layers and feed-forward convolutional layers is initialized as 0.001 and that of other previously learned layers is initialized as 0.0001. For training based on "Co-CNN", the learning rate of all layers is initialized as 0.001. All weight matrices used in the LG-LSTM layers are randomly initialized from a uniform distribution of [-0.1, 0.1]. In each LG-LSTM layer, eight spatial LSTMs for different spatial dimensions and one depth LSTM are deployed in each position, and each LSTM predicts d = 64 dimension hidden cells and memory cells. We only use five LG-LSTM layers for all models since significant improvements are not observed by using more LG-LSTM layers which also cost more computation resources. The weights of all convolutional layers are initialized with Gaussian distribution with standard deviation as 0.001. We train all the models using stochastic gradient descent with a batch size of 2 images, momentum of 0.9, and weight decay of 0.0005. We fine-  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Results and Comparisons</head><p>We compare the proposed LSTM architecture with the strong baselines on three public datasets.</p><p>Horse-Cow Parsing dataset <ref type="bibr" target="#b30">[31]</ref>: <ref type="table">Table 1</ref> shows the performance of our models and comparisons with three state-of-the-art methods on the overall metrics. The proposed LG-LSTM architecture can outperform three baselines with significant gains: 9.47% over SPS <ref type="bibr" target="#b30">[31]</ref>, 3.74% over HC <ref type="bibr" target="#b10">[11]</ref>, 2.43% over Joint <ref type="bibr" target="#b32">[33]</ref> in terms of overall pixel accuracy for the horse class. Our method also gives a huge boost in average IOU: LG-LSTM achieves 68.73%, 6.75% better than HC <ref type="bibr" target="#b10">[11]</ref> and 3.71% better than Joint <ref type="bibr" target="#b32">[33]</ref> for the horse class. The large improvement, i.e. 5.61% increase by LG-LSTM in IOU over the best performing stateof-the-art method, can also be observed from the comparisons on cow class. This superior performance achieved by LG-LSTM demonstrates that utilizing stacked LG-LSTM layers is very effective in capturing the complex contextual patterns within images that are critical for distinguishing and segmenting different semantic parts from an instance with homogeneous appearance such as a horse or a cow.</p><p>ATR dataset <ref type="bibr" target="#b14">[15]</ref>: <ref type="table" target="#tab_2">Table 2</ref> and <ref type="table">Table 5</ref> show the performance of our models and comparisons with five stateof-the-arts on overall metrics and F-1 scores of individual semantic labels, respectively. The proposed LG-LSTM can significantly outperform five baselines, particularly, 80.97% vs 64.38% of ATR <ref type="bibr" target="#b14">[15]</ref> and 76.95% of Co-CNN <ref type="bibr" target="#b18">[19]</ref> in terms of average F-1 score. Following <ref type="bibr" target="#b18">[19]</ref>, we also take the additional 10,000 images in <ref type="bibr" target="#b18">[19]</ref> as the supplementary train-</p><formula xml:id="formula_4">Input VGG16</formula><p>LG-LSTM  ing images and report the results as "</p><p>LG-LSTM (more)". The "</p><p>LG-LSTM (more)" can also improve the average F-1 score by 3.98% over "Co-CNN (more)". We show the F-1 scores for each label in <ref type="table">Table 5</ref>. Generally, our LG-LSTM shows much higher performance than other methods. In terms of predicting semantic labels with small regions such as hat, belt, bag and scarf, our method offers a very large gain. This demonstrates that our LG-LSTM architecture performs very well on this human parsing task. Fashionista dataset <ref type="bibr" target="#b36">[37]</ref>: <ref type="table" target="#tab_3">Table 3</ref> gives the comparison results on the 229 test images of the Fashionista dataset. All results of the state-of-the-art methods were reported in <ref type="bibr" target="#b14">[15]</ref> and <ref type="bibr" target="#b18">[19]</ref>. Following <ref type="bibr" target="#b14">[15]</ref>, we only report the performance by training on the same large ATR dataset <ref type="bibr" target="#b14">[15]</ref> and then testing on the 229 images of the Fashionista dataset. Our LG-LSTM architecture can substantially outperform the baselines by the large gains over all metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Discussions</head><p>We further evaluate different network settings to verify the effectiveness of the important components in our LG-LSTM architecture, presented in <ref type="table" target="#tab_4">Table 4</ref>.</p><p>Comparison with using convolutional layers: To strictly evaluate the effectiveness of using the proposed LG-LSTM layers, we report the performance of purely using convolutional layers, i.e., "VGG16", indicating the results of the basic network architecture we use with one extra feed-forward convolution layer with 1 × 1 filters attached to output pixel-wise confidence maps. By comparing "VGG16" with "LG-LSTM", 4.6% improvement in IOU on horse class can be observed, which demonstrates the superiority of using more LG-LSTM layers to jointly address the long-distance and short-distance contextual information. Note that, since the LSTMs are deployed in each position in each LG-LSTM layer and more parameters are introduced for model learning, one possible alternative solution is just using more convolutional layers instead of LG-LSTM layers. We thus report the performance of using more convolutional layers on the basic network structure, i.e. "VGG16 extra conv". To make fair comparison with our usage of five LG-LSTM layers, five extra convolutional layers are utilized containing 576 = 64 × 9 convolutional filters with size 3 × 3 in each convolutional layer, because nine LSTMs are used in LG-LSTM layers and each of them has 64 hidden cell outputs. Compared with "LG-LSTM", the "VGG16 extra conv" decreases the mean IOU by 2.78% and 4.86% on horse and cow classes, respectively. It speaks well for the superiority of using LG-LSTM layers to harness complex long-distances patterns and memorize long-period hidden states over purely convolutional layers.</p><p>Local connections in LG-LSTM: Note that in LG-LSTM, we use eight spatial neighboring connections to capture local contextual information for each position. To further validate the effectiveness of LG-LSTM, we also report the performance of using two local connections and four local connections, i.e. "LG-LSTM local 2" and "LG-LSTM local 4". For "LG-LSTM local 2", the top and left neighboring positions with respect to each position are used. For "LG-LSTM local 4", the local information is propagated from four neighboring positions in top, left, top-left and top-right directions. It can be observed that our LG-LSTM architecture ("LG-LSTM") significantly outperforms "LG-LSTM local 2" and "LG-LSTM local 4" by 4.19% and 2.94% in IOU on cow class, respectively. Eight spatial connections with neighboring pixels for each position enable LG-LSTM to capture richer information from neighboring context, which are more informative than other limited local spatial connections. As illustrated in the first row of <ref type="figure">Figure 6,</ref> LG-LSTM gives more consistent parsing results by incorporating sufficient local connections while Co-CNN produces many fragments of semantic regions.</p><p>Global connections in LG-LSTM: "LG-LSTM w/o global" in <ref type="table" target="#tab_4">Table 4</ref> indicates the results without using global hidden cells as the LSTM inputs for each position. Compared with "LG-LSTM", 1.27% and 1.81% decreases in <ref type="table">Table 5</ref>. Per-Class Comparison of F-1 scores with five state-of-the-art methods on ATR <ref type="bibr" target="#b14">[15]</ref>. LG-LSTM <ref type="figure">Figure 6</ref>. Comparison of object parsing results of our LG-LSTM architecture and the Co-CNN <ref type="bibr" target="#b18">[19]</ref> on ATR dataset <ref type="bibr" target="#b14">[15]</ref>.</p><p>IOU occur with "LG-LSTM w/o global" on horse and cow classes, respectively. This demonstrates well the effectiveness of global contextual information for inferring the prediction of each position. These global features provide an overview perspective of the image to guide the pixel-wise labeling. As shown in the second row of <ref type="figure">Figure 6</ref>, by gathering global information from the whole image, LG-LSTM successfully distinguishes the combination of upper-clothes and skirt with dress while Co-CNN often confuses them only from local cues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">More Visual Comparison</head><p>The qualitative comparisons of parsing results on Horse-Cow dataset and ATR dataset are visualized in <ref type="figure" target="#fig_4">Figure 5</ref> and <ref type="figure">Figure 6</ref>, respectively. Because the previous state-ofthe-art methods do not publish their codes, we only compare our method with the "VGG16" on Horse-Cow parsing dataset. As can be observed from these visualized comparisons, our LG-LSTM architecture outputs more semantically meaningful and precise predictions than "VGG16" and "Co-CNN" despite the existence of large appearance and position variations. For example, the small regions (e.g. tails) can be successfully segmented out by LG-LSTM from neighboring similar semantic regions (i.e. body and legs) on the Horse-Cow dataset.</p><p>LG-LSTM can successfully handle the confusing labels such as skirt vs dress and legs vs pants on the human parsing dataset. The regions with similar appearances can be recognized and separated by the guidance from global contextual information, while the local boundaries for different semantic regions are preserved well by using local connections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions and Future Work</head><p>In this work, we proposed a novel local-global LSTM architecture for semantic object parsing. The LG-LSTM layers jointly capture the long-distance and short-distance spatial dependencies by using global hidden cells from the whole maps and local hidden cells from eight spatial dimensions and one depth dimension. Extensive results on three public datasets clearly demonstrated the effectiveness of the proposed LG-LSTM in generating pixel-wise semantic labeling. In the future, we will explore how to develop a pure LG-LSTM network architecture where all convolutional layers are replaced with well designed LG-LSTM layers. It will produce more complex neural units in each layer, which can hierarchically exploit local and global connections of the whole image, and enables to remember longperiod hidden states to capture complex visual patterns.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Examples for semantic object parsing by our LG-LSTM architecture. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>The proposed LG-LSTM architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Illustration of the proposed LG-LSTM layer. For local interactions, the prediction of each pixel (black point) depends on the features of all eight neighboring positions (green points), shown in the left column. The proposed LG-LSTM layer is shown in the right column. To connect the convolutional layers with LG-LSTM layers, we first feed convolutional feature maps into nine LSTMs to generate corresponding hidden cells. Then, local hidden cells comprise the hidden cells passed from eight neighboring directions (blue arrows) and that position (purple arrows) in the previous layer, or from the transition layer for the first LG-LSTM layer. Global hidden cells are constructed from hidden cells of all positions. We feed them both into nine LSTMs to produce individual hidden cells along different spatial dimensions (green arrows) and the depth dimension (purple arrow). Through recurrent connections by using stacked layers, the feature of each pixel can capture context information from a much larger local region and the global perspective of the whole image. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Global hidden cells in LG-LSTM. The whole hidden cell maps are spatially partitioned into nine grids and the global max pooling is performed on each grid to generate the global hidden cells, which well capture global contextual information.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Comparison of object parsing results by our LG-LSTM architecture and the baseline that only fine-tunes the model based on VGG-16 network. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 2 .</head><label>2</label><figDesc>Comparison of human parsing performance with five state-of-the-art methods when evaluating on ATR<ref type="bibr" target="#b14">[15]</ref>.</figDesc><table>Method 
Acc. F.g. acc. Avg. prec. Avg. recall Avg. F-1 score 

Yamaguchi et al. [37] 84.38 55.59 
37.54 
51.05 
41.80 
PaperDoll [36] 
88.96 62.18 
52.75 
49.43 
44.76 
M-CNN [21] 
89.57 73.98 
64.56 
65.17 
62.81 
ATR [15] 
91.11 71.04 
71.69 
60.25 
64.38 
Co-CNN [19] 
95.23 80.90 
81.55 
74.42 
76.95 
Co-CNN (more) [19] 96.02 83.57 
84.95 
77.66 
80.14 
CRFasRNN [38] 
96.34 85.10 
84.00 
80.70 
82.08 

LG-LSTM 
96.18 84.79 
84.64 
79.43 
80.97 
LG-LSTM (more) 96.85 87.35 
85.94 
82.79 
84.12 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 .</head><label>3</label><figDesc></figDesc><table>Comparison of human parsing performance with four 
state-of-the-art methods on the test images of Fashionista [37]. 

. 

Method 
Acc. F.g. acc. Avg. prec. Avg. recall Avg. F-1 score 

Yamaguchi et al. [37] 87.87 58.85 
51.04 
48.05 
42.87 
PaperDoll [36] 
89.98 65.66 
54.87 
51.16 
46.80 
ATR [15] 
92.33 76.54 
73.93 
66.49 
69.30 
Co-CNN [19] 
96.08 84.71 
82.98 
77.78 
79.37 
Co-CNN (more) [19] 97.06 89.15 
87.83 
81.73 
83.78 

LG-LSTM 
96.85 87.71 
87.05 
82.14 
83.67 
LG-LSTM (more) 97.66 91.35 
89.54 
85.54 
86.94 

tune the networks on VGG-16 for roughly 60 epochs and 
it takes about 1 day for the Horse-Cow parsing dataset. For 
training based on "Co-CNN" from the scratch, it takes about 
4-5 days for the human parsing datasets. In the testing stage, 
one image takes 0.3 second on average. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4 .</head><label>4</label><figDesc></figDesc><table>Comparison of object parsing performance with differ-
ent variants of the proposed LG-LSTM architecture on Horse-Cow 
object parsing dataset [31]. 

Horse 
Method 
Bkg head body leg 
tail 
Fg IOU Pix.Acc 

VGG16 
87.73 60.25 80.06 56.74 35.87 80.19 64.13 89.00 
VGG16 extra conv 88.60 62.02 81.39 58.10 39.63 80.92 65.95 89.80 
LG-LSTM local 2 88.58 64.63 82.53 58.53 39.33 81.32 66.72 89.99 
LG-LSTM local 4 88.97 66.43 83.42 59.49 40.06 81.64 67.68 90.41 
LG-LSTM w/o global 89.06 65.18 83.07 59.26 40.76 81.71 67.46 90.39 
LG-LSTM 
89.64 66.89 84.20 60.88 42.06 82.50 68.73 90.92 

Cow 
Method 
Bkg head body leg 
tail 
Fg IOU Pix.Acc 

VGG16 
87.62 57.97 75.39 47.21 13.89 81.78 56.41 87.73 
VGG16 extra conv 87.46 60.88 76.60 47.82 16.93 82.69 57.93 87.83 
LG-LSTM local 2 87.57 61.06 76.82 48.32 19.21 82.78 58.60 87.95 
LG-LSTM local 4 88.14 63.34 78.57 49.90 19.29 83.54 59.85 88.67 
LG-LSTM w/o global 88.77 64.59 80.11 52.21 19.24 84.19 60.98 89.41 
LG-LSTM 
89.71 68.43 82.47 53.93 19.41 85.41 62.79 90.43 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head></head><label></label><figDesc>MethodHat Hair S-gls U-cloth Skirt Pants Dress Belt L-shoe R-shoe Face L-leg R-leg L-arm R-arm Bag Scarf Input Co-CNN LG-LSTM Input Co-CNN LG-LSTM Input Co-CNN Input Co-CNN LG-LSTM</figDesc><table>Yamaguchi et al. [37] 8.44 59.96 12.09 
56.07 
17.57 55.42 40.94 14.68 38.24 
38.33 72.10 58.52 57.03 45.33 46.65 24.53 11.43 
PaperDoll [36] 
1.72 63.58 0.23 
71.87 
40.20 69.35 59.49 16.94 45.79 
44.47 61.63 52.19 55.60 45.23 46.75 30.52 2.95 
M-CNN [21] 
80.77 65.31 35.55 
72.58 
77.86 70.71 81.44 38.45 53.87 
48.57 72.78 63.25 68.24 57.40 51.12 57.87 43.38 
ATR [15] 
77.97 68.18 29.20 
79.39 
80.36 79.77 82.02 22.88 53.51 
50.26 74.71 69.07 71.69 53.79 58.57 53.66 57.07 
Co-CNN [19] 
72.07 86.33 72.81 
85.72 
70.82 83.05 69.95 37.66 76.48 
76.80 89.02 85.49 85.23 84.16 84.04 81.51 44.94 
Co-CNN more [19] 
75.88 89.97 81.26 
87.38 
71.94 84.89 71.03 40.14 81.43 
81.49 92.73 88.77 88.48 89.00 88.71 83.81 46.24 

LG-LSTM (more) 
81.13 90.94 81.07 
88.97 
80.91 91.47 77.18 60.32 83.40 
83.65 93.67 92.27 92.41 90.20 90.13 85.78 51.09 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Scene Labeling with LSTM Recurrent Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Byeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Breuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Raue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liwicki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Texture classification using 2d lstm networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Byeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liwicki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Breuel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Detect what you can: Detecting and representing objects using holistic models and body parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Mindś eye: A recurrent visual representation for image caption generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A deformable mixture parsing model with parselets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">The pascal visual object classes challenge 2010 (voc2010) results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multidimensional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fernandez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICANN</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Speech recognition with deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICASSP</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="6645" to="6649" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Offline handwriting recognition with multidimensional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="545" to="552" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Hypercolumns for object segmentation and fine-grained localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Grid long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.01526</idno>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Deep human parsing with active template regression. TPAMI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Towards computational baby learning: A weakly-supervised approach for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Reversible recursive instance-level object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Proposal-free network for instance-level object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.02636</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Human parsing with contextualized convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Fashion parsing with video context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1347" to="1358" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Matching-CNN Meets KNN: Quasi-Parametric Human Parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Semantic image segmentation via deep parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.4038</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Parsing semantic parts of cars using graphical models and segment appearance consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A High Performance CRF Model for Clothes Parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Simo-Serra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">End-to-end people detection in crowded scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Parallel multi-dimensional lstm, with application to fast biomedical volumetric image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Stollenga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Byeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liwicki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.07452</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.03478</idno>
		<title level="m">Generative image modeling using spatial lstms</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Semantic part segmentation using compositional model combining shape and appearance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Who blocks who: Simultaneous clothing segmentation for grouping images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Joint object and part segmentation using deep learned potentials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Discriminative hierarchical part-based models for human parsing and action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3075" to="3102" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2048" to="2057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Paper doll parsing: Retrieving similar styles to parse clothing items</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yamaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kiapour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Parsing clothing in fashion photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yamaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kiapour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ortiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jayasumana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
		<title level="m">Conditional random fields as recurrent neural networks. In ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
