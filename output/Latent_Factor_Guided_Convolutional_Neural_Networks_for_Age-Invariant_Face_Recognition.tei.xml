<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Latent Factor Guided Convolutional Neural Networks for Age-Invariant Face Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Wen</surname></persName>
							<email>yd.wen@siat.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Electronic and Information Engineering</orgName>
								<orgName type="institution">South China University of Technology</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institutes of Advanced Technology</orgName>
								<orgName type="institution">Shenzhen Key Lab of Comp. Vis. &amp; Pat. Rec</orgName>
								<address>
									<settlement>Shenzhen</settlement>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">CAS</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Li</surname></persName>
							<email>zhifeng.li@siat.ac.cn</email>
							<affiliation key="aff1">
								<orgName type="department">Institutes of Advanced Technology</orgName>
								<orgName type="institution">Shenzhen Key Lab of Comp. Vis. &amp; Pat. Rec</orgName>
								<address>
									<settlement>Shenzhen</settlement>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">CAS</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
							<email>yu.qiao@siat.ac.cn</email>
							<affiliation key="aff1">
								<orgName type="department">Institutes of Advanced Technology</orgName>
								<orgName type="institution">Shenzhen Key Lab of Comp. Vis. &amp; Pat. Rec</orgName>
								<address>
									<settlement>Shenzhen</settlement>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">CAS</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Latent Factor Guided Convolutional Neural Networks for Age-Invariant Face Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:42+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>While considerable progresses have been made on face recognition, age-invariant face recognition (AIFR) still remains a major challenge in real world applications of face recognition systems. The major difficulty of AIFR arises from the fact that the facial appearance is subject to significant intra-personal changes caused by the aging process over time. In order to address this problem, we propose a novel deep face recognition framework to learn the ageinvariant deep face features through a carefully designed CNN model. To the best of our knowledge, this is the first attempt to show the effectiveness of deep CNNs in advancing the state-of-the-art of AIFR. Extensive experiments are conducted on several public domain face aging datasets (MORPH Album2, FGNET, to demonstrate the effectiveness of the proposed model over the state-ofthe-art. We also verify the excellent generalization of our new model on the famous LFW dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Face recognition is one of the most active areas in computer vision community. It has been studied for several decades with substantial progresses. Most of the existing research focuses on general face recognition. There are very limited work directly on age-invariant face recognition (AIFR), which aims to address the face matching problem in the presence of remarkable aging variations <ref type="bibr" target="#b25">[26]</ref>.</p><p>AIFR has many useful and practical applications. For instance, it can be applied in finding missing children after years or checking whether the same person has been issued multiple government documents in different ages. However, it still remains a challenging problem in real world applications of face recognition systems. The major challenge of AIFR is mostly attributed to the great changes in face appearance caused by aging process over time. <ref type="figure" target="#fig_0">Figure 1</ref> is a * Corresponding author.  <ref type="bibr" target="#b0">[1]</ref>. One can see the significant intra-personal variation therein.</p><p>typical example, in which the cross-age face images from the same person have significant intra-personal changes. The previous research on AIFR falls into two categories: generative approaches and discriminative approaches. The generative approaches <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b23">24]</ref> first synthesis face that matches target age and then perform recognition. Due to the strong parametric assumptions and the complexity in modeling aging process, these methods are computationally expensive and the results are often unstable in real-world face recognition scenarios. Recently discriminative approaches <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17]</ref> draw increasing attentions. However, the features they use still contain age information, which is detrimental to the subsequent classification. To separate the age and the identity information, <ref type="bibr" target="#b6">[7]</ref> proposes the hidden factor analysis (HFA) method. It formulates face feature as the linear combination of an identity component and an age component. Then the identity component is used for face recognition. <ref type="bibr" target="#b7">[8]</ref> reports a more effective maximum entropy feature descriptor for AIFR, and proposes a more robust identity matching framework. However, most of the existing methods in AIFR rely heavily on the hand-crafted feature descriptors to extract the dense features for age-invariant face recognition, which may limit the performance of these methods. Designing an effective age-invariant face features still remains an open problem in AIFR.</p><p>As one of the most promising feature learning tools nowadays, deep convolution neural networks (CNNs) have been successfully applied to a variety of problems in computer vision, including object detection and classification <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b27">28]</ref>, and face recognition <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b35">36]</ref>, etc. So it is desirable to use the deep learning model to address the AIFR problem. Surprisingly, there is no such work showing the superiority of deep learning on AIFR in the literature, to the best of our knowledge. A possible reason is the lack of a very suitable face aging dataset that can be used to train a robust deep learning model specifically for AIFR. For all the existing face aging datasets, each subject has very limited number of training samples across different ages, which are not suitable to serve as the training data in deep models. If we use the large scale web-collected face images to train the deep CNNs, the learned deep features will inevitably contain both identity-related component (e.g. ethnicity, gender) and identity-unrelated component (e.g. age, noise) <ref type="bibr" target="#b31">[32]</ref>. Ideally, we expect the resulting deep feature contains only the identity-related components, reducing the variations caused by the aging process as much as possible.</p><p>In this paper, we explore the use of deep CNNs in AIFR and propose a latent factor guided CNN (LF-CNN) framework to learn the age-invariant deep face features. Specifically, we extract the age-invariant deep features from convolutional features by a carefully designed fully connected layer, termed as latent factor fully connected (LF-FC) layer. For this purpose, we develop a latent variable model, called latent identity analysis (LIA), to separate the variations caused by the aging process from the identity-related components in the convolutional features. The parameters of the LIA model are used to update the parameters of LF-FC layer. Moreover, the LIA model and the loss function in CNNs constitute the age-invariant identity loss, which is used to guide the learning of the LF-CNNs. In this way, our model is more adapted to age-invariant face recognition problem, as supported by our experimental results in Section 4. <ref type="figure">Figure 2</ref> is a visualization example of the ageinvariant deep features and the convolutional features, from which we can clearly see that the convolutional features are still age sensitive, while the age-invariant features are robust to aging process.</p><p>The major contributions of this paper are summarized as follows:</p><p>• We propose a robust age-invariant deep face recognition framework. To the best of our knowledge, it is the first work to show the effectiveness of deep CNNs in AIFR and achieve the best results to date.</p><p>• Instead of directly applying deep learning model to AIFR, we propose a new model called latent factor guided convolutional neural network (LF-CNN) to specifically address the AIFR task. By coupled learning the parameters in CNNs and LIA, the age-invariant deep face features can be extracted, which are more robust to the variations caused by the aging process over the time.</p><p>• Extensive experiments have shown that the proposed approach significantly outperforms the state-of-the-art on all the three face aging datasets (MORPH Album2 <ref type="bibr" target="#b26">[27]</ref>, FG-NET <ref type="bibr" target="#b0">[1]</ref> and CACD-VS <ref type="bibr" target="#b3">[4]</ref>), even beating the human voting performance on the CACD-VS dataset. We further demonstrate the excellent generalizability of our approach on the famous LFW <ref type="bibr" target="#b10">[11]</ref> dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Convolutional Networks for Face Recognition</head><p>CNNs play a significant role in recent advances of face recognition. DeepFace <ref type="bibr" target="#b32">[33]</ref> reports that a deeply-learned face representation achieves the accuracy close to humanlevel performance on LFW dataset <ref type="bibr" target="#b10">[11]</ref>. <ref type="bibr" target="#b31">[32]</ref> learns a deep CNN with the identification-verification supervisory signal and further adds supervision to early convolutional layers, greatly boosting the face recognition accuracy. FaceNet <ref type="bibr" target="#b28">[29]</ref>   <ref type="figure">Figure 3</ref>. The architecture of the proposed LF-CNNs and its training process. Frozen layer only performs regular forward and backward calculations, but does not update their parameters (in other words, the parameters of this layers are fixed). The outside data Y ia and the training data Y ia are trained differently. Specifically, Y ia and Y ia are used for training the convolutional unit and the LF-FC layer respectively, following different pipelines. The two parallel convolution units are corresponding to a physical module in two stages (frozen and not frozen).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Latent Variable Model</head><p>The Latent variable model finds the latent variable that are not directly observed but inferred through a statistical model from observations. Latent variable models are widely used in recommended systems <ref type="bibr" target="#b9">[10]</ref>. A few recent work <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref> apply the latent variable model to face recognition and achieves impressive performance. However, how to smoothly apply latent variable model in CNNs to achieve robust recognition still remains to be explored.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The Proposed Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">The LF-CNNs Model</head><p>The LF-CNN model is composed of two key components: convolution unit for convolution feature learning and LF-FC layer for age-invariant deep feature learning. The architecture of the LF-CNNs is shown in <ref type="figure">Figure 3</ref>.</p><p>The structure of the convolution unit in LF-CNNs follows typical CNNs, alternatively stacking convolution layer, nonlinearity layer, and optional pooling layer. We construct the convolution unit with 5 convolution layers as shown in <ref type="figure">Figure 3</ref>. The convolution kernel size and stride are set as 3×3 and 1 respectively, to capture more facial details in raw images <ref type="bibr" target="#b29">[30]</ref>. In the 4th and 5th layer, the weights of convolution are locally shared to learn different mid-level and high-level features from different regions <ref type="bibr" target="#b32">[33]</ref>. The five convolution layers output 128, 128, 128, 256 and 256 feature maps respectively. The nonlinear function is the Para-metric Rectified Linear Unit (PReLU) <ref type="bibr" target="#b8">[9]</ref>, which improves model fitting. The max pooling is used for enhancing the robustness to potential translation and subsampling.</p><p>Next we focus on the construction of the LF-FC layer. Notice that the FC layer is equivalent to the matrix multiplication: F f c = W F conv + b where F f c is the output of FC layer, F conv is the convolutional feature, and W , b are the parameters of the FC layer. We leverage such equivalence property to design a set of W , b that can extract the age-invariant feature from F conv . Note that, no nonlinearity layer will cascade to the LF-FC layer according to the LIA model. The feature dimension of F f c is 512. Instead of iteratively updating the all the parameters in LF-CNNs by stochastic gradient descent (SGD), we design an Latent Identity Analysis (LIA) method to learn W and b for the LF-CNN model, as elaborated in the following subsection. For the parameters of the convolution unit, we fix W , b to update them via standard SGD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Latent Identity Analysis</head><p>The latent identity analysis model can infer the unobserved latent factors (one of them being the identity factor) from the observed features in a supervised fashion. The general model of latent identity analysis is formulated as</p><formula xml:id="formula_0">v = d i=1</formula><p>Uixi +v <ref type="bibr" target="#b0">(1)</ref> where v ∈ R n×1 denotes the observed facial features, U i ∈ R n×pi is the corresponding matrix whose columns span the subspace of different variation and x i ∈ R pi×1 denotes the latent variable with prior zero-mean Gaussian distribution.v ∈ R n×1 is the mean of all the facial features. The intuition behind this model is very clear. Each facial feature is viewed as the combination of different components according to different supervised signals.</p><p>Such an idea is useful to achieve robust face recognition in practical. In the cross-age face recognition, we usually decompose the facial features into two latent components and a noise variable. So the model can be simplified as v = U id x id + U ag x ag + U e x e +v where x id , x ag satisfy standard Gaussian distribution N (0, I) and x e satisfies N (0, σ 2 I). Note that U e is set to be a unix matrix since x e stands for noise. U id x id is the identity-related component which is key to achieve age-invariant face recognition.</p><p>U ag x ag is the age-related component, representing the age variations. A set of model parameters θ = {U id , U ag , σ 2 ,v} can be learned by maximizing the following maximum likelihood function L(θ):</p><formula xml:id="formula_1">L(θ) = i,j Li,j(θ) = i,j ln P (v j i , xid,j, xag,j|θ)<label>(2)</label></formula><p>where v j i is the feature of the ith subject at jth age group. x id,i and x ag,j are the corresponding identity and age factors respectively. The summation is over all the available training samples from all subjects at all age groups. We use the expectation-maximization (EM) algorithm to estimate these model parameters. To perform the EM algorithm, the Q function is given by</p><formula xml:id="formula_2">Q(θ, θ (i) ) = i,j E ln P (v j i , xid,j, xag,j|θ) = i,j x id,i ,x ag,j P (xid,j, xag,j|θ (i) , V )Li,j(θ)<label>(3)</label></formula><p>where V denotes the observed features of all the training samples, θ (i) is the given model parameters and θ is the parameters to be estimated. With the given parameter θ (i) , we can compute the posterior distribution of the latent variables P (x id,j , x ag,j |θ (i) , V ). With the given posterior distribution, we can maximize the Q function to obtain a new θ. The EM algorithm is performed in an iterative fashion. E</p><p>Step. Given the model parameter θ (i) and training data V = {v j i } i=1,··· ,N ;j=1,··· ,M , we first compute all the necessary first and second conditional moments of P (x id,j |θ (i) , V ) and P (x ag,j |θ (i) , V ) for the posterior distribution P (x id,j , x ag,j |θ (i) , V ):</p><formula xml:id="formula_3">µ1(xid,i) = UidΣ −1 Ni N i k=1 (v k i −v) (4) µ1(xag,j) = UagΣ −1 Mj M j k=1 (v j k −v) (5) µ2(xid,i, xid,i) = I − U T id Σ −1 Uid Ni + µ1(xid,i) µ1(xid,i) T (6) µ2(xag,j, xag,j) = I − U T ag Σ −1 Uag Mj + µ1(xag,j) µ1(xag,j) T (7) µ2(xid,i, xag,j) = − U T ag Σ −1 Uid NiMj + µ1(xid,i) µ1(xag,j) T (8) µ2(xag,j, xid,i) = − U T id Σ −1 Uag NiMj + µ1(xag,j) µ1(xid,i) T<label>(9)</label></formula><p>where Σ = σ 2 I + U id U T id + U ag U T id , N i and M j are the numbers of training samples for the ith subject and the kth age group, respectively. M Step. We maximize the Q function to estimate the θ (i+1) . The maximization is shown as follows:</p><formula xml:id="formula_4">θ (i+1) = arg max θ Q(θ, θ (i) )<label>(10)</label></formula><p>To solve this optimization, the model parameter θ (i+1) is given by</p><formula xml:id="formula_5">Uid = (C − DB −1 E)(A − F B −1 E) −1 Uag = (D − CA −1 F )(B − EA −1 F ) −1 σ 2 = 1 N n i,j v j i −v − Uidµ1(xid,i) − Uagµ1(xag,j) T (v j i −v)<label>(11)</label></formula><p>in which</p><formula xml:id="formula_6">A = ij µ2(xid,i, xid,i), B = i,j µ2(xag,j, xag,j), C = ij (v j i −v) µ1(xid,i) T , D = ij (v j i −v) µ1(xag,j) T , E = ij µ2(xag,j, xid,i), F = i,j µ2(xid,i, xag,j).<label>(12)</label></formula><p>After the model parameters and the posterior distribution are estimated by the EM algorithm, the identity factor for the ith subject can be inferred by the first moment of x id,i , namely µ 1 (x id,i ). One can observe that the form of µ 1 (x id,i ) is actually identical to an LF-FC layer with the following parameters:</p><formula xml:id="formula_7">W = U T id Σ −1 , b = −U T id Σ −1v (13) wherev = 1 N i,j v j i and Σ = σ 2 I + U id U T id + U ag U T id .</formula><p>The feature v comes from the convolution unit. Because v is updated with new parameters of the identity factor guided FC layer, W , b are also updated using the new v. The procedure iteratively goes on. The detailed learning framework is elaborated in the next subsection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Coupled Learning Algorithm for LF-CNNs</head><p>Input: Outside training data Yi with identity label, cross-age training data Yia with both age and identity label. Output: The parameters θ f and θg.</p><p>1: i ← 0. 2: Initialize the parameters θ i ← i + 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>7:</head><p>Fix the θ </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Learning Framework</head><p>In LF-CNN model, the convolution unit maps a raw input image F img to convolutional feature F conv by F conv = f (F img ), and then the LF-FC layer computes the ageinvariant feature F f c via F f c = g(F conv ). The ageinvariant feature F f c is used for the AIFR. The parameters in the convolution unit and the LF-FC layer are denoted by θ f , θ g respectively. In our framework, f (·) and g(·) characterize different properties of AIFR and the original learning methods based on CNNs are not appropriate, we adopt a coupled learning framework to optimize the LF-CNN model. Concretely, after initializing the networks, the parameters θ f of convolutional unit is updated by SGD with the fixed θ g . Then we fix θ f and use the LIA model to learn the parameters θ g for the LF-FC layer. We alternatively update θ f and θ g until the stopping condition is satisfied. The procedure is summarized in Algorithm 1.</p><p>Learning parameters for LF-FC layer. We use the training data Y ia with both age and identity label to train the LF-FC layer. Specifically, the convolutional feature F f c is taken as the observed features V in LIA. The LIA model learns the parameters θ g = {U id , U ag , σ 2 ,v}, and then computes W , b for the LF-FC layer via Eq. (13).</p><p>Learning parameters for convolution unit. To learn the parameters θ f for the convolution unit, we need to fix the parameters θ g of the LF-FC layer. Then we use the SGD to train the LF-CNNs with outside training data Y i . Note that, we use both the softmax loss and the contrastive loss to strength the supervision in learning, similar to <ref type="bibr" target="#b30">[31]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Discussion</head><p>The learning process of LF-CNNs has the following advantages. First, the coupled learning is very beneficial to AIFR. The joint objective functions consists of minimizing the classification error (softmax loss and contrastive loss) and maximizing the likelihood probability that the training samples are generated by the latent factors. The former aims to learn discriminant feature representations for classification while the later improves the robustness of age-invariant features. Both of them consistently contribute to the gain in the AIFR task. Second, we update the LF-FC layer by LIA instead of SGD, which largely reduces the parameter scale and prevents potential overfitting. LIA plays an essential role in LF-CNNs by inferring the effective identity factor to guide the parameter estimation of the LF-FC layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we conduct extensive experiments to demonstrate the effectiveness of the proposed LF-CNNs on several challenging face aging databases, including MORPH Album 2 <ref type="bibr" target="#b26">[27]</ref> (the largest face aging database available in the pubic domain), FG-NET <ref type="bibr" target="#b0">[1]</ref> (a publicdomain face aging dataset), and the subset of Cross-Age Celebrity Dataset (CACD-VS) <ref type="bibr" target="#b3">[4]</ref>. To further demonstrate the generalization of our model, we also evaluate our model on the Labeled Faces in the Wild (LFW) database <ref type="bibr" target="#b10">[11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation Details</head><p>Preprocessing. For each face image, we use the recently proposed algorithms <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b2">3]</ref> to detect the facial landmarks in images. Then the faces are globally cropped to 112 × 96 according to the 5 facial landmarks (two eyes, nose and two mouth corners) by similarity transformation.</p><p>Training Data. The training data used in this paper is composed of two parts: outside training data Y i (that only contains the identity information) and face aging training data Y (that contains both the age and identity information). For the outside training data Y i , we use the large scale web-collected face data, including CASIA-WebFace <ref type="bibr" target="#b33">[34]</ref>, CACD <ref type="bibr" target="#b3">[4]</ref>, and Celebrity+ <ref type="bibr" target="#b20">[21]</ref>). When testing our model on CACD-VS, we remove all the identities in CACD from the training data. The images are horizontally flipped for data augmentation. For the face aging training data Y ia , we use the MORPH Album 2 dataset, as described in Section 4.2.</p><p>Detailed setting in LF-CNN model. We implement the LF-CNN model using the Caffe library <ref type="bibr" target="#b11">[12]</ref> with our modifications. Unless otherwise specified, the batch size is 150. When training convolution unit with outside data, the learning rate is 1e-1, 1e-2, 1e-3 and is switched when the error plateaus. The total number of epochs is about 12 for our model.</p><p>Classifier. To better evaluate the performance of the proposed age-invariant deep face features, our model uses the simple Euclidean Distance and the Nearest Neighbor rule as the classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Experiments on the MORPH Album 2 Dataset</head><p>The MORPH Album 2 database is the largest face aging dataset available in the public domain, consisting of about 78,000 face images of 20,000 persons with age ranging from 16 to 77. Following the same training and testing split scheme in <ref type="bibr" target="#b6">[7]</ref>, 10,000 subjects are used for training and the remaining 10,000 subjects are used for testing. There is no overlapping subject between the training set and the testing set. For each subject, two face images with the youngest age and the oldest age are selected as gallery and probe set respectively. For fair comparison, we also train a baseline CNN model with the same networks as LF-CNNs, and learn all the parameters by SGD. The experimental results are shown in <ref type="table" target="#tab_2">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Rank-1 Identification Rates HFA (2013) <ref type="bibr" target="#b6">[7]</ref> 91.14% CARC (2014) <ref type="bibr" target="#b3">[4]</ref> 92.80% MEFA (2015) <ref type="bibr" target="#b7">[8]</ref> 93.80% MEFA+SIFT+MLBP (2015) <ref type="bibr" target="#b7">[8]</ref> 94.59% Method (2015) in <ref type="bibr" target="#b15">[16]</ref> 87.13% LPS+HFA (2016) <ref type="bibr" target="#b16">[17]</ref> 94.87% CNN-baseline 89.68% CNN-baseline 95.13% (fine-tuned by MORPH training data) LF-CNNs 97.51% (fine-tuned by MORPH training data) In <ref type="table" target="#tab_2">Table 1</ref>, we compare our LF-CNN model against (i) the CNN-baseline model, (ii) the CNN-baseline model (fintuned by MORPH training data), and (iii) several recently developed top-performing AIFR algorithms in the literature. From these results, we have the following observations. First, the result of the CNN-baseline is only 89.68%, which is inferior to the other results in <ref type="table" target="#tab_2">Table 1</ref>. This confirms that directly applying the deep CNN model to address the AIFR problem is indeed not a good choice. Second, the performance of CNN-baseline can be improved to 95.13% by fine-tuning with the additional MORPH training data. However, this result (95.13%) is near to the top-performing result in the literature (94.59%). The lack of a significant improvement over the state-of-the-art reflects the limitation of the CNN-baseline model. So it is desirable to design a new deep CNN model to address the AIFR problem. Finally, it is encouraging to see that the proposed LF-CNN model obtains a significant performance improvement over the other results in <ref type="table" target="#tab_2">Table 1</ref>  To further evaluate the performance of our LF-CNN model, we design an experiment to report the recognition results of our model in each iteration of the coupled learning process, as illustrated in <ref type="figure" target="#fig_3">Figure 4</ref>. The two parameters θ f and θ g are updated in a step-wise manner. With one fixed, update the other one, and vice versa. <ref type="figure" target="#fig_3">Figure 4</ref> clearly shows that the coupled learning process consistently contributes to the performance improvement of AIFR, converging to a good result quickly. <ref type="figure" target="#fig_4">Figure 5</ref> shows some examples of the failed retrievals using our approach in MORPH Album 2 dataset. We can see that although our results are incorrect in these cases, the probe images appear to be more similar to the incorrect retrievals than the gallery images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Experiments on the FG-NET Dataset</head><p>The FG-NET dataset consists of 1002 face images from 82 different subjects, with each subject having multiple face images at different ages (ranging from 0 to 69). Following the testing scheme in <ref type="bibr" target="#b17">[18]</ref>, we compare our LF-CNNs with the state-of-the-art approaches on this dataset. The comparative results are reported in <ref type="table">Table 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Rank-1 Identification Rates <ref type="bibr" target="#b23">Park et al. (2010)</ref>  <ref type="bibr" target="#b23">[24]</ref> 37.4% <ref type="bibr" target="#b17">Li et al. (2011)</ref>  <ref type="bibr" target="#b17">[18]</ref> 47.5% HFA (2013) <ref type="bibr" target="#b6">[7]</ref> 69.0% MEFA (2015) <ref type="bibr" target="#b7">[8]</ref> 76.2% CNN-baseline 84.4% LF-CNNs 88.1% <ref type="table">Table 2</ref>. Performance of different methods on FG-NET.</p><p>As can be seen from <ref type="table">Table 2</ref>, LF-CNNs achieve the highest recognition accuracy (88.1%) among all the results, sig- Interestingly, MORPH and FG-NET have different age distributions. In FGNET, roughly 61% samples are less than 16 years old. But for the MORPH dataset, all the persons are more than 16 years old. So it is desirable to exploit the influence of different age distributions on the proposed approach. In <ref type="table">Table 3</ref> we give the rank-1 identification rates in different age groups. The results in <ref type="table">Table 3</ref> show that the proposed LF-CNNs consistently outperforms the CNN-baseline model on all the age groups. This further confirms the advantage of our LF-CNN model over the CNN-baseline model in AIFR task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Experiments on the CACD Verification Subset</head><p>The CACD dataset is a recently released dataset for AIFR, containing 163,446 images from 2,000 celebrities with labeled ages. It includes varying illumination, pose variation and makeup and better simulates practical scenario. However, the entire CACD dataset contains some incorrectly labeled samples, and some duplicate images. Following the state-of-the-art configuration <ref type="bibr" target="#b3">[4]</ref>, we test LF-CNNs on a subset of CACD <ref type="bibr" target="#b3">[4]</ref>, CACD-VS, which consists of 4000 image pairs (2000 positive pairs and 2000 negative pairs) and have been carefully annotated. We follow the same training strategy as in section 4.3. Note that, the identities in CACD-VS are excluded from the outside training data in this experiment. So only 400,000 training samples are used. According to the ten-fold cross-validation rule, we calculate the Euclidean Distance of each pairs and choose the best threshold using nine training folds, then testing on the leftover fold. We compute the face verification rate and compare our result with the existing methods in this dataset, as shown in <ref type="figure" target="#fig_5">Figure 6</ref> and <ref type="table">Table 4</ref>. Again, the proposed LF-CNN model significantly outperforms all the published results in this dataset, even surpassing the human-level performance with a clear margin. It further demonstrates the effectiveness of the proposed ageinvariant deep features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Acc.</p><p>High-Dimensional LBP (2013) <ref type="bibr" target="#b4">[5]</ref> 81.6% HFA (2013) <ref type="bibr" target="#b6">[7]</ref> 84.4% CARC (2014) <ref type="bibr" target="#b3">[4]</ref> 87.6% Human, Average <ref type="formula" target="#formula_1">(2015)</ref> 85.7% Human, Voting <ref type="formula" target="#formula_1">(2015)</ref> 94.2% LF-CNNs 98.5% <ref type="table">Table 4</ref>. Performance of different methods on CACD-VS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Experiments on the LFW Dataset</head><p>To evaluate the generalization performance of LF-CNNs, we further conduct an experiment on the famous LFW dataset <ref type="bibr" target="#b10">[11]</ref>. This dataset contains 13,233 face images from 5749 different subjects, collecting from uncontrolled conditions. Following the unrestricted with labeled outside data protocol <ref type="bibr" target="#b10">[11]</ref>, we train on the outside dataset and test on 6,000 face pairs. People overlapping between the outside training data and the LFW testing data are excluded. We respectively train 25 networks with 25 different image patches, and concatenate the output features from these networks into a long feature vector. We then apply PCA on the long feature vector to obtain a compact feature vector for classification. In <ref type="table">Table 5</ref> we compare our results against the recent state-of-the-art results. From the results we can see that our approach can obtain comparable results to the state-of-theart approaches using relatively small training data, demonstrating the excellent generalization ability of our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Images Networks Acc.</p><p>DeepFace <ref type="formula" target="#formula_1">(2014)</ref>   <ref type="table">Table 5</ref>. Performance of different methods on LFW.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this paper, we have proposed an age-invariant deep face recognition framework, referred to as LF-CNNs. Unlike the existing deep learning models in face recognition community, the proposed new model constructs a latent identity analysis (LIA) module to guide the learning of the CNNs parameters. By coupled learning the CNNs parameters and the LIA parameters, our model can extract the ageinvariant deep face features, which are well suitable for the AIFR task. Extensive experiments are conducted on several public-domain face aging databases to demonstrate the significant performance improvement of our new model over the state-of-the-art. We have also performed experiments on the famous LFW dataset to demonstrate the excellent generalization ability of our new model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledge</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Cross-age face images for one of the subjects in the FGNET dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>=</head><label></label><figDesc>{Uid, Uag, σ 2 ,v} where σ 2 = 0.1,v = 0 and Uid, Uag are randomly initialized from −0.1 to 0.1. 4: Compute W (1) , b (1) for the LF-FC layer via Eq. (13). 5: while not converge do 6:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>input the training data Yia to obtain the convolutional features Fconv, which are taken as the observed features V . parameters W (i+1) , b (i+1) for the LF-FC layer via the Eq. (13). 11: end while</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>The recognition rates in each iteration of the coupled learning algorithm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Some failed retrievals in MORPH Album 2. The first row: the probe images. The second row: the incorrect retrievals using our approach. The third row: the corresponding gallery images for the probe images.nificantly outperforming the top-performing result (76.2%) in<ref type="bibr" target="#b7">[8]</ref> by 11.9%. Moreover, the proposed LF-CNNs outperform the CNN-baseline method by a clear margin. This confirms what we observe from the MORPH Album 2 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>ROC comparisons of different methods on CACD-VS.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>Figure 2. Cross-age faces processed by the proposed LF-CNNs. We visualize<ref type="bibr" target="#b34">[35]</ref> the convolutional features and the age-invariant features.</figDesc><table>9 years old 

14 years old 

22 years old 

Original 
Testing Imgae 

Convolutional 
Feature 

Age-Invariant 
Deep Feature 

Convolution 
Unit 

Latent Factor 
FC Layer 
Loss 
Input 
LF-CNN 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>achieves 99.63% verification accuracy on LFW with a deep CNN trained to directly optimize the embedding itself, rather than an intermediate bottleneck layer as in previous deep learning approaches. More recently, [20] achieves a new record in verification accuracy: 99.77% on LFW with a two-stage approach that combines a multi-patch deep C-NN and deep metric learning.</figDesc><table>c 
o 
u 
p 
l 
e 
d 
l 
e 
a 
r 
n 
i 
n 
g 

Pool4 
2x2+2(S) 

Pool5 
2x2+2(S) 

Local Conv4 
3x3+1(S) 
Shared Size 
4x4 

Local Conv5 
3x3+1(S) 
Shared Size 
2x2 

Conv1 
3x3+1(S) 

Pool2 
2x2+2(S) 

Conv2 
3x3+1(S) 

Pool3 
2x2+2(S) 

Conv3 
3x3+1(S) 

Contrastive 

Image Pair with 
Identity label 

Softmax 

Contrastive_Loss 

Softmax_Loss 

Latent Factor 
FC Layer 

(Frozen) 

Convolution 
Unit 

Age-Invariant 
Feature 

Convolution 
Unit (Frozen) 

Images with 
identity and age 
label 

Age-Invariant Identity Loss 
Convolutional Feature Learning 

supervise 

apply LIA to update FC parameters 

A Pair of Age-
Invariant 
Features 

Conv Layer 1 
Conv Layer 2 
Conv Layer 3 
Conv Layer 4 
Conv Layer 5 

+ 

Latent Identity 
Analysis 

Update Parameters 
Update Parameters 

Yia 

Yi 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 1 .</head><label>1</label><figDesc>Performance of different methods on MORPH.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head></head><label></label><figDesc>, demonstrating a new state-of-the-art (97.51%) on the MORPH Album 2 database.</figDesc><table>0 
1 
2 
3 
4 
5 
6 
7 
8 
9 

0 

1 

2 

3 

4 

5 

6 

7 

8 

9 

90.25% 

91.35% 
95.06% 

96.58% 

97.05% 

97.25% 

97.48% 

97.37% 

97.53% 

97.5% 

95.18% 

96.69% 

97.13% 

97.3% 

97.52% 

97.5% 

97.5% 

97.51% 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head></head><label></label><figDesc>Age group Amount CNN-baseline LF-CNNsTable 3. Performance of different age groups on FG-NET.</figDesc><table>0 -4 
193 
51.81% 
60.10% 
5 -10 
218 
84.86% 
88.53% 
11 -16 
201 
91.04% 
94.03% 
17 -24 
182 
94.51% 
97.80% 
25 -69 
208 
99.04% 
99.52% 
0 -16 
612 
76.47% 
81.37% 
17 -69 
390 
96.93% 
98.72% 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Fg-net aging database</title>
		<ptr target="http://www.fgnet.rsunit.com/" />
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Robust discriminative response map fitting with constrained local models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Asthana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2013 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3444" to="3451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Incremental face alignment in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Asthana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1859" to="1866" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Face recognition and retrieval using cross-age reference coding with cross-age celebrity dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TMM</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="804" to="815" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Blessing of dimensionality: High-dimensional feature and its efficient compression for face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2013 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3025" to="3032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Automatic age estimation based on facial aging patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Smith-Miles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2234" to="2240" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Hidden factor analysis for age invariant face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A maximum entropy feature descriptor for age invariant face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.01852</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Latent class models for collaborative filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Puzicha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Labeled faces in the wild: A database for studying face recognition in unconstrained environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
		<idno>07-49</idno>
		<imprint>
			<date type="published" when="2007" />
			<pubPlace>Amherst</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Massachusetts</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Caffe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<title level="m">Convolutional architecture for fast feature embedding</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Face recognition across time lapse: On learning feature subspaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Klare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCB</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Toward automatic simulation of aging effects on face images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lanitis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">F</forename><surname>Cootes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="442" to="455" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning compact feature descriptor and adaptive matching framework for face recognition. Image Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2736" to="2745" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Aging face recognition: A hierarchical learning model based on local patterns selection. Image Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2146" to="2154" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A discriminative model for age invariant face recognition. Information Forensics and Security</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1028" to="1037" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Face verification across age progression using discriminative methods. Information Forensics and Security</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="82" to="91" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Targeting ultimate accuracy: Face recognition via deep embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhengping</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.07310</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.7766</idno>
		<title level="m">Deep learning face attributes in the wild</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">How does aging affect facial components? In ECCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Otto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deepid-net: Deformable deep convolutional neural networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Age-invariant face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="947" to="954" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision</title>
		<meeting>the British Machine Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Computational methods for modeling facial aging: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Biswas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Visual Languages &amp; Computing</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="131" to="144" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Morph: A longitudinal image database of normal adult age-progression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ricanek</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tesafaye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FG</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1" to="42" />
		</imprint>
	</monogr>
	<note>Imagenet large scale visual recognition challenge. IJCV</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep learning face representation by joint identification-verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1988" to="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deeply learned face representations are sparse, selective, and robust</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deepface: Closing the gap to human-level performance in face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.7923</idno>
		<title level="m">Learning face representation from scratch</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2014</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="818" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Naive-deep face recognition: Touching the limit of lfw benchmark or not?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1501.04690</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Face detection, pose estimation, and landmark localization in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2879" to="2886" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
