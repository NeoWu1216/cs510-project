<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">VLAD 3 : Encoding Dynamics of Deep Features for Action Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingwei</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>San Diego</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weixin</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>San Diego</settlement>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Mahadevan</surname></persName>
							<email>vmahadev@yahoo-inc.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>San Diego</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>San Diego</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yahoo</forename><surname>Research</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>San Diego</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">VLAD 3 : Encoding Dynamics of Deep Features for Action Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:48+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Previous approaches to action recognition with deep features tend to process video frames only within a small temporal region, and do not model long-range dynamic information explicitly. However, such information is important for the accurate recognition of actions, especially for the discrimination of complex activities that share sub-actions, and when dealing with untrimmed videos. Here, we propose a representation, VLAD for Deep Dynamics (VLAD 3 ), that accounts for different levels of video dynamics. It captures short-term dynamics with deep convolutional neural network features, relying on linear dynamic systems (LDS) to model medium-range dynamics. To account for long-range inhomogeneous dynamics, a VLAD descriptor is derived for the LDS and pooled over the whole video, to arrive at the final VLAD 3 representation. An extensive evaluation was performed on Olympic Sports, UCF101 and THUMOS15, where the use of the VLAD 3 representation leads to stateof-the-art results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Object and action recognition are two important problems in computer vision. Over the last decade, the dominant representation for both problems was the bag-of-words model, combining histograms of descriptors such as HoG or HoF and pooling over space-time volumes <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b31">32]</ref>. More recently, substantial gains in object recognition have been reported with the use of deep convolution neural networks (CNNs) <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b29">30]</ref>. This has motivated several attempts to apply CNNs to the action recognition problem <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b30">31]</ref>. However, for action, the gains over state-of-the-art bag-ofwords approaches <ref type="bibr" target="#b31">[32]</ref> have not been as impressive. This can be at least partially explained by the structure of the video signal.</p><p>While most images tend to exhibit localized spatial structure, the temporal structure of video is constrained by the laws of Newtonian mechanics, which determine the motion of objects in natural scenes. Since this motion tends to be smooth, the video dynamics are homogeneous over a substantial number of frames. We refer to this as the medium-range structure of video, which tends to be homogeneous. On the other hand, over the long-range, natural scene dynamics can be very inhomogeneous since videos frequently depict a number of actions. An example is given in <ref type="figure">Figure 1</ref>, where the action of interest ("javelin throw") is embedded in video that also contains actions that precede (athlete warmups) and follow (crowd shot, shot of the score board, etc.) it. This video is only marginally informative about the action of interest (depicts actions shared by all track events). Overall, the statistics of the video can be represented at various levels of granularity, giving rise to the hierarchy of <ref type="figure">Figure 1</ref>.</p><p>In the short-term, a video can be characterized as a sequence of frames with characteristic motion patterns. For example, the optical flow of the "pole-fly" scene is very distinct from that of the "score board" scene. We refer to this as short-term video dynamics. At the next level, the video can be grouped into shots of dynamics that are discriminative for the target action. Since these dynamics tend to span a substantial number of frames (at least seconds, sometimes minutes) they are denoted as medium-range dynamics. Note that, as shown in <ref type="figure">Figure 1</ref>, a single action, such as the "javelin throw," can span several shots, including the sub-actions "run," "throw," "pole flight," and "pole landing." Hence, the dynamics of a single action can frequently be decomposed into a sequence of states, which typically have themselves homogeneous mid-range dynamics. Finally, the target action is embedded into marginally or totally unrelated video. Hence, the long-range dynamics of video tend to be highly inhomogeneous. While chunks of these dynamics correspond to the action of interest, their location can vary from one video to the next.</p><p>Since the temporal structure of video statistics follows the hierarchy of <ref type="figure">Figure 1</ref>, it is sensible to consider a similar hierarchy for video models. The lowest level in this hierarchy includes models whose temporal support accounts for a few video frames. This includes approaches based on hand-crafted descriptors and a number of recent CNN models, which use a few frames as inputs and a purely feedforward structure <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b30">31]</ref>. This is denoted as the shortterm level of the model hierarchy. The next class of models includes recurrent neural networks (RNNs) and their vari-</p><formula xml:id="formula_0">Figure 1:</formula><p>The VLAD 3 is inspired by the hierarchical structure of video dynamics. A short-term stage captures shortterm appearance and motion patterns with deep features. A medium-range stage models the dynamics of segments of deep features, using an LDS. Finally, a long-range stage computes and pools a VLAD descriptor, derived from the LDS.</p><p>ants <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b8">9]</ref>, or a combination of a short-term representation, such as CNN activations, and a pooling operator along video trajectories <ref type="bibr" target="#b32">[33]</ref>. These representations have some ability to capture the medium-range dynamics of <ref type="figure">Figure 1</ref>, but not completely. While they can model longer segments of homogeneous dynamics, it is less clear that they can account for actions composed of multiple distinct segments, such as the "javelin throw" of <ref type="figure">Figure 1</ref>. They can thus be considered as representations that fall between short and medium range. Beyond them, the medium-range level of the hierarchy includes models that explicitly account for a hidden state, such as the hidden Markov model (HMM) <ref type="bibr" target="#b28">[29]</ref> or the linear dynamic system (LDS) <ref type="bibr" target="#b1">[2]</ref>, which allows them to model actions composed of multiple states corresponding to shots of sub-actions. Finally, the third-level of the hierarchy includes models that can account for short-term motion, medium range dynamics with multiple states and highly inhomogeneous long-range dynamics. We refer to this as the long-range level of the hierarchy.</p><p>In this work, we propose a representation for video that encompasses the three-level hierarchy of <ref type="figure">Figure 1</ref>. At the short-term level, this representation extracts features from a small temporal window over video frames, jointly capturing their appearance and motions. It consists of a deep CNN, whose layers implement spatiotemporal filters of reduced temporal support (16 frames). Semantically, this level of the representation accounts for action segments, e.g. "arm movement" or "running". At the medium-range level, the CNN features extracted by the short-term level are processed by a linear dynamic system (LDS). This accounts for medium-range dynamics by modeling the CNN feature as a sequence of observations from a stochastic pro-cess with a hidden state. By transitioning through states, it can account for non-stationary dynamics, e.g. that throwing a javelin consists of a temporal sequence of states such as "running," "throwing," "pole flight," and "pole landing". As the state process has a Gauss-Markovian structure, this model has much better scalability than recurrent networks and can be easily learned from much longer time sequences. In this work, LDSs can be learned from features spanning the whole video sequence. Finally, the last stage of the representation is motivated by observations that the VLAD descriptor <ref type="bibr" target="#b0">[1]</ref> performs well for data with non-homogeneous statistics, e.g. image classification <ref type="bibr" target="#b0">[1]</ref> or even prior work on action recognition <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b33">34]</ref>. We derive a VLAD descriptor for the LDS likelihood of CNN responses and use it as the final, long-range level representation of the video hierarchy. This representation is denoted as the VLAD for Deep Dynamics (VLAD 3 ).</p><p>Overall, this work includes three main contributions. First, at the level of statistical modeling, we derive the VLAD descriptor for the LDS model. Second, at the level of video representation, we study the hypothesis that effective action representations must be discriminative in short-term, scalable enough to capture medium range dynamics, and robust to the heterogeneity of long-range dynamics. This leads to the proposed combination of short-term CNN features, medium-range LDS models, and global VLAD descriptor. Finally, we test the hypothesis through a large scale video recognition experiment which shows that the proposed representation achieves state-of-the-art performance on three challenging datasets -Olympic sports, UCF101, and THUMOS15 -which have been the subject of substantial prior research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Long-range video dynamics are difficult to capture with CNN models with small temporal support, e.g. spacetime models implemented by application of a spatial CNN to a small set of stacked video frames <ref type="bibr" target="#b10">[11]</ref>, combinations of CNNs operating on image and optical flow information <ref type="bibr" target="#b25">[26]</ref>, or even models that implement layers of 3D convolutional filters <ref type="bibr" target="#b30">[31]</ref>. As these representations scale poorly in their temporal support, the resulting classifiers typically account for only 10 to 20 video frames. This is not sufficient to characterize the dynamics of the underlying scene, which can unfold over tens of seconds or even minutes. While these techniques are frequently mapped into a holistic video-level representation by application of a global pooling operation, this representation is a summary of the short-term statistics of the video sequence, not a model of its long-range dynamics.</p><p>The modeling of such dynamics can, in principle, be achieved with more sophisticated deep learning models. In fact, a branch of the deep learning literature has evolved to address this problem. This includes methods based on recurrent neural networks (RNNs) <ref type="bibr" target="#b5">[6]</ref>, or variants such as the long short-term memory (LSTM) of <ref type="bibr" target="#b8">[9]</ref>. Such models have recently started to appear in the action recognition literature, e.g. by stacking an LSTM upon a CNN <ref type="bibr" target="#b3">[4]</ref> so as to learn the high-level temporal structure of low-level visual features, or by using an LSTM to model the dynamics of CNN activations <ref type="bibr" target="#b18">[19]</ref>. While models such as the RNN or LSTM can, in principle, model sequences of infinite length, they are usually trained with relatively small temporal support. While this observation holds even for datasets that are sizable, such as Sports1M <ref type="bibr" target="#b18">[19]</ref>, it is conceivable that the introduction of larger ones will eliminate the problem. This is unclear at this point. Similarly to the CNN approaches, these models have so far only been learned from sequences of 16 <ref type="bibr" target="#b3">[4]</ref> to 30 <ref type="bibr" target="#b18">[19]</ref> frames.</p><p>Alternatively, there has been interest in combining deep learning features with statistical representations of better temporal scalability. The hypothesis is that, while deep features are highly discriminative for short-term dynamics, action recognition will benefit from their combination with models of long-range dynamics. We refer to this as the long-range dynamics hypothesis. For example, <ref type="bibr" target="#b32">[33]</ref> proposed to pool deep features along video trajectories, a procedure commonly used in the bag-of-words literature <ref type="bibr" target="#b31">[32]</ref>. This can, in principle, exploit the temporal scalability of hand-crafted trajectories to substantially expand the temporal support of the video representation. However, because trajectories are obtained by tracking, they can be quite sensitive to the drift problem. In practice, this limits the maximum trajectory length, which is 15 frames in <ref type="bibr" target="#b32">[33]</ref> and still below the temporal extent of the dynamics of most scenes.</p><p>While the long-range dynamics hypothesis has moti-vated the deployment of methods such as LSTM <ref type="bibr" target="#b3">[4]</ref> or pooling of deep features along motion trajectories <ref type="bibr" target="#b32">[33]</ref>, the modeling of the long-range dynamics of deep learning features has so far received limited attention in the vision literature. In the broader context of action recognition, dynamics are frequently captured by rather straightforward operations, e.g. spatiotemporal extensions of spatial pyramid pooling <ref type="bibr" target="#b14">[15]</ref>, or latent support vector machines (SVM) that rely on anchor points to delimit motion segments <ref type="bibr" target="#b19">[20]</ref>. Such approaches can only capture coarse dynamics. Finer grained models usually rely on a more elaborate treatment of sequences of features, usually through generative models. Most generative models of video dynamics have been proposed for attribute-based representations <ref type="bibr" target="#b16">[17]</ref>. These characterize the video in terms of elementary semantic units, such as "leg motion," "raised arm," etc. The emphasis on attribute dynamics can be partly explained by the discrete nature of these variables, which enables simple learning and inference with widely used statistical models. For example, <ref type="bibr" target="#b28">[29]</ref> models attribute sequences with the combination of a Hidden Markov Model (HMM) and the associated Fisher vector. However, discrete dynamic models tend to underperform their continuous counterparts <ref type="bibr" target="#b1">[2]</ref>. For example, by connecting state and observations through a PCA-like transformation, the LDS extracts a much more compact representation of the scene dynamics. This usually guarantees better generalization than what is possible with an HMM. The difficulty is that, due to the non-Euclidean nature of attribute spaces, the LDS is not a suitable model for attribute data.</p><p>This has motivated non-Euclidean extensions of the LDS. For example, <ref type="bibr" target="#b15">[16]</ref> introduced the binary dynamic system (BDS), which is basically an LDS for discrete observations. However, such non-Euclidean variants introduce a non-trivial difficulty. While the LDS has exact inference, through the efficient Kalman smoothing computations <ref type="bibr" target="#b24">[25]</ref>, this is only possible because the state and observation distributions are both Gaussian, forming a conjugate pair. This conjugacy is broken for non-Euclidean observations, rendering exact inference impossible. This, in turn, makes it impossible to compute a VLAD vector for models such as BDS without resorting to complex Markov Chain Monte Carlo procedures or some form of approximate inference.</p><p>The proposed VLAD 3 representation is related to all these previous efforts. Similarly to <ref type="bibr" target="#b28">[29]</ref>, we propose a generative model of dynamics and the associated Fisher vector (albeit we use the simpler VLAD <ref type="bibr" target="#b20">[21]</ref>). However, rather than the discrete HMM we rely on a continuous model of dynamics. While conceptually this makes the proposed approach more similar to the BDS, we eliminate the difficulties of this approach by eliminating its reliance on human defined attributes. Instead, we propose to apply the dynamic model (LDS) directly to CNN features, which are power-ful in discriminating short-term dynamics. Overall, we rely on the CNN features for discrimination and on the LDS to capture the medium-range dynamics of these features. Besides eliminating the need for (additional) attribute annotations and classifiers, this model supports exact inference via Kalman smoothing. We exploit this property to design an efficient algorithm for the computation of the VLAD descriptor, which is not available for the BDS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">LDS for deep feature dynamics</head><p>In this section, we briefly review the linear dynamic system (LDS) model and its extensions that are of interest for this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Linear Dynamic System</head><p>The LDS is composed by a hidden Gauss-Markov state process and Gaussian observation process, according to</p><formula xml:id="formula_1">x t+1 = Ax t + v t , (1a) y t = Cx t + u + w t ,<label>(1b)</label></formula><p>where y ∈ R m is the observation and x ∈ R n the hidden state. This model is parametrized by the state transition matrix A ∈ R n×n , the observation matrix C ∈ R m×n , and the bias vector u ∈ R m , and includes two noise components, the state noise v t ∼ N (0, Q) and the observation noise w t ∼ N (0, R). These are Gaussian processes of zero mean and covariance matrices Q and R, respectively. Finally, the initial state is</p><formula xml:id="formula_2">x 1 = µ + v 0 ∼ N (µ, S).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">LDS learning</head><p>The LDS parameters Ω = {C, A, u, µ, R, Q, S} can be learned by maximum likelihood (ML) using the expectation-maximization (EM) algorithm. An adaptation of the LDS, known as the dynamic texture model <ref type="bibr" target="#b4">[5]</ref>, is popular for video representation in computer vision. Dynamic texture (DT) learning <ref type="bibr" target="#b4">[5]</ref> uses a much simpler approximation to estimate the LDS parameters, via a two-step algorithm. This starts by performing a principal component analysis (PCA) of the observation sequence {y t }. The resulting principal components are interpreted as the columns of the observation matrix C and the associated coefficients as an estimate of the hidden state variables {x t }. The second step then learns the transition matrix A and the noise parameters by solving a least squares problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">LDS codebook learning</head><p>Besides learning a single LDS, several methods have been proposed to learn mixtures or codebooks of LDSs. Again, an ML solution can be obtained with recourse to the EM algorithm, leading to the mixture of dynamic textures model <ref type="bibr" target="#b2">[3]</ref>. However, a simpler alternative is provided by the bag-of-models clustering (BMC) procedure of <ref type="bibr" target="#b15">[16]</ref>. This procedure clusters observation sequences in model space, rather than in the observation space. Given a sample set of</p><formula xml:id="formula_3">sequences D = {z i } N i=1 , where z = {y t } τ t=1</formula><p>is an individual sequence, BMC iterates between a cluster assignment and a cluster refinement step.</p><p>The assignment step operates on the representation of each sequence z i as a LDS model. The sequence is first subject to the mapping</p><formula xml:id="formula_4">f M : Z ⊇ {z} → M ∈ M<label>(2)</label></formula><p>from the space of observation sequences Z to the model space M. This mapping is implemented with an LDS learning algorithm, as discussed in Section 3.2. The resulting LDS, f M (z i ), is then assigned to one of the models in the LDS codebook, according to</p><formula xml:id="formula_5">q i = arg min j D M (f M (z i ), w j ),<label>(3)</label></formula><p>where w j ∈ M, the j th model in the codebook, is an LDS P (z; Ω j ) of parameters Ω j and</p><formula xml:id="formula_6">D M : M × M → R<label>(4)</label></formula><p>is a distance metric on model space M. The refinement step updates the codebook models according to</p><formula xml:id="formula_7">w j = f M ({z i : q i = j}).<label>(5)</label></formula><p>This consists of gathering all sequences z i assigned to each model and relearning the model parameters with the LDS learning algorithm of Section 3.2.</p><p>In this work, we rely on the BMC procedure to learn LDS codebooks, using the dynamic texture procedure to implement the mapping of (2) and the popular Martin distance of <ref type="bibr" target="#b17">[18]</ref> as the distance of (4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">VLAD encoding for deep dynamics</head><p>In this section, we review the VLAD descriptor and introduce the VLAD encoding for Deep Dynamics (VLAD 3 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">VLAD review</head><p>The vector of linearly aggregated descriptors (VLAD) is a simplified version of the Fisher vector descriptor of <ref type="bibr" target="#b9">[10]</ref>. A codebook V = {w j } V j=1 of V generative models w j = P (z; Ω j ) of parameters Ω j is first learned with recourse to a clustering algorithm. In this work, this is the BMC procedure of Section 3.3. The VLAD is an efficient encoding of the first-order statistics of a sample D = {z i } N i=1 with respect to this codebook. Each sample point z i is first assigned to a subset of the codewords according to</p><formula xml:id="formula_8">q ij = 1, if w j is in the k-nearest neighborhood set of z i , 0, otherwise</formula><p>where the k-nearest neighborhood of w j is defined by the distance of (3), which is the Martin distance in this work. The VLAD encoding φ(D) is the vector</p><formula xml:id="formula_9">φ(D) = [φ 1 (D), . . . , φ V (D)]<label>(6)</label></formula><p>such that</p><formula xml:id="formula_10">φ j (D) = n i=1 q ij ∂ log P (z i ; Ω) ∂Ω Ω=Ωj ,<label>(7)</label></formula><p>where ∂ log P (zi;Ω) ∂Ω is the gradient of the log-likelihood log P (z i ; Ω) with respect to the parameters in Ω. In summary, φ(D) pools the log-likelihood gradients of the sample</p><formula xml:id="formula_11">D = {z i } N i=1 with association q ij .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">LDS log-likelihood gradients</head><p>In the context of this work, the observation z is a sequence y τ 1 = {y t } τ t=1 of localized spatiotemporal features extracted from a video sequence of length τ with a deep CNN, and P (z; Ω) = P (y τ 1 ; Ω) is the LDS of (1). Using the chain rule of probability and the Markovian property of the hidden LDS states, this has likelihood</p><formula xml:id="formula_12">P (y τ 1 ) = P (y τ 1 |x τ 1 )P (x τ 1 ) dx τ 1 (8) = τ t=1 P (y t |x t ) τ t=2 P (x t |x t−1 )P (x 1 ) dx τ 1 ,</formula><p>where P (y t |x t ) = G(y t ; Cx t + u, R),</p><formula xml:id="formula_13">(9) P (x t |x t−1 ) = G(x t ; Ax t−1 , Q),<label>(10)</label></formula><formula xml:id="formula_14">P (x 1 ) = G(x 1 ; µ, S),<label>(11)</label></formula><p>and G(x; µ, Σ) is a Gaussian distribution of mean µ and covariance Σ. One of the important properties of the LDS is that all the terms of (8) are Gaussian. As is usual for the VLAD, we only consider the gradients with respect to parameters that affect the mean of these distributions, namely the vectors µ, u, and matrices A, C. Consider the gradient with respect to C. Using</p><formula xml:id="formula_15">∂ log P (y τ 1 ) ∂C = 1 P (y τ 1 ) ∂P (y τ 1 ) ∂C<label>(12)</label></formula><p>and</p><formula xml:id="formula_16">∂P (y τ 1 ) ∂C = ∂ ∂C τ t=1 P (y t |x t ) P (x τ 1 ) dx τ 1 =   τ t=1 ∂P (y t |x t ) ∂C j =t P (y j |x j )   P (x τ 1 ) dx τ 1 = τ t=1 1 P (y t |x t ) ∂P (y t |x t ) ∂C P (y τ 1 |x τ 1 )P (x τ 1 ) dx τ 1 it follows that ∂ log P (y τ 1 ) ∂C = τ t=1 ∂ log P (y t |x t ) ∂C P (x τ 1 |y τ 1 )dx τ 1 = τ t=1 E xt|y τ 1 ∂ log P (y t |x t ) ∂C .<label>(13)</label></formula><p>Similarly, it can be shown that</p><formula xml:id="formula_17">∂ log P (y τ 1 ) ∂A = τ t=2 E x t t−1 |y τ 1 ∂ log P (x t |x t−1 ) ∂A ,<label>(14)</label></formula><formula xml:id="formula_18">∂ log P (y τ 1 ) ∂u = τ t=1 E xt|y τ 1 ∂ log P (y t |x t ) ∂u ,<label>(15)</label></formula><formula xml:id="formula_19">∂ log P (y τ 1 ) ∂µ = E x1|y τ 1 ∂ log P (x 1 ) ∂µ .<label>(16)</label></formula><p>Using (9)-(11) and the Gaussian log-likelihood derivatives</p><formula xml:id="formula_20">∂ log G(x; P s + b, Σ) ∂b = Σ −1 (x − P s − b),<label>(17)</label></formula><formula xml:id="formula_21">∂ log G(x; P s + b, Σ) ∂P = Σ −1 (x − P s − b)s ⊺ ,<label>(18)</label></formula><p>it follows that</p><formula xml:id="formula_22">∂ log P (y τ 1 ) ∂C = R −1 τ t=1 (y t − u)α ⊺ t − Cβ t,t , (19) ∂ log P (y τ 1 ) ∂A = Q −1 τ t=2 β t,t−1 − Aβ t−1,t−1 , (20) ∂ log P (y τ 1 ) ∂u = R −1 τ t=1 [(y t − u) − Cα t ] ,<label>(21)</label></formula><formula xml:id="formula_23">∂ log P (y τ 1 ) ∂µ = S −1 (α 1 − µ)<label>(22)</label></formula><p>with</p><formula xml:id="formula_24">α t = E[x t |y τ 1 ], β t1,t2 = E[x t1 x ⊺ t2 |y τ 1 ].<label>(23)</label></formula><p>These expectations are part of the standard LDS inference, and can be computed efficiently with recourse to Kalman smoothing <ref type="bibr" target="#b24">[25]</ref>, which only requires a single forward and a single backward pass through the sequence y τ 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">VLAD 3 encoding</head><p>To derive the VLAD 3 encoding of a long sequence y T 1 , the sequence is first decomposed into a set of overlapping segments {z i } N i=1 , where z i is a subsequence of y T 1 of length τ , and an LDS is learned per subsequence. Given a codebook V = {w j } V j=1 of V LDSs, where w j = P (z; Ω j ), the sample association matrix q ij of Section 4.1 is computed, using (3) and the Martin distance to define the k-nearest neighborhood of each z i . The log-likelihood gradients with respect to model w j are then computed by using y τ 1 = z i and C = C j , A = A j , u = u j , and µ = µ j , in (19)- <ref type="bibr" target="#b22">(23)</ref>. The expectations of (23) are obtained with the Kalman smoothing filter. Finally, the gradient information is pooled across the video-subsequences extracted from y T 1 , according to <ref type="formula" target="#formula_9">(6)</ref> and <ref type="bibr" target="#b6">(7)</ref>. Following <ref type="bibr" target="#b33">[34]</ref>, the resulting vector is also post-processed with intra-normalization, powernormalization, and l 2 normalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>In this section, we describe the datasets used and discuss several experiments conducted to evaluate the performance of VLAD 3 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Datasets</head><p>Three datasets were used in our experiments: UCF101, Olympic Sports, and THUMOS15. UCF101 <ref type="bibr" target="#b27">[28]</ref> is widely used for video classification. It consists of 13,320 videos of 101 human action classes, covering a broad set of activities such as sports, musical instruments, and human-object interaction. However, most of the activities (e.g. "Knitting", "Drumming") are short, simple, and repetitive. Some of these activities can be discriminated without any modeling of dynamics, e.g. because they occur against a very specific background scene, or due to the presence of certain objects. In terms of the hierarchy of <ref type="figure">Figure 1</ref>, this dataset is somewhere between the short-term and medium-range levels. We adopt the three train-test splits suggested in <ref type="bibr" target="#b27">[28]</ref>.</p><p>Olympic Sports <ref type="bibr" target="#b19">[20]</ref> contains YouTube videos of 16 sport activities, for a total of 783 videos. Each video is trimmed to contain only the activity of interest, but this can be a complex activity, composed by a sequence of simpler sub-actions. These sub-actions can be shared by different activities. For example, "long jump" and "triple jump" have very similar action segments ("running," "jumping"), only differing in their temporal sequencing and length (a single long jump vs. three shorter hops). Hence, temporal dynamics are critical for discrimination on this dataset, which can be confidently considered a dataset at the medium-range level of the hierarchy of <ref type="figure">Figure 1</ref>. On Olympic, we adopt the train-test split of <ref type="bibr" target="#b19">[20]</ref>.</p><p>The task of the THUMOS challenge <ref type="bibr" target="#b7">[8]</ref> is to recognize human action classes in open source videos. The validation set of THUMOS 2015 contains 2,104 untrimmed videos of 101 activity classes, which are the same as those of UCF101. However, in THUMOS15 each video includes one/multiple instances of one/multiple actions, in varying temporal locations. The irrelevant video segments can be considered as semantic noise and the video statistics can be highly non-homogeneous. This datasets is an example of the long-range level of <ref type="figure">Figure 1</ref>. Since groundtruth is not available for the test set of THUMOS15, we followed the 5-fold cross-validation train-test strategy of <ref type="bibr" target="#b34">[35]</ref> on the validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Experimental set-up</head><p>In all experiments, the short-term representation of the VLAD 3 was based on the C3D features of <ref type="bibr" target="#b30">[31]</ref>, extracted from a 16-frame video window. The temporal stride was set to 16 for THUMOS15 and 4 on the other datasets. C3D is a 3D-convolutional deep network learned from a large video dataset. We used the 4096-d fc6 feature vector, which was l 2 normalized and dimensionality reduced into a 256d vector by PCA. Given the sequence of CNN vectors extracted from the video sequence, we defined a temporal sliding window of length τ and stride three. The τ feature vectors within the window centered at time t composed the sequence z t . A set of such sequences, collected from a random subset of the training video sequences was used to learn an LDS codebook of size V = 128, as discussed in Section 3.3. Each LDS codeword had a hidden state of dimension n ∈ {6, 8, 10, 20, 24}. The k-nn parameter of VLAD encoding was set to 5. The maximum τ used in the experiments was 40, corresponding to a temporal support of 656 frames for THUMOS15 and 176 frames for the other datasets. We cross-validated n and τ for each action class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">The importance of modeling dynamics</head><p>A number of experiments were performed to evaluate the importance of dynamic modeling in action recognition. In these experiments we considered a family of representations that cover different levels of the hierarchy of <ref type="figure">Figure 1</ref>. They are all based on the C3D deep features, differing only in the modeling of video dynamics. The first, denoted T3, pools feature responses with the temporal pyramid (of scale 3) of <ref type="bibr" target="#b14">[15]</ref>. Since this is a very crude representation of video dynamics, this model is representative of the short-term level of <ref type="figure">Figure 1</ref>. The second, denoted CTR <ref type="bibr" target="#b1">[2]</ref>, consists of the spectral signature of an LDS learned from the entire video sequence. It can capture long-range dynamics, but assumes that these are homogeneous. It can thus be seen as a representative of the medium-range level of <ref type="figure">Figure 1</ref>. Finally, we considered, two representatives of the long-range level, the HMM Fisher vector of <ref type="bibr" target="#b28">[29]</ref> and the proposed VLAD 3 . These are very similar, differing only in the use of discrete or continuous state variables.</p><p>In all cases, the classifier was a 1-vs-rest linear SVM with cross-validated parameter C. Performance was evaluated by the mean average precision (mAP) metric. For the UCF101 dataset, where performance is usually reported in terms of the accuracy (Acc), we also computed this metric. A common strategy to boost action recognition performance is the fusion of different models. Following <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b20">21]</ref>, we also tried to late-fuse the representations above with a holistic representation obtained by average pooling the CNN fea-  <ref type="figure">Figure 1</ref>. A ⋆ is used to identify short-term representations/video, a † for medium-range and a ♦ for long-range.</p><p>These results confirm the hypothesis that the modeling of continuous dynamics is beneficial for action recognition. As expected, the gains of this modeling depend on the class of dynamics present in each dataset. While, for example, there is no significant difference between T3 and CTR on UCF101, where most video only has short-term dynamics, the difference becomes much more significant on THU-MOS15. On Olympic and THUMOS15, where dynamics are more important, the gains of the long-range models (HMM-FV and VLAD 3 ) are substantial. Among these models, the VLAD 3 has clearly better performance. This confirms the gains previously observed in <ref type="bibr" target="#b1">[2]</ref> for continuous dynamic models. Overall, the proposed VLAD 3 representation achieves significant gains in all datasets. Compared to T3, these increase from 5% on the short-term level UCF101, to 10% in the medium-range level Olympic Sports, to 20% on the long-range THUMOS15.</p><p>The increase from 5% in UCF101 to 20% on THU-MOS15 is particularly relevant, since the two datasets have exactly the same activity classes, differing only on the inhomogeneity of their dynamics. This supports the hypothesis that 1) there are benefits to modeling dynamics, and 2) the dynamic model must account for both the long-term nature of these dynamics and their inhomogeneity. More detailed evidence in support of this hypothesis is given in <ref type="figure" target="#fig_0">Figure 2</ref>, which shows a plot of the per-class average precision for Olympic Sports. Classes, such as bowling or diving platform 10m, that differ from the rest in terms of short-term motion patterns are perfectly discriminated by the holistic C3D representation. On the other hand, for classes with similar short-term motion patterns, e.g. hammer-throw, high-jump, triple jump and the remaining track activities, the dynamic modeling of the VLAD 3 enables very significant gains in classification performance.</p><p>Finally, while late-fusion decreases the gap between the methods, it does not change the conclusions above. On the most challenging datasets, the gains of VLAD 3 are significant even after late-fusion. In fact, in all datasets, the top performance achieved by late-fusion of any of the shortterm and medium-range methods is at most equal (and usually inferior) to that of the vanilla VLAD 3 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Role of long-range dynamics and heterogeneity</head><p>In addition to establishing a video representation that covers all levels of the hierarchy of <ref type="figure">Figure 1</ref>, the VLAD 3 can be used as a tool to investigate the importance of each "step" on this hierarchy, i.e. from short-term to long-range dynamics and from homogeneous to noisy dynamics. In fact, this can be done by measuring the performance of the VLAD 3 as a function of the temporal support τ of the subsequences used to compute it. In the limit of τ = 1, the VLAD 3 includes no modeling of medium or long-range dynamics, reducing to a standard Gaussian-VLAD. As τ increases, and the LDS accounts for mid-range dynamics, performance is expected to improve. Finally, for large τ , as these dynamics become highly inhomogeneous, performance is expected to degrade. In the limit, it is possible to fit a single LDS to the full video y T 1 (which could have hundreds or thousands of frames, e.g. average T for THU-MOS15 is 6620), rather than fitting various LDSs to subsequences of size τ . This is denoted the single-LDS descriptor and expected to be sensitive to inhomogeneous dynamics. <ref type="figure">Figure 3</ref> presents the results of this procedure on the three datasets. In all cases, performance increases with τ demonstrating the benefits of medium-range level modeling. However, performance starts to saturate for a value of τ that matches the temporal support of the dynamics in the video and decreases after that. This can be seen by the fact that, in all cases, the single-LDS descriptor underperforms the VLAD 3 of optimal τ . These experiments also confirm the categorization of the datasets in <ref type="table">Table 1</ref>. On Olympic Sports and THUMOS15, whose video is composed of complex events and/or has inhomogeneous dynamics, the VLAD 3 of optimal τ clearly outperforms the single-LDS. However, on UCF101, which is a short-term dataset, the single-LDS is almost as good.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Comparison with the state-of-the-art</head><p>A comparison to state-of-the-art results in the literature can be difficult, since different methods use different components that are not always comparable. In fact, the best results are usually obtained by fusing different representations. For example, as noted in <ref type="bibr" target="#b30">[31]</ref>, hand-crafted features such as iDT <ref type="bibr" target="#b31">[32]</ref> are complementary to the C3D feature -the iDT encodes low-level gradients by tracking op-   <ref type="figure">Figure 3</ref>: VLAD 3 performance as a function of the LDS temporal support τ . UCF101 (Acc%) Olympic Sports (mAP%) THUMOS15(mAP%) Tran et al. <ref type="bibr" target="#b30">[31]</ref> 90. <ref type="bibr" target="#b3">4</ref> Wang and Schmid <ref type="bibr" target="#b31">[32]</ref> 91. <ref type="bibr" target="#b0">1</ref> Xu et al. <ref type="bibr" target="#b34">[35]</ref> 74.6 Lan et al. <ref type="bibr" target="#b13">[14]</ref> 89. <ref type="bibr" target="#b0">1</ref> Gaidon et al. <ref type="bibr" target="#b6">[7]</ref> 85. <ref type="bibr" target="#b4">5</ref> Qiu et al. <ref type="bibr" target="#b22">[23]</ref> 70.0 Simonyan and Zisserman <ref type="bibr" target="#b25">[26]</ref> 88.0 Lan et al. <ref type="bibr" target="#b13">[14]</ref> 91. <ref type="bibr" target="#b3">4</ref> Ning and Wu <ref type="bibr" target="#b11">[12]</ref> 65.5 Wang et al. <ref type="bibr" target="#b32">[33]</ref> 91. <ref type="bibr" target="#b4">5</ref> Peng et al. <ref type="bibr" target="#b21">[22]</ref> 93.8 VLAD 3 76.8 VLAD 3 +iDT 90.9 VLAD 3 +iDT 96.6 VLAD 3 +iDT 80.8 VLAD 3 +iDT(fisher) 92.2 <ref type="table">Table 2</ref>: Comparison to state-of-the-art results.</p><p>tical flow while C3D features are more abstract and capture high level semantics. Inspired by this observation, we tested the late-fusion of the scores produced by VLAD 3 , which is built on top of C3D features, and iDT. <ref type="table">Table 2</ref> compares this approach to the state-of-the-art for the three datasets considered in this work. In all cases, the fusion of VLAD 3 and iDT achieves the best performance. For example, on UCF101, replacing the state-of-the-art trajectorypooled deep descriptors (TDD+iDT (fisher)) of <ref type="bibr" target="#b32">[33]</ref> by the VLAD 3 +iDT (fisher) combination, improves the state-ofthe-art from 91.5% to 92.2%. This is a non-trivial gain, given the amount of research that has addressed this dataset. Similarly, on Olympic Sports, the VLAD 3 +iDT combination, outperforms approaches that attempt to stack multiple layers of feature representation. Since THUMOS15 is a relatively new dataset, we only list the results using the top three approaches (with only visual features) reported in the competition. Note that these approaches often require fusion of many different features, while VLAD 3 by itself can lead to better performance, clearly demonstrating the benefits of a representation that models long-range dynamics.</p><p>Again, the VLAD 3 +iDT achieves the state-of-the-art results by a big margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this work, we proposed a new video representation, the VLAD 3 , that models video dynamics at three hierarchical levels. The resulting encoding leverages discriminative deep features for short-term dynamics, the LDS model for medium-range dynamics, and a novel VLAD descriptor, derived from the LDS, for long-range dynamics. This enables it to model video whose dynamics are homogeneous over short and medium-range time scales, but inhomogeneous over long time spans. An evaluation on three datasets showed that explicit modeling of long-range dynamics is important for action recognition, and demonstrated superior performance of the proposed VLAD 3 compared to state-ofthe-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Acknowledgments</head><p>This work was partially funded by NSF grant IIS-1208522. We also thank NVIDIA for GPU donation through their academic program.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Average Precision(AP) per class on Olympic Sports.</figDesc></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">All about vlad</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arandjelovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2013 IEEE Conference on</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1578" to="1585" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Recognition of complex events: Exploiting temporal dynamics between underlying concepts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Kalayeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note>IEEE</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Modeling, clustering, and segmenting video with mixtures of dynamic textures. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="909" to="926" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2015 IEEE Conference on</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="2625" to="2634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Dynamic textures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Doretto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chiuso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="109" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Finding structure in time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Elman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive science</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Activity representation with motion hierarchies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gaidon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="219" to="238" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">THUMOS challenge: Action recognition with a large number of classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gorban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Idrees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Exploiting generative models in discriminative classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jaakkola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Haussler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="487" to="493" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Zjudcd submission at thumos challenge 2015</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">W</forename><surname>Ning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR workshop</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Beyond gaussian pyramid: Multi-skip feature stacking for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Raj</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2015 IEEE Conference on</title>
		<imprint>
			<date type="published" when="2015-06" />
			<biblScope unit="page" from="204" to="212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning realistic human actions from movies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marszałek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rozenfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Recognizing activities via bag of words for attribute dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sawhney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2013 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Recognizing human actions by attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kuipers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="3337" to="3344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A metric for arma processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Martin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1164" to="1170" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
	<note>Signal Processing</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Beyond short snippets: Deep networks for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2015 IEEE Conference on</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="4694" to="4702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Modeling temporal structure of decomposable motion segments for activity classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2010</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Bag of visual words and fusion methods for action recognition: Comprehensive study and good practice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1405.4506</idno>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Action recognition with stacked fisher vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2014</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="581" to="595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Msr asia msm at thumos challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR workshop</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">High-dimensional signature compression for large-scale image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1665" to="1672" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">An approach to time series smoothing and forecasting using the em algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">H</forename><surname>Shumway</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Stoffer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of time series analysis</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">UCF101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Roshan</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno>CRCV-TR-12-01</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Active: Activity concept transitions in video event classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV), 2013 IEEE International Conference on</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2015 IEEE Conference on</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">C3d: generic features for video analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.0767</idno>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Action recognition with improved trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV), 2013 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="3551" to="3558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Action recognition with trajectory-pooled deep-convolutional descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2015 IEEE Conference on</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="4305" to="4314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A discriminative cnn video representation for event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2015 IEEE Conference on</title>
		<imprint>
			<date type="published" when="2015-06" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Uts-cmu at thumos 2015</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR workshop</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
