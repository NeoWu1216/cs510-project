<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Structured Scene Parsing by Learning with Image Descriptions</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
							<email>linliang@ieee.org</email>
							<affiliation key="aff0">
								<orgName type="department">School of Data and Computer Science</orgName>
								<orgName type="institution">Sun Yat-sen University</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangrun</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Data and Computer Science</orgName>
								<orgName type="institution">Sun Yat-sen University</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Data and Computer Science</orgName>
								<orgName type="institution">Sun Yat-sen University</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruimao</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Data and Computer Science</orgName>
								<orgName type="institution">Sun Yat-sen University</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Data and Computer Science</orgName>
								<orgName type="institution">Sun Yat-sen University</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Structured Scene Parsing by Learning with Image Descriptions</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:39+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper addresses a fundamental problem of scene understanding: How to parse the scene image into a structured configuration (i.e., a semantic object hierarchy with object interaction relations) that finely accords with human perception. We propose a deep architecture consisting of two networks: i) a convolutional neural network (CNN) extracting the image representation for pixelwise object labeling and ii) a recursive neural network (RNN) discovering the hierarchical object structure and the inter-object relations. Rather than relying on elaborative user annotations (e.g., manually labeling semantic maps and relations), we train our deep model in a weakly-supervised manner by leveraging the descriptive sentences of the training images. Specifically, we decompose each sentence into a semantic tree consisting of nouns and verb phrases, and facilitate these trees discovering the configurations of the training images. Once these scene configurations are determined, then the parameters of both the CNN and RNN are updated accordingly by back propagation. The entire model training is accomplished through an Expectation-Maximization method. Extensive experiments suggest that our model is capable of producing meaningful and structured scene configurations and achieving more favorable scene labeling performance on PASCAL VOC 2012 over other state-of-theart weakly-supervised methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Scene understanding started with the goal of creating systems that can infer meaningful configurations (e.g., parts, objects and their compositions with relations) from imagery like humans <ref type="bibr" target="#b9">[10]</ref>. In computer vision research, significant progresses have been made in semantic scene labeling / segmentation (i.e., assigning the label for each pixel of the scene image) <ref type="bibr" target="#b13">[14]</ref>[32] <ref type="bibr" target="#b16">[17]</ref> <ref type="bibr" target="#b24">[25]</ref>. However, the problem of structured scene parsing (i.e., producing meaningful scene configurations) remains a challenge due to the following difficulties.  <ref type="figure">Figure 1</ref>. An illustration of our structured scene parsing. An input scene image is automatically parsed into a hierarchical configuration that comprises hierarchical semantic objects (black labels) and the interaction relations (red labels) of objects.</p><p>• The representations of nested hierarchical structure in scene images are often ambiguous, e.g., a configuration may have more than one way of parsing. Conducting these parsing results to finely accord with human perception is an interesting yet fundamental problem.</p><p>• Training a scene parsing model usually relies on very expensive manual annotations, e.g., including semantic maps and structured configurations.</p><p>To address these above issues, we develop a novel deep neural network architecture that automatically parses an input scene into a structured and meaningful configuration. <ref type="figure">Fig. 1</ref> shows an illustration of our structured scene parsing, where our model identifies salient semantic objects in the scene and generates the hierarchical scene structure with the interaction relations among objects. Our model is inspired by the effectiveness of two widely successful deep learning techniques: convolutional neural networks (CNNs) <ref type="bibr" target="#b12">[13]</ref> <ref type="bibr" target="#b16">[17]</ref> and recursive neural network (RNNs) <ref type="bibr" target="#b28">[29]</ref>. The former category of models is widely applied for generating powerful feature representations in various vision tasks such as im-age classification and object recognition. Meanwhile, the RNN models (such as <ref type="bibr" target="#b28">[29]</ref>[25] <ref type="bibr" target="#b23">[24]</ref>) have demonstrated as an effective class of models for predicting hierarchical and compositional structures in image and natural language understanding <ref type="bibr" target="#b29">[30]</ref>. One important property of RNNs is the ability to recursively learn the representations in a semantically and structurally coherent way. In our deep CNN-RNN architecture, the CNN and RNN models are collaboratively integrated for accomplishing the scene parsing from complementary aspects. We utilize the CNN to layerwise extract features from the input scene image and generate the representations of semantic objects. Then, the RNN is sequentially stacked based on the CNN feature representations, generating the structured configuration of the scene.</p><p>On the other hand, to avoid relying on the elaborative annotations, we propose to train our CNN-RNN model by leveraging the image descriptions. Our approach is partially motivated but different with the recently proposed methods for image-sentence embedding <ref type="bibr" target="#b11">[12]</ref> <ref type="bibr" target="#b35">[36]</ref>. In particular, we distill knowledge from the sentence descriptions for discovering scene structural configurations.</p><p>In the initial stage, we decompose each sentence into a normalized semantic tree consisting of nouns and verb phrases by using a standard parser <ref type="bibr" target="#b27">[28]</ref> and the WordNet <ref type="bibr" target="#b17">[18]</ref>. Afterward, based on these semantic trees and their associated scene images, we train our model by developing an Expectation-Maximization method. Specifically, the semantic tree facilitates discovering the latent scene configuration in the two following aspects. i) The entities (i.e., nouns) determine the object category labels existing in the scene, and ii) the relations (i.e., verb phrases) over the entities assist to produce the scene hierarchy and object interactions. The two proportions of knowledge are incorporated into our learning objective together with the CNN and the RNN, respectively. Therefore, once the scene configuration is fixed, the parameters of the two neural networks are updated accordingly by the back propagation.</p><p>The main contributions of our work are summarized as follows. i) We present a novel CNN-RNN framework for generating meaningful and hierarchical scene representations, which gains a deeper understanding of the objects in the scene compared to traditional scene labeling. The integration of CNN and RNN models is general to be extended to other high-level computer vision tasks. ii) We present a EM-type training method by leveraging text descriptions that associate with the training images. This method is costeffective yet beneficial to introducing rich contexts and semantics. iii) Our extensive experiments on PASCAL VOC 2012 demonstrate that the parsed scene representations are useful for scene understanding and our generated semantic segmentations are more favorable than those by other weakly-supervised scene labeling methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Scene understanding is arguably considered as the most fundamental problem in computer vision, which actually involves several tasks of different level. In current research, a myriad of different methods focus on what general scene type the image shows (classification) <ref type="bibr" target="#b6">[7]</ref>[4] <ref type="bibr" target="#b36">[37]</ref>, what objects and their locations are in a scene (semantic labeling or segmentation) <ref type="bibr" target="#b22">[23]</ref>[8] <ref type="bibr" target="#b18">[19]</ref> <ref type="bibr" target="#b32">[33]</ref>. These methods, however, ignore or over-simplified the compositional object representations and would fail to gain a deeper scene understanding.</p><p>Meanwhile, as a higher-level task, structured scene parsing has also attracted much attention. A pioneer work was proposed by Tu et al., <ref type="bibr" target="#b33">[34]</ref>, in which they mainly focused on faces and texture patterns by a Bayesian inference framework. In <ref type="bibr" target="#b9">[10]</ref>, Han et al., proposed to hierarchically parse the indoor scene images by developing a generative grammar model. A hierarchical model was proposed in <ref type="bibr" target="#b38">[39]</ref> to represent the image recursively by contextualized templates at multiple scales, and the rapid inference was realized based on dynamic programming. Ahuja et al., <ref type="bibr" target="#b0">[1]</ref> developed a connected segmentation tree for object and scene parsing. Some other related works <ref type="bibr" target="#b25">[26]</ref>[9] investigated the approaches for RGB-D scene understanding, achieving impressive results.</p><p>With the resurgence of neural network models, the performances of scene understanding have been improved substantially. The representative works, the fully convolutional network (FCN) <ref type="bibr" target="#b16">[17]</ref> and its extensions <ref type="bibr" target="#b2">[3]</ref>, demonstrate effectiveness in pixel-wise scene labeling. A recurrent neural network model was proposed in <ref type="bibr" target="#b37">[38]</ref>, which improves the segmentation performance by incorporating the mean-field approximate inference, and similar idea was also explored in <ref type="bibr" target="#b15">[16]</ref>. For the problem of structured scene parsing, recursive neural networks (RNNs) were studied in <ref type="bibr" target="#b28">[29]</ref> <ref type="bibr" target="#b23">[24]</ref>. For example, Socher et al. <ref type="bibr" target="#b28">[29]</ref> proposed to predict hierarchical scene structures by using a max-margin RNN model. The differences between these existing RNN-based parsing models and our model are two-fold. First, they mainly focused on parsing only the semantic entities (e.g., buildings, bikes, trees) and the scene configurations generated by ours include not only the objects but also the interaction relations of objects. Second, we incorporate convolutional feature learning into our deep model for joint optimization.</p><p>Most of the existing scene labeling / parsing models are studied in the context of supervised learning, and they rely on expensive annotations. To overcome this issue, one can develop alternative methods that train the models from weakly annotated training data, e.g., image-level tags and contexts <ref type="bibr" target="#b34">[35]</ref>[21] <ref type="bibr" target="#b19">[20]</ref>. Among these methods, one inspiring us is <ref type="bibr" target="#b19">[20]</ref>, which adopts an EM learning algorithm for training the model with image-level semantic labels. This algorithm alternates between predicting the latent pixel labels subject to the weak annotation constraints and optimizing The input image is directly fed into the CNN to produce score map of each semantic category and feature representation of each pixel. Then the model applies score maps to classify the pixels, and groups pixels with same labels to obtain feature representation v of objects. After that v is fed into the RNN, where it is first mapped onto a semantic space and used to predict the tree structure and relations between objects. x denotes the mapped feature.</p><p>the neural network parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">CNN-RNN Architecture</head><p>Structured scene parsing aims to infer the following three forms of outputs from an image: i) the location of semantic entities, ii) interaction relations and iii) the hierarchical configuration among the semantic entities. To this end, we propose a novel deep architecture by integrating the convolutional neural network (CNN) and recursive neural network (RNN). In our CNN-RNN architecture, the CNN model is introduced to perform semantic segmentation by assigning an entity label to each pixel, and the RNN model is introduced to discover hierarchical structure and interaction relations among entities. Moreover, the CNN model also produces the feature representation for each entity, which will be fed to the RNN model for generating parsing tree. <ref type="figure" target="#fig_1">Fig. 2</ref> illustrates the the proposed CNN-RNN architecture for structured scene parsing. First, the input image is directly fed into our revised VGG-16 network <ref type="bibr" target="#b26">[27]</ref> to produce a score map for each entity category. Based on the softmax normalization of the score maps, each pixel is labeled with an entity category. We further group the adjacent pixels with the same label into a semantic entity category, and generate feature representations for entities. By feeding feature representations of entities to the RNN, a bottom-up greedy aggregation algorithm is used to construct the parsing tree. To model interaction relations and hierarchical structure of entities, the parsing tree includes three types of nodes, where a leaf node represents an individual entity, and a higher-level node is generated by recursively combining child nodes to represent part of the scene, and finally the root node is introduced to represent the whole scene. Different from the RNN architecture in <ref type="bibr" target="#b28">[29]</ref>[24], our model predicts the relation between these two nodes when they are combined into a higher-level node.</p><p>In the following, we provide the more detailed explana-tions on the proposed CNN and RNN networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">CNN Model</head><p>The CNN model is designed to accomplish two tasks: semantic labeling and generating feature representations for entities. For semantic labeling, we adopt the fully convolutional network with parameters W C to yield K + 1 score maps {s 0 , ..., s k , s K }, corresponding to one extra background category and K object categories. The score s k j is further normalized using softmax to obtain the corresponding classification score:</p><formula xml:id="formula_0">σ(s t j ) = exp(s t j ) ∑ K k=1 exp(s k j )<label>(1)</label></formula><p>where σ(s t j ) denotes the probability of j-th pixel belonging to t-th object category with</p><formula xml:id="formula_1">∑ K t=1 σ(s t j ) = 1. C = {c j } M</formula><p>j=1 denotes the labels of pixels in the image I, where c j ∈ {1, ..., K} and M is the number of pixels of image I. With σ(s t j ), the label of the j-th pixel can be predicted by:</p><formula xml:id="formula_2">c j = arg max t σ(s t j )<label>(2)</label></formula><p>For generating feature representation for each entity category, we group the adjacent pixels with the same label into a semantic entity category.</p><p>Considering that the pixel numbers vary with the semantic entity categories, in order to obtain feature representation with fixed length for any entity category, we use Log-Sum-Exp(LSE) <ref type="bibr" target="#b1">[2]</ref>, a convex approximation of the max function, to fuse the features of pixels</p><formula xml:id="formula_3">v k = 1 π log   1 Q k ∑ cj =k exp(πv j )  <label>(3)</label></formula><p>where v k denotes the feature representation of the k-th entity category,v j denotes the feature representation of the j-th pixel by concatenating all feature maps at the layer before softmax at position j into a vector, Q k is the total number of pixels of the k-th object category, and π is a hyperparameter to control smootheness. With higher value of π, the function tend to preserve the max value for each dimension in the feature, while with lower value the function behaves like a averaging function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">RNN Model</head><p>With the feature representations of object categories produced by CNN, the RNN model is designed to generate the image parsing tree for predicting object interaction relations and hierarchical scene structure. The RNN model consists of four sub-networks: (semantic mapper, combiner, categorizer and scorer). Therefore, the parameters of the RNN also includes four parts, denoted as W R = {W sem , W com , W cat , W score }.</p><p>Following <ref type="bibr" target="#b28">[29]</ref> and <ref type="bibr" target="#b23">[24]</ref>, object feature v k produced by CNN is first mapped onto a semantic space by the Semantic mapper, which is a one-layer fully-connected network.</p><formula xml:id="formula_4">x k = F sem (v k ; W sem )<label>(4)</label></formula><p>where x k is the mapped feature, F sem is the network transformation and W sem is the network parameter. The features of two child nodes are fed to the Combiner and generate their parent node feature.</p><formula xml:id="formula_5">x kl = F com ([x k , x l ]; W com )<label>(5)</label></formula><p>where x k and x l indicate the two child features and x kl denotes their parent feature in the parsing tree. F com is the network transformation and W com is the corresponding parameter. Parent node feature encode semantic information of the combination of its two child nodes, as well as the structural information of this specific merging operation. The parent node feature has the same dimensionality as the child node feature, allowing the procedure can be applied recursively and eventually the root feature can be used to represent the whole image. When two nodes are merged into a parent node, the Categorizer sub-network determines the relation of these two nodes. Categorizer is a softmax classifier that takes parent node feature x kl as input, and predict the relation label y kl ,</p><formula xml:id="formula_6">y kl = sof tmax(F cat (x kl ; W cat ))<label>(6)</label></formula><p>where y kl is the predicted relation probability vector, F cat denotes the network transformation and W cat denotes the network parameter.</p><p>The Scorer sub-network measures the confidence of a merging operation between two nodes. It takes the parent node feature x kl as input and outputs a real value h kl .</p><formula xml:id="formula_7">h kl = F score (x kl ; W score )<label>(7)</label></formula><p>where F score denotes the network transformation and W score denotes the network parameter. The merging score q kl of node {kl} is computed as q kl = 1 1+exp(h kl ) . Merging score is used to optimize the structure discovery in training, as described in Sect. 4.2.</p><p>Similar to <ref type="bibr" target="#b28">[29]</ref>, we use the RNN model to construct the parsing tree with a greedy algorithm. The procedure begins with a initial set of leaf nodes. In each iteration, the algorithm enumerates all possible merging pairs and computes merging scores for each. The algorithm chooses the pair with highest score to merge, replacing the pair of nodes with their parent node. The algorithm iterates until there is only one root node left.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Weakly-supervised Model Training</head><p>Compared with some other weak annotations such as labels and attributes, sentences usually provide richer semantics and structured contexts (e.g., object interactions and relations). More importantly, describing images by sentences finely accords with the process of human perception, and it thus contributes to meaningful representation learning.</p><p>In the initial stage of model training, we first convert each sentence into a normalized tree by using common techniques, as discussed above. Formally, a semantic tree T includes entity labels (i.e., nouns) and the relations (i.e., verb phrases).</p><p>Since the scene configurations are unavailable for the training images, we need to estimate them to training our CNN and RNN. Thus, we train the model with a EM type algorithm. This algorithm alternates between predicting the latent scene configurations (via transferring knowledge from the semantic trees), and optimizing the neural network parameters.</p><p>Our model performs two tasks: semantic labeling and scene structure discovery. Thus we define the loss function as the sum of two terms: semantic label loss J C produced by CNN, and scene structure loss J R produced by RNN. With a training set containing Z image-tree pairs {(I 1 , T 1 ), ..., (I Z , T Z )}. The overall loss function is as fol-  lows,</p><formula xml:id="formula_8">J (W ) = 1 Z Z ∑ i=1 (J C (W C ; I i , T i ) + J R (W ; V i , T i )) (8)</formula><p>where I i is the i-th image and T i is the tree sructure produced from the descriptive sentence. V i is the set of semantic entity features produced by CNN from the i-th image. V takes the form V = {v k |k ∈ ψ(T )}, where ψ(T ) is set of object categories mentioned in T . W is all model parameters, W C is model parameters of the CNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Semantic Label Loss</head><p>Given intermediate label map C, the semantic label task performed by CNN can be optimized as a pixel-wise classfication problem. We first perform an inference step to obtain an estimated ground truth label map C, which is used as supervision (see Sect. 4.3 for more details). Let c j ∈ C denote the estimated category label of pixel j, the loss function of semantic labeling for image I is defined as,</p><formula xml:id="formula_9">J C (W C ; I, T ) = − 1 M ( M ∑ j=1 K ∑ k=1 1( c j = k) log σ(s k j ) + (1 − 1( c j = k)) log(1 − σ(s k j ))) + ∥W C ∥ 2<label>(9)</label></formula><p>where M denotes the total number of pixels in the image I. As defined in Eq.(1)function σ(s k j ) outputs the probability of j-th pixel for the k-th entity category predicted by the CNN. Note that {s 0 , ..., s K } represent the score maps of image I produced by the fully convolutional network with parameters W C .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Scene Structure Loss</head><p>The scene structure discovery task is performed by the RNN, and can be further divided into two sub-tasks: tree structure construction and relation categorization. Thus we define the RNN loss to be the sum of loss from the two tasks,</p><formula xml:id="formula_10">J R (W ; V i , T i ) = J struc (W ; V i , T i ) + J rel (W ; V i , T i )<label>(10</label></formula><p>) Tree Structure Construction. The goal of tree structrue construction is to learn a transformation I → P I according to the tree structure T . We define an image parsing tree as valid if the sequence of two regions merges is consistent with the merging order in the text parsing tree. From a valid parsing tree, we extract a sequence of "correct" merging operations as A(V, T ) = {a 1 , ..., a PT }. P T is the total number of merging operation in the text parsing tree T . This implies a contraint that the nubmer of merging operation in a tree structure always equals nubmer of merging operation in the corresponding text parse tree.</p><p>We define a loss based on the merging score q produced by scorer sub-network as described in Sect. 3.2. For convenience, we denote merging score of operation a given V and T as q(a). Intuitively, we encourage the correct merging operation a to have a larger merging score than that of incorrect merging operation a. Thus we have q(a) ≥ q( a) + △, where △ is a constant margin. We define the loss function for scene structrue discovery as,</p><formula xml:id="formula_11">J struc (W ; V, T ) = 1 P T PT ∑ p=1 [ max ap / ∈A(V,T ) q( a p ) − q(a p ) + △ ] + λ 2 ||W || 2<label>(11)</label></formula><p>where λ is the weight of regularization term. Intuitively, this loss objective function maximizes the score of correct merging operation and minimizes incorrect merging operations. To improve efficiency, we do not minimize all incorrect merging operations, but only the one with highest score. Relation Categorization. The relation categorization task can be optimized as a softmax classification problem. We define the object function of relation categorization for image I as,</p><formula xml:id="formula_12">J rel (W ; V, T ) = − 1 |U T | ( ∑ {kl} S ∑ s=1 1(r kl = s) log G s (θ kl (V, W )) + (1 − 1(r kl = s)) log(1 − G s (θ kl (V, W ))) + ∥W ∥ 2<label>(12)</label></formula><p>|U T | denotes the number of relation appearing in the tree structre T . {kl} denotes a node merged from node k and l. S is the total number of relation categories. r kl denotes the ground truth relations provided by tree structure T between two semantic entities. G s (θ kl (V, W )) is the categorizer sub-network in the RNN(see Sect. 3.2), which outputs the probability that node {kl} belongs to relation category s.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Learning Algorithm</head><p>The Expectation-Maximization method is adopted to optimize the loss in Eq. <ref type="bibr" target="#b7">(8)</ref>. In the E-step, guided by the sentence description, we update the intermediate label maps C and the latent structured configurations together with the CNN and RNN losses. In the M-step, the parameters are updated using the back-propagation algorithm. In summary, our learning algorithm can be conducted by iteratively performing the following tree steps:</p><p>(i) Updating intermediate label mapsĈ and the CNN loss. Given image I and its semantic tree T , we compute the classification probability of each pixel according to Eq.(1). Inspired by the work of cardinality potentials <ref type="bibr" target="#b30">[31]</ref> <ref type="bibr" target="#b14">[15]</ref>, the score of pixel j belonging to the label k is calculated by</p><formula xml:id="formula_13">f j (k) = σ(s k j ) + δ k , where σ(s k j ) is defined in Eq.(1)</formula><p>. δ k is entity-dependent biases, which is set adaptively according to the prescribed proportion areas of background or foreground entity classes in the image <ref type="bibr" target="#b19">[20]</ref>, regarding the set of entities in T . The final classification result of pixel j is computed by c j = arg max k f j (k). Finally, the CNN loss is computed according to Eq.(9).</p><p>(ii) Updating latent scene structures and the RNN loss. Given the label of each pixel, we group the pixels into semantic objects and obtain the object feature representations with the method described in Sect. 3.1. Then we use the RNN model to infer the interaction relations and hierarchical configuration of objects, and compute the RNN loss according to Eq.(11) and Eq. <ref type="bibr" target="#b11">(12)</ref>.</p><p>(iii) Updating the CNN and RNN parameters. Given the intermediate label maps and latent scene structure, we can compute the gradient of the overall loss in Eq.(8) w.r.t. the CNN and RNN parameters. With the BP algorithm, the gradients from the semantic label loss propagate backward through all layers of CNN. The gradients from the scene structure loss first propagate recursively through the layers of RNN, and then propagate through the object features to the CNN. Thus, all the parameters of our CNN-RNN model can be learned in an end-to-end manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiment</head><p>We first introduce the implementation details and then evaluate the performance of our proposed method for semantic labeling and structured scene parsing.</p><p>Datasets. We conduct our experiments on PASCAL VOC 2012 segmentation benchmark <ref type="bibr" target="#b5">[6]</ref>, which contains 4,369 images from three subsets: training (1,464 images), validation (1,449 images) and test(1,456 images). PASCAL VOC 2012 dataset has 20 foreground categories and 1 background category. To suit our task, we randomly divide images in the training and validation sets into 5 groups, and asked 5 annotators to provide one description for each image in each group respectively. Since the groundtruth labeling is unavailable for test images, we did not annotate the test set. In the semi-supervised experiments, the training set is further divided into two subsets, where one is the strongly-annotated subset and the other is the PASCAL VOC 2012 training set with sentence description. Considering the Semantic Boundaries Dataset (SBD) <ref type="bibr" target="#b10">[11]</ref> provides pixel-wise labels for images from PASCAL VOC 2011, we use part of the SBD to constitute the strongly-annotated subset, which includes at most 1,464 of the 10,582 training images in our experiments.</p><p>Preprocessing. Constituency trees from the Stanford Parser <ref type="bibr" target="#b27">[28]</ref> still contains irrelevant words that do not describe object category or interaction relations(e.g., adjectives). Therefore, we need to convert constituency trees into semantic trees, which only contains semantic entities and scene structure (as illustrated in <ref type="figure">Fig. 5</ref>).</p><p>The conversion process generally involves three steps. Given a constituency tree (top tree in <ref type="figure">Fig. 5)</ref>, we first filter the leaf nodes by their part-of-speech, preserving only nouns as object candidates, and verbs and prepositions as relation candidates. Second, nouns are combined and converted to object categories. Annotators sometimes use different nouns for the same category (e.g. "cat" and "kitten"). Thus we use the lexical relation data in WordNet <ref type="bibr" target="#b17">[18]</ref> to unify the synonyms belonging to same defined category, and convert them to the corresponding object category. Annotators may mention entities that are not in any defined object categories (e.g. "grass" in "a sheep stands on the grass"), which will be also removed from the trees.</p><p>Third, relations should also be recognized and refined. Denote by R a set of defined relations, and T the triplets in the form of (entity1, verb/prep, entity2). We construct a mapping T → R to recognize relation. R also contains two special relation categories: "other" and "background". The "other" serves as a placeholder for undefined relations. The "background" deals with the special cases where only one entity is recognized in a tree. In this case we merge the entity with an additional "background" entity, and assign "background" relation to their parent node.  <ref type="figure">Figure 5</ref>. An illustration of the tree conversion process. The top tree is the constituency tree generated by language parser. The middle tree is the constituency tree after POS tag filtering. The bottom tree is the converted relation tree.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Semantic Labeling</head><p>In this section, we report the results for the conventional semantic labeling task which assigns semantic label to each pixel. We consider two experimental settings, i.e. weakly-supervised learning and semi-supervised learning, and adopt the pixel-wise intersection-over-union(IoU) used in PASCAL VOC segmentation challenge <ref type="bibr" target="#b5">[6]</ref> as the performance indicator. Note that our description annotation does not cover the exact same object classes in each image as in the pixel-wise annotation, making only partial class labels are used for training. For fair comparison, we modified the training and validation images by assigning background category to the object categories not mentioned in description sentences. Due to the labels of the test set is not available, we cannot modify the test set and thus only report the results on the modified validation set. Visualized labeling results are shown in <ref type="figure" target="#fig_4">Fig. 6</ref>.</p><p>Weakly-supervised Learning. <ref type="table">Table 1</ref> shows the results under the setting of weakly-supervised learning. We compare our method with MIL-ILP <ref type="bibr" target="#b21">[22]</ref>, MIL-FCN <ref type="bibr" target="#b16">[17]</ref>, and DeepLab <ref type="bibr" target="#b19">[20]</ref>, a state-of-the-art weakly-supervised method using image labels as supervision. We perform experiments with the publicly available code of DeepLab, and our own implementation of MIL-ILP and MIL-FCN. Our method obtains the IoU of 35.2%, outperforming DeepLab <ref type="bibr" target="#b19">[20]</ref> by 4.9%. If we fix the parameters of the RNN with random initialization, a 1.7% drop of IoU is observed, indicating that the RNN does help in learning the CNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>IoU MIL-ILP <ref type="bibr" target="#b21">[22]</ref> 29.4% MIL-FCN <ref type="bibr" target="#b16">[17]</ref> 28.3% DeepLab(weakly) <ref type="bibr" target="#b19">[20]</ref>  Semi-supervised Learning. In this setting, we have access to both pixel-level (strongly) annotated data and image-level (weakly) annotated data, and our method can take advantage of both types of supervision information in the training procedure. We consider two semi-supervised strategies: waterfall and fusion. For the waterfall strategy, we first perform 8,000 iterations of strongly-supervised pre-training on the CNN, followed by 16,000 iterations of weakly-supervised training on the CNN and RNN. For the fusion strategy, we use a weighted sum of stronglysupervised and weakly-supervised loss functions to train the CNN and RNN, where we use 280 strong samples together with weak training samples, and the loss weight is set as 1:1.5 (strong:weak). <ref type="table">Table 2</ref> shows the result on the PASCAL VOC 2012 validation set. We observe that all methods benefit significantly from semi-supervised learning. The improvement of IoU compared to weakly supervised learning is 10.1% with 280 strongly annotated samples (strong:weak = 1:5), and is 17.7% with 1464 strongly annotated samples (strong:weak = 1:1). Our method outperforms DeepLab <ref type="bibr" target="#b19">[20]</ref> by 2.8% with 280 strong samples and fusion strategy.</p><p>Given the same number of strongly annotated data, the fusion strategy outperforms the waterfall strategy by 10.2% in terms of IoU. We observe that the accuracy of pretraining step in waterfall strategy is very high (over 95%) on the training set. This indicates that the separated pretraining with small amount of data causes the model overfitted, making pre-training contribute little to performance improvement. Nevertheless, the fusion strategy trains the model with a combined loss for better tradeoff of the two types of supervision information, and thus can exploit the strongly annotated data without suffering from overfitting.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Structured Scene Parsing</head><p>In this section, we evaluate the structured scene parsing performance of the proposed method, which is measured with two metrics: relation accuracy and structure accuracy. Relation accuracy is computed recursively. Denote by T a tree and P = {T, T 1 , T 2 , . . . , T m } the set of enumerated sub-tress (including T ) of T . A leaf T i is considered to be correct if it is of the same object category as the one in the ground truth tree. A non-leaf T i (with two subtrees T l and T r ) is considered to be correct if and only if T l and T r are both correct and the relation label r T is correct. Then, the relation accuracy is calculated as (#of correctsubtrees) m+1 , and the structure accuracy is a simplification of the relation accuracy by ignoring the relation labels in the evaluation of the correctness of T .</p><p>Note that not all images in the PASCAL VOC 2012 validation set can be used for structure and relation accuracy, e.g. the images containing only one object, and these images should not be counted in the experiments.</p><p>To get detailed understanding of our method, we study the effect of two factors, i.e. joint CNN/RNN learning and end-to-end learning, and conduct experiments with the following configurations: i) Fixed the other parameters of the CNN except for the top two layers, we update all parameters of the RNN; ii) Fixed all parameters of RNN with randomly initialized values, we update all parameters of the CNN; iii) We separate the learning of CNN and RNN, i.e. we first update the CNN for 16000 iterations with the fixed RNN, and then update RNN for 16000 iterations with the fixed CNN; iv) We update both CNN and RNN in the whole process with an end-to-end and joint learning manner. <ref type="bibr">CNN</ref>   <ref type="table">Table 3</ref> shows the results on the PASCAL VOC 2012 validation set. Our method with end-to-end and joint learning performs best among all training settings. The training setting with fixed RNN performs much worse than one with fixed CNN, indicating that the RNN plays a more important role for structure and relation prediction. This is reasonable since structure and relation is finally obtained by RNN. Learning CNN and RNN separately performs better than learning with either fixed, but is still worse than endto-end and joint learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We have introduced a structured scene parsing method based on a deep CNN-RNN architecture, and a costeffective mode training method by transferring knowledge from image-level descriptive sentences. We have demonstrated the effectiveness of our framework by i) generating hierarchical and relation-aware configurations from the scene images and ii) achieving more favorable scene labeling results compared to other state-of-the-art weaklysupervised methods.</p><p>There are several directions in which we intend to extend this work, such as improving our system by adding a component for object attribute parsing. Deeply combining with some language processing techniques also would be a possible way.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>A glance into our proposed CNN-RNN architecture for structured scene parsing.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>An illustrate of recursive neural network in our CNN-RNN architecture. This network calculates the score for merging decision and predicts the relation category of two merged regions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>An illustration of the training process with our CNN-RNN architecture. The learning objective consists of two proportions: the semantic object labeling via the CNN, and the structure prediction via the RNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>Visualized semantic labeling results. (a) The input images; (b) The groundtruth lebeling results; (c) Our proposed method (weakly-supervised); (d) Deeplab(weaklysupervised)<ref type="bibr" target="#b19">[20]</ref>; (e)MIL-ILP(weakly-supervised)<ref type="bibr" target="#b21">[22]</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head></head><label></label><figDesc>Table 1. PASCAL 2012 val result of weakly supervised methods</figDesc><table>30.3% 
Ours(fixed-RNN) 
33.5% 
Ours 
35.2% 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Annotation. Direct annotation of the structured parsing trees for images is time-consuming, since it requires carefully designed tools and user interface. To save annotation cost, we use the natural language descriptions instead of trees. The sentence description of an image naturally provides a tree structure to indicate the major objects along with their interaction relations<ref type="bibr" target="#b4">[5]</ref>. Here we use the Stanford Parser<ref type="bibr" target="#b27">[28]</ref> to parse sentences and produce constituency trees, which are two-way trees with each word in a sentence as a leaf node and can serve as suitable alternative of structured image tree annotation.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Connected segmentation tree-a joint representation of region layout and hierarchy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Todorovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Convex optimization. Cambridge university press</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Vandenberghe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Semantic image segmentation with deep convolutional nets and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Discriminative models for multi-class object layout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">95</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Distributed representations, simple recurrent networks, and grammatical structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Elman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1991" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="195" to="225" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The pascal visual object classes challenge: A retrospective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M A</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning visual attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Class segmentation and object localization with superpixel neighborhoods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fulkerson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Perceptual organization and recognition of indoor scenes from rgb-d images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Bottom-up/top-down image parsing with attribute grammar</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="59" to="73" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Semantic contours from inverse detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep visual-semantic alignments for generating image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Pylon model for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">S</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">High order regularization for semisupervised learning of structured output problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Semantic image segmentation via deep parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Introduction to word net. An Online Lexical Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Miler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Beckwith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fellbuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Miller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Feedforward semantic segmentation with zoom-out features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mostajabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yadollahpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shakhnarovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Weakly-and semi-supervised learning of a deep convolutional network for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Fully convolutional multi-class multiple instance learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">From image-level to pixellevel labeling with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">O</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning a classification model for segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep hierarchical parsing for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Recursive context propagation network for semantic scene labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Indoor segmentation and support inference from rgbd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<idno>ECCV. 2012. 2</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Parsing with compositional vector grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Parsing natural scenes and natural language with recursive neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning continuous phrase representations and syntactic parsing with recursive neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Deep Learning and Unsupervised Feature Learning Workshop</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Fast exact inference for recursive cardinality models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">J</forename><surname>Frey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Eighth Conference on Uncertainty in Artificial Intelligence</title>
		<meeting>the Twenty-Eighth Conference on Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="825" to="834" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Superparsing -scalable nonparametric image parsing with superpixels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tighe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="329" to="349" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Scene parsing with object instances and occlusion ordering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tighe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niethammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Image parsing: Unifying segmentation, detection, and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="113" to="140" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Weakly supervised structured output learning for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vezhnevets</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Buhmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Tell me what you see and I will show you where it is</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A codebook-free and annotation-free approach for fine-grained image categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">R</forename><surname>Bradski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jayasumana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conditional random fields as recurrent neural networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Recursive segmentation and recognition templates for image parsing. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="359" to="371" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
