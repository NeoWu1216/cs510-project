<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Video2GIF: Automatic Generation of Animated GIFs from Video</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Gygli</surname></persName>
							<email>gygli@vision.ee.ethz.ch</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">CVL</orgName>
								<orgName type="institution" key="instit2">ETH Zurich Zurich</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yale</forename><surname>Song</surname></persName>
							<email>yalesong@yahoo-inc.com</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">CVL</orgName>
								<orgName type="institution" key="instit2">ETH Zurich Zurich</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yahoo</forename><surname>Research</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">CVL</orgName>
								<orgName type="institution" key="instit2">ETH Zurich Zurich</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>USA</roleName><forename type="first">New</forename><surname>York</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">CVL</orgName>
								<orgName type="institution" key="instit2">ETH Zurich Zurich</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Video2GIF: Automatic Generation of Animated GIFs from Video</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Liangliang Cao Yahoo Research New York, USA</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:48+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce the novel problem of automatically generating animated GIFs from video. GIFs are short looping video with no sound, and a perfect combination between image and video that really capture our attention. GIFs tell a story, express emotion, turn events into humorous moments, and are the new wave of photojournalism. We pose the question: Can we automate the entirely manual and elaborate process of GIF creation by leveraging the plethora of user generated GIF content? We propose a Robust Deep RankNet that, given a video, generates a ranked list of its segments according to their suitability as GIF. We train our model to learn what visual content is often selected for GIFs by using over 100K user generated GIFs and their corresponding video sources. We effectively deal with the noisy web data by proposing a novel adaptive Huber loss in the ranking formulation. We show that our approach is robust to outliers and picks up several patterns that are frequently present in popular animated GIFs. On our new large-scale benchmark dataset, we show the advantage of our approach over several state-of-the-art methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Animated GIF is an image format that continuously displays multiple frames in a loop, with no sound. Although first introduced in the late 80's, its popularity has increased dramatically in recent years on social networks, such as Tumblr and reddit, generating numerous famous Internet memes and creative Cinemagraphs <ref type="bibr" target="#b0">[1]</ref>. In response, various websites have been created to provide easy-to-use tools to generate GIF from video, e.g., GIFSoup, Imgflip, and Ezgif. However, while becoming more prevalent, the creation of GIF remains an entirely manual process, requiring the user to specify the timestamps of the beginning and the end of a video clip, from which a single animated GIF is generated. This way of manually specifying the exact time range makes existing solutions cumbersome to use and re- * This work was done while the author was an intern at Yahoo! Inc. <ref type="figure">Figure 1</ref>. Our goal is to rank video segments according to their suitability as animated GIF. We collect a large-scale dataset of animated GIFs and the corresponding video sources. This allows us to train our Robust Deep RankNet using over 500K pairs of GIF and non-GIF segment pairs, learning subtle differences between video segments using our novel adaptive rank Huber loss. quires extensive human effort.</p><p>In this paper, we introduce the novel problem of automatically generating animated GIFs from video, dubbed Video2GIF. From the computer vision perspective, this is an interesting research problem because GIFs have some unique properties compared to conventional images and videos: A GIF is short, entirely visual with no sound, expresses various forms of emotions, and sometimes contains unique spatio-temporal visual patterns that make it appear to loop forever. The task has some connections to existing computer vision problems -such as visual interestingness <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b10">11]</ref>, creativity <ref type="bibr" target="#b30">[31]</ref>, video highlights <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b39">40]</ref> and summarization <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b32">33]</ref> -but differs from them due to the unique properties described above. Apart from research interest, the task is supported by real-world demand and has many practical application scenarios including photojournalism, advertising, video sharing and preview, as well as video promotion on social media.</p><p>To handle this task, we propose a novel RankNet that, given a video, produces a ranked list of segments accord-ing to their suitability as animated GIF. Our framework has several novel components designed to learn what content is frequently selected for popular animated GIFs. First, to capture the highly dynamic spatio-temporal visual characteristics of GIFs, we use 3D convolutional neural networks <ref type="bibr" target="#b35">[36]</ref> to represent each segment of a video. Second, to unravel the complex relationships and learn subtle differences between segments of a given video, we construct a ranking model that learns to compare pairs of segments and find the ones that are more suitable as GIF. Third, to make our learning task robust to the noisy web data, we design a new robust adaptive Huber loss function in the ranking formulation. Lastly, to account for different degrees of quality in user generated content, we encode the popularity measure of GIFs on social media directly into our loss.</p><p>Crucial to the success of our approach is our new largescale animated GIF dataset: We collected more than 100K user generated animated GIFs with their corresponding video sources from online sources. There are hundreds of thousands of GIFs available online and many provide a link to the video source. This allows us to create a dataset that is one to two orders of magnitude larger than existing datasets in the video highlight detection and summarization literature <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b11">12]</ref>. We use this dataset to train our deep neural network by making comparisons between more than 500K GIF and non-GIF pairs. Experimental results suggest that our model successfully learns what content is suitable for GIFs, and that our model generalizes well to other tasks, namely video highlight detection <ref type="bibr" target="#b34">[35]</ref>.</p><p>In summary, we make the following contributions: 1. We introduce the task of automatically generating animated GIFs from video. This is an interesting computer vision research problem that, to the best of our knowledge, has not been addressed before. 2. We propose a Robust Deep RankNet with a novel adaptive Huber loss in the ranking formulation. We show how well our loss deals with noisy web data, and how it encodes the notion of content popularity to account for different degrees of content quality. 3. We collect a new, large-scale benchmark dataset of over 100K user generated animated GIFs and their video sources. The dataset is one to two orders of magnitude larger than existing video highlighting and summarization datasets. The dataset is publicly available 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Our work is closely related to image aesthetics and interestingness, as well as video highlight detection and summarization. We review some of the most relevant work and discuss the differences. We also review and make connections to recent efforts on learning deep neural networks for 1 https://github.com/gyglim/video2gif_dataset ranking and trained on large-scale weakly-labeled data. Image aesthetics and interestingness. Finding the best images in a collection has been studied from several angles. Early approaches aimed at predicting the quality <ref type="bibr" target="#b18">[19]</ref> or aesthetics <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref> of an image. More recently, several approaches for predicting visual interestingness of an image have been proposed <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b6">7]</ref>. While interestingness is a subjective property assessed by the viewer, there is considerable consistency across annotated ratings <ref type="bibr" target="#b10">[11]</ref>. This makes it possible to model interestingness with computational means, but ground truth is typically noisy. Fu et al. <ref type="bibr" target="#b6">[7]</ref> propose an approach accounting for this, by learning a ranking model and removing outliers in a joint formulation. Khosla et al. <ref type="bibr" target="#b19">[20]</ref> analyze the related property of image popularity. Using a large-scale dataset of Flickr images, they analyze and predict what types of images are more popular than others, surfacing trends similar to those of interestingness <ref type="bibr" target="#b10">[11]</ref>. In a similar direction is the work of Redi et al. <ref type="bibr" target="#b30">[31]</ref>, which analyzes creativity. Rather than analyzing images, however, they focus on Vines videos, whose lengths are restricted to 6 seconds. Video summarization. A thorough discussion of earlier research can be found in <ref type="bibr" target="#b36">[37]</ref>. Here, we discuss two recent trends, (i) using web-image priors <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b26">27]</ref> and (ii) supervised learning-based methods <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b26">27]</ref>. Methods using web-image priors are based on the observation that web images for a specific topic or query are often canonical visual examples for the topic. This allows one to compute frame scores as the similarity between a frame and a set of web images <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b32">33]</ref>. Learning-based methods, on the other hand, use supervised models to obtain a scoring function for frames <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b26">27]</ref> or segments <ref type="bibr" target="#b12">[13]</ref>. Lee et al. <ref type="bibr" target="#b24">[25]</ref> learn a regression model and combine it with a clustering approach to diversify the results. Instead, <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b12">13]</ref> directly learn an objective function that scores a set of segments, based on relative importance between different aspects of a summary (e.g. balancing highlights and diversity). Crucial to these learning-based methods is some notion of importance or interestingness of a segment. Next, we will discuss methods focusing only on this part while ignoring diversity and information coverage. Video highlights. The definition of highlight is both subjective and context-dependent <ref type="bibr" target="#b36">[37]</ref>. Nevertheless, it has been shown that there exists some consistency among human ratings for this task <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b11">12]</ref>. Several methods exploit, for example, that close-ups of faces are generally of interest <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b11">12]</ref>. But these approaches are limited in that they rely on a few hand-crafted features for capturing highlights in highly diverse settings. Instead, several approaches for domain-specific models have been proposed. In particular, in sport games highlight is more clearly defined (e.g. scoring a goal) which has been exploited in many works (see <ref type="bibr" target="#b36">[37]</ref> for an overview). Recently, Sun et al. <ref type="bibr" target="#b34">[35]</ref> and Potapov et al. <ref type="bibr" target="#b29">[30]</ref> proposed a more general approach. Based on annotated videos for a specific topic (e.g. surfing), they use machine learning on top of generic features to train a highlight predictor. In order to train their model, <ref type="bibr" target="#b29">[30]</ref> uses a large, manually annotated dataset for action recognition. Instead, <ref type="bibr" target="#b34">[35]</ref> use a smaller dataset obtained by crawling YouTube data. They find pairs of raw and edited videos, used in training, by matching all pairs of videos within a certain category (e.g. gymnastics). The size of their dataset is, however, limited by the availability of domain-specific videos in both raw and edited forms.</p><p>Obtaining a large-scale video highlight dataset is difficult. Thus, Yang et al. <ref type="bibr" target="#b39">[40]</ref> propose an unsupervised approach for finding highlights. Relying on an assumption that highlights of an event category are more frequently captured in short videos than non-highlights, they train an auto-encoder. Our work instead follows a supervised approach, introducing a new way to obtain hundreds of thousands of labeled training videos (10x larger than the unlabeled dataset of <ref type="bibr" target="#b39">[40]</ref>), which allows us to train a deep neural network with millions of parameters.</p><p>Learning to rank with deep neural networks. Several works have used CNNs to learn from ranking labels. The loss function is often formulated over pairs <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b8">9]</ref> or triplets <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b22">23]</ref>. Pairwise approaches typically use a single CNN, while the loss is defined relatively over the output. For example, Gong et al. <ref type="bibr" target="#b8">[9]</ref> learn a network to predict image labels and require the scores of correct labels to be higher than the scores of incorrect labels. Triplet approaches, on the other hand, use Siamese networks. Given an image triple (query, positive, negative), a loss function requires the learned representation of the query image to be closer to that of the positive, rather than the negative image, according to some metric <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b22">23]</ref>.</p><p>Supervised deep learning from noisy labels. Several previous works have successfully learned models from weak labels <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b26">27]</ref>. Liu et al. <ref type="bibr" target="#b26">[27]</ref> considers the video search scenario. Given click-through data from Bing, they learn a joint embedding between query text and video thumbnails in order to find semantically relevant video frames. In contrast, <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b37">38]</ref> use labels obtained through automatic methods to train neural networks. Karpathy et al. <ref type="bibr" target="#b17">[18]</ref> train a convolutional neural network for action classification in videos. Their training data is obtained from YouTube where it is labeled automatically by analyzing meta data associated with the videos. Wang et al. <ref type="bibr" target="#b37">[38]</ref> learn a feature representation for fine-grained image ranking. Based on existing image features they generate labels used for training the neural network. Both approaches obtain state-of-the-art performance, showing the strength of large, weakly-labeled datasets in combination with deep learning.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Video2GIF Dataset</head><p>Inspired by the recent success with large, weakly-labeled datasets applied in combination with deep learning, we harvest social media data with noisy, human generated annotations. We use websites that allow users to create GIFs from video (Make-a-GIF and GIFSoup). Compared to edited videos used in <ref type="bibr" target="#b34">[35]</ref>, GIFs have the intriguing property that they are inherently short and focused. Furthermore they exist in large quantities and typically come with reference to the initial video, which makes alignment scale linearly in the number of GIFs. Aligning GIFs to their source videos is crucial, as it allows us to find non-selected segments, which serve as negative samples in training. In addition, videos provide a higher frame-rate and fewer compression artifacts, ideal for obtaining high quality feature representations.</p><p>Using these GIF websites, we collected a large-scale dataset with more than 120K animated GIFs and more than 80K videos, with a total duration of 7,379 hours. This is one to two orders of magnitude larger than the highlight datasets of <ref type="bibr" target="#b39">[40]</ref> and <ref type="bibr" target="#b34">[35]</ref>. We will show further statistics on the dataset after discussing the alignment process.</p><p>Alignment. We aligned the GIFs to their corresponding videos using frame matching. In order to do this efficiently, we encoded each frame with a perceptual hash based on the discrete cosine transform <ref type="bibr" target="#b40">[41]</ref>. The perceptual hash is <ref type="bibr">Figure 3</ref>. Most frequent video tags on the used dataset. We can observe that not all tags are equally informative. While several describe a specific visual concept ( e.g. cat or wrestling) others describe abstract concepts that cannot be expected to help the task at hand.  fast to compute and, given its binary representation, can be matched very efficiently using the Hamming distance. We matched the set of GIF frames to the frames of its corresponding video. This approach requires O(nk) distance computations, where n, k is the number of frames in the video and GIF, respectively. Since the GIFs are restricted in length and have a low frame-rate, they typically contain only a few frames (k &lt; 50). Thus, this method remains computationally efficient while it allows for the alignment to be accurate. In order to test the accuracy of our alignment process, we manually annotated a small random set of 20 GIFs with ground-truth alignments and measured the error. Our method has a mean alignment error of 0.34 seconds (median 0.20 seconds), which is accurate enough for our purpose. In comparison, Sun et al. <ref type="bibr" target="#b34">[35]</ref> aligned blocks of 50 frames (≈ 2 seconds), i.e. on a much coarser level.</p><p>Dataset Analysis. We analyze what types of video are often used to create animated GIFs. <ref type="figure">Figure 3</ref> shows the most frequent tags of videos in our dataset, and <ref type="figure" target="#fig_2">Figure 4</ref> shows the category distribution of the videos. Several tags give a sense of what is present in the videos, which can potentially help GIF creation, e.g. cute and football. Others are not visually informative, such as 2014 or YouTube. <ref type="figure" target="#fig_1">Figure 2</ref> shows a histogram of video lengths (median: 2m51s, mean: 5m12s). As can be seen, most source videos are rather short, with a median duration of less than 3 minutes.</p><p>Splits. From the full dataset we used videos with a maximal length of 10 minutes. Longer videos are discarded as the selected GIF segments become too sparse and the videos are more affected by chronological bias <ref type="bibr" target="#b32">[33]</ref>. We split the data into training and validation sets, with about 65K and 5K videos, respectively. For the test set, we use videos with Creative Commons licence, which allows us to distribute the source videos for future research. As the task is trivial for videos shorter than 30sec we only consider videos of longer duration. The final test set consists of 357 videos. <ref type="table">Table 1</ref> shows the statistics of the dataset we used in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Method</head><p>This sections presents our approach to the Video2GIF task, with a novel adaptive Huber loss in the ranking formulation to make the learning process robust to outliers; we call our model the Robust Deep RankNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Video Processing</head><p>We start by dividing a video into a set of non-overlapping segments S = {s 1 , · · · , s n }. We use the efficient shot boundary detection algorithm of Song et al. <ref type="bibr" target="#b32">[33]</ref>, which solves the multiple change point detection problem to detect shot boundaries.</p><p>The segments are not necessarily aligned perfectly with the boundaries of the actual animated GIF segments. We determine whether a segment s belongs to GIF segment s ⋆ by computing how much of it overlaps with s ⋆ . A segment is considered as a GIF segment only if the overlap is larger than 66%. Segments without any overlap serve as negatives. The segments are then fed into our robust deep ranking model, described next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Robust Deep RankNet</head><p>Architecture overview. <ref type="figure">Figure 5</ref> illustrates the architecture of our model. During training, the input is a pair of GIF and non-GIF segments. The model learns a function h : R d → R that maps a segment s to its GIF-suitability score h(s). This score is of course unknown even during training; we learn the function by comparing the training segment pairs so that a GIF segment gets a higher score than a non-GIF segment. During testing, the model is given a single  <ref type="figure">Figure 5</ref>. The architecture of our Robust Deep RankNet. We train the green-colored layers from scratch. Each hidden layer is followed by a ReLu non-linearity <ref type="bibr" target="#b28">[29]</ref>. The final scoring layer is a linear function of the last hidden layer. The rank loss acts on pairs of segments and is non-zero, unless s + scores higher than s − by a margin of 1. To emphasize that the loss acts on pairs of segments, we show the two passes separately, but we use a single network.</p><p>. segment and computes its GIF-suitability score using the learned scoring function. We compute the score h(s) for all segments s ∈ S and produce a ranked list of the segments for their suitability as an animated GIF. Feature representation. Animated GIFs contain highly dynamic visual content; it is crucial to have feature representation that captures this aspect well. To capture both the spatial and the temporal dynamics of video segments, we use C3D <ref type="bibr" target="#b35">[36]</ref> pretrained on the Sports-1M dataset <ref type="bibr" target="#b17">[18]</ref> as our feature extractor. C3D extends the image-centric network architecture of AlexNet <ref type="bibr" target="#b21">[22]</ref> to the video domain by replacing the traditional 2D convolutional layers with a spatio-temporal convolutional layer, and has been shown to perform well on several video classification tasks <ref type="bibr" target="#b35">[36]</ref>. Inspired by previous methods using category specific models <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b29">30]</ref>, we optionally add contextual features to the segment representation. These can be considered metainformation, supplementing the visual features. They have the potential to disambiguate segment rankings and allow a model to score segments conditioned on the semantic category of a video. The features include the category label, a semantic embedding of the video tags (mean over their word2vec representation <ref type="bibr" target="#b27">[28]</ref>) and positional features. For positional features, we use the timestamp, rank and the relative position of the segment in the video. Problem formulation. A straightforward way to formulate our problem is by posing it as a classification problem, i.e., treat GIF and non-GIF segments as positive and negative examples, respectively, and build a binary classifier that separates the two classes of examples. This formulation, however, is inadequate for our problem because there is no clear cut definition of what is a good or a bad segment. Rather, there are various degrees of GIF suitability that can only be inferred by comparing GIF and non-GIF pairs. A natural formulation is therefore posing it as a ranking problem. We can define a set of rank constraints over the dataset D, where we require GIF segments s + to rank higher than non-GIF segments s − , i.e.</p><formula xml:id="formula_0">h(s + ) &gt; h(s − ), ∀ s + , s − ∈ D.</formula><p>This formulation compares two segments even if they are from different videos. This is problematic because a comparison of two segments is meaningful only within the context of the video, e.g., a GIF segment in one video may not be chosen as a GIF in another. To see this, some videos contain many segments of interest (e.g. compilations), while in others even the selected parts are of low quality. The notion of GIF suitability is thus most meaningful only within, but not across, the context of a single video.</p><p>To account for this, we revise the above video-agnostic ranking formulation to be video-specific, i.e.</p><formula xml:id="formula_1">h(s + ) &gt; h(s − ), ∀ s + , s − ∈ S.</formula><p>That is, we require a GIF segment s + to score higher than negative segments s − that come from the same video only. Next we define how we impose the rank constraints. Loss function. One possible loss function for the ranking problem is an l p loss, defined as</p><formula xml:id="formula_2">l p (s + , s − ) = max 0, 1 − h(s + ) + h(s − ) p ,<label>(1)</label></formula><p>where p = 1 <ref type="bibr" target="#b16">[17]</ref> and p = 2 <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b23">24]</ref> are the most popular choices. The l p loss imposes the ranking constraint by requiring a positive segment to score higher than its negative counterpart by a margin of 1. If the margin is violated, the incurred loss is linear in the error for the l 1 loss, while for the l 2 loss it is quadratic. One drawback of the l 1 loss, compared to the l 2 loss, is that it over-penalizes small margin violations. The l 2 loss does not have such problem, but it quadratically penalizes margin violations, and thus is more affected by outliers (see <ref type="figure" target="#fig_3">Figure 6</ref>).</p><p>Our dataset contains animated GIF contents created by online users, so some of the contents will inevitably be of low quality; these can be considered as outliers. This motivates us to propose a novel robust rank loss, which is an adaption of the Huber loss formulation <ref type="bibr" target="#b14">[15]</ref> to the ranking setting. This loss gives a low penalty to small violations of the margin (where the ranking is still correct), and is more robust to outliers compared to the l 2 loss. We define our loss as</p><formula xml:id="formula_3">l Huber (s + , s − ) = 1 2 l 2 (s + , s − ), if u ≤ δ δl 1 (s + , s − ) − 1 2 δ 2 , otherwise<label>(2)</label></formula><p>where u = 1 − h(s + ) + h(s − ). Thus, if the margin is violated, the loss corresponds to a Huber loss, which is squared for small margin-violations and linear for stronger violations. The parameter δ defines the point at which the loss becomes linear. We illustrate the three different forms loss functions in <ref type="figure" target="#fig_3">Figure 6</ref>.</p><p>Considering the source of our dataset (social media), not all GIFs are expected to be of equal quality. Some might be casually created by beginners and from mediocre videos, while others are carefully selected from a high quality source. Thus, some GIFs can be considered more reliable as positive examples than others. We take this into account by making the parameter δ GIF dependent: We assign a higher value to δ to more popular GIFs. Our intuition behind this adaptive scoring scheme is that popular GIFs are less likely to be outliers and therefore do not require a loss that becomes linear early on. Objective function. Finally, we define our objective as the total loss over the dataset D and a regularization term with the squared Frobenius norm on the model weights W:</p><formula xml:id="formula_4">L(D, W) = Si∈D (s + ,s − )∈Si l Huber (s + , s − )+λ||W|| 2 F ,<label>(3)</label></formula><p>where λ is the regularization parameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Implementation Details</head><p>We experimented with various network architectures. While the loss function turned out to be crucial, we empirically found that performance remains relatively stable for different depths of a network. Thus, we opt for a simple 2 hidden layer fully-connected model, where each hidden unit is followed by a ReLu non-linearity <ref type="bibr" target="#b28">[29]</ref>. We use 512 units in the first and 128 in the second hidden layer. The final prediction layer, which outputs h(s), is a simple single linear unit, predicting an unnormalized scalar score. The final network has 2,327,681 parameters.</p><p>We minimize the objective in Eq. 3 using mini-batch stochastic gradient descent with backpropagation <ref type="bibr" target="#b31">[32]</ref>. We use mini-batches of 50 pairs. In order to accelerate convergence, we apply Nesterov's Accelerated Momentum <ref type="bibr" target="#b1">[2]</ref> for updating the weights. The momentum is set to 0.9 and λ = 0.001 (weight decay). We initialize training with a learning rate of 0.001 and reduce it every 10th epoch. The learning is stopped after 25 epochs. We apply dropout <ref type="bibr" target="#b33">[34]</ref> regularization to the input (0.8) and after the first hidden layer (0.25). Dropout is a simple, approximate way to do model averaging that increases robustness of the model <ref type="bibr" target="#b33">[34]</ref>.</p><p>We obtain the training set segment pairs (s + , s − ) by using all positive segments, randomly sampling k = 4 negatives per video, and combining them exhaustively. We limit the negatives in order to balance the positive-negative pairs per video. Finally, we obtain 500K pairs for training. For the Huber loss with a fixed δ we set δ = 1.5 based on the performance on the validation set. For the adaptive Huber loss, we set δ = 1.5 + p, where p is normalized viewcount proposed in <ref type="bibr" target="#b19">[20]</ref>.</p><p>In order to further decrease the variance of our model, we use model averaging, where we train multiple models from different initializations and average their predicted scores. The models were implemented using Theano <ref type="bibr" target="#b2">[3]</ref> with Lasagne <ref type="bibr" target="#b5">[6]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>We evaluate our method against several state-of-the art methods on our dataset. In Section 5.3 we further evaluate cross-task performance on the highlight dataset of <ref type="bibr" target="#b34">[35]</ref>. Evaluation metrics. Two popular performance metrics used in video highlight detection are mean Average Precision (mAP) <ref type="bibr" target="#b34">[35]</ref> and average meaningful summary duration (MSD) <ref type="bibr" target="#b29">[30]</ref>. Both mAP and MSD are, however, sensitive to video length: the longer the video is, the lower the score (think about finding the needle in the haystack). To compensate for a variety of video lengths in our dataset (see <ref type="figure" target="#fig_1">Figure 2</ref>), we propose a normalized version of MSD.</p><p>The normalized MSD (nMSD) corresponds to the relative length of the selected GIF at a recall rate of α. We define it as:</p><formula xml:id="formula_5">nM SD = |G * | − α|G gt | |V| − α|G gt | ,<label>(4)</label></formula><p>where |.| denotes the length of a GIF or video, and G * is the GIF with α recall w.r.t. the ground truth GIF G gt . The score is normalized with the length of the ground truth GIF and the video V, such that it is 0 if the selection equals to G gt (is perfect), and 1 if the ground truth has the lowest predicted score. The added normalization helps make the scores of different videos more comparable, in contrast to mAP, which is strongly affected by the length of the ground truth GIF, relative to the video. To account for inaccuracies in segmentation we set α = 0.5. For videos with multiple GIFs we use their mean nMSD as the video score. In addition to nMSD, we also evaluate performance using the traditional mAP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Compared Methods</head><p>We compare our method to three state-of-the-art methods in highlight detection. We also provide an approximate upper bound that puts the results in perspective.</p><p>Domain-specific highlights <ref type="bibr" target="#b34">[35]</ref>. We learn a domainspecific rankSVM per video category. Sun et al. <ref type="bibr" target="#b34">[35]</ref> use an EM-like approach to handle long, loosely selected highlights. For our dataset, this problem does not occur because the GIFs are already short and focused. We therefore simply train a rankSVM <ref type="bibr" target="#b23">[24]</ref> per video category using C3D features. We set C = 1 for all models.</p><p>Deep visual-semantic embedding <ref type="bibr" target="#b26">[27]</ref>. We train a network using triplets of segment, true and random titles (s + , t + , t − ). The titles are embedded into R 300 using word2vec <ref type="bibr" target="#b27">[28]</ref>. In contrast to our method, the loss of <ref type="bibr" target="#b26">[27]</ref> is defined over positive and negative titles and uses only positive segments (or images in their case) for training.</p><p>Category-specific summarization <ref type="bibr" target="#b29">[30]</ref>. This approach trains a one-vs-all SVM classifier for each video category. Thus, the classifier learns to separate one semantic class from the others. At test time it uses the classifier confidence to assign each segment an importance score, which we use to obtain a ranked list.</p><p>Approximate upper bound. This bound provides a reference for how well an automatic method can perform. To obtain the upper bound, we first find all videos in our dataset that have animated GIFs from multiple creators. We then evaluate the performance of one GIF w.r.t. the remaining ones from the same video. Thus, the approximate upper bound is the performance users achieve in predicting the GIFs of other users. And it allows us to put the performance of automatic methods in perspective. We note, however, that this bound is only approximate because it is obtained in a very different setting than other methods. <ref type="table">Table 2</ref> summarizes the results. <ref type="figure">Figure 7</ref> shows qualitative results obtained using our method. As can be seen, our method ("Ours" in <ref type="table">Table 2</ref>) outperforms the baseline methods by a large margin in terms of nMSD. The strongest baseline method is domain-specific rankSVM <ref type="bibr" target="#b34">[35]</ref>. Their learning objective is similar to ours, i.e., they use pairs of positive and negative segments from the same video for training. In contrast, two other baselines <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b26">27]</ref> use a "proxy" objective, i.e., learn semantic similarity of segments to video category <ref type="bibr" target="#b29">[30]</ref> or segments to video title <ref type="bibr" target="#b26">[27]</ref>. We believe this different training objective is crucial, allowing both our method and rankSVM <ref type="bibr" target="#b34">[35]</ref> to significantly outperform the two baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Results and Discussions</head><p>Domain-specific rankSVM <ref type="bibr" target="#b34">[35]</ref> with C3D features performs fairly well; but our method outperforms it. We believe the reason for this performance difference is two-fold: (1) the l 2 loss in <ref type="bibr" target="#b34">[35]</ref> is not robust enough to outliers; and Method nMSD ↓ mAP ↑ Joint embedding <ref type="bibr" target="#b26">[27]</ref> 54.38% 12.36% Category-spec. SVM <ref type="bibr" target="#b29">[30]</ref> 52.98% 13.46% Domain-spec. rankSVM <ref type="bibr" target="#b34">[35]</ref> 46 (2) the learning capabilities of <ref type="bibr" target="#b34">[35]</ref> are limited by the use of a linear model, compared to highly nonlinear neural nets. Next, we analyze different configurations of our method in greater detail and discuss impacts of each design choice. The configurations differ in terms of used inputs, network architecture and objective. What loss function is most robust? We analyze performance with different loss functions and training objectives discussed in Section 4.2. As expected, classification models always performs poorly compared to ranking models. Also, using video agnostic training data performs poorly. This indicates that the definition of a highlight is most meaningful within the video. When comparing l 1 loss and l 2 loss, we find that l 1 loss penalizes small margin violations (i.e., 0 &lt; h(s + ) − h(s − ) &lt; 1) too strongly, while the l 2 loss is affected by outliers. Our Huber rank loss avoids the two issues by combining the robustness to outliers (l 1 loss) and the decrease in the gradient for small margin violations (l 2 loss); it thus performs better than the other losses.</p><p>The role of context. Inspired by previous methods using category specific models <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b29">30]</ref> we used contextual information as input to our model (category label, a semantic of the video tags and positional features). When comparing the performance with and without context, we find that they perform similarly <ref type="table">(Table 2)</ref>. We believe that most of the information about the context is already present in the segment representation itself. This is supported by <ref type="bibr" target="#b25">[26]</ref> who show that the context can be extracted from the segment itself with high accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Cross Dataset Performance</head><p>As discussed, automatic GIF creation is related to video highlight detection. Of course, they are not identical: GIFs have a different focus and often depict funny or emotional content rather than highlights only. Nonetheless, it is inter- <ref type="figure">Figure 7</ref>. Qualitative results. Examples of top 3 and bottom 3 predicted segments. Our approach picks up aspects that are related to GIF suitability. For example, it learns that segments with people in motion are suitable for GIFs (e.g., (a) and (c)), while low contrast segments without any (main) objects are not (e.g., (a) the 4th image). It also scores segments showing the goal area of soccer games higher than the crowd in the stadium (b). We show a failure case (d): the network scores the segments with people on the ground higher than the landing plane (4th image). We provide more examples in GIF format on http://video2gif.info esting to see how well our method generalizes to this task. We evaluate our model on the dataset of <ref type="bibr" target="#b34">[35]</ref>, which contains videos from hand-selected categories such as surfing and skiing. We also evaluate the best performing baseline, domain-specific rankSVM <ref type="bibr" target="#b34">[35]</ref>, trained on our dataset and tested on the highlight dataset. The results are summarized in <ref type="table">Table 3</ref> (we borrow previously reported results <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b34">35]</ref>).</p><p>Our method outperforms rankSVM by a large margin, which suggests that our model generalizes much better than the baseline method. It also significantly outperforms the method of Yang et al. <ref type="bibr" target="#b39">[40]</ref>, which trains an auto-encoder model for each domain. Sun et al. <ref type="bibr" target="#b34">[35]</ref> tops the performance, but they use video category labels (which are handpicked) and learn multiple models, one per category, directly on the highlight dataset. Instead, our method learns a single global model on the GIF data, with much more diverse video categories. Nonetheless, it shows competitive performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We introduced the problem of automatically generating animated GIFs from video, and proposed a Robust Deep RankNet that predicts the GIF suitability of video segments. Our approach handles noisy web data with a novel adaptive Huber rank loss, which has the advantage of being robust to outliers and able to encode the notion of content quality directly into the loss. On our new dataset of animated GIFs we showed that our method successfully learns to rank seg-  <ref type="table">Table 3</ref>. Cross-dataset results (mAP). We train on our dataset and test on the video highlight dataset of <ref type="bibr" target="#b34">[35]</ref>. Our method outperforms rankSVM and <ref type="bibr" target="#b39">[40]</ref>, which learns an unsupervised model for each domain. Sun et al. <ref type="bibr" target="#b34">[35]</ref> performs best, but it is directly trained on their dataset and learns multiple models, one per category. Instead, we learn a single global model for GIF suitability.</p><p>ments with subtle differences, outperforming existing methods. Furthermore, it generalizes well to highlight detection.</p><p>Our novel Video2GIF task, along with our new largescale dataset, opens the way for future research in the direction of automatic GIF creation. For example, more sophisticated language models could be applied to leverage video meta data, as not all tags are informative. Thus, we believe learning an embedding specifically for video tags may improve a contextual model. While this work focused on obtaining a meaningful ranking for GIFs, we only considered single segments. Since some GIFs range over multiple shots, it would also be interesting to look at when to combine segments or even do joint segmentation and selection.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Length distribution of the input videos.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Distribution over video categories. Note how the categories are highly imbalanced and often not specific. e.g. Entertainment is an extremely broad category with strong visual and semantic variation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 .</head><label>6</label><figDesc>Rank loss comparison. Ours Huber rank loss combines the robustness w.r.t. to small margin violations of the l2 loss with the robustness to outliers of the l1 loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head></head><label></label><figDesc>Table 2. Experimental results. A lower nMSD and higher mAP represent better performance.</figDesc><table>.40% 
16.08% 
Classification 
61.37% 
11.78% 
Rank, video agnostic 
53.71% 
13.25% 
Rank, l 1 loss 
44.60% 
16.09% 
Rank, l 2 loss 
44.43% 
16.10% 
Rank, Huber loss 
44.43% 
16.22% 
Rank, adaptive Huber loss 
44.58% 
16.21% 
Rank, adaptive Huber loss 
+ context (Ours) 
44.19% 
16.18% 

Ours + model averaging 
44.08% 
16.21% 
Approx. bounds 
38.77% 
21.30% 

</table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Fast, cheap, and good: Why animated GIFs engage us</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bakhshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shamma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>De Juan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Kaye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CHI</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Advances in optimizing recurrent networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Boulanger-Lewandowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Theano: a CPU and GPU math expression compiler</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Breuleux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bastien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SciPy</title>
		<meeting>SciPy</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Studying aesthetics in photographic images using a computational approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">High level describable attributes for predicting aesthetics and interestingness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Lasagne: First release</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schlüter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Olson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sønderby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Nouri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maturana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Thoma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Battenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kelly</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Interestingness Prediction by Robust Learning to Rank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Diverse sequential subset selection for supervised video summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-L</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Deep convolutional ranking for multilabel image annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<idno>abs/1312.4894</idno>
		<imprint>
			<date type="published" when="2013" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Visual Interestingness in Image Sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Grabner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Nater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>ACM MM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gygli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Grabner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Riemenschneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Nater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<title level="m">The Interestingness Of Images. ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Creating Summaries from User Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gygli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Grabner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Riemenschneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Video Summarization by Learning Submodular Mixtures of Objectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gygli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Grabner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep semantic ranking based hashing for multi-label image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Robust estimation of a location parameter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Huber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Mathematical Statistics</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="1964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Understanding and Predicting Interestingness of Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Optimizing search engines using clickthrough data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<publisher>ACM SIGKDD</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The design of high-level features for photo quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Large-Scale Video Summarization Using Web-Image Priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hamid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sundaresan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Joint Summarization of Large-scale Collections of Web Images and Videos for Storyline Reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">ImageNet Classification with Deep Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Simultaneous feature learning and hash coding with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Large-scale linear rankSVM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-P</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Discovering important people and objects for egocentric video summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Summarizing While Recording : Context-Based Highlight Detection for Egocentric Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">I</forename><surname>Morariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV Workshop</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Multi-Task Deep Visual-Semantic Embedding for Video Thumbnail Selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Distributed Representations of Words and Phrases and their Compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Category-specific video summarization. ECCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Potapov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Redi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">O</forename><surname>Hare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schifanella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Trevisiol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jaimes</surname></persName>
		</author>
		<title level="m">Seconds of Sound and Vision : Creativity in Micro-Videos. CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning representations by back-propagating errors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Willams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vallmitjana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Stent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jaimes</surname></persName>
		</author>
		<title level="m">TVSum : Summarizing Web Videos Using Titles. CVPR</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Ranking Domain-Specific Highlights by Analyzing Edited Videos. ECCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Seitz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning Spatiotemporal Features with 3D Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Video abstraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">T</forename><surname>Truong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venkatesh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>ACM TOMCCAP</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning Fine-grained Image Similarity with Deep Ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Unsupervised Learning of Visual Representations Using Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Unsupervised Extraction of Video Highlights Via Robust Recurrent Auto-encoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Implementation and benchmarking of perceptual image hash functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zauner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
