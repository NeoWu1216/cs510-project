<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Recurrently Target-Attending Tracking</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Cui</surname></persName>
							<email>zhen.cui@seu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Research Center for Learning Science</orgName>
								<orgName type="institution">Southeast University</orgName>
								<address>
									<postCode>210096</postCode>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengtao</forename><surname>Xiao</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
							<email>jshfeng@gmail.com</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">360 Artificial Intelligence Institute</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Recurrently Target-Attending Tracking</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:45+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Robust visual tracking is a challenging task in computer vision. Due to the accumulation and propagation of estimation error, model drifting often occurs and degrades the tracking performance. To mitigate this problem, in this paper we propose a novel tracking method called Recurrently Target-attending Tracking (RTT). RTT attempts to identify and exploit those reliable parts which are beneficial for the overall tracking process. To bypass occlusion and discover reliable components, multi-directional Recurrent Neural Networks (RNNs)  are employed in RTT to capture long-range contextual cues by traversing a candidate spatial region from multiple directions. The produced confidence maps from the RNNs are employed to adaptively regularize the learning of discriminative correlation filters by suppressing clutter background noises while making full use of the information from reliable parts. To solve the weighted correlation filters, we especially derive an efficient closedform solution with a sharp reduction in computation complexity. Extensive experiments demonstrate that our proposed RTT is more competitive over those correlation filter based methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Visual object tracking is a fundamental research topic in computer vision with a wide range of applications, including video surveillance, traffic monitoring, and augmented reality <ref type="bibr" target="#b44">[45]</ref>, etc. Despite the great progress that has been made over the past decades, it remains very challenging due to the existing unpredictable appearance variations such as partial occlusion, illumination change, geometric deformation, background clutter, fast motion, etc.</p><p>A typical pipeline of visual tracking starts with an initial location (e.g., a rectangle bounding box) of the object in the first video frame, and then the location of the specified target in the subsequent frames is predicted. Among the existing tracking works, recent part-based methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b30">31]</ref> have been studied actively due to their robustness to local appearance variations, particularly partial occlusion. By partitioning the candidate region of the tracking target into several parts, part-based methods attempt to extract some useful cues from those identified reliable parts. For example, Kwon et al. <ref type="bibr" target="#b25">[26]</ref> used the topology structure of local patches to find those reliable parts. Zhang et al. <ref type="bibr" target="#b45">[46]</ref> employed a locality-constrained low-rank and sparse prior to establish correspondences between parts across frames. More recently, Liu et al. <ref type="bibr" target="#b30">[31]</ref> proposed to learn one response function for each part, and integrated response maps of all parts to generate the final tracking confidence. However, those methods might suffer from some difficulties in capturing large-range spatial dependencies between parts, especially for those objects with a large homogenous region.</p><p>In addition to the above methods, many holistic tracking methods have also been developed. In particular, correlation filter (CF) based methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b22">23]</ref> arouse increasing interest due to their excellent efficiency and robustness. CF based methods usually learn a group of correlation filters, which produce correlation peaks for the targets in the scene and meanwhile yield low responses to background regions. The correlation filters are trained by scanning the candidate regions using a circular sliding window. As they adopt the holistically convolutional representation, the whole candidate region is identically treated without any identification during training. This might produce inaccurate filters which cause the learnt tracker to drift away from its correct trajectory, especially when the candidate region embodies cluttered background.</p><p>To address the above problems, we propose a novel tracking method called Recurrently Target-attending Tracking (RTT), which attempts to identify and exploit those reliable parts throughout the process of model learning. To discover reliable components, RTT employs multi-directional Recurrent Neural Networks (RNNs) to spatially encode all parts from four different angles. The multi-directional RNN offer the following advantages for tracking objects robustly. (i) The spatial recurrent models can learn long-range contextual dependencies between parts, and further produce more accurate detection confidence maps associated with the parts. (ii) Encoding from multiple directions can significantly alleviate negative effects of occlusions that occur in one separated direction. (iii) The generated representation of a target is translation-invariant to some extent as the spatial networks are recurrently performed on local parts. (iv) The multi-directional RNNs are very simple and easy to implement compared to those graphic models with complex structures. Benefited from these aforementioned characteristics, multi-directional RNNs are able to provide a reasonable confidence prediction for the target and background region.</p><p>The confidence maps produced from multi-directional RNNs are further used to weight correlation filters in order to suppress negative effects of cluttered background and enhance learning from reliable parts. To this end, we reformulate the correlation filter learning into a regularized version by using confidence maps as the weighting factor. To solve the weighted correlation filters, we propose to factorize the high-dimensional space spanned by those concatenated multi-channel features into low-dimensional spaces on the single channel features, and finally derive an efficient analytical solution to the problem of filter learning. The solution sharply reduces the computation complexity of its direct original solution by a factor of d 3 , where d is the number of feature channels. Extensive experiments on the public tracking benchmark dataset demonstrate that our proposed RTT outperforms those existing correlation filter based methods.</p><p>In summary, our main contributions are three folds: (i) we propose a part-based confidence map learning method to discover reliable target parts and cluttered background regions; (ii) we develop an adaptively weighted correlation filter method to improve tracking performance by using more reliable information during model updating; and (iii) we derive an efficient closed-form solution to the learning of weighted correlation filters with a sharp reduction in computation complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Our work is related to the object tracking methods, especially those part-based methods and correlation filter based methods, as well as the current popular recurrent neural network technique.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Object Tracking</head><p>Video object tracking has been extensively studied in computer vision over the past decade <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b30">31]</ref>. Generally, they fall into two categories: genera-tive models and discriminative models. Generative methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b34">35]</ref> search for the most similar region to the tracked target. The target is often represented by a series of templates or spanned as a subspace. Discriminative methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b22">23]</ref> treat the object tracking as a classifier problem, which learns to distinguish the target from background.</p><p>The main related work is the part based and correlation filter based methods. Typically, the part based trackers <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b30">31]</ref> divide the entire target into several parts. Adam et al. <ref type="bibr" target="#b0">[1]</ref> represented the object by the grid of fragments, and then voted the target position from these fragments. Jia et al. <ref type="bibr" target="#b24">[25]</ref> used l 1 sparsity to search the closest candidate patches in the next frame. Besides, many methods explore the topology representation of local parts, such as tree structure <ref type="bibr" target="#b26">[27]</ref> or graph structure on superpixels <ref type="bibr" target="#b5">[6]</ref>. Different from these methods, we use recurrent neural units to model dependencies of parts from multiple directions, which can not only reduce effects of partial occlusions but also build long-range textural dependencies. Besides, RNNs are simpler and easier to be controlled than those models with tree or graph structures.</p><p>Correlation filters have made great progress in visual object tracking <ref type="bibr" target="#b7">[8]</ref>. Especially after Minimum Output Sum of Squared Error (MOSSE) <ref type="bibr" target="#b3">[4]</ref> filter was proposed, numerous correlation filter methods have been developed. Henriques et al. <ref type="bibr" target="#b21">[22]</ref> introduced the kernel trick strategy, and Danelljan et al. <ref type="bibr" target="#b12">[13]</ref> used color attributes to better represent the input information. To deal with the scale problem, SAM-F <ref type="bibr" target="#b27">[28]</ref>, DSST <ref type="bibr" target="#b10">[11]</ref> and an improved KCF <ref type="bibr" target="#b22">[23]</ref> have been proposed subsequently and achieved state-of-the-art performance. With more methods developed <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b28">29]</ref>, correlation filter based trackers have proven their high-efficiency and robustness. Especially, a regularization work of correlation filter <ref type="bibr" target="#b11">[12]</ref> was synchronously developed. But different from <ref type="bibr" target="#b11">[12]</ref>, RTT adaptively learns more reliable filters by using RNNs, and meanwhile employs high-efficient optimization for filter learning.</p><p>Besides, some deep learning based tracking methods <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b42">43]</ref> have been proposed more recently. They usually focus on learning more robust features by training Convolutional Neural Networks (CNN) on external large-scale datasets. In contrast, the focus of this work is on the pure tracking task where one can only use the first frame. It is straightforward to incorporate CNN into our framework, e.g., directly replacing HOG (used below) with CNN feature maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Recurrent Neural Network</head><p>Traditional RNNs learn complex temporal dynamics by mapping input sequences to a sequence of hidden states, and <ref type="figure">Figure 1</ref>. Illustration of our proposed RTT tracker. To identify and exploit those reliable components during tracking, a confidence map is estimated by using multi-directional RNNs, and further used to regularize correlation filters. The dash lines denote the work flow of RNNs. The ⊙ is an element-wise multiplication operation. More details are described in Section 3.</p><p>hidden states to outputs via the recurrence equations:</p><formula xml:id="formula_0">h t = σ(W xh x t + W hh h t−1 + b h ), (1) o t = σ(W ho h t + b o ),<label>(2)</label></formula><p>where σ is an element-wise non-linear activation function (e.g., a sigmoid or hyperbolic tangent), x t is the t-th input (frame), h t is the corresponding hidden state, and o t is the predicted output at time t. Given an input sequence</p><formula xml:id="formula_1">{x 1 , x 2 , · · · , x T } with length T , the inference is computed sequentially as {h 1 , o 1 , h 2 , o 2 , · · · , h T , o T } with the ini- tial hidden state h 0 = 0.</formula><p>Recently, RNNs <ref type="bibr" target="#b35">[36]</ref>, including the long-term version called Long Short-Term Memory (LSTM) model <ref type="bibr" target="#b23">[24]</ref> and Gated Recurrent Unit (GRU) <ref type="bibr" target="#b8">[9]</ref>, have attracted increasing attention in modeling sequential data. The applications cover multilingual machine translation <ref type="bibr" target="#b40">[41]</ref>, action recognition <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref>, scene labeling <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b33">34]</ref>, speech recognition <ref type="bibr" target="#b18">[19]</ref>, etc. More recently, the conventional RNNs are generalized into more complex structure models, such as 2D RNNs <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b36">37]</ref>, multi-dimensional RNNs <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b4">5]</ref>, tree RNNs <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b47">48]</ref>, etc. Our method is a simple generalization of their methods, and to the best of our knowledge, it is the first attempt to adapt RNNs to modeling the complex long-range dependencies for the tracking task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Recurrently Target-Attending Tracker</head><p>In this section, first we give an overview of the proposed RTT tracker. Then we illustrate the generation of confidence maps and the learning process of discriminative filters in details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Overview</head><p>An overview on the RTT framework is shown in <ref type="figure">Fig. 1</ref>. Given a video frame, we first determine a small candidate region of 2.5 times the size of the bounding box surrounding the localization result in the previous frame, considering that the motion between continuous video frames is usually subtle. For this candidate region, a grid-like partition is used to produce visual parts, and the feature from each part is extracted for the next tracking. In practice, some descriptors pooled on spatial grids such as HOG <ref type="bibr" target="#b16">[17]</ref> or high-level features from CNN <ref type="bibr" target="#b37">[38]</ref> can be utilized. Thereafter we can obtain the part-based feature X ∈ R h×w×d of d channels for each candidate region, where h, w are the height and the width of spatial parts/grids. RTT attempts to identify those reliable parts and then utilize them for robust tracking. As intimate interactions exist among those spatially adjacent parts and even disjoint parts, between-part relationships can offer valuable contextual information beyond purely relying on a single part. However, the interactions of parts in a 2D space are far more complex than Markov chain structures. Here we employ recurrent neural network to characterize the parts and their complex dependencies, since it is simper and capable of gleaning long-range contextual cues. Moreover, to compensate the inadequacy of a single RNN used in a 2D space, we use several spatial RNNs (e.g., quaddireactional RNNs) to traverse the spatial candidate region from different angles. Such a strategy can effectively alleviate the contamination from partial occlusion or local appearance variation during tracking. The spatial RNNs produces confidence scores for each part, which compose a confidence map for the whole candidate region. The confidence map factually represents the probability of every part being background or target. Thus the confidence map may be used to predict the existence of occlusions and guide the model update. More details about the confidence map generation are described in Section 3.2.</p><p>Furthermore, the confidence map can be incorporated in-to the learning of a discriminative tracker. As the conventional correlation filter trackers usually treat all parts identically, the incremental learning tends to produce results deviating from the expected trajectory due to its sensitiveness to noises from clutter background or occluded regions. Fortunately, the confidence map produced from RNNs can reflect the reliability of candidate region to some extent. Thus the confidence map may be employed to adaptively mask correlation filters to resist those negative effects of cluttered backgrounds or partial occlusions during filter learning. The filter weighting strategy makes RTT more robust to alleviate model drifting due to the use of reliable components during model updating. Similar to those correlation filter based methods, RTT conducts the learning process in the frequency domain. More details about the learning of weighted filters are given in Section 3.3.</p><p>As aforementioned, during model training and updating, RTT learns more discriminative correlation filters by adaptively regularizing the filters with the confidence map. In testing, RTT simply employs the learnt discriminative filters to detect the target as discriminative information has adaptively permeated into the tracker.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Spatial Confidence Map</head><p>The recurrent neural network has the memory ability endowed by its repetitive connections. A popular model is the recently proposed long short-term memory (LST-M) <ref type="bibr" target="#b23">[24]</ref>. However, LSTM has a high-freedom parameter space. Searching parameters in such a large space suffers from the risk of overfitting specifically for the online tracking task, where training samples are usually scarce. To address this problem, here we choose the conventional RNN unit, which has a few model parameters to be solved.</p><p>Concretely, we suppose the candidate parts X are represented by a graph G = {V, E}, where V = {x ij }(i = 1, · · · , h, j = 1, · · · , w) denotes the vertex set of parts indexed by their spatial coordinates, and E = {e ij,kl } represents the edges of spatial neighboring parts. By traversing through G with a predefined forward evolution sequence, we can define the input state and previous states for a recurrent neural network unit. Here the only requirement for traversing is that one node cannot be processed until the processing of its predecessors is finished. Formally, the adopted multidirectional RNNs in RTT can be written as</p><formula xml:id="formula_2">h r ij = σ 1 (U r x ij + ∑ (k,l)∈Nij W r h r kl + b r ), (3) o ij = σ 2 ( ∑ r∈D V r h r ij + b),<label>(4)</label></formula><p>where x ij , h r ij , o ij respectively denote the representation of input, hidden and output node at the location of (i, j), N ij is the set of predecessors of the vertex (i, j) in the graph G, and r represents a traversing direction in D. h r ij collects information of all the predecessors of the current state (i, j), and the output summarizes the stimulus from all directions D. The learned parameters {U r , W r , b r , V r , b} are recurrently utilized in traversing the graph G. Here the non-linear function σ 1 for hidden layers is ReLU <ref type="bibr" target="#b9">[10]</ref>.</p><p>To make the traversing information from traversing processes mutually complementary, we consider four traversing directions starting from four angular points. For example, the directional traversing from the top-left corner is responsible for capturing contextual cues about the topleft areas, with the adjacent predecessor set N ij = {(i, j − 1), (i − 1, j − 1), (i − 1, j)}. Thus, in a 2D spatial plane, by connecting contiguous parts and traversing these parts respectively from four directions, four directed acyclic chains can be generated to represent the 2D neighborhood system. With successive propagations in chains as formulated in Eqn. <ref type="formula">(3)</ref>, the interactions among parts can be achieved.</p><p>To obtain a probability map in the output layer, we use the standard softmax function, i.e., σ 2 (x) = exp x i ∑ k exp x k . Thus the cross entropy loss can be naturally used as the objective function:</p><formula xml:id="formula_3">E = − ∑ (i,j) ∑ c∈C y c ij ln P r(c|x ij ),<label>(5)</label></formula><p>where y ∈ C = {0, 1} is the expected binary indicator of being background or target regions, and P r(·) is the output probability of this model. In model training and updating, we simply assign the label 1 to those parts within the localized bounding box while 0 for outside parts because we do not have accurate labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Weighted Correlation Filter</head><p>The correlation filter based methods are to learn a group of filters {f k }, k = 1, · · · , d, each for one feature channel in X = {x 1 , x 2 , · · · , x d }. The learning of a weighted correlation filter can be formally written as minimizing the following loss function:</p><formula xml:id="formula_4">ς(f ) = ∥ d ∑ k=1 x k * f k − y∥ 2 + d ∑ k=1 ∥w ⊙ f k ∥ 2 ,<label>(6)</label></formula><p>where * denotes a spatial convolution, ⊙ is an elementwise multiplication operation, f k convolves with the k-th channel features, and the weight w regularizes the correlation filters by using the confidence map produced from the multidirectional RNNs. According to Parseval theorem, the objective function in Eqn. (6) is equivalent to the following loss function in the frequency domain:</p><formula xml:id="formula_5">ς( f ) = ∥ d ∑ k=1 x k ⊙ f k − y∥ 2 + λ d ∑ k=1 ∥ w * f k ∥ 2 ,<label>(7)</label></formula><p>where · denotes the FFT of the involved variable, and the constant factor λ is a balance parameter, which is simply set to 1 h 2 w 2 according to the practical implementation of FFT <ref type="bibr" target="#b22">[23]</ref>.</p><p>To vectorize the objective function, we introduce some extra notations. Let X = diag( x) denote the diagonal matrix with diagonal elements from the vector x. W represents a circulant matrix by shifting its basis vector w in each row, with the first row being set to w. Therefore, the objective function in Eqn. <ref type="bibr" target="#b6">(7)</ref> can be written as</p><formula xml:id="formula_6">ς( f ) = ∥ d ∑ k=1 X k f k − y∥ 2 + λ d ∑ k=1 ∥W f k ∥ 2 . (8)</formula><p>However, the above equation is defined in a complex domain, where some theories cannot directly apply from the real space. To transform ς( f ) into the real domain, we decompose each complex value in the vector/matrix into two real values corresponding to its real and imaginary part respectively. Concretely, the matrix and the vector are respectively expanded in the following formulation:</p><formula xml:id="formula_7">   a 11 + b 11 i, · · · a 1n + b 1n i . . . . . . . . . a n1 + b n1 i, · · · a nn + b nn i   →        a 11 , −b 11 , · · · a 1n , −b 1n b 11 , a 11 , · · · b 1n , a 1n . . . . . . . . . . . . . . . a n1 , −b n1 , · · · a nn , −b nn b n1 , a n1 , · · · b nn , a nn        , [ c 11 + d 11 i, · · · c 1n + d 1n i ] ⊺ → [ c 11 , d 11 , · · · c 1n , d 1n ] ⊺ .</formula><p>Thus the objective function in Eqn. <ref type="bibr" target="#b7">(8)</ref> equates with the following function in the real domain:</p><formula xml:id="formula_8">ς( f ) = ∥ d ∑ k=1 X k f k − y∥ 2 + λ d ∑ k=1 ∥ W f k ∥ 2 ,<label>(9)</label></formula><p>where X, W ∈ R 2n×2n , f , y ∈ R 2n are real-valued matrices/vectors corresponding to X, W, f , y, and n = h × w. The loss function in Eqn. <ref type="bibr" target="#b8">(9)</ref> can be further simplified by introducing matrix calculation.</p><p>By defining the concatenation matrix</p><formula xml:id="formula_9">X = [ X 1 , · · · , X d ] ∈ R 2n×2nd , f = [( f 1 ) ⊺ , · · · , ( f d ) ⊺ ] ⊺ ∈ R 2nd , W = diag([ W, W, · · · , W]) ∈ R 2nd×2nd ,</formula><p>where W is a block diagonal matrix with the diagonal element being W, we have a loss function in a more concise form,</p><formula xml:id="formula_10">ς(f ) = ∥Xf − y∥ 2 + λ∥Wf ∥ 2 .<label>(10)</label></formula><p>Obviously, this loss function has a closed-form minimum solution obtained by setting its differential to be zero, i.e.,</p><formula xml:id="formula_11">f = (X ⊺ X + λW ⊺ W) −1 X ⊺ y.<label>(11)</label></formula><p>However, we have to calculate an inverse operation on a real matrix of size 2nd × 2nd , which dominates the computation cost of the optimization. As multiple channel features are usually employed, the computation complexity on the matrix inverse is usually O(n 3 d 3 ), which is too expensive for real-world applications. Fortunately, we develop an efficient solution which only requires computing the inverse of a matrix of size 2n × 2n. It reduces the computation complexity by a factor of d 3 , which is quite significant for high-dimensional features. The solution is presented in the following proposition.</p><formula xml:id="formula_12">Proposition 1. Suppose W is invertible 1 . The optimal so- lution f to Eqn. (9) is f = 1 λ G(I − (λI + H) −1 H) y,<label>(12)</label></formula><formula xml:id="formula_13">where G = [ X 1 ( W ⊺ W) −1 , · · · , X d ( W ⊺ W) −1 ] ⊺ and H = ∑ d k=1 X k ( W ⊺ W) −1 ( X k ) ⊺ . Proof.</formula><p>As the matrix W is invertible, the matrix P = W ⊺ W is symmetrical and positive definite. Performing Singular Value Decomposition (SVD) on P can be written as P = SVS ⊺ , where S is an orthonormal matrix and V is a diagonal matrix with nonnegative elements. Let Q denote the term (X ⊺ X + λW ⊺ W) in Eqn. <ref type="bibr" target="#b10">(11)</ref>, according to general matrix algebras we have</p><formula xml:id="formula_14">Q −1 = (X ⊺ X + λW ⊺ W) −1 = (X ⊺ X + λSVVS ⊺ ) −1 = SV −1 (V −1 S ⊺ X ⊺ XSV −1 + λI) −1 V −1 S ⊺ .<label>(13)</label></formula><p>Let U XSV −1 , by using the Woodbury matrix identity, we have</p><formula xml:id="formula_15">Q −1 = SV −1 (U ⊺ U + λI) −1 V −1 S ⊺ = SV −1 ( 1 λ (I − U ⊺ (λI + UU ⊺ ) −1 U))V −1 S ⊺ .(14)</formula><p>According to the definition of W and X, and ( W ⊺ W) −1 = SV −1 V −1 S ⊺ , we can derive the solution in Eqn. <ref type="bibr" target="#b11">(12)</ref>.</p><p>As X k is produced from a diagonal complex matrix according to the above expansion, it is highly sparse. Thus the matrix multiplication with X k can be implemented by simply switching matrix rows/columns and performing an element-wise multiplication operation. Therefore, the overall computation complexity is O(n 3 +dn 2 ), which is significantly smaller than O(d 3 n 3 ) of directly solving Eqn. (11).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Implementation</head><p>Here we present some implementation details including feature extraction, occlusion decision, scale estimation and model updating.</p><p>Feature Extraction. Here we use a variant of HOG <ref type="bibr" target="#b16">[17]</ref>, which is popular in the tracking task. The HOG features are sampled from a series of spatial grids with 4 × 4 pixels and then are quantized into 31 bins. We do not use other features such as color information or even convolutional features, even though more robust features can promote the tracking performance, since our aim is to explore some intrinsic effects to mitigate model drifting problem in the tracking task.</p><p>Scale Estimation. Similar to <ref type="bibr" target="#b27">[28]</ref>, we use the multiple scale searching technique to estimate changes of the target size, where the scaling factors are defined as <ref type="figure" target="#fig_5">{0.985, 0.99, 0.995, 1.0, 1.005, 1.01, 1.015}</ref>.</p><p>Occlusion Decision. The confidence map produced from spatial RNNs is used to predict the existence of occlusion. When the object is predicted to be occluded with a high probability, the model is not updated. Concretely, we define the entire confidence score as an accumulation of the probability values within the target region. If the current score is less than a certain ratio τ of the average score of previous frames, the current frame is considered to be occluded. In practice, the threshold τ is set to 0.85.</p><p>RNN Training and Updating. The standard Back-Propagation Through Time (BPTT) strategy is used for training RNN. In spatial RNNs, the dimension of hidden layers is the same as the number of channels. As the training samples are insufficient, we employ the first five frames to train spatial RNNs with a learning rate of 0.02. The RNNs are updated in the subsequent frames with a fixed interval of five frames. To avoid over-fitting in the current frame, we employ a small learning rate 0.001 and a few iteration times 100 in fine-tuning. The learning momentum is fixed as 0.9.</p><p>Filter Updating. The update procedure is straightforward except that the first frame is used to initialize the model. Similar to previous correlation filter based methods, we linearly combine the new filter with the old one as below:</p><formula xml:id="formula_16">f = θf new + (1 − θ)f ,<label>(15)</label></formula><p>where the learning factor θ is set to 0.025.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>In the following experiments, two evaluation criteria are used. The first one is mean center location error (CLE), i.e., the difference between ground truth and prediction results. A smaller CLE means a more accurate tracking result. The second one is the Pascal VOC Overlap ratio (VOR) <ref type="bibr" target="#b15">[16]</ref>, which is defined as V OR = Area(B T ∩ B G )/Area(B T ∪ B G ), where B G is the bounding box of ground truth, and B T is the predicted bounding box. A bigger value indicates a more accurate prediction. We employ all the 51 video sequences in the popular benchmark <ref type="bibr" target="#b43">[44]</ref> to extensively evaluate our method. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Appearance Variation-Attending Prediction</head><p>To investigate the effectiveness of multi-directional RNNs for predicting appearance variations, in <ref type="figure" target="#fig_0">Fig. 2</ref> we show two video sequences covering diverse appearance variations including object occlusion, deformation and illumination changes. We take the average value of scores of parts within the bounding box as the response value of the bounding box. To predict the existence of occlusion, we use the average of previous responses as a reference value, as depicted in real lines in <ref type="figure" target="#fig_0">Fig. 2</ref>. The valleys below the moving average lines indicate that there are dramatic appearance variations in the corresponding frame. For example, the two valleys in <ref type="figure" target="#fig_0">Fig. 2(a)</ref> are caused by partial occlusion and glaring lights. Based on the observation, we can conclude that adaptively updating the model according to the current state is necessary for reducing the artifacts incurred by dramatic appearance variations.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Comparison with Correlation Filter Based Trackers</head><p>To evaluate the performance gain of our proposed RTT, we compare six correlation filter based methods, including CN <ref type="bibr" target="#b12">[13]</ref>, CSK <ref type="bibr" target="#b21">[22]</ref>, KCF <ref type="bibr" target="#b22">[23]</ref>, SA <ref type="bibr" target="#b27">[28]</ref>, MF <ref type="bibr" target="#b27">[28]</ref> and SAM-F <ref type="bibr" target="#b27">[28]</ref>. As RTT only employs the HOG feature, SA should be a standard baseline. SA is a scale adaptive correlation filter method that also uses HOG feature. <ref type="figure" target="#fig_2">Fig. 3</ref> shows VOR curves and CEL curves of One-Pass Evaluation (OPE) <ref type="bibr" target="#b43">[44]</ref> for these compared trackers on the benchmark dataset. Although all of these trackers use the circulant filters, their tracking performance is quite different. CSK only employs the raw features, thus it gives worst results among the compared methods. CN exploits the color features and improves the performance. The remaining compared methods adopt the robust HOG feature. Moreover, SAMF and MF also fuse gray and color information as their features. Compared with the standard baseline SA, the performance gains of RT-T are about 5% and 10% respectively in terms of VOR and CEL metrics. Among previous CF based methods, SAMF achieves the best performance with a VOR score of 56.7%  and a CEL score of 77.4%. Our proposed RTT approach outperforms the SAMF tracker by about 2.1% and 4.7% in terms of VOR and CEL curves respectively. The experiments suggest that using reliable part information can improve the tracking performance. <ref type="figure" target="#fig_4">Fig. 4</ref> provides the comparisons between RTT and well-established state-of-the-art methods on the benchmark dataset <ref type="bibr" target="#b43">[44]</ref>, including KCF <ref type="bibr" target="#b22">[23]</ref>, SCM <ref type="bibr" target="#b46">[47]</ref>, STRUCK <ref type="bibr" target="#b20">[21]</ref>, CN <ref type="bibr" target="#b12">[13]</ref>, SAMF <ref type="bibr" target="#b27">[28]</ref>, TGPR <ref type="bibr" target="#b17">[18]</ref>, and DSST <ref type="bibr" target="#b10">[11]</ref>. Only the results of the top five trackers are reported. Correspondingly, the performance on the specific video sequences including object deformation, occlusion, and out-of-plane is shown in <ref type="figure" target="#fig_5">Fig. 5</ref>. From the results, we can observe that the proposed RTT achieves very appealing performance. Especially, the location errors of our proposed RTT are largely reduced on the sequences with dramatic appearance variations, which benefits from regularizing the correlation filter on those reliable parts. As shown in these VOR curves, RTT has slightly inferior performance on estimating accurate bounding boxes with an overlap threshold larger than 0.8. A possible reason may be that the confidence map estimated by multi-directional RNNs incorporates a few errors, which might contaminate the scale estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Comparison with State-of-the-art Trackers</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Discussion</head><p>The proposed model predicts confidence scores on local parts, so it gives a rough probability estimation for the candidate regions. More accurate estimations at finer-grained level such as superpixel level should be meaningful for more robust tracking. A possible direction is to simultaneously perform object segmentation and tracking. However, this will lead to higher computation cost. Currently, our implementation runs about 3∼4 Fps using the non-optimized python code on a general PC (2.80GHz, 16G Memory). The main time cost is still spent on the matrix inversion even though we have reduced the computation complexity by a factor of d 3 . A future work to speed up the algorithm by approximating matrix inversion needs to be explored.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we introduce Recurrently Target-attending Tracker (RTT) to identify and utilize those reliable components and achieve better tracking results. To efficiently find those reliable components, we employ the quaddirectional spatial recurrent neural networks to traverse the whole can-didate region from different angles. Due to modeling local parts and their dependencies, recurrent networks are shown to be able to capture some invariant and reliable information even though partial occlusion exists. The produced confidence map from recurrent neural networks is shown to be effective for predicting the existence of occlusion. At the same time, the confidence map is used to weight the correlation filters during training, which successfully suppresses some clutter background information and makes full use of reliable components. To learn discriminative filters, we provide an accurate analytic solution with low computation complexity. Finally, we obtain encouraging empirical results from our extensive experiments compared with several state-of-the-art trackers.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Examples of confidence maps. The blue line represents the probability of the current bounding box, and the red line denotes the average probability of the previous observation. The valleys below the average line means the possible occurrence of dramatic appearance variations such as occlusion, deformation and illumination changes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Comparisons with correlation filter based methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Comparisons with five state-of-the-art methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>The plot curves on the tracking targets with deformation, occlusion and out-of-plane rotation. Numbers in parentheses are quantity of corresponding videos.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">* This work was performed when Zhen Cui was a Postdoctoral Fellow at National University of Singapore.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">In practice, a small regularization term ϵI may be used to avoid its singularity.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Robust fragmentsbased tracking using the integral histogram</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rivlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Shimshoni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Avidan</surname></persName>
		</author>
		<title level="m">Support vector tracking. T-PAMI</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="1064" to="1072" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Visual tracking with online multiple instance learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Babenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Visual object tracking using adaptive correlation filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Bolme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Beveridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Draper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">M</forename><surname>Lui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Byeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Breuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Raue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liwicki</surname></persName>
		</author>
		<title level="m">Scene labeling with lstm recurrent neural networks. In CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Structured visual tracking with dynamic graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Robust visual tracking using an adaptive coupled-layer visual model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cehovin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kristan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Leonardis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">T-PAMI</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="941" to="953" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">An experimental survey on correlation filter-based tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<idno>arX- iv:1509.05520</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1078</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Improving deep neural networks for lvcsr using rectified linear units and dropout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Accurate scale estimation for robust visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Häger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning spatially regularized correlation filters for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">Shahbaz</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Adaptive color attributes for real-time visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Van De Weijer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.4389</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Hierarchical recurrent neural network for skeleton based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Object detection with discriminatively trained partbased models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1627" to="1645" />
		</imprint>
	</monogr>
	<note type="report_type">T-PAMI</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Transfer learning based visual tracking with gaussian processes regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Speech recognition with deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICASSP</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Offline handwriting recognition with multidimensional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Struck: Structured output tracking with kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saffari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Exploiting the circulant structure of tracking-by-detection with kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caseiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Batista</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Highspeed tracking with kernelized correlation filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caseiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Batista</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">T-PAMI</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="583" to="596" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Visual tracking via adaptive structural local sparse appearance model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Tracking of a non-rigid object via patch-based dynamic appearance modeling and adaptive basin hopping monte carlo sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Highly nonrigid object tracking via patch-based dynamic appearance modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">T-PAMI</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2427" to="2441" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A scale adaptive kernel correlation filter tracker with feature integration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV Workshops</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Reliable patch trackers: Robust visual tracking by exploiting reliable patches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Hoi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Robust tracking using local sparse appearance model and k-selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kulikowsk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Real-time part-based visual tracking via adaptive correlation filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Intelligence</title>
		<imprint>
			<biblScope unit="page">2345390</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Hierarchical convolutional features for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Long-term correlation tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Recurrent convolutional neural networks for scene labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Incremental learning for robust visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R.-S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="125" to="141" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Paliwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2673" to="2681" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Quaddirectional 2d-recurrent neural networks for image labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1990" to="1994" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Parsing natural scenes and natural language with recursive neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Parallel multi-dimensional lstm, with application to fast biomedical volumetric image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Stollenga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Byeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liwicki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.07452</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Improved semantic representations from tree-structured long short-term memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">S</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.00075</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Visual tracking with fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Online object tracking: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Object tracking: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yilmaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Javed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acm computing surveys (CSUR)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">38</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Partial occlusion handling for visual tracking via robust part matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Robust object tracking via sparsity-based collaborative model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Long short-term memory over tree structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sobhani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.04881</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
