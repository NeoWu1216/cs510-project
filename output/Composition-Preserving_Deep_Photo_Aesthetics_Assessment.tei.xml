<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Composition-preserving Deep Photo Aesthetics Assessment</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Mai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Portland State University</orgName>
								<orgName type="institution" key="instit2">Portland State University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hailin</forename><surname>Jin</surname></persName>
							<email>hljin@adobe.com</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Portland State University</orgName>
								<orgName type="institution" key="instit2">Portland State University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adobe</forename><surname>Research</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Portland State University</orgName>
								<orgName type="institution" key="instit2">Portland State University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Liu</surname></persName>
							<email>fliu@cs.pdx.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Portland State University</orgName>
								<orgName type="institution" key="instit2">Portland State University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Composition-preserving Deep Photo Aesthetics Assessment</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:38+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Photo aesthetics assessment is challenging. Deep convolutional neural network (ConvNet) methods have recently shown promising results for aesthetics assessment. The performance of these deep ConvNet methods, however, is often compromised by the constraint that the neural network only takes the fixed-size input. To accommodate this requirement, input images need to be transformed via cropping, scaling, or padding, which often damages image composition, reduces image resolution, or causes image distortion, thus compromising the aesthetics of the original images. In this paper, we present a composition-preserving deep Con-vNet method that directly learns aesthetics features from the original input images without any image transformations. Specifically, our method adds an adaptive spatial pooling layer upon the regular convolution and pooling layers to directly handle input images with original sizes and aspect ratios. To allow for multi-scale feature extraction, we develop the Multi-Net Adaptive Spatial Pooling ConvNet architecture which consists of multiple sub-networks with different adaptive spatial pooling sizes and leverage a scenebased aggregation layer to effectively combine the predictions from multiple sub-networks. Our experiments on the large-scale aesthetics assessment benchmark (AVA [29]) demonstrate that our method can significantly improve the state-of-the-art results in photo aesthetics assessment.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Subjective photo quality and aesthetics assessment is challenging. Existing photo aesthetics assessment methods extract visual features and then employ various machine learning algorithms to predict photo aesthetic values <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b50">51]</ref>. Feature extraction is a critical step for aesthetics assessment. Early methods manually design aesthetics features according to people's aesthetics perception and photography rules <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b16">17]</ref>. Manually designing effective aesthetics features, however, is still a challenging task although these features have shown encouraging results. Other approaches have been developed to leverage more generic im- While padding and scaling keeps the original aspect ratio, it sometimes leads to the loss of the image clarity. In this example, the spots on the ladybug is difficult to see in the padding result. The added boundaries between the image and the padding area can also confuse a deep learning algorithm.</p><p>age features such as Fisher Vector <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b35">36]</ref> and bag of visual words <ref type="bibr" target="#b41">[42]</ref> to predict photo aesthetics. While obtaining promising performance, the image representation provided by those generic features may not be optimal for photo aes-thetics as they are designed to represent natural images in general, not specifically for aesthetics assessment.</p><p>Deep learning methods, which have shown great success in various computer vision tasks, have recently been used to extract effective aesthetics features <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b46">47]</ref>. However, applying existing deep learning algorithms, such as deep convolutional network and deep belief network, to aesthetics feature learning is non-trivial. One major challenge is posed by the fixed input size restriction required by the neural networks. This restriction poses a particular challenge for applying a deep neural network algorithm to aesthetics assessment. To meet this restriction, input images need to be transformed via cropping, scaling, or padding before feeding into the neural network. These transformations often compromise the aesthetics of the original images. As illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>, cropping can sometimes negatively change the image composition, such as turning a well-composed photo in (a) that originally follows rule of thirds into an ill-composed one. Scaling distorts the salient object in (b) and padding plus uniformly scaling reduces the original image resolution and compromises the detail clarity of the important object as shown in (c). Padding also introduces artificial boundaries between the original image and the padding area, and moreover, the locations of these boundaries vary over different images, which could possibly confuse the neural network. Finally, for deep learning, assigning the aesthetics label of an original image to its transformed versions during training will likely make the data more ambiguous and thus compromise the ability of the network to learn good discriminative features.</p><p>Existing methods address this fixed-size restriction by designing dedicated convolutional neural network architectures to simultaneously take multiple versions of the transformed images as input <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref>. These dedicated networks show promising results; however, they still learn from transformed inputs and it is unclear whether the aesthetics labels of the original images can be transferred to the collection of their transformed versions.</p><p>In this paper, we present a deep Multi-Net Adaptive Spatial Pooling Convolutional Neural Network (MNA-CNN) method for photo aesthetics assessment that can directly process the original images without any image transformation. Our method adds an adaptive spatial pooling layer upon regular convolutional and pooling layers. This adaptive spatial pooling layer can handle input images with different sizes and aspect ratios. To allow for multi-scale feature extraction, our deep network architecture consists of multiple sub-networks, each having an adaptive spatial pooling layer with a different pooling size. We further construct a scene-aware aggregation layer to effectively combine the predictions from these multiple sub-networks.</p><p>Our MNA-CNN method has a major advantage in that it can directly handle images with their native sizes and aspect ratios, which is critical for aesthetics assessment. Our study shows that by learning from the original images without any transformations, our MNA-CNN network can learn to capture some subtle compositions that are important for aesthetics. Our method is also capable of extracting features at multiple scales and naturally incorporating scene categories for aesthetics assessment. As shown in our experiments, our method can significantly improve the state-of-the-art results for photo aesthetics assessment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Early methods for image quality assessment measure image quality by detecting and measuring various distortions, including blocking, ringing, mosaic patterns, blur, noise, ghosting, jerkiness, smearing, etc <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b39">40]</ref>. While they are effective for measuring quality loss due to compression or data loss during transmission, these low-level distortion measurement-based metrics sometimes do not well reflect people's subjective perception of image quality.</p><p>Subjective image quality assessment methods have also been developed <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b50">51]</ref>. Many of these methods represent images using manually crafted features that are carefully designed to approximate a number of photographic and psychological aesthetics rules, such as rule of thirds, visual balance, rule of simplicity, etc <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b16">17]</ref>. A classifier is then trained using those features to label an input image as low or high quality. Some other approaches directly use generic image features such as Fisher Vector <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b35">36]</ref> and bag of visual words <ref type="bibr" target="#b41">[42]</ref> to predict photo aesthetics.</p><p>Recently, deep learning methods have shown great success in various computer vision tasks, such as object recognition, object detection, and image classification <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b53">54]</ref>. Deep learning methods, such as deep convolutional neural network and deep belief network, have also been applied to photo quality/aesthetics assessment and have shown good results <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b46">47]</ref>. As most deep neural network architectures require fixed-size inputs, recent methods <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref> transform input images via cropping, scaling, and padding, and design dedicated deep network architectures, such as double-column or multi-column networks, to simultaneously take multiple transformed versions as input. Since transformation often affect the aesthetics quality of the original images as discussed in Section 1, this paper designs a dedicated deep Multi-Net Adaptive Spatial Pooling Convolutional Neural Network (MNA-CNN) architecture that can directly process the images with its native size and aspect ratio, thus preserving the quality of the original images.</p><p>The design of our MNA-CNN network is inspired by the success of the SPP-Net for visual recognition <ref type="bibr" target="#b10">[11]</ref>. Like SPP-Net, our method also constructs an adaptive spatial pooling layer to allow our network to accept as input images at its original size and aspect ratio. Compared to SPP-Net, our method has two main differences. First, unlike SPP-Net that adopts the training strategy that use multiple fixed-size inputs during training, our method allows arbitrary-size input to be used both in training and testing. Second, instead of using the spatial pyramid pooling layer which concatenates adaptive spatial pooling layers with different sizes together, our method contains multiple sub-networks for different pooling sizes. That allows these sub-networks to learn effective feature detectors for images with different resolutions and aspect ratios and at the same time simplify the fully connected layer which makes the network effective even with a limited amount of training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Composition-preserving Deep Network for Photo Aesthetics Assessment</head><p>We first briefly review how the conventional deep convolutional neural network (ConvNet) can be applied to aesthetics assessment and then describe our dedicated deep Multi-Net Adaptive Spatial Pooling ConvNet for photo aesthetics assessment. Background. A deep ConvNet consists of a number of convolutional and pooling layers, followed by some fully connected layers. In this paper, we employ the supervised feature transfer technique that has lead to many successful computer vision applications. Instead of designing and training a new ConvNet from scratch, we reuse a classification ConvNet architecture pre-trained on a large collection of images such as ImageNet <ref type="bibr" target="#b5">[6]</ref>. We then modify the top layer of the network to adapt to our aesthetics classification task. In particular, we modify an ImageNet network by turning their 1000-way softmax prediction layer into a single linear unit followed by a sigmoid activation <ref type="figure">(Figure 2</ref>(a)).</p><p>The resulted ConvNet represents the mapping function f W : I → P (Q I = high|I), where Q I represents the aesthetics quality of the image I. Let f c l (I) be the output of the last fully connected layer, the sigmoid activation unit models the posterior probability of the input image having a high aesthetics quality as</p><formula xml:id="formula_0">P (Q I = high|I) = 1 1 + e −f c l (I)<label>(1)</label></formula><p>The model is trained on a collection of training examples S = {I n , y n }, where y n is the binary aesthetics label (high or low) of the image I n . Let W be the set of weights from all the layers of the network. During training, the optimal value of W is determined by minimizing the following binary cross-entropy objective function typically using a stochastic gradient descend algorithm. Adjustable-sized receptive field</p><p>Fixed output dimension (b) Adaptive spatial pooling layer <ref type="figure">Figure 2</ref>: ConvNet architecture for aesthetics assessment and adaptive spatial pooling layer. Similar to the conventional pooling layer, the adaptive spatial pooling layer (ASP) performs the pooling (e.g. max pooling) operator over local image regions. However, instead of fixing the receptive field's size, ASP fixes the output dimension while adjusting the size of the receptive field to handle images with different sizes and aspect ratios.</p><p>The major challenge in applying existing ConvNets to photo aesthetics assessment is the fixed-size constraint. Due to this restriction, input images need to be transformed to the pre-defined size before given to the network. As discussed in Section 1, such transformation can sometimes severely affect the ability of the network to learn useful features for aesthetics analysis because transforming the images would often compromise the important factors of the image aesthetics perception such as composition, detail clarity, and/or image content.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Composition-preserving Deep ConvNet</head><p>To remove the fixed-size constraint, we employ the adaptive spatial pooling strategy <ref type="bibr" target="#b10">[11]</ref> to enable the ConvNet to operate on an image in its original form during both training and testing. Below, we first describe the concept of adaptive spatial pooling and then elaborate how our method incorporates adaptive spatial pooling and develop a Multi-Net Adaptive-Pooling ConvNet that works with images with their original sizes and aspect ratios.</p><p>Adaptive Spatial Pooling. As discussed in previous work <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b22">23]</ref>, the requirement of fixed-size input imposed by existing ConvNets is due to the last pooling layer of the convolutional structure. The last pooling layer produces the inputs for the subsequent fully-connected layer which demands fixed-size inputs. The main problem is that conventional pooling layers pre-define the size of the local receptive fields (i.e. the local regions on which to pool). The fixed-size receptive field constraint makes the output dimension of a conventional pooling layer depend on its input dimension. As a result, the input image size needs to be fixed in order for the network to generate the input of the dimension required by the fully-connected layers.</p><p>Inspired by the spatial pyramid pooling method <ref type="bibr" target="#b10">[11]</ref>, we relax that problematic fixed-size constraint by employing an alternative pooling strategy: adaptive spatial pooling. As illustrated in <ref type="figure">Figure 2</ref>(b), the adaptive spatial pooling layer performs the pooling operator (e.g. max pooling) over local image regions similarly to the conventional pooling layer. However, different from the conventional pooling layers where the size of the receptive field is fixed, the adaptive spatial pooling layer instead fixes the output dimension, and adjust the receptive field size accordingly. This allows the adaptive spatial pooling layer to generate the fixed-size output from input with various sizes. Any existing ConvNet structure can then be modified to accept arbitrary-size input images by replacing the last conventional pooling layer with the adaptive spatial pooling layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Multi-Net Adaptive-Pooling ConvNet</head><p>The importance of multi-scale feature extraction has been emphasized in various computer vision and deep learning research <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21]</ref>. Using a single pre-defined size for the top pooling layer often restricts the scale at which the lower level features are extracted. The recent SPP-Net method addresses this problem with a spatial pyramid strategy that uses multiple adaptive pooling layers with different sizes and concatenates their outputs <ref type="bibr" target="#b10">[11]</ref>. While such a spatial pyramid pooling method allows multi-scale pooling, it restricts all pooling components to share the same lowerlevel features, which makes it more difficult to learn dedicated features specifically for different pooling layer sizes. More importantly, as the upper fully connected layer is connected to all the pooling layer, it often needs to learn the interactions between them, which makes the learning task more complex and thus requires a large amount of training data. To address these problems, we develop a Multi-Net Adaptive-Pooling method to combine adaptive pooling layers of different sizes.</p><p>Our Multi-Net Adaptive-Pooling ConvNet (MNA-CNN) consists of multiple sub-networks, each of which is a copy of the base network with the last pooling layer replaced by an adaptive spatial pooling layer with a specific scale, as illustrated in <ref type="figure" target="#fig_2">Figure 3</ref>. All sub-networks share the same input image and their outputs are combined with the average operator to obtain the overall prediction. During training, we train each sub-network separately instead of training the whole architecture at the same time so as to minimize the correlation among those networks, which has been shown to improve the ensemble performance <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b54">55]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Scene-Aware Multi-Net Aggregation</head><p>Our MNA-CNN method averages the prediction results of multiple sub-networks as the final output. While taking the average can leverage the complementary among the subnetworks to improve the overall prediction results as shown in Section 4.2, it treats all sub-networks equally regardless of the image content. Previous research has shown that taking the scene category of the image into account can improve the aesthetics prediction accuracy <ref type="bibr" target="#b47">[48]</ref>. Accordingly, we enhance our MNA-CNN method with a learningbased aggregation component to combine the results of subnetworks in a scene-aware manner. Specifically, we augment our MNA-CNN network with a state-of-the-art scene-categorization deep network <ref type="bibr" target="#b53">[54]</ref>. We replace the average operator in the MNA-CNN network with a new aggregation layer that takes the concatenation of the sub-network predictions and the image scenecategorization posteriors as input and output the final aesthetics prediction. <ref type="figure" target="#fig_3">Figure 4</ref> shows our scene-aware MNA-CNN network that implements the scene-aware aggregation component using a fully-connected layer with 50 neurons.  Conceptually the scene-aware aggregation layer can be trained end-to-end along with the sub-networks. In our implementation, we simplify the computational complexity in both training and testing by first training the MNA-CNN sub-networks and then training the aggregation on the validation data with the sub-networks fixed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Scene-CategorizaƟon CNN</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Scene-Aware AggregaƟon layer</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Implementation Details</head><p>Our implementation of the MNA-CNN architecture uses k = 5 adaptive pooling layer sizes. Specifically, we implement our sub-networks with the adaptive pooling layer sizes of 12 × 12, 9 × 9, 6 × 6, 4 × 4, and 2 × 2, respectively. All the network training and testing are done using the Torch deep learning package 1 . The networks are trained with the standard back-propagation algorithm. During training, we fix the learning rate at 0.05 without learning rate shrinkage.</p><p>We use the VGG network (VGG-Net) <ref type="bibr" target="#b40">[41]</ref> pre-trained on the ImageNet dataset as our base network architecture for supervised feature transfer. VGG-Net is one of the state-ofthe-art object-recognition networks that has been adopted with great success to many different computer vision problems. Our experiments show that combining VGG-Net architecture with our MNA-CNN method can significantly improve the aesthetics assessment accuracy compared to the state-of-the-art photo aesthetics methods. The pre-trained VGG network model used in our implementation is obtained from the BVLC CAFFE model zoo 2 .</p><p>For the scene-categorization ConvNet component, we </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We experimented with our method on the AVA benchmark <ref type="bibr" target="#b28">[29]</ref>, which, to our best knowledge, is the largest publicly available aesthetics assessment benchmark. The AVA benchmark provides about 250,000 images in total. The aesthetics quality of each image in the dataset was rated on average by roughly 200 people with the ratings ranging from one to ten, with ten indicating the highest aesthetics quality. For a fair comparison, we use the same partition of training data and testing data as the previous work <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b28">29]</ref>. That is, we allocate 235,599 images for training and 19,930 images for testing.</p><p>We follow the same procedure as the previous work <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b28">29]</ref> to assign a binary aesthetics label to each image in the benchmark. Specifically, images with mean ratings smaller than 5 − δ are labeled as low quality and those with mean ratings larger than or equal to 5 + δ are labeled as high quality. Images in the middle range [5 − δ, 5 + δ] are considered ambiguous and discarded. Two different values of δ: δ = 0 and δ = 1 are used to generate the ground truth labels for the training images and δ = 0 is used for all testing images, as suggested in <ref type="bibr" target="#b28">[29]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Comparison with the State of the Art</head><p>We compare our methods MNA-CNN and scene-aware MNA-CNN to the state-of-the-art methods <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b28">29]</ref>. Here <ref type="bibr" target="#b28">[29]</ref> provides the state-of-the-art result for methods that use manually designed features and/or generic image features for aesthetics assessment. <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b25">26]</ref> are the very recent methods that also design a dedicated deep ConvNet for aesthetics assessment. The results of these methods are obtained from their papers. As shown in <ref type="table" target="#tab_1">Table 1</ref>, both our methods outperform the state-of-the-art methods for aesthetics assessment. The comparisons, especially those between our methods and the existing deep ConvNet methods <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b25">26]</ref>, show that preserving the original image size and aspect ratio can most likely lead to improved aesthetics assessment performance. <ref type="figure" target="#fig_4">Figure 5</ref> shows some examples of the test images that are considered of the highest and lowest aesthetics values by our scene-aware MNA-CNN method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Effectiveness of Adaptive Spatial Pooling</head><p>To examine the effectiveness of the adaptive spatial pooling layers in our composition-preserving deep aesthetics methods, we compare our methods to the baseline methods with fixed-size inputs. In particular, we experiment with three VGG-Net based aesthetics assessment methods, each operating on a different type of transformed input. VGG-Crop: The input of the network is obtained by randomly cropping the original input image with a 224 × 224 cropping window. This cropping window size is the fixed size required by the VGG-Net architecture. During training, we extract five random crops for each image in the training set and train the network on all the crops with their corresponding aesthetics labels. For each testing image, we follow the previous work <ref type="bibr" target="#b24">[25]</ref> to predict the aesthetics quality for 50 random crops obtained from the image and take their average as the final prediction result. VGG-Scale: The input of the network is obtained by scaling the original input image to the fixed size of 224 × 224. Both training and testing are conducted on the scaled version of the input images. VGG-Pad: The original image is uniformly resized such that the larger dimension becomes 224 and the aspect ratio is preserved. The 224 × 224 input is then formed by padding the remaining dimension of the transformed image with zero pixels.</p><p>We also experiment with an alternative compositionaware method that makes use of the spatial pyramid pooling layer (SPP-CNN) <ref type="bibr" target="#b10">[11]</ref> to handle images with different sizes and aspect ratios. We note that different from the method presented in <ref type="bibr" target="#b10">[11]</ref> that only allows an arbitrary-size input during testing, we implement an SPP-CNN network to allow arbitrary-size inputs to be used both during training and testing, which is critical for aesthetics assessment, as discussed in Section 1. <ref type="table" target="#tab_3">Table 2</ref> compares the performance of the above deep network aesthetics assessment methods in terms of three metrics: classification accuracy, F-measure, and area under the ROC curve (AUC score). The accuracy and F-measure of a network are obtained by binarizing the network's outputs with the threshold value of 0.5 and comparing the results to  the gound-truth binary aesthetics labels. The ROC curve is obtained by varying the binarization threshold value from 0 to 1 and computing the true positive rate and false positive rate at each threshold. The area under the ROC curve (AUC) is computed to assess the performance of the network over a wide range of binarization threshold values. All the networks are trained using the training dataset obtained with δ = 0. The results show that both SPP-CNN and MNA-CNN can significantly improve the aesthetics assessment performance over the three fixed-size networks. In addition, our multi-net based architecture MNA-CNN performs better than the spatial pyramid pooling based architecture SPP-CNN. As discussed earlier in Section 3.1, while the SPP-CNN architecture allows multi-scale processing to be performed by the upper fully connected layer of the network, it requires the learning process to capture the complex interaction among different lower pooling layers, which demands a larger amount of data and longer training time to learn successfully. Our MNA-CNN architecture, on the other hand, trains a sub-network for each scale and then aggregate them together, enabling easy training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Composition-preserving Analysis</head><p>It is interesting to examine if our MNA-CNN network has learned to respond to the change in image composition, especially those caused by cropping and scaling. To test this, we collect 20 high-quality images from the AVA benchmark. For each original image, we generate a cropped version where the original image is cropped to its center using a square cropping windows whose side equals to the smaller dimension of the image, and a scaled version where the image is scaled along the longer side to make it square, as illustrated in <ref type="figure" target="#fig_5">Figure 6</ref>. In this way, we have 60 images in total. We then ask five users to rate each of these 60 images with an aesthetics score ranging from 1 to 5, with 5 indicating the highest aesthetics value. We randomize the order of these 60 images and show one image to each user at each time. We average the scores from the five users as the final score for each image. Finally, for each of the 20 original images, we pair it with one of the transformed images and obtain 40 pairs in total. We label a pair of images as desc if the transformed image is rated with a higher score than the original one and asc otherwise.</p><p>We then use our MNA-CNN method to rate each image I with the output probability P (I = high) and ob- tain the desc or asc label for the above 40 pairs according to the predicted scores. We found that for all these 40 pairs of original/transformed images, the labels from our method agree with the ground-truth labels computed from the user scores. This shows that our MNA-CNN is able to reliably respond to the change of image composition caused by cropping or scaling. On the other hand, the baseline deep networks VGG-crop, VGG-pad, and VGG-scale only agree with the user ratings for 59.5%, 61.5%, and 63.1%, respectively. <ref type="figure" target="#fig_5">Figure 6</ref> shows two example images used in the study and their transformed versions, along with the average user given scores and our MNA-CNN predicted posteriors. Automatic cropping. We conduct another study to visually evaluate how our method can be used to guide image cropping. Specifically, given an image, we slide a cropping window through the whole image with the step size of 20 pixels. We then employ our MNA-CNN method to predict the score for each cropping result. <ref type="figure" target="#fig_6">Figure 7</ref> shows some example images and their highest-and lowest-scored cropping results in (c) and (d), respectively. We also create a quality map (b) by assigning the score of each cropping window to the image pixel corresponding to its center. The map is then smoothed for better visualization. The high values in the map indicate the locations on the image that our method suggests a cropping window should be centered at to create a high-quality cropping result. It is interesting to note that our method not only tends to capture important content in the photo (e.g. the human face, the bird's head, and the bridge) but also learns where to position the main subject to create a well-composed cropping result. For example, even with a small cropping window, our method tends to select the cropping window such that the main subject is positioned a little off-center in the resulted image, which agrees with common photography techniques such as rule of thirds <ref type="bibr" target="#b17">[18]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Scene-Aware MNA-CNN Performance</head><p>To examine the effectiveness of the scene-aware aggregation component, we train the MNA-CNN network on half of the available training data, and use the other half to train the scene-based aggregation model as described in Section 3.1.2. <ref type="table" target="#tab_1">Table 1</ref> shows that incorporating the scene category prediction information (MNA-CNN-Scene) improves the aggregation performance of our MNA-CNN network. <ref type="figure" target="#fig_7">Figure 8</ref> further explains the improvement of scenebased aggregation upon our MNA-CNN method. For each of the predicted seven scene categories, this figure shows the performance of each individual sub-network of the MNA-CNN architecture taken independently as well as the performance after aggregation. We can see that the relative performance among the sub-networks vary across different scene categories. Taking the scene prediction as augmented information can therefore help the aggregation model effectively combine the prediction from individual sub-networks to produce the better overall prediction.  Discussion. As expected, padding an image performs better than cropping, as reported in <ref type="table" target="#tab_3">Table 2</ref>. However, it is interesting why padding performs slightly worse than scaling given that it does not crop off or distort image content. We suspect that there are two reasons. First, padding adds artificial boundaries between the real image and the padding areas and the boundaries vary over different images. This could confuse a deep learning algorithm. Second, padding is typically coupled with uniform scaling, which can make the small yet interesting content difficult to appreciate as shown in <ref type="figure" target="#fig_0">Figure 1</ref> (c). While this is not directly related to our method, it will be interesting to study in the future. Our MNA-CNN-Scene model is trained on all the training data and uses the scene information only at the top aggregation layer. Previous studies <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b47">48]</ref> have found it beneficial to learn different aesthetics models specifically for each scene category. Training scene-specific deep neural networks is challenging as it requires a large amount of training data and very accurate scene categorization results. We plan to study this problem in our future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>This paper presents a scene-aware Multi-Net Adaptive Spatial Pooling ConvNet (MNA-CNN) for photo aesthetics assessment. This MNA-CNN deep ConvNet is trained and tested with images at their original sizes and aspect ratios without first transforming them into a fixed size and thus preserves the aesthetics of the original images. This sceneaware MNA-CNN has three enabling features. First, it uses an adaptive spatial pooling layer upon regular convolutional and pooling layers. This adaptive spatial pooling layer has a fixed-size output while having a variable receptive field size to handle images with different sizes and aspect ratios. Second, it uses multiple sub-networks to capture aesthetics features at multiple scales. Finally, it uses a scene-aware aggregation layer to combine these sub-networks into a powerful one. Our experiments on the large-scale AVA benchmark show that our scene-aware MNA-CNN can significantly improve the state of the art in photo aesthetics assessment.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Effect on image transformation on photo aesthetics. (a): Cropping compromises the composition of the originally well-composed image that follows rule of thirds. (b): Scaling distorts the important object. (c):</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>yn log(fW (In)) + (1 − yn) log(1 − fW (In))</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Multi-Net Adaptive-Pooling ConvNet (MNA-CNN). Our MNA-CNN network contains multiple subnetworks, each being a copy of the base network with the last pooling layer replaced by an adaptive spatial pooling layer (ASP) with a specific scale. All sub-networks share the same input image and their outputs are combined with the average operator to obtain the overall prediction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Scene-Aware Multi-Net Aggregation. We augment our network with a scene-categorization deep network. The top-level classifier takes the sub-network predictions and the image scene-categorization posteriors as feature vectors and produce the final aesthetics classification.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Aesthetics quality prediction. The top and the bottom show the images with the highest predicted aesthetics values and those with the lowest predicted aesthetics values in the testing dataset, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Test on composition changes. Each input image is cropped and scaled to change the photo composition. The posterior predicted by our MNA-CNN method and the user score for each image is shown below each image. This test shows that our MNA-CNN method can reliably rate images with different compositions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Automatic cropping. We slide a cropping window through the whole image with the step size of 20 pixels. Each cropping result is scored by our MNA-CNN method. We show the highest rated cropping results in (c) and the lowest-rated cropping results in (d). (b) is a cropping quality map with high values indicating the locations on the image that our method suggests a cropping window should be centered at to create a good cropping result.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Scene-based aggregation effect. This figure shows the performance of each sub-network, the average aggregation result (MNA-CNN) and the scene-aware aggregation result (MNA-CNN-Scene).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 :</head><label>1</label><figDesc>CNN-Scene 76.5% 77.4% Comparison with the state of the art methods. * This result is not reported in the original paper<ref type="bibr" target="#b25">[26]</ref>.</figDesc><table>1 http://torch.ch/ 
2 https://github.com/BVLC/caffe/wiki/Model-Zoo 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Comparison between deep ConvNets with and 
without spatial pooling layers. 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">http://places.csail.mit.edu/downloadCNN.html 4 https://www.flickr.com/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgment. This work was supported by NSF IIS-1321119 and CNS-1218589.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deepedge: A multiscale bifurcated deep network for top-down contour detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A framework for photo-quality assessment and enhancement based on visual aesthetics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimedia</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="271" to="280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">No-reference quality assessment of h. 264/avc encoded video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brandão</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Queluz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="1437" to="1447" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Image quality assessment based on a degradation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Damera-Venkata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Geisler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="636" to="650" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Studying aesthetics in photographic images using a computational approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="288" to="301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009-06" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">High level describable attributes for predicting aesthetics and interestingness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="1657" to="1664" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Ensemble methods in machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">G</forename><surname>Dietterich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First International Workshop on Multiple Classifier Systems</title>
		<meeting>the First International Workshop on Multiple Classifier Systems</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning hierarchical features for scene labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Farabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Najman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1915" to="1929" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1904" to="1916" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Automatic aesthetic value assessment in photographic images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Loui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cerosaletti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Multimedia and Expo (ICME), 2010 IEEE International Conference on</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="920" to="925" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Aesthetics and emotions in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Fedorovskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q.-T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="94" to="115" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
	<note>IEEE</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Recognizing image style</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Trentacoste</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hertzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Winnemoeller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep visual-semantic alignments for generating image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3128" to="3137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1725" to="1732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The design of high-level features for photo quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2006-06" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="419" to="426" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">The Art of Composition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">P</forename><surname>Krages</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<publisher>Allworth Communications, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1106" to="1114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Data-driven depth map refinement via multi-scale sparse representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Visual saliency based on multiscale deep features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Blind image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Image Processing</title>
		<meeting>International Conference on Image Processing</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="449" to="453" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Rating image aesthetics using deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2021" to="2034" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Rapid: Rating pictorial aesthetics using deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Conference on Multimedia</title>
		<meeting>the ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep multipatch aggregation network for image style, aesthetics, and quality estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Photo and video quality evaluation: Focusing on the subject</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="386" to="399" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Assessing the aesthetic quality of photographs using generic image descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Marchesotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Larlus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Csurka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="1784" to="1791" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Ava: A largescale database for aesthetic visual analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Marchesotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Considering temporal variations of spatial visual distortions in video quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ninassi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">Le</forename><surname>Meur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">Le</forename><surname>Callet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Barba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Signal Processing</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="253" to="265" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Aesthetic quality classification of photographs based on color harmony</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nishiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Okabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="33" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">What makes a professional video? a computational aesthetics approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1037" to="1049" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Perceptual quality assessment of video considering both frame rate and quantization artifacts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1" to="1" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deepid-net: Deformable deep convolutional neural networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Fisher kernels on visual vocabularies for image categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dance</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2007-06" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Improving the fisher kernel for large-scale image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="143" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deep learning in neural networks: An overview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1404.7828</idno>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="85" to="117" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Published online. based on. cs.NE</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Overfeat: Integrated recognition, localization and detection using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Motion tuned spatiotemporal quality assessment of natural videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Seshadrinathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="335" to="350" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">No-reference quality assessment using natural scene statistics: Jpeg2000</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cormack</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1918" to="1927" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition. CoRR, abs/1409.1556</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Scenic photo quality assessment with bag of aesthetics-preserving features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Kao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">H</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-Y</forename><surname>Chien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimedia</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1213" to="1216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Deep learning face representation from predicting 10,000 classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Deeply learned face representations are sparse, selective, and robust</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Deepface: Closing the gap to human-level performance in face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Blind image quality assessment using semi-supervised rectifier networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kapoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2877" to="2884" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Content-based photo quality assessment. Multimedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1930" to="1943" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Learning no-reference quality metric by examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Multimedia Modelling</title>
		<meeting>International Conference on Multimedia Modelling</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Image quality assessment: From error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Learning to predict the perceived visual quality of photos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Conputer Vision</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="225" to="232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">3d shapenets: A deep representation for volumetric shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1912" to="1920" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">How transferable are features in deep neural networks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lipson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and K. Weinberger</editor>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3320" to="3328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Learning deep features for scene recognition using places database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Zhou</surname></persName>
		</author>
		<title level="m">Ensemble Methods: Foundations and Algorithms. Chapman &amp; Hall/CRC</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note>1st edition</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
