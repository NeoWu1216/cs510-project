<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Automatic Content-Aware Color and Tone Stylization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon-Young</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<address>
									<country>Kweon KAIST</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adobe</forename><surname>Research</surname></persName>
							<affiliation key="aff0">
								<address>
									<country>Kweon KAIST</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kalyan</forename><surname>Sunkavalli</surname></persName>
							<affiliation key="aff0">
								<address>
									<country>Kweon KAIST</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adobe</forename><surname>Research</surname></persName>
							<affiliation key="aff0">
								<address>
									<country>Kweon KAIST</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
							<affiliation key="aff0">
								<address>
									<country>Kweon KAIST</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adobe</forename><surname>Research</surname></persName>
							<affiliation key="aff0">
								<address>
									<country>Kweon KAIST</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
							<affiliation key="aff0">
								<address>
									<country>Kweon KAIST</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adobe</forename><surname>Research</surname></persName>
							<affiliation key="aff0">
								<address>
									<country>Kweon KAIST</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Automatic Content-Aware Color and Tone Stylization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:37+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce a new technique that automatically generates diverse, visually compelling stylizations for a photograph in an unsupervised manner. We achieve this by learning style ranking for a given input using a large photo collection and selecting a diverse subset of matching styles for final style transfer. We also propose an improved technique that transfers the global color and tone of the chosen exemplars to the input photograph while avoiding the common visual artifacts produced by the existing style transfer methods. Together, our style selection and transfer techniques produce compelling, artifact-free results on a wide range of input photographs, and a user study shows that our results are preferred over other techniques.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Photographers often stylize their images by editing their color, contrast and tonal distributions -a process that requires a significant amount of skill with tools like Adobe Photoshop. Instead, casual users use preset style filters provided by apps like Instagram to stylize their photographs. However, these fixed sets of styles do not work well for every photograph and in many cases, produce poor results.</p><p>Example-based style transfer techniques <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b2">3]</ref> can transfer the look of a given stylized exemplar to another photograph. However, the quality of these results is tied to the choice of the exemplar used, and the wrong choices often result in visual artifacts. This can be avoided in some cases by directly learning style transforms from inputstylized image pairs <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b27">28]</ref>. However, these approaches require large amounts of training data, limiting them to a small set of styles.</p><p>Our goal is to make the process of image stylization adaptive by automatically finding the "right" looks for a photograph (from potentially hundreds or thousands of different styles), and robustly applying them to produce a diverse set of stylized outputs. In particular, we consider stylizations that can be represented as global transformations of color and luminance. We would also like to do this in an unsupervised manner, without the need for input-stylized example pairs for different content and looks.</p><p>We introduce two datasets to derive our stylization technique. The first is our manually curated target style database, which consists of 1500 stylized exemplar images that capture color and tonal distributions that we consider as good styles. Given an input photograph, we would like to automatically select a subset of these style exemplars that will guarantee good stylization results. We do this by leveraging our second dataset -a large photo collection that contains millions of photographs and spans the range of styles and semantic content that we expect in our input photographs (e.g., indoor photographs, urban scenes, landscapes, portraits, etc.). These datasets cannot be used individually for stylization; the style dataset is small and does not span the full content-style space, and the photo collection is not curated and contains both good and poorlystylized images. The key idea of our work is that we can use the large photo collection to learn a content-to-style mapping and bridge the gap between the source photograph and the target style database. We do this in a completely unsupervised manner, allowing us to easily scale to a large range of image content and photographic styles.</p><p>We segment the large photo collection into content-based clusters using semantic features, and learn a ranking of the style exemplars for each cluster by evaluating their style similarities to the images in the cluster. At run time, we determine the semantic clusters nearest to the input photograph, retrieve their corresponding stylized exemplar rankings, and sample this set to obtain a diverse subset of relevant style exemplars.</p><p>We propose a new robust technique to transfer the global color and tone statistics of the chosen exemplars to the input photo. Doing this using previous techniques can produce artifacts, especially when the exemplar and input statistics are very disparate. We use regularized color and tone mapping functions, and use a face-specific luminance correction step to minimize artifacts in the final results.</p><p>We introduce a new benchmark dataset of 55 images with manually stylized results created by an artist. We compare our style selection method with other variants as well as the artist's results through a blind user study. We also evaluate the performance of a number of current statistics-based style transfer techniques on this dataset, and show that our style transfer technique produces better results than all of them. To the best of our knowledge, this is the first extensive quantitative evaluation of these methods.</p><p>The technical contributions of our work include: 1. A robust style transfer method that captures a wide range of looks while avoiding image artifacts, 2. An unsupervised method to learn a content-specific style ranking using semantic and style similarity, 3. A style selection method to sample the ranked styles to ensure both diversity and quality in the results, and 4. A new benchmark dataset with professional stylizations and a comprehensive user evaluation of various style selection and transfer techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Example-based Style Transfer One popular approach for image stylization is to transfer the style of an exemplar image to the input image. This approach was pioneered by Reinhard et al. <ref type="bibr" target="#b21">[22]</ref> who transferred color between images by matching the statistics of their color distributions. There are several subsequent work <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b20">21]</ref> that improves this technique. There are several methods that compute transfer functions from correspondences <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b0">1]</ref>. Please refer to <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b9">10]</ref> for a detailed survey of different color transfer methods. We base our chrominance transfer function on the work of Pitié et al. <ref type="bibr" target="#b17">[18]</ref> but add a regularization term to make it robust to large differences in the color distributions being matched. Style transfer techniques also match the contrast and tone between images. Bae et al. <ref type="bibr" target="#b2">[3]</ref> propose a two-scale technique to transfer both global and local contrast. Aubry et al. <ref type="bibr" target="#b1">[2]</ref> demonstrate the use of local Laplacian pyramids for contrast and tone transfer. Shih et al. <ref type="bibr" target="#b22">[23]</ref> use a multiscale local contrast transfer technique to stylize portrait photographs. We propose a parametric luminance reshaping curve that is designed to be smooth and avoids artifacts in the results. In addition, we propose a face luminance correction method that is specifically designed to avoid artifacts for portrait shots. Learning-based Stylization and Enhancement Another approach for image stylization is to use supervised methods to learn style mapping functions from data consisting of input-stylized image pairs. Wang et al. <ref type="bibr" target="#b24">[25]</ref> introduce a method to learn piece-wise smooth non-linear color mappings from image pairs. Yan et al. <ref type="bibr" target="#b27">[28]</ref> uses deep neural networks to learn local nonlinear transfer functions for a variety of photographic effects. There are also several automatic learning-based enhancement techniques. Kang et al. <ref type="bibr" target="#b14">[15]</ref> present a personalized image enhancement framework using distance metric learning. It was extended by <ref type="bibr" target="#b5">[6]</ref>, which proposes collaborative personalization. Bychkovsky et al. <ref type="bibr" target="#b4">[5]</ref> build a reference dataset of input-output image pairs. Hwang et al. <ref type="bibr" target="#b11">[12]</ref> propose a context-based local image enhancement method. Yan et al. <ref type="bibr" target="#b26">[27]</ref> account for the intermediate decisions of a user in the editing process. While these learning-based methods show impressive adjustment results, collecting training data and generalizing them to a large number of styles is very challenging. In contrast, our technique to learn content-specific style rankings is completely unsupervised and easily generalizes to a large number of content and style classes.</p><p>Our technique is similar in spirit to two papers that leverage large image collections to restore/stylize the color and tone of photographs. Dale et al. <ref type="bibr" target="#b6">[7]</ref> find visually similar images in a large photo collection, and use their aggregate color and tone statistics to restore the input photograph. This aggregation causes a regression to the mean that is appropriate for image restoration but not stylization. Liu et al. <ref type="bibr" target="#b16">[17]</ref> use a user-specified keyword to search for images that are used to stylize the input photo. The final results are highly dependent on the choice of the keyword and it can be challenging to predict the right keywords to stylize a photograph. Our technique automatically predicts the right styles for the input photograph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Overview</head><p>Given an input photograph, I, our goal is to automatically create a set of k stylized outputs O 1 , O 2 , · · · , O k . In particular, we focus on stylizations that can be represented as global transformations of the input color and luminance values. The styles we are interested in are captured by a curated set of exemplar images S 1 , S 2 , · · · , S n , (n &gt;&gt; k). Using images as style examples makes it intuitive for users to specify the looks they are interested in.</p><p>We use an example-based style transfer algorithm to transfer the look of a given exemplar image to the input photograph. While example-based techniques can produce compelling results <ref type="bibr" target="#b9">[10]</ref>, they often cause visual artifacts when there are strong differences in the input and exemplar images being processed. In this work, we develop regularized global color and tone mapping functions (Sec. 4) that are expressive enough to capture a wide range of effects, but sufficiently constrained to avoid such artifacts.</p><p>The quality of the stylized result O is also closely tied to the choice of the exemplar S. Using an outdoor landscape image, for example, to stylize a portrait could lead to poor transfer results (see <ref type="figure" target="#fig_0">Fig. 1(b)</ref>). It is therefore important to choose the "right" set of exemplar images based on the content of the input photograph. We use a semantic similarity metric -that we learned using a convolutional neural network (CNN) -to match images with similar content. Given this semantic similarity measure, one approach would be to use it directly to find exemplar images with content similar to an input photograph and stylize it. However, the curated exemplar dataset is limited and unlikely to contain style examples for every content class. Using the semantic similarity metric to find the closest stylized exemplar to an input photograph will not guarantee a good match, and as illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>(c), could lead to poor stylizations.</p><p>In order to learn a content-specific style ranking, we crawl a large collection of Flickr interesting photos P 1 , P 2 , · · · , P m , (m &gt;&gt; n) that cover a wide range of different content with varying styles and levels of quality. A straightforward way of stylizing an input photograph could be to use the semantic similarity measure to directly find matching images from this large collection and transfer their statistics to the input photograph. However, this large collection of photos is not manually curated, and contains images of both good and bad quality. Performing style transfer using the low-quality photographs in the database can lead to poor stylizations, as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>(d). While these results can be improved by curating the photo collection, this is an infeasible task given the size of the database.</p><p>We leverage the large photo collection to learn a style ranking for each content class in an unsupervised way. We cluster the photo collection into a set of semantic classes using the semantic similarity metric (Sec. 5.1). For each image in a semantic class, we vote for the best matching stylized exemplar using a style similarity metric (Sec. 5.2). We aggregate these votes across all the images in the class to build a content-specific ranking of the stylized exemplars.</p><p>At run time, we match an input photograph to its closest semantic classes and use the pre-computed style ranking for these classes to choose the exemplars. We use a greedy sampling technique to ensure a diverse set of examples (Sec. 5.3), and transfer the statistics of the sampled exemplars to the input photograph using our robust example-based transfer technique. As shown in <ref type="figure" target="#fig_0">Fig. 1(e)</ref>, our style selection technique chooses stylized exemplars that are not necessarily semantically similar to the input photograph, yet have the "right" color and tone statistics to transfer, and produces results that are significantly better than the approaches of directly searching for semantically similar images in the style database or the photo collection. <ref type="figure" target="#fig_2">Fig. 2</ref> illustrates the overall framework of our stylization system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Robust Example-based Style Transfer</head><p>We stylize an input photograph, I, by applying global transforms to match its color and tonal statistics to those of a style example, S. This space of transformations encompasses a wide range of stylizations that artists use, including color mixing, hue and saturation shifts, and nonlinear tone adjustments. While a very flexible transfer model can capture a wide range of photographic looks, it is also important that it can be robustly estimated and does not cause artifacts; this is particularly important in our case, where the images being mapped may differ significantly in their content. With this in mind, we design color and contrast mapping functions that are regularized to avoid artifacts.</p><p>To effectively stylize images with global transforms, we first compress the dynamic ranges of the two images using a γ (= 2.2) mapping and convert the images into the CIELab colorspace (because it decorrelates the different channels  well). Then, we stretch the luminance (L channel) to cover the full dynamic range after clipping both the minimum and the maximum 0.5 percent pixels of luminance levels, and apply different transfer functions to the luminance and chrominance components.</p><p>Chrominance Our color transfer method maps the statistics of the chrominance channels of the two images. We model the chrominance distribution of an image using a multivariate Gaussian, and find a transfer function that creates the output image O by mapping the Gaussian statistics N S (µ S , Σ S ) of the style exemplar S to the Gaussian statistics N I (µ I , Σ I ) of the input image I as:</p><formula xml:id="formula_0">c O (x) = T (c I (x) − µ I ) + µ S s.t. T Σ I T ⊤ = Σ S ,<label>(1)</label></formula><p>where T is a linear transformation that maps chrominance between the images and c(x) is the chrominance at pixel x. Following Pitié and Kokaram <ref type="bibr" target="#b17">[18]</ref>, we solve for the color transform using the following closed form solution:</p><formula xml:id="formula_1">T = Σ −1/2 I Σ 1/2 I Σ S Σ 1/2 I 1/2 Σ −1/2 I .<label>(2)</label></formula><p>This solution is unstable for low input covariance values, leading to color artifacts when the input has low color variation. To avoid this, we regularize this solution by clipping diagonal elements of Σ I as:</p><formula xml:id="formula_2">Σ ′ I = max(Σ I , λ r I),<label>(3)</label></formula><p>and substitute it into Eq. (2). Here I is an identity matrix. This formulation has the advantage that it only regularizes colors channels with low variation without affecting the others. We use a regularization of λ r = 7.5.</p><p>Luminance We match contrast and tone using histogram matching between the luminance channels of the input and style exemplar images. Direct histogram matching typically results in arbitrary transfer functions and may produce artifacts due to non-smooth mapping or excessive stretching/compressing of the luminance values. Instead, we design a new parametric model of luminance mapping that allows for strong expressiveness and regularization simultaneously. Our transfer function is defined as:</p><formula xml:id="formula_3">l O (x) = g(l I (x)) = arctan( m δ ) + arctan( l I (x)−m δ ) arctan( m δ ) + arctan( 1−m δ ) ,<label>(4)</label></formula><p>where l I (x) and l O (x) are the input and output luminance respectively, and m and δ are the two parameters of the mapping function. m determines the inflection point of the mapping function and δ determines the degree of luminance stretching around the inflection point. This parametric function can represent a diverse set of tone mapping curves and we can easily control the degree of stretching/compressing of tone. Since the derivative of Eq. <ref type="formula" target="#formula_3">(4)</ref> is always positive and continuous, it is guaranteed to be a smooth and monotonically increasing curve. This ensures that this mapping function generates a proper luminance mapping curve for any set of parameters.</p><p>We extract a luminance feature, L, that represents the luminance histogram with uniformly sampled percentiles of the luminance cumulative distribution function (we use 32 samples). We estimate the tone-mapping parameters by minimizing the cost function: where L I and L S represent the input and style luminance features, respectively.L is an interpolation of the input and exemplar luminance features and represents how closely we want to match the exemplar luminance distribution. We set τ to 0.4 and minimize this cost using parameter sweeping in a branch-and-bound scheme. <ref type="figure" target="#fig_4">Fig. 3</ref> compares the quality of our style transfer method against three recent methods: the N-dimensional histogram matching technique of Pitié et al. <ref type="bibr" target="#b18">[19]</ref>, the linear Monge-Kantarovich solution of Pitié and Kokaram <ref type="bibr" target="#b17">[18]</ref>, and the three-band method of Bonneel et al. <ref type="bibr" target="#b3">[4]</ref>. While each of these algorithms has its strengths, only our method consistently produces visually compelling results without any artifacts. We further evaluate all these methods via a comprehensive user study in Sec. 6. Face exposure correction In the process of transferring tonal distributions, our luminance mapping method can over-darken some regions. When this happens to faces, it detracts from the quality of the result, as humans are sensitive to facial appearance. We fix this using a facespecific luminance correction. We detect face regions in the input image, given by center p and radius r, using the Haar cascade classifier implemented in the OpenCV. If the median luminance in a face region,l, is lower than a threshold l th , we correct the luminance as:</p><formula xml:id="formula_4">l(x) = (1 − w(x)) · l(x) + w · l(x) γ ifl &lt; l th , w(x) = exp(−α r (x − p)/r 2 ) exp(−α c c −c 2 ), γ = max(γ th ,l/l th ).<label>(6)</label></formula><p>This technique applies a simple γ-correction to the luminance, where γ th determines the maximum level of exposure correction. We would like to apply it to the entire face; however, the face region is given by a coarse box and applying the correction to the entire box will produce artifacts. Instead we interpolate the corrected luminance with the original luminance using weights w(x). We compute these weights based on spatial distance from the face center, and chrominance distance from the median face chrominance value,c (to capture the color of the skin). α r and α c are normalization parameters that control the weights of the spatial and chrominance kernels respectively. We set {l th , γ th , α r , α c } to {0.5, 0.5, 0.45, 0.001}. <ref type="figure">Fig. 4</ref> shows an example of our face exposure correction results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Content-aware Style Selection</head><p>Given the target style database 1 , we can use the method described in Sec. 4 to transfer the photographic style of a style exemplar to an input photograph. However, as noted in Sec. 3 and illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>, it is important that we choose the right set of style exemplars. Motivated by the fact that images with different semantic content require different styles, we attempt to learn the set of good styles (or their 1 a curated dataset of 1500 exemplar style images ranking) for each type of semantic content separately.</p><p>To achieve this, we prepare a large photo collection consisting of one million photographs downloaded from Flickr's daily interesting photograph collection 2 . As noted in Sec. 3, the curated style dataset does not contain examples for all content classes and cannot be directly used to stylize a photograph. However, by leveraging the large photo collection, we can learn style rankings of the curated style dataset even for content classes that are not represented in it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Semantic clustering</head><p>Inspired by recent breakthroughs in the use of CNN <ref type="bibr" target="#b15">[16]</ref>, we represent the semantic information of an image using a CNN feature, trained on the ImageNet dataset <ref type="bibr" target="#b7">[8]</ref>. We modified the CaffeNet <ref type="bibr" target="#b13">[14]</ref> to have fewer nodes in the fully-connected layers 3 and fine-tuned the modified network. This results in a 512-dimensional feature vector for each image. We empirically found that this smaller CNN captures more style diversity in each content cluster compared to the original CaffeNet or AlexNet <ref type="bibr" target="#b7">[8]</ref> which sometimes "oversegments" content into clusters with low style variation.</p><p>We perform k-means clustering on the CNN feature vectors for each image in the large photo collection to obtain semantic content clusters. A small number of clusters leads to different content classes being grouped in the same cluster, while a large number of clusters lead to the style variations of the same content class of images being split into different clusters. In our experiments, we found that using 1000 clusters was a good balance between these two aspects. <ref type="figure" target="#fig_5">Fig. 5</ref> shows images from four different semantic clusters. The images in a single cluster share semantically similar content but have diverse appearances (including both good and bad styles). These intra-class style variations allow us to learn the space of relevant styles for each class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Style ranking</head><p>To choose the best style example for each semantic cluster, we compute style similarity between each style example and the images in a cluster, and use this measure to rank the styles for that cluster. As explained in Sec. 4, we represent a photograph's style using chrominance and luminance statistics. Following this, we define the style similarity measure between cluster photograph P and style image S as:</p><formula xml:id="formula_5">R(P, S) = exp − De(LP ,LS ) 2 λ l exp − D h (NP ,NS ) 2 λc ,<label>(7)</label></formula><p>where D e represents the Euclidean distance between the two luminance features, and λ l and λ c are normalization parameters. We set λ l = 0.005 and λ c = 0.05 to generate all our results. D h is the Hellinger distance <ref type="bibr" target="#b19">[20]</ref> defined as:</p><formula xml:id="formula_6">D h (N P , N S ) = 1 − |Σ P Σ S | 1/4 |Σ| 1/2 exp − 1 8μ ⊤Σ−1μ s.tμ = |µ P − µ S | + ǫ,Σ = Σ P +Σ S 2 ,<label>(8)</label></formula><p>where N P = (µ P , Σ P ) are the multivariate Gaussian statistics of chrominance channel for an image. We chose the Hellinger distance to measure the overlap between two distributions because it strongly penalizes large differences in covariance even if the means are close enough. ǫ = 1 is added to the difference between the means to additionally penalize small covariance images.</p><p>We measure the compatibility of a stylized exemplar S, with a semantic cluster C K , by aggregating the style similarity measure over all the images in the cluster as R K (S) = P ∈C K R(P, S).</p><p>For each semantic cluster, we computeR for all the style exemplars and determine the style example ranking by sortingR in decreasing order. This voting scheme measures how often a particular exemplar's color and tonal statistics occurs in the semantic cluster. Poorly stylized cluster images are implicitly filtered out because they do not vote for any style exemplar. Meanwhile, well stylized images in the cluster vote for their corresponding exemplars, giving us a "histogram" of the style exemplars for that cluster. <ref type="figure" target="#fig_2">Figs. 2 and 6</ref> show the results of each stage of our stylization pipeline. As these figures illustrate, our semantic similarity term is able to find clusters with semantically similar content (see <ref type="figure">Fig. 6(b)</ref>). Our technique does not require the selected style exemplars to be semantically similar to the input image (see <ref type="figure">Fig. 6(c)</ref>). While this might seem counter-intuitive, the final stylized results do not suffer from any artifacts because the highly-ranked styles have the same style characteristics as a large number of "auxiliary exemplars" in the training photo collection that, in turn, share the same content as the input (see <ref type="figure">Fig. 6(d)</ref>). This is an important property of our style selection scheme, and is what allows it to generalize a small style dataset to arbitrary content.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Style sampling</head><p>Given an input photograph, we can extract its semantic feature and assign it to the nearest semantic cluster. We can retrieve the pre-computed style ranking for this cluster and use the top k style images to create a set of k stylized renditions of the input photograph. However, this strategy could lead to outputs that are similar to each other. In order to improve the diversity of styles in the final results, we propose the following multi-cluster style sampling scheme. Adjacent semantic clusters usually share similar highlevel semantics but different low-level features such as object scale, color, and tone. Therefore we propose using multiple nearest semantic clusters to capture more diversity. We merge the style lists for the chosen semantic clusters and order them by the aggregate similarity measure (Eq. (9)). To avoid redundant styles, we sample this merged style list in order (starting with the top-ranked one) and discard styles that are within a specified threshold distance from the styles that have already been chosen.</p><p>We define a new similarity measure for this sampling process that computes the squared Fréchet distance <ref type="bibr" target="#b8">[9]</ref>:</p><formula xml:id="formula_8">D f (N P , N Q ) = µ P − µ Q 2 + tr[Σ P + Σ Q − 2(Σ P Σ Q ) 1/2 ].</formula><p>We use this distance because it measures optimal transport between distributions and is more perceptually linear. We use three semantic clusters and set the threshold to 7.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Results and Discussion</head><p>We have tested our automatic stylization results on a wide range of input images, and show a subset of our results in Figs. 1, 2, 6, and 7. Please refer to the supplementary material and video for more examples, comparisons, and a real-time demo of our technique. As can be seen from these results, our stylization method can robustly capture fairly aggressive visual styles without creating artifacts, and is able to generate diverse stylization results. Figs. 1, 2, 6, and 7 also show the automatically chosen style examples that were used to stylize the input photographs. As expected, in most cases, the style examples chosen have different semantics from the input image, but the stylizations are still of high-quality. This verifies the advantage of our method when given only a limited set of stylized exemplars. User study Due to the subjective nature of image stylization, we validated our stylization technique through user studies that evaluate our style selection and style transfer strategies. For the study, we created a benchmark dataset of 55 images -50 images were randomly chosen from the FiveK dataset <ref type="bibr" target="#b4">[5]</ref> and the rest were downloaded from Flickr. We resized all test images to 500-pixels wide on the long edge and stored them using an 8-bit sRGB JPEG format.</p><p>We asked a professional artist to create five diverse stylizations for every image in our benchmark dataset as a baseline for evaluation. The artist was told to only use tools that globally edit the color and tone; he used the 'Levels', 'Curves', 'Exposure', 'Color Balance', 'Hue/Saturation', 'Vibrance', and 'Black and White' tools in Adobe Photoshop. Creating five different looks for every photograph is challenging even for professional artists. Instead, our artist first constructed 27 different looks, each of which evoked a particular theme (like 'old photo', 'sunny', 'romantic', etc.), applied all of them to all the images in the dataset, and picked the five diverse styles that he preferred the most.</p><p>We performed two user studies. In Study 1, we evaluated two style selection methods, our style selection and direct semantic search which directly searches for semantically similar images in the style database. We also explored directly searching in the photo collection using semantic similarity, but its results were consistently poor, which led us to drop this selection method in the larger study. To assess the effect of the size of the style database on the selection algorithm, we tested against two style databases: the full database with 1500 style exemplars, and a small database with 50 style exemplars randomly chosen from the full database.</p><p>We compared five different groups of stylization results including: the reference dataset retouched by a professional (henceforth, PRO), our style selection with the full style database (OURS 1500) and the small style database (OURS 50), direct semantic search on the full style database (DIRECT 1500) and the small style database (DIRECT 50). For both our style selection and direct semantic search, we apply the same style sampling in Sec. 5.3 to achieve the similar levels of style diversity and create the results using the same style transfer technique (Sec. 4). Please see the supplementary material for all these results.</p><p>For each image in the benchmark dataset, we showed users five groups of five stylized results (one set each from OURS 1500, OURS 50, DIRECT 1500, DIRECT 50, and PRO). Users were asked to rate the stylization quality of each group of results on a five-point Likert scale ranging from 1 (worst) to 5 (best). A total of 37 users participated in this study, and a total of 1498 different image groups were rated, giving us an average of 27.24 ratings per group. <ref type="figure">Fig. 8(a)</ref> shows the result of Study 1. In this study, OURS 1500 (3.820 ± 0.403) outperforms all the other techniques. We reported the mean of all user ratings and the standard deviation of the average scores of each of the 55 benchmark images. DIRECT 1500 (3.169 ± 0.444) is substantially worse than OURS 1500. When the style database becomes smaller, the performance of direct search drops dramatically (2.421 ± 0.436 for DIRECT 50) while our style selection stays stable (3.620 ± 0.413 for OURS 50). We believe that this is a result of our novel two-step style ranking algorithm that is able to learn the mapping between semantic content and style even with very few style examples. On the other hand, direct search fails to find good semantic matches when the size of the style database is reduced significantly. Interestingly, we found that even when direct search finds a semantically meaningful match, this does not guarantee a good style transfer result. An example of this is shown in <ref type="figure" target="#fig_8">Fig. 9</ref>, where the green in the background of the exemplar image influences the global statistics and causes the girl's skin to take on an undesirable green tone. Our technique aggregates style similarity across many images giving it robustness to such scenarios.</p><p>It is also worth noting that PRO (2.881 ± 0.480) got a lower mean score than {OURS 1500, OURS 50, DIRECT 1500} with the largest standard deviation of scores. We attribute this to two reasons. First, the artist-created filters do not adapt to the content of the image in the same way our example-based style transfer technique does. Second, image stylization tends to be subjective in nature; some of users might be uncomfortable with the aggressive stylizations of a professional, while our style selection is learned from a more 'natural' style database and does not have the same level of stylization.</p><p>In Study 2, we compare our style transfer technique with four different statistics-based style transfer techniques: MK, which computes an affine transform in CIELab <ref type="bibr" target="#b17">[18]</ref>, SMH, which combines three different affine transforms in different luminance bands with a non-linear tone curve <ref type="bibr" target="#b3">[4]</ref>, PDF, which use 3-d histogram matching in CIELab <ref type="bibr" target="#b18">[19]</ref>, and PHR, which progressively reshapes the histograms to make them match <ref type="bibr" target="#b20">[21]</ref>. We used our implementation for the MK method and used the original authors' code for the other methods. Style exemplars are chosen by our style selection and these methods are used only for the transfer. We showed users an input photograph, an exemplar, and a randomly arranged set of five stylized images created using the techniques, and asked them to rate the results in terms of style transfer and visual quality on a five-point Likert scale ranging from 1 (worst) to 5 (best). 27 participants from the same pool as (Study 1) participated in this study; they rated 1554 results in total giving us 5.65 ratings per input-style pair and 28.25 rating per input. <ref type="figure">Fig. 8(b)</ref> shows the result of Study 2. In this study, OURS (4.002 ± 0.336) records the best rating, while MK (3.730 ± 0.440) is ranked second. SHM (2.949 ± 0.545), PDF (2.494 ± 0.577), and PHR (2.286 ± 0.452) are less favored by users. These three techniques have more expressive color transfer models leading to over-fitting and poor results in many cases. This demonstrates the importance of the style transfer technique for high-quality stylization; our technique balances expressiveness and robustness well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>In this work, we have proposed an automatic technique to stylize photographs based on their content. Given a set of target photographic styles, we leverage a large collection of photographs to learn a content-specific style ranking in a completely unsupervised manner. At run-time, we use the learned content-specific style ranking to adaptively stylize images based on their content. Our technique produces a diverse set of compelling, high-quality stylized results. We have extensively evaluated both style selection and transfer components of our technique and studies show that users clearly prefer our results over other variations of our pipeline.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Stylization results with different choices of the exemplar images. All exemplars are shown in insets in the top-left corner.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>The overall framework of our system.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>(m,δ) = arg min m,δ g(L I ) −L 2 , s.t.L = L I + (L S − L I ) τ min(τ,|L S −L I |∞) , (5)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 .</head><label>3</label><figDesc>Examples of our style transfer results compared with previous statistics-based transfer methods. Exemplars are shown in insets in the top-left corner of input images. (a) Input photograph (b) Our stylization (c) Our stylization and face correction Figure 4. Face exposure correction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Examples of semantic clusters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Images in the cluster with the highest votes for the chosen exemplars in (c) (e) Stylization results by transferring the styles of the exemplars in (c)Figure 6. Intermediate steps of style selection. The input (a) can be semantically different from the selected exemplars (c) (second and third example especially). However, the cluster images with the highest votes for these style exemplars (d), are both semantically similar to the input and stylistically similar to the chosen exemplars. This ensures input-exemplar compatibility and leads to artifact-free stylizations (e).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 .Figure 8 .</head><label>78</label><figDesc>Our stylization results. The left most images are input photographs and the right images are our automatically stylized results. Results of our two user studies.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 .</head><label>9</label><figDesc>Failure case of direct search.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://www.flickr.com/services/api/flickr.interestingness.getList.html<ref type="bibr" target="#b2">3</ref> We reduced the number of nodes in the FC6 layer from 4096 to 512 and removed the FC7 layer.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">User-controllable color transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pellacini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CGF</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="263" to="271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Fast local laplacian filters: Theory and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aubry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Paris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Hasinoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
		<idno>167:1-167:14</idno>
		<imprint>
			<date type="published" when="2002" />
			<publisher>ACM TOG</publisher>
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Two-scale tone management for photographic look</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Paris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM TOG (Proc. SIGGRAPH)</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="637" to="645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Examplebased video color grading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bonneel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sunkavalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Paris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pfister</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM TOG (Proc. SIGGRAPH)</title>
		<imprint>
			<date type="published" when="2013-07" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning photographic global tonal adjustment with a database of input/output image pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Bychkovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Paris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="97" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Collaborative personalization of image enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Caicedo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">B</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Image restoration using online photo collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sunkavalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Matusik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pfister</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="2217" to="2224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The fréchet distance between multivariate normal distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dowson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Landau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Multivariate Analysis</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="450" to="455" />
			<date type="published" when="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A survey of color mapping and its applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Faridul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pouli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chamaret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Stauder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Trémeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Reinhard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eurographics</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="43" to="67" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Non-rigid dense correspondence with applications for image enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hacohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Goldman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM TOG (Proc. SIGGRAPH)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page">70</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Context-based automatic local image enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">B</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">7572</biblScope>
			<biblScope unit="page" from="569" to="582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Color transfer using probabilistic moving least squares</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Caffe: An open source convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<ptr target="http://caffe.berkeleyvision.org/" />
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Personalization of image enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">B</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1799" to="1806" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Autostyle: Automatic style transfer from image collections to users&apos; images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Uyttendaele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rusinkiewicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CGF</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="21" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The linear monge-kantorovitch linear colour mapping for example-based colour transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pitié</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kokaram</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVMP</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">N-dimensional probability density function transfer and its application to color transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pitié</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Kokaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dahyot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1434" to="1439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">A user&apos;s guide to measure theoretic probability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pollard</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<publisher>Cambridge University Press</publisher>
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Progressive color transfer for images of arbitrary dynamic range</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pouli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Reinhard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comp. &amp; Graph</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="67" to="80" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Color transfer between images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Reinhard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ashikhmin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gooch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shirley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CG&amp;A</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="34" to="41" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Style transfer for headshot portraits</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Paris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
		<idno>148:1-148:14</idno>
	</analytic>
	<monogr>
		<title level="m">ACM TOG (Proc. SIGGRAPH)</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Soft color segmentation and its applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-K</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1520" to="1537" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Example-based image color and tone style enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-Q</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM TOG (Proc. SIGGRAPH)</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2002" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page">64</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Performance evaluation of color correction approaches for automatic multi-view image and video stitching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mulligan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="263" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A learning-torank approach for image color enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">B</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2987" to="2994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Automatic photo adjustment using deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Paris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<idno>abs/1412.7725</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
