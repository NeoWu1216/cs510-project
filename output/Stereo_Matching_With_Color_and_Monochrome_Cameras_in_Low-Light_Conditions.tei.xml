<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Stereo Matching with Color and Monochrome Cameras in Low-light Conditions</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hae-Gon</forename><surname>Jeon</surname></persName>
							<email>hgjeon@rcv.kaist.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Robotics and Computer Vision Lab</orgName>
								<orgName type="institution">KAIST</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon-Young</forename><surname>Lee</surname></persName>
							<email>jolee@adobe.com</email>
							<affiliation key="aff1">
								<orgName type="department">Adobe Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunghoon</forename><surname>Im</surname></persName>
							<email>shim@rcv.kaist.ac.krhwha@rcv.kaist.ac.kriskweon@kaist.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Robotics and Computer Vision Lab</orgName>
								<orgName type="institution">KAIST</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyowon</forename><surname>Ha</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Robotics and Computer Vision Lab</orgName>
								<orgName type="institution">KAIST</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">In</forename><forename type="middle">So</forename><surname>Kweon</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Robotics and Computer Vision Lab</orgName>
								<orgName type="institution">KAIST</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Stereo Matching with Color and Monochrome Cameras in Low-light Conditions</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:46+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Consumer devices with stereo cameras have become popular because of their low-cost depth sensing capability. However, those systems usually suffer from low imaging quality and inaccurate depth acquisition under low-light conditions. To address the problem, we present a new stereo matching method with a color and monochrome camera pair. We focus on the fundamental trade-off that monochrome cameras have much better light-efficiency than color-filtered cameras. Our key ideas involve compensating for the radiometric difference between two crossspectral images and taking full advantage of complementary data. Consequently, our method produces both an accurate depth map and high-quality images, which are applicable for various depth-aware image processing. Our method is evaluated using various datasets and the performance of our depth estimation consistently outperforms state-of-the-art methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Stereo camera systems allow us to estimate depth information and have many advantages over active range sensors such as ToF (time-of-flight) cameras and laser scanners. Stereo cameras are cost-effective and can work in both indoor and outdoor environments. This is why they have been widely used in the computer vision and robotics fields for several decades. Recently, consumer devices having a stereo camera <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4]</ref> have been released for depth-aware image editing applications.</p><p>Despite the advantages, estimating accurate depth map in low-light conditions leads to severe image noise; thus is still challenging and limits the usefulness of stereo systems. Although a long exposure time or a flash light may alleviate the problem, they can induce other imaging problems such as motion blur or specular reflections <ref type="bibr" target="#b18">[19]</ref>. To overcome these issues, multi-modal and multi-spectral imaging approaches such as a color and infrared camera pair <ref type="bibr" target="#b15">[16]</ref> and cross-channel matching <ref type="bibr" target="#b25">[26]</ref> have been proposed. However, (a) A pair of color and monochrome images (b) Estimated disparity map (c) Recovered color image (d) Image refocusing (e) Image stylization <ref type="figure">Figure 1</ref>. Given the pair of images (a), we estimate the accurate disparity map (b) and recover the high-quality color image (c). Our result is applicable to various depth-aware image processing (d, e). these approaches require high manufacturing cost and specialized hardware.</p><p>In this paper, we present a stereo matching framework with a color and monochrome image pair ( <ref type="figure" target="#fig_3">Fig. 1(a)</ref>). Our system is designed to estimate an accurate depth map under low-light conditions without additional light sources ( <ref type="figure">Fig. 1(b)</ref>). In order to obtain reliable correspondence, we exploit the fundamental trade-off between color sensing capability and light efficiency of color cameras and monochrome cameras, respectively. Because monochrome cameras respond to all colors of light, they have much better light efficiency than Bayer-filtered color cameras <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b2">3]</ref>.</p><p>In general, image luminance recorded from a color camera is not consistent with that from a monochrome camera due to spatially-varying illumination and different spectral sensitivities of the cameras. This degrades the performance of stereo matching. To solve this problem, we sample appropriate decolorization parameters of a color image and perform locally adaptive radiometric alignment and correspondences augmentation iteratively. After estimating a depth map, we recover a high-quality color image by colorizing the image from a light-efficient monochrome camera. For accurate colorization, we introduce a local chrominance consistency measure. Herein, we demonstrate the superior stereo matching performance over state-of-theart methods. In addition, we show our result can be applied to depth-aware image processing algorithms such as image stylization and digital refocusing ( <ref type="figure">Fig. 1(c)</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Our method is related to cross-spectral stereo matching and colorization. Prior to introducing previous studies, we refer the reader to <ref type="bibr" target="#b13">[14]</ref> for a comprehensive discussion of stereo matching with radiometric and noise variation.</p><p>Cross-spectral stereo matching has been studied extensively to find correspondence between multi-modal and color-inconsistent stereo images. Heo et al. <ref type="bibr" target="#b11">[12]</ref> analyzed a color formation model and proposed an adaptive normalized cross correlation for stereo matching, that would be robust to various radiometric changes. This was extended in <ref type="bibr" target="#b12">[13]</ref>, which presented an iterative framework to simultaneously achieve both depth estimation and color consistency. Pinggera et al. <ref type="bibr" target="#b22">[23]</ref> presented depth map estimation with cross-spectral stereo images, which uses dense gradient features based on the HOG descriptor <ref type="bibr" target="#b8">[9]</ref>. Kim et al. <ref type="bibr" target="#b17">[18]</ref> designed a dense descriptor for multimodal correspondences by leveraging a measure of adaptive self-correlation and randomized receptive field pooling. Holloway et al. <ref type="bibr" target="#b14">[15]</ref> proposed an assorted camera array and a cross-channel point correspondence measure using normalized gradient cost.</p><p>Colorization is a process of adding color channels to a grayscale image and video. Levin et al. <ref type="bibr" target="#b20">[21]</ref> presented a user-guided colorization method that takes partial color information from user scribbles and automatically propagates the given seed color to make a complete color image. Yatziv and Sapiro <ref type="bibr" target="#b29">[30]</ref> proposed a fast colorization method using the geodesic distance between neighboring pixels. Gastal and Oliveira <ref type="bibr" target="#b9">[10]</ref> introduced an edge-aware filter in a transformed domain and showed a colorization result as one of its applications. Irony et al. <ref type="bibr" target="#b16">[17]</ref> proposed an examplebased colorization based on an assumption that similarly textured regions have similar colors.</p><p>In this study, we focused on simultaneously reconstructing an accurate depth-map and a noise-free color image using a color + monochrome image pair. We achieve this by taking the advantage of our cross-spectral stereo system. We created a locally adaptive spectral alignment algorithm that allows us to estimate an accurate disparity map without complex optimizations for intensity  <ref type="figure">Figure 2</ref>. (a) Spectral sensitivity of the color and monochrome camera <ref type="bibr" target="#b0">[1]</ref> used in our prototype stereo system. (b) An example image pair captured by our stereo system. Note that there is visible difference of image noise due to the gap of light-efficiency between two cameras.</p><p>equalization. As will be demonstrated in the experimental section (Sec. 7), our method is highly effective for accurate disparity estimation and significantly outperforms the stateof-the-art algorithms <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b14">15]</ref>. In colorization, most approaches concentrate on propagating limited numbers of user-defined seeds, while we have lots of seed pixels with outliers around occlusion boundaries. To handle this issue, we introduced a new weighting term to correct inaccurate seed pixels and successfully recover a high-quality color image.</p><p>Recent work by <ref type="bibr" target="#b6">[7]</ref> presented the concept of an alternative camera sensor that samples color information very sparsely. They recover a full color image by propagating the sparsely sampled colors into an entire image. This work shares the same philosophy with our work that takes the advantage of light-efficient monochrome sensors, but the concept may suffer from color noise that leads to an erroneous color image. Moreover, we adopt the idea in a stereo system and obtain an accurate depth-map and a noisefree color image simultaneously.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Stereo System with Color and Mono Cameras</head><p>Most color cameras use a color filter array called a Bayer array to capture color information. The Bayer array is positioned over the pixels of an image sensor and separates the incoming light into one of three primary colors (red, green, or blue) by filtering the light spectra according to wavelength range. This process is effective for capturing color information, but it amplifies image noise under low-light conditions because the array occludes a lot of incoming light. It may also reduce image sharpness as a result of using an anti-aliasing filter or optical low-pass filter to avoid aliasing or moiré artifacts during the demosaicing process.</p><p>Unlike color cameras, monochrome cameras receive all the incoming light at each pixel and need no demosaicing process. Therefore, they have much better light efficiency  and provide shaper images. In <ref type="figure">Fig. 2</ref>, we compare the imaging quality of a color and a monochrome camera. The comparison of spectral sensitivity ( <ref type="figure">Fig. 2</ref>(a)) and the example image-pair captured under the same conditions ( <ref type="figure">Fig. 2</ref>(b)) prove the large difference in light efficiency and image quality between the two types of cameras. That is, a color + monochrome camera pair is highly suitable for achieving a noise-free color image in addition to accurate depth estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Overview</head><p>Our stereo setup is a kind of cross-spectral system that includes a color and a monochrome (RGB-W) camera. In this section, we present an overview of our framework for the RGB-W stereo setup. Our key ideas are compensating for the spectral/radiometric difference between RGB-W images by locally adaptive radiometric alignment, and aggregating reliable correspondences by robust and noisetolerant stereo matching. From these, we are able to utilize a disparity map to reconstruct high-quality color and depth images with applications to depth-aware image processing. <ref type="figure" target="#fig_2">Fig. 3</ref> depicts an overview of our framework. To account for spectral differences between a pair of RGB-W images, we first decolorize the color input image (Sec. 5.1). Because two cameras have different spectral sensitivities and viewpoints, there is no global mapping function that can explain the radiometric difference between two images without any assumptions. Moreover, estimation of local mapping functions is unstable and tends to diverge without reliable and dense correspondences. Instead, we use a candidate set of global decolorization functions that serve to preserve contrast distinctiveness and to suppress noise amplification simultaneously.</p><p>Then, we estimate disparities based on brightness constancy and edge similarity constraints, and each constraint term is designed to be robust to image noise and non-linear intensity changes, respectively (Sec. 5.2). Because the estimated disparity may contain outliers due to radiometric difference or image noise, we retain reliable correspondences with a left-right consistency check and aggregate them from all candidate decolorized images.</p><p>After that, we have a set of reliable correspondences and use them to augment additional correspondences by iterative gain compensation and disparity estimation. Given the grayscale input and aligned decolorized image, we match the brightness of the input image to the decolorized image by estimating a local gain map (Sec. 5.3). Because our decolorization is performed to preserve the contrast distinctiveness of a color image, it can capture important local edges better than the grayscale input image, where edges may be ambiguous due to the mixing of spectral information. Therefore, this iterative process provides increases the number of reliable correspondences.</p><p>Last, we fuse the RGB-W stereo input image with the estimated disparity map and obtain a high-quality color image (Sec. 6). We show experimental validation and additional applications of our method in Sec. 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Stereo Matching with RGB-W Images</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Image Decolorization</head><p>A decolorization is a dimension-reduction process that converts three dimensional data to one dimension in the same range. Existing decolorization studies <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b21">22]</ref> mostly considered contrast preservation and visual distinctiveness. We propose a decolorization method for our cross-spectral stereo setup by considering contrast preservation and noise suppression together.</p><p>We assume that a decolorized image I γ is constructed by the weighted sum of three color channels of an input color image I as:</p><formula xml:id="formula_0">I γ = ω r I r + ω g I g + ω b I b s.t. ω r + ω g + ω b = 1, ω r ≥ 0, ω g ≥ 0, ω b ≥ 0,<label>(1)</label></formula><p>where I r , I g and I b are three color channels, and ω r , ω g and ω b are their weighting parameters.</p><p>We discretize each of ω r , ω g and ω b with an interval of 0.1, and make 64 candidate parameter sets Γ n where n ∈ {1, 2, · · · , 64}, because finer discretization produces indistinguishable differences in output for most cases, as shown in <ref type="bibr" target="#b24">[25]</ref>. Then, we choose a set of appropriate parameters with contrast preservation and noise suppression constraints for accurate stereo matching.</p><p>Contrast Preservation. Contrast preservation is a key property in decolorization because it is effective for preserving rich color information and for reducing the perceptual difference between color and decolorized images. To estimate good decolorization parameters, we adopt the contrast preserving measure in <ref type="bibr" target="#b24">[25]</ref>, as follows.</p><p>To measure the contrast difference between a color image I and its decolorized image I γ , which is also robust to image noise, we reconstruct a color imageĨ γ having contrast of I γ by applying the guided filter <ref type="bibr" target="#b10">[11]</ref> at each pixel i as:</p><formula xml:id="formula_1">I γ i = G i (I, I γ ) = j 1 |Ω| 2 w ij (I γ )I j , w ij (I γ ) = k|(i,j)∈Ω k 1 + (I γ i − µ k )(I γ j − µ k ) σ 2 k + ǫ ,<label>(2)</label></formula><p>where |Ω| is the number of pixels in Ω k , ǫ is a regularization parameter, µ k and σ k are the mean and standard deviation of I γ in a 5×5 window Ω k centered at the pixel k respectively. Then, we measure a contrast preserving cost E c (γ) for each decolorization parameter γ as:</p><formula xml:id="formula_2">E c (γ) = G(I, I) −Ĩ γ 1 , ∀γ ∈ Γ,<label>(3)</label></formula><p>where G(I, I) is the guided output image of the color input image I with a guidance of itself. After computing the cost E c (γ), we linearly interpolate the scattered data in the Γ space and find the local minima from the interpolated cost map. We denote the set of weighting parameters at the local minima as Γ 1 .</p><p>Noise Suppression. Because we consider low-light conditions where images suffer from large noise <ref type="bibr" target="#b7">[8]</ref>, the contrast preserving measure is not enough for producing properly decolorized images. As a complementary measure, we adopt the normalized sparsity measure, which was originally proposed for motion deblurring <ref type="bibr" target="#b19">[20]</ref>.</p><p>We use the normalized sparsity measure to estimate noise amplification during the decolorization process. It is defined as:</p><formula xml:id="formula_3">E n (γ) = ∇ x I γ 1 + ∇ y I γ 1 ∇ x I γ 2 + ∇ y I γ 2 , ∀γ ∈ Γ,<label>(4)</label></formula><p>where I γ is the decolorized image with a parameter γ, ∇ is a gradient magnitude of x or y direction, and · 1 and · 2 are the L 1 and L 2 norm, respectively. The normalized sparsity measure computes the normalized L 1 norm of the image gradient which makes it scaleinvariant, while the conventional L 1 norm of gradient, widely used in denoising <ref type="bibr" target="#b26">[27]</ref>, imposes signal sparsity and is scale-variant that can be simply minimized by reducing the entire signal. We denote the set of decolorization parameters resulting in low normalized sparsity values in Eq. (4) as Γ 2 . We empirically took the 20 percent of parameter subset in Γ as Γ 2 for this paper. <ref type="figure">Fig. 4</ref> shows the comparison of two sparsity measures. While the L 1 norm fails to select a good parameter because it favors low-intensity images, the normalized sparsity chooses a proper decolorized image that adjusts a good balance between signal power and image noise.</p><p>Decolorization parameters. Based on the two measures, E c and E n , we determine the candidate set of decolorization parameters Γ d as the intersection of Γ 1 and Γ 2 (Γ d = Γ 1 ∩ Γ 2 ). In our experiment, usually 5∼7 decolorization parameters were selected through this process.</p><p>To validate the effectiveness of our decolorization process, we perform stereo matching without post-processing in <ref type="figure" target="#fig_4">Fig. 5</ref>. The decolorized image with our parameter produces a greater number of reliable correspondences than those with the baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Disparity Estimation</head><p>In stereo matching, the brightness constancy assumption shows promising results in the presence of strong image noise because summing over a patch acts as a mean filter. Edge similarity assumption works well for the case of nonlinear intensity variations due to camera gain and changes in gamma <ref type="bibr" target="#b13">[14]</ref>.</p><p>To achieve robust stereo matching results, we combine two complementary costs; the sum of absolute differences (SAD) as a brightness constancy measure and the sum of informative edges (SIE) as an edge similarity measure. Our cost volume V at pixel x is defined as:</p><formula xml:id="formula_4">V(x, l) = αV SAD (x, l) + (1 − α)V SIE (x, l),<label>(5)</label></formula><p>where l represents a cost label and α ∈ [0, 1] is a balancing parameter between a brightness constancy term V SAD and an informative edges term V SIE .</p><p>The brightness constancy term V SAD is defined as:</p><formula xml:id="formula_5">V SAD (x, l) = x∈Ωx min(|I L (x) − I γ R (x + d)|, τ 1 ),<label>(6)</label></formula><p>where Ω x is a 7 × 7 support window centered at x, I L is a monochrome input image, I γ R is a decolorized image from a color input image, d is a disparity, and τ 1 is a truncation value for robustness.</p><p>The informative edges term V SIE is defined as:</p><formula xml:id="formula_6">V SIE (x, l) = x∈Ωx min |J(I L (x)) − J(I γ R (x + d))|, τ 2 , s.t. J(I) = | x∈Ωx ∇I(x)| x∈Ωx |∇I(x)| + 0.5 ,<label>(7)</label></formula><p>where τ 2 is a truncation value. We adopt J as a criterion to represent informative edges, which was introduced in deblurring research <ref type="bibr" target="#b27">[28]</ref>. In the definition of J, the sum of signed gradients in the numerator cancels out image noise, while the sum of absolute gradient magnitudes in the denominator, computes how strong the edges are, around a pixel location. The constant value (0.5) prevents production of a large edge response in homogeneous regions. The informative edge response J always results in a normalized value in the range of [0, 1], and is robust to nonlinear intensity changes and image noise. Therefore, we use Eq. (7) as our edge similarity measure. <ref type="figure" target="#fig_5">Fig. 6</ref> shows the effectiveness of the informative edge measure. While the conventional gradient map in <ref type="figure" target="#fig_5">Fig. 6</ref>(b) fails to detect distinctive edge responses, the informative edge map in <ref type="figure" target="#fig_5">Fig. 6</ref>(c) captures important edge responses even under lowlight conditions.</p><p>As a sequential step, we refine every cost slice in Eq. (5) by applying an edge-preserving filter that aggregates labels over the monochrome guidance image I L <ref type="bibr" target="#b23">[24]</ref>. Then we determine a disparity map using the winner-takes-all strategy. We reject outliers by the left-right consistency check, which marks pixels out if the disparity of a pixel on the left view is not consistent with the disparity of the corresponding pixel on the right view. We add such reliable correspondences by iteratively performing disparity estimation and gain adjustment that will be explained in the following section, and finally optimize the disparity map using minimum spanning tree-based filtering <ref type="bibr" target="#b28">[29]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Gain Adjustment</head><p>In order to locally match the intensity levels between I L and I γ R , we estimate a local gain map that adjusts brightness of the monochrome input image I L . We achieve this by solving a constrained linear least-squares problem.</p><p>Given two images, I L and I γ R , we first divide I L into unit blocks of 15 × 20 pixels where each block is assumed to be a uniform disparity. Each block is assigned one gain value, therefore we need to estimate a gain map Π = {π 1 , π 2 , · · · , π n b } where n b is the number of blocks. The gain map is computed by solving the problem:</p><formula xml:id="formula_7">argmin π t ϕ ν δ ν (I γ RL ) π t ν β ν (I L ) − β ν (I γ RL ) 2 s.t. 0.8π t−1 ν ≤ π t ν ≤ 1.2π t−1 ν , π 0 ν = µ(β ν (I γ RL )) µ(β ν (I L )) ,<label>(8)</label></formula><p>where ν is a block index, β ν is the set of intensities of correspondences in a block ν, I γ RL represents the aligned image of I γ R to I L with its disparity, and t is an iteration index of the disparity estimation and gain adjustment loop. δ ν (·) is an indicator function activated when the number of correspondences in block ν is larger than three. We use the indicator function to avoid over-fitting to outliers.</p><p>After solving Eq. (8), we estimate a dense and smooth gain map by propagating Π to the entire image I L using the local affinity model <ref type="bibr" target="#b20">[21]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">High-quality Color Image Recovery</head><p>The next step of stereo matching is merging the RGB-W stereo image to recover a high-quality color image. This is straightforward because monochrome cameras have better light efficiency than color cameras as discussed in Sec. <ref type="bibr" target="#b2">3</ref>.</p><p>In this process, we use the Y U V colorspace, which is composed of one luminance channel Y and two chrominance channels, U and V . We directly use the monochrome input image as the luminance channel of a recovered color image and reconstruct its color information by combining the chrominance channels of the color input image according to the estimated disparity.</p><p>(a) Initial color mapping (b) Colorization using <ref type="bibr" target="#b20">[21]</ref> (c) Local color consistency (d) Proposed <ref type="figure">Figure 7</ref>. Comparison of colorization results. <ref type="figure" target="#fig_3">Fig. 7(a)</ref> shows the reconstructed color image by this initial color mapping. It already shows a pretty good result thanks to our accurate disparity estimation, but there are color bleeding errors in occluded regions and conventional colorization <ref type="bibr" target="#b20">[21]</ref> cannot handle the problem because the algorithm <ref type="bibr" target="#b20">[21]</ref> is specialized for color propagation not for color correction (see <ref type="figure">Fig. 7(b)</ref>).</p><p>To resolve the problem, we introduce a simple but effective weight term and modify the algorithm <ref type="bibr" target="#b20">[21]</ref> to correct color bleeding errors. We segment the luminance channel into super-pixels <ref type="bibr" target="#b5">[6]</ref> and compute the confidence of initial chrominance mapping at the pixel i as:</p><formula xml:id="formula_8">w d i = exp C∈{U,V } − (C i − median S (C i )) 2 2σ 2 C ,<label>(9)</label></formula><p>where median S (C i ) is the median chrominance of a super pixel containing the pixel i and σ C is a control parameter. <ref type="figure">Fig. 7</ref>(c) shows the confidence map computed from <ref type="figure" target="#fig_3">Fig. 7(a)</ref>. The confidence value is used as an additional weighting term of the colorization method <ref type="bibr" target="#b20">[21]</ref>. Specifically, we recover color-corrected chrominance channels by minimizing an objective function defined as:</p><formula xml:id="formula_9">argmin C∈{Û ,V } i w d i Ĉ i − C i 2 +λ s i j∈Ni Ĉ i − w s ij WĈ j 2 , s.t. w s ij = exp − (Y i − Y j ) 2 2σ 2 Ni , W = j∈Ni w s ij ,<label>(10)</label></formula><p>where λ s is a balancing parameter between the data term and the smoothness term, N i represents the eight neighboring pixels of i, and σ 2</p><p>Ni is the variance of the luminances in N i . Following <ref type="bibr" target="#b20">[21]</ref>, this objective function can be color camera monochrome camera illum. exp. noise std. illum. exp. noise std. <ref type="table">Table 1</ref>. Two setups of the Middlebury stereo benchmark. We simulate our stereo system by taking a image of a given illumination and exposure level from the Middlebury dataset. We add additional signal dependent Gaussian noise with a given standard deviation where κ represents the noise-free signal intensity <ref type="bibr" target="#b4">[5]</ref>    <ref type="table">Table 1.</ref> efficiently solved as:</p><formula xml:id="formula_10">Setup 1 1 0 0.03 √ κ 2 1 0.01 √ κ Setup 2 3 0 0.07 √ κ 1 2 0.01 √ κ</formula><formula xml:id="formula_11">(W d + λ s L)ĉ = W d c s.t. L = I − W s ,<label>(11)</label></formula><p>where W d is a diagonal matrix consisting of the data weight w d p , L is a Laplacian matrix, I is an identity matrix, W s is a matrix form of the smoothness term, and c andĉ are vectorized forms of C andĈ respectively. <ref type="figure">Fig. 7</ref>(d) shows our colorization result in which color bleeding is recovered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Experiments</head><p>We implemented our method in MATLAB and it takes about 5 minutes to process one dataset of 1390 × 1110 resolution on an I7 3.4GHz machine. Among all the steps, disparity estimation in Sec. 5.2 uses most of the processing time. We expect that the computational time can be significantly reduced using GPU parallelization.</p><p>For the evaluation, we compare our method with stateof-the-art methods of multi-spectral or cross-channel stereo matching; DASC <ref type="bibr" target="#b17">[18]</ref>, CCNG <ref type="bibr" target="#b14">[15]</ref>, ANCC <ref type="bibr" target="#b11">[12]</ref> and JDMCC <ref type="bibr" target="#b12">[13]</ref>. For a fair comparison, we used the original  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.">Middlebury Stereo Benchmark</head><p>We quantitatively evaluated our method using the Middlebury stereo benchmark <ref type="bibr" target="#b13">[14]</ref>. For realistic simulations, we took two images captured under different illuminations to simulate different spectral sensitivities and add additional noise to simulate low-light conditions. To imitate the lightefficiency difference between color and monochrome cameras, we used longer exposure images as monochrome input images, and added more noise to the color input images. We configured two different setups for this experiment. The details are summarized in <ref type="table">Table 1</ref>.</p><p>Two examples of stereo matching results are shown in <ref type="figure" target="#fig_8">Fig. 8</ref>. The quantitative comparison is presented in <ref type="figure" target="#fig_7">Fig. 9</ref>. We use the bad pixel rate as an evaluation criterion, which is defined as the percentage of pixels for which the absolute disparity error is greater than 1. In this experiment, our method largely outperformed all the competing methods for all the test datasets. The NCC (normalized cross correlation)-based methods <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b11">12</ref>] are vulnerable to low intensity level and severe noise <ref type="figure" target="#fig_7">Fig. 9(b)</ref>, as demonstrated in <ref type="bibr" target="#b13">[14]</ref>. The JDMCC <ref type="bibr" target="#b12">[13]</ref>   <ref type="figure">Figure 11</ref>. Quantitative evaluation using a controlled indoor scene captured from our prototype system. The odd rows show the results with image pairs captured under bright and dark illumination conditions, respectively. The even rows are corresponding bad pixel error maps. among the competing methods; however, it exhibited large quantization errors, as shown in <ref type="figure" target="#fig_8">Fig. 8</ref>. We guess that the absence of color information leads to failure of color equalization of the JDMCC, and that this causes large errors in the JDMCC results. We also evaluated the effectiveness of our colorization process. For the quantitative evaluation, we measure the PSNR and SSIM of three different colorization methods: initial color mapping with an estimated disparity map, conventional colorization <ref type="bibr" target="#b20">[21]</ref>, and our colorization with chrominance consistency weight. <ref type="figure" target="#fig_9">Fig. 10</ref> shows the evaluation result, and that our method outperforms the competing methods and consistently improves colorization quality in terms of both PSNR and SSIM. This is because the newly proposed method corrects the color bleeding errors in outof-plane regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.">Experiment with our Prototype System</head><p>We implemented our prototype system using two Point-Grey Flea3 cameras, one color and one monochrome camera, with baseline of 5cm, and maximum disparity of about 80 pixels. The stereo system was pre-calibrated and  <ref type="figure" target="#fig_2">Figure 13</ref>. Applications to depth-aware image processing using the accurate disparity map and high-quality color image of our method. Note that the input images were captured in challenging low-light conditions and we amplify the brightness of the results for the visualization purpose.</p><p>images from the cameras were rectified using the MATLAB bulit-in camera calibration toolbox.</p><p>First, we investigated performance degradation with respect to illumination conditions. As shown in <ref type="figure">Fig. 11</ref>, we captured the same scene under two different illuminations in a controlled laboratory environment. For quantitative evaluation, we also estimated a ground-truth disparity map using a structured-light 3D scanner. <ref type="figure">Fig. 11</ref> shows the comparison of the estimated disparity maps with their bad pixel rates. Compared to the competing methods, our method achieved the best results regardless of illumination conditions, with the least degradation of performance. <ref type="figure" target="#fig_10">Fig. 12</ref> shows the results of outdoor datasets captured at night (i.e., low-light conditions). All the state-of-the-art methods produced reasonable results; however, our method achieved the most accurate disparity map among them while the other methods suffered from holes and errors in dark regions. Note that our method reconstructs both depth discontinuities and fine structure, such as the horse stone statue in the 1 st row, and the branches in the 2 nd row of <ref type="figure" target="#fig_10">Fig. 12.</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.">Applications to Depth-aware Image Processing</head><p>An accurate disparity map can facilitate many applications. As examples, we show photographic editing applica-tions such as digital refocusing and image stylization.</p><p>Digital refocusing that shifts the in-focus region after taking a photo <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4]</ref> is one of the most popular depthaware processing techniques. An accurate disparity map is necessary to create a realistic refocused image. In <ref type="figure" target="#fig_2">Fig. 13</ref>(b), we added synthetic blurs to the images using our disparity estimates and produced a shallow depth of field image.</p><p>Another emerging application is image stylization, which changes the photographic look of an image. When a disparity map is given, we can easily change the color of a certain depth range and produce visually pleasing photographic looks as shown in <ref type="figure" target="#fig_2">Fig. 13</ref>(c). Our application results show distinctively realistic photographic effects even in night scenes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Conclusions</head><p>We have proposed a new stereo framework for highquality depth and color image acquisition under low-light conditions. We achieved this by utilizing a fundamental trade-off of the advantages of color and monochrome cameras and validated the effectiveness of the proposed framework through extensive quantitative and qualitative evaluation. We expect that the proposed framework could become popular as a robust stereo system for mobile phones in the near future.</p><p>In this study, we found some challenges that should be overcome, and which will be considered in future work. First, the performance of our method is not guaranteed for datasets with low-texture and refractive media. This is considered a fundamental issue of stereo matching. Second, we need to account for a narrower baseline system than our prototype for practical utility in mobile devices. Last, the current computational burden is another problem to be solved.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>The overview of our algorithm. Our method produces the accurate depth map (f) and the high-quality color image (g), while the baseline stereo matching method with the luminance channels of the input images results in the poor disparity map (a).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>( a )</head><label>a</label><figDesc>Gradient sparsity by L1 norm (b) Normalized sparsity Figure 4. Decolorized images with two different gradient sparsity measures.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Results of stereo matching between a gray reference image and decolorized images with three different methods: (a) the "rgb2gray" in MATLAB, (b) the contrast preserving measure in Eq. (3), and (c) the proposed measure. A detailed description for the dataset is presented in Sec. 7.1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Effectiveness of the informative edge measure. (a) Color image. (b) Conventional gradient map. (c) Informative edge map.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 .</head><label>9</label><figDesc>Quantitative evaluation results on the Middlebury stereo benchmark. The experimental setup is summarized in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 .</head><label>8</label><figDesc>Comparison of estimated disparity maps under the "Setup 2" on the Middlebury Benchmark.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 .</head><label>10</label><figDesc>Evaluation for each of the colorization methods.authors' code and chose the best performing parameters using a parameter sweep. We used the same set of parameters {α, τ 1 , τ 2 , σ C , λ s } = {0.5, 0.1, 0.1, 0.01, 3} to generate all the results of our method. Please refer to the supplementary material for more results and comparisons with state-of-the-art methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 12 .</head><label>12</label><figDesc>Comparison of disparity estimation results on outdoor scenes captured from our prototype system.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>worked relatively well</figDesc><table>40.13% 

ANCC 
44.83% 

17.07% 

JDMCC 
24.23% 

11.79% 

14.48% 
Ours 

19.20% 

CCNG 
24.45% 

19.52% 

41.71% 
DASC 

Ground Truth 

Input images 

Structured Light 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Flea3 gige imaging performance specification</title>
		<ptr target="http://www.ptgrey.com/support/downloads/10109/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Htc One</surname></persName>
		</author>
		<ptr target="http://www.htc.com/us/smartphones/htc-one-m8/" />
		<imprint>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huawei</surname></persName>
		</author>
		<ptr target="http://consumer.huawei.com/minisite/worldwide/p8/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Venue 8 7000 series</title>
		<ptr target="http://www.dell.com/en-us/shop/productdetails/dell-venue-8-7840-tablet/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multiplexing for optimal lighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lucchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Susstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI)</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1339" to="1354" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Slic superpixels compared to state-of-the-art superpixel methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lucchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Susstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="2274" to="2282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Rethinking color cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chakrabarti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zickler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computational Photography (ICCP)</title>
		<meeting>IEEE International Conference on Computational Photography (ICCP)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Noise suppression in low-light images through joint denoising and demosaicing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chatterjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">B</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Matsushita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE International Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE International Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Domain transform for edgeaware image and video processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">S</forename><surname>Gastal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Oliveira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page">69</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<title level="m">Guided image filtering. IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1397" to="1409" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Robust stereo matching using adaptive normalized cross-correlation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">S</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">U</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="807" to="822" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Joint depth map and color consistency estimation for stereo images with different illuminations and cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">S</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">U</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1094" to="1106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Evaluation of stereo matching costs on images with radiometric differences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hirschmüller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Scharstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI)</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="1582" to="1599" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Generalized assorted camera arrays: Robust crosschannel registration and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Holloway</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Koppal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Veeraraghavan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing (TIP)</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="823" to="835" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multispectral pedestrian detection: Benchmark dataset and baseline</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE International Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Colorization by example</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Irony</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eurographics Symp. on Rendering</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Dasc: Dense adaptive self-correlation descriptor for multimodal and multi-spectral correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ryu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">N</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE International Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Dark flash photography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page">96</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Blind deconvolution using a normalized sparsity measure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE International Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Colorization using optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="689" to="694" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Contrast preserving decolorization with perception-based quality metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="222" to="239" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">On crossspectral stereo matching using dense gradient features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pinggera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Breckon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of British Machine Vision Conference (BMVC)</title>
		<meeting>British Machine Vision Conference (BMVC)</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Fast cost-volume filtering for visual correspondence and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rhemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hosni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bleyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gelautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE International Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Decolorization: is rgb2gray () out?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH Asia Technical Briefs</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Picam: an ultra-thin high performance monolithic camera array</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Venkataraman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lelescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Duparré</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mcmahon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Molina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chatterjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mullis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">166</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A new alternating minimization algorithm for total variation image reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Imaging Sciences</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="248" to="272" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Two-phase kernel estimation for robust motion deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Stereo matching using tree filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="834" to="846" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Fast image and video colorization using chrominance blending</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yatziv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing (TIP)</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1120" to="1129" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
