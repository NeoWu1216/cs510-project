<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Fast Temporal Activity Proposals for Efficient Detection of Human Actions in Untrimmed Videos</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Caba Heilbron</surname></persName>
							<email>fabian.caba@kaust.edu.sa</email>
							<affiliation key="aff0">
								<orgName type="institution">King Abdullah University of Science and Technology (KAUST)</orgName>
								<address>
									<country key="SA">Saudi Arabia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
							<email>jniebles@cs.stanford.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Universidad del Norte</orgName>
								<address>
									<country key="CO">Colombia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
							<email>bernard.ghanem@kaust.edu.sa</email>
							<affiliation key="aff0">
								<orgName type="institution">King Abdullah University of Science and Technology (KAUST)</orgName>
								<address>
									<country key="SA">Saudi Arabia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Fast Temporal Activity Proposals for Efficient Detection of Human Actions in Untrimmed Videos</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:40+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In many large-scale video analysis scenarios, one is interested in localizing and recognizing human activities that occur in short temporal intervals within long untrimmed videos. Current approaches for activity detection still struggle to handle large-scale video collections and the task remains relatively unexplored. This is in part due to the computational complexity of current action recognition approaches and the lack of a method that proposes fewer intervals in the video, where activity processing can be focused. In this paper, we introduce a proposal method that aims to recover temporal segments containing actions in untrimmed videos. Building on techniques for learning sparse dictionaries, we introduce a learning framework to represent and retrieve activity proposals. We demonstrate the capabilities of our method in not only producing high quality proposals but also in its efficiency. Finally, we show the positive impact our method has on recognition performance when it is used for action detection, while running at 10FPS.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>With the growth of online and personal media archives, people are generating, storing and consuming very large collections of videos. A need that arises from such large sources of video is the ability to process them for contentbased indexing and search. In particular, many such applications would benefit from automatic recognition of events, actions, and activities in continuous video streams. To achieve this, computer vision algorithms are required to temporally localize the activities of interest within long video sequences. Such visual recognition setting corresponds to the well-known task of action/activity detection.</p><p>Current methods for action temporal localization rely on applying action classifiers at every time location and at multiple temporal scales, in a temporal sliding window fashion.  However, due to the computational complexity of such classifiers and the large number of possible temporal locations and scales, the sliding window approach is computationally infeasible for large-scale video analysis applications.</p><p>In order to avoid this exhaustive evaluation of video classifiers, a number of researchers have recently introduced the idea of spatial or spatiotemporal proposals for the task of action recognition <ref type="bibr" target="#b31">[31,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b9">10]</ref>. Within this paradigm, a video is first processed to produce a set of candidate video segments or proposals, which are likely to contain a human action or activity as <ref type="figure" target="#fig_1">Figure 1</ref> illustrates. These proposals are then used as a reduced candidate set, on which more sophisticated action classifiers can be applied for recognition. A good method for generating proposals should therefore have the following properties: (a) it should recover the true temporal locations of actions with high recall and relatively good precision, and (b) it should produce proposals quickly.</p><p>These two requirements make proposal generation a computationally challenging vision task. In addition, the proposal algorithm should be versatile enough to find candidates for any action or activity class, and simultaneously provide potential starting and ending times for each candidate activity. The large variation in motion, scenes, and objects involved, styles of execution, camera viewpoints, camera motion, background clutter and occlusions impose additional burden to the proposal generation process.</p><p>Although the concept of action proposals in video has been introduced in previous work, most existing methods target spatiotemporal proposals, which are very helpful in localizing and disambiguating actions that occur in the same time. However, as we will see in the experimental section, these methods only achieve marginal recall improvement over simple baselines, such as uniform random sampling, and are too computationally expensive to scale to large datasets like THUMOS <ref type="bibr" target="#b13">[14]</ref> or ActivityNet <ref type="bibr" target="#b3">[4]</ref>. These two observations motivate our proposed work on localizing proposals only in time, which we expect to be a fundamental building block in the development of scalable and practical action/activity detection algorithms in the future.</p><p>Contributions. In this paper, we introduce a new method that produces temporal proposals in untrimmed videos. Our work has the following contributions. First, we propose a sparse learning framework for scoring temporal segments according to how likely they are to contain an action. Second, we experimentally show that current and state-of-the-art proposal methods are not well suited for generating temporal proposals in realistic and large-scale scenarios. Third, we show empirical evidence that our proposed sparse learning framework achieves high recall and efficiency on several benchmark datasets. Finally, we incorporate our proposal generation method into a standard activity detection framework and show that it can significantly boost the overall action detection performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>We first describe the importance of using candidate regions from the object domain perspective. Then, we relate our work with previous approaches on action proposals.</p><p>For object detection in images, the use of a sliding window (exhaustive search) strategy to localize objects is no longer popular due to the high computational cost that it entails. Instead, generic or class-specific object proposals are used to quickly find possible locations of an object in an image. Only these locations are in turn tested by an object classifier to recognize whether or not they contain a specific class of object. Since these proposal methods have very high recall and a low false positive rate, their use in object detection has significantly reduced the runtime of otherwise slow object classifiers <ref type="bibr" target="#b8">[9]</ref>. Some popular object proposal methods are SelectiveSearch <ref type="bibr" target="#b30">[30]</ref>, MCG <ref type="bibr" target="#b1">[2]</ref>, Objectness <ref type="bibr" target="#b0">[1]</ref>, and EdgeBoxes <ref type="bibr" target="#b41">[41]</ref>. We refer the reader to <ref type="bibr" target="#b11">[12]</ref> for an extensive review of the advances in object proposals in the image domain.</p><p>Only very recently have proposal methods been extended to the video domain. For example, several works attempt to produce spatiotemporal tubes as proposals for objects in video <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b6">7]</ref>. However, very limited research has targeted temporal proposals for activities in video; even though a proposal generation method would be crucial for efficient activity detection in long untrimmed videos. Currently, activity detection methods simply apply a computationally expensive activity classifier to a temporally sliding window (or randomly sampled temporal chunks). Very recent work generates spatiotemporal proposals in video, including tubelets <ref type="bibr" target="#b12">[13]</ref>, action tubes <ref type="bibr" target="#b9">[10]</ref>, the actionness measure <ref type="bibr" target="#b5">[6]</ref>, proposals from dense trajectories <ref type="bibr" target="#b31">[31]</ref>, the fast proposal method <ref type="bibr" target="#b35">[35]</ref>, and Bag of Fragments <ref type="bibr" target="#b19">[20]</ref>. All these methods rely either on dense trajectories or use hierarchical grouping approaches for generating the proposals. This tends to violate efficiency constraints for creating action proposals in a large-scale scenario.</p><p>Moreover, previous action proposal methods are not developed or designed to propose temporal segments where a human activity can be confined. In fact, current approaches have not been evaluated beyond simple short videos. So, their scalability and detection performance in real-world scenarios is uncertain. To overcome the existing limitations, our proposed algorithm generates temporal activity candidates using an efficient scoring function that can be adapted to varying activity types of interest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Temporal Activity Proposals</head><p>To be applicable at large-scales and in practical scenarios, a useful activity proposal method is driven by two competing goals. (i) The proposal method must be computationally efficient, in representing, encoding, and scoring a temporal segment. (ii) The proposal method must be discriminative of activities that we are interested in, so as to only retrieve temporal segments that contain visual information indicative of these activity classes. On one hand, sampling a long video uniformly without using any content-based information can generate proposals very quickly; however, more often than not, this strategy will retrieve segments that are not related to the activity classes we seek. On the other hand, executing the most successful activity recognition methods in the literature will not be feasible. Although extracting dense trajectories and encoding them using Fisher vectors have become the defacto standard in the majority of state-of-the-art methods for trimmed activity recognition <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b2">3]</ref>, this strategy is too slow for extracting activity proposals, especially at large-scale. In this work, we propose a temporal proposal strategy that is a successful and, more importantly, tunable tradeoff between these two goals. In what follows, we give a detailed account of our proposed method and emphasize how each goal is mindfully considered in its design. <ref type="figure">Fig 2 shows</ref> an overview of our approach for generating temporal activity proposals. Given a set of training videos, we extract features that captures spatial and temporal appearance. We next learn a universal dictionary that encodes discriminative information for a set of activity classes. After constructing this dictionary, we can use it to efficiently . . .  <ref type="figure">Figure 2</ref>. At training time, our approach describes trimmed action instances using STIPs and learns a dictionary that encodes their visual contents. Given a test sequence, we first generate a large set of candidate temporal segments. These candidates are described using STIPs and ranked using the learned dictionary. Finally, a subset of the candidate segments set is selected as our activity proposals based on the predicted activity proposal ranking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Trimmed videos</head><p>encode temporal segments in unseen video sequences. To do this, we first generate a large set of candidates temporal segments. To output the final temporal activity proposals, these segments are efficiently ranked according to how well they are represented by the dictionary and only the most representative among them are retrieved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Candidate Proposals</head><p>Our method starts with an initial set of candidates, from which we eventually select and retrieve proposals. This candidate proposal set is usually much larger in cardinality than the generated proposals. In what follows, we describe how these candidate proposals are extracted and represented.</p><p>Generating candidate proposals: Within an input video sequence, we sample temporal segments of different lengths from the entire video. This sampling is done uniformly over time, but we sample likely proposal lengths from a distribution compiled from a set of training videos containing temporally localized activity classes. In doing so, we partition the input video into a large pre-defined number of proposal candidates, which can overlap in time.</p><p>Feature extraction: As a tradeoff between computational efficiency and representation/discrimination power, we decide to use Spatio Temporal Interest Points (STIPs) <ref type="bibr" target="#b16">[17]</ref>, which are extracted in the aforementioned proposal candidates. We follow the standard practice and encode each STIP point using Histogram of Oriented Gradients (HOG) and Histogram of Optical Flow (HOF) to characterize its spatial and temporal appearance. Note that STIPs have been successfully applied in previous work to action recognition <ref type="bibr" target="#b26">[27]</ref> and video summarization <ref type="bibr" target="#b40">[40]</ref>. In practice, we further speedup this extraction and representation process by performing it in parallel across the entire set of candidates. As such, each proposal candidate is represented as a set of feature descriptors x i ∈ R 172 , which can be concatenated for convenience in matrix form as X k = [x 1 | · · · |x n k ], where n k is the total number of STIPs detected in the k th proposal candidate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Learning to Represent Proposals</head><p>Inspired by seminal works in image classification <ref type="bibr" target="#b34">[34]</ref> and action recognition <ref type="bibr" target="#b10">[11]</ref>, we assume that each STIP feature in a proposal candidate X k can be represented linearly using a sparse set of dictionary/vocabulary elements, which are learned offline from a large video corpus with temporally annotated activity instances. Moreover, we expect that the representations of different STIP features within the same proposal candidate, which originate from a particular activity class, would not be independent, but instead share commonalities. In fact, the merits of representing interest points jointly instead of independently are well-studied in the image classification literature <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b27">28]</ref>. To combine all these requirements together, two design choices emerge, both of which we will explore. (a) class-independent proposal learning: We seek to compute an over-complete dictionary that can jointly represent STIP features within the same proposal candidate using a sparse set of dictionary elements. Here, the focus is solely on representation and is agnostic to any supervised information that is possibly available. (b) class-induced proposal learning: We seek a dictionary similar to the one in (a), but that also leads to a proposal representation that is discriminative of the activity classes we are interested in. Both choices are viable and each has its own merits <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b20">21]</ref>. However, for the purpose of retrieving meaningful activity proposals to be used in activity detection, we will experimentally show in Section 4 that (b) is superior to (a). Previous work in image classification has also reached this conclusion <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Class-Independent Proposal Learning</head><p>From the training set, we compile all temporal segments X l that belong to annotated activities to form the data matrix X = [X 1 | · · · |X l ], where l corresponds to the total number of trimmed instances in the training set. Then, we cast the dictionary learning problem of D U as follows.</p><formula xml:id="formula_0">(DU , A * ) = arg min D,A 1 n X − DA 2 F + λ A 2,1,<label>(1)</label></formula><p>where X ∈ R 172×n , D ∈ R 172×d , A ∈ R d×n with d equal to the number of dictionary elements, and n equal to the total number of STIP points in the training subset. Note that, A = [A 1 | · · · |A k ] is a stacked matrix that contains the reconstruction coefficients for all STIPs in all the activity instances. Inspired by the results of <ref type="bibr" target="#b40">[40]</ref> on video summarization, we use a ℓ 1 /ℓ 2 matrix regularizer to encourage joint sparsity in the representations of each activity instance. In fact, this regularization scheme encourages that the STIPs in each temporal segment X k share the same sparse support in representation, i.e. they use similar dictionary elements for reconstruction. We control the reconstruction quality with the tradeoff parameter λ which we set in practice to λ = 0.05. This joint representation scheme is a form of multi-task learning (MTL), where tasks that share dependencies in features or learning parameters are jointly solved in order to capitalize on their inherent relationships. In our case, coding each individual STIP is considered a single task. Note that this type of learning has been successfully applied to classical problems (e.g. image annotation <ref type="bibr" target="#b24">[25]</ref>, image classification <ref type="bibr" target="#b37">[37]</ref>, and object tracking <ref type="bibr" target="#b39">[39,</ref><ref type="bibr" target="#b38">38]</ref>) and has outperformed state-of-the-art methods that resort to independent learning. To solve Eq (1), we follow a conventional strategy of fixed point optimization, which iteratively updates each of the variables D U and A separately by fixing one of them at a time. So at every iteration, two update steps are required and we summarize them next. We initialize D U using K-Means. The iterative method is terminated when the relative change in objective is smaller than a pre-defined threshold.</p><p>Updating A: This is referred to as the coding step. It requires the solution to Eq 2, which is a non-smooth convex program that can be optimized using the Alternating Direc-tion Method of Multipliers (ADMM).</p><formula xml:id="formula_1">min A 1 n X − DA 2 F + λ A 2,1<label>(2)</label></formula><p>Updating D U : This update requires the solution to Eq (3), which is a linear least squares problem in matrix form. It can be optimized by solving a set of linear systems.</p><formula xml:id="formula_2">min D X − DA 2 F<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Class-Induced Proposal Learning</head><p>The second design choice for the dictionary learning is to incorporate supervised information (i.e. activity class labels) into the learning process. Here, we describe how we learn a universal dictionary in a supervised fashion using the available class labels in the training set. We formulate the problem in Eq <ref type="formula" target="#formula_3">(4)</ref>.</p><formula xml:id="formula_3">(DS, A * , W * ) = arg min D,A,W 1 n X − DA 2 F + λ1 A 2,1 + λ2 W T A − Y 2 F + λ3 W 2 F ,<label>(4)</label></formula><p>where W ∈ R d×c , and Y ∈ {0, 1} c×n with c equal to the number of classes. Here, Y is the label matrix for all the STIPs in the training set, whereby each STIP inherits the label of the activity instance it belongs to. The matrix W is a column-wise concatenation of c one-vs-all linear classifiers. The main difference between this formulation and Eq (1) is the training classification loss term, W T A − Y 2 F , which empowers the dictionary D S with discriminative properties. Of course, different forms of this loss term can be used, including the hinge loss used in SVMs or the logistic loss used in logistic regression. We choose this simple ℓ 2 loss for computational reasons. To be less sensitive to overfitting and for better classifier generalization, we add an energy regularizer on W. Similar to class-independent proposal learning, we use alternating optimization to solve Eq (4) and the same initialization for D S .</p><p>Updating W: This requires solving Eq (5), which is a linear least squares problem. The classifiers learned in this step will not be used for activity recognition later. They are merely intermediate variables that guide the dictionary to being more discriminative.</p><formula xml:id="formula_4">min W W T A − Y 2 F + λ2 W 2 F<label>(5)</label></formula><p>Updating A: This coding step requires solving Eq (6), which can be solved using ADMM.</p><formula xml:id="formula_5">min A 1 n X − DA 2 F + λ1 A 2,1 + λ2 W T A − Y 2 F<label>(6)</label></formula><p>Updating D: This step is the same as Eq (3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Retrieving Proposals</head><p>Once we have learned a dictionary-based proposal representation, our aim is to retrieve activity proposals from unseen input videos which contain a human activity that is similar to activity instances in the training set. As described in Section 3.1, this input video is segmented into a large number of proposal candidates. Then, we jointly encode each proposal candidate by solving Eq (7) using the learned dictionary D (either D U or D S ). Since this procedure is done online, we achieve further speedup by using the sparse codes A k of one proposal candidate X k as an initialization to the sparse codes A j of another proposal candidate X j that overlaps with it. This is valid, since Eq (7) is convex and it is guaranteed to converge to the global solution no matter what the initialization. Finally, the average reconstruction error 1 n k X k − DA k 2 F is used to rank the proposal candidates. A small error value indicates that the candidate X k can be represented well with the learned dictionary and thus it is likely to belong to one of the activity classes belonging to the training set. Obviously, we retrieve the final activity proposals as the candidates with lowest reconstruction error.</p><formula xml:id="formula_6">A * k = arg min A k 1 n k X k − DA k 2 F + λ A k 2,1.<label>(7)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results</head><p>In this section, we test our proposal generation method under two different settings. We generate and rank proposals using a dictionary learned in an class-independent (Section 3.2.1) fashion, as well as, using a dictionary learned in a class-induced setting (Section 3.2.2).</p><p>We evaluate the performance of our temporal activity proposal method from three perspectives. First, we study the quality of our activity proposals by measuring the ability of our method to retrieve proposals that overlap with the occurrence of actions in long videos in Section 4.2. Second, we evaluate the processing speed of our method to assess how quickly it can generate activity proposals in Section 4.3. Finally, we apply our proposal generation method to the task of action detection in Section 4.4, and show how our method can contribute to improving the performance of existing methods in the action detection task. Throughout the experiments, we compare our method to baseline and state-of-the-art proposal methods. Our results show that our framework consistently achieves improved performance over the state-of-the-art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Setup</head><p>Datasets: We test our proposal method on two different datasets for action temporal localization. First, we con-duct experiments on the MSR-II Action dataset <ref type="bibr" target="#b36">[36]</ref>. It contains 54 video sequences with an average length of 51 seconds each. These videos depict three action classes: Boxing, Clap, and Wave. Following the standard evaluation protocol, we use videos in the KTH <ref type="bibr" target="#b26">[27]</ref> dataset for training. Second, we test the quality of our method on the labeled untrimmed videos from the challenging THUMOS 2014 Detection Challenge dataset <ref type="bibr" target="#b13">[14]</ref>. This dataset compiles videos from YouTube from 20 sport actions, and is considered one of the most challenging datasets for action detection. Videos in this dataset have an average length of 3 minutes with the actions usually confined to a small fraction of the video. We use 200 untrimmed videos from the validation subset to train our proposal method. For testing, we evaluate on the remaining 213 videos that are provided with temporal activity annotations.</p><p>Baselines: We compare our methods to two baselines and three state-of-the-art techniques. (1) Uniform sampling: it ranks each candidate proposal uniformly at random. (2) Binary Proposal Classifier (BPC): from the raw video descriptors, a binary linear classifier is learned to discriminate between action vs non-action. We use STIPs from the trimmed video as positive instances and STIPs from background segments as negative instances. To rank proposals, we perform mean pooling over all the classifier scores within each candidate proposal. The same candidate proposal generation approach described in Section 3.1 is used to feed (1) and <ref type="bibr" target="#b1">(2)</ref>. (3) Action localization Proposals from dense Trajectories (APT) <ref type="bibr" target="#b31">[31]</ref>; (4) Fast Action Proposals for Human action Detection and Search (FAP) <ref type="bibr" target="#b35">[35]</ref>; and (5) Bag of Fragments (BoFrag) <ref type="bibr" target="#b19">[20]</ref>. Note that APT and FAP are originally designed to generate spatiotemporal proposals, so we project their resulting proposals to the temporal dimension only and discard the spatial information. This process may result in highly overlapping or duplicate temporal proposals, so we remove these for a more fair comparison against our method. While these methods target spatiotemporal proposals and not directly compute temporalonly proposals, we choose to compare to them as they are the closest methods in the literature that are applicable to our problem. We obtain the pre-computed proposals for APT and FAP on MSR-II directly from the authors. On Thumos14, we run an implementation of APT available online to extract proposals, we obtain pre-computed proposals for BoFrag directly from the authors, but we were not able to obtain an implementation for FAP.</p><p>Matching criterion: In order to evaluate the quality of proposals generated by each method, we measure the overlap between each proposal and the ground truth temporal annotations. To do this, we compute the temporal Intersection over Union (tIoU) as the intersection over union of the two time intervals. If the tIoU of a proposal is above a predefined threshold, the proposal is considered a true positive.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Recall Analysis</head><p>As discussed earlier, a good action proposal method should retrieve as many true activity segments in a video as possible, i.e. it should achieve a high recall rate. We analyze the quality of our temporal proposals from two perspectives. First, we study how tight our proposals are in comparison to the ground truth temporal locations. Second, we evaluate the quality of our proposal ranking by measuring recall on the top-ranked proposals generated by our method. All throughout, we report the performance of the baselines and state-of-the-art methods for comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Proposal Localization Quality</head><p>In this experiment, we obtain a fixed number of proposals from each method and measure recall by comparing the retrieved proposals to the ground truth at various tIoU thresholds. We only allow one detection per ground truth instance. <ref type="figure" target="#fig_2">Figure 3</ref> plots the recall of each method against the tIoU threshold. Consider for instance the results on the MSR-II dataset in <ref type="figure" target="#fig_2">Figure 3 (left)</ref>. For a tIoU threshold of 0.8, we see that our class-induced method achieves a recall of 80%, while the second-best is APT with a recall of 65%. We observe the same behaviour across the entire range of tIoU thresholds for both datasets, which indicates that our proposal generation method achieves the highest action localization quality.</p><p>As expected, the proposals scored with the class-independent approach achieve lower recall rates than the ones scored in the class-induced fashion. We attribute the resulting gap in the curves to the fact that our classinduced approach is empowered with discriminative properties. Moreover, our approach clearly outperforms the Uniform sampling baseline. For example, when tIoU is fixed to 0.8, our class-induced method achieves improvements in recall of 40% and 38% on MSR-II and Thumos14 respectively with respect to that baseline. We note that the BPC baseline obtains good recall at low tIoU thresholds; however, this behavior is not consistent at high tIoU thresholds. From <ref type="figure" target="#fig_2">Figure 3</ref> (left), it is clear that the Fast Proposals (FAP) approach is not well suited for proposing temporal segments at high tIoU thresholds. When the tIoU threshold is greater than 0.4 the recall rate decreases dramatically. For example, its recall rate is 0.6 at a tIoU of 0.5, which is lower than what we obtained by uniform sampling.</p><p>As compared to APT, the quality of our class-induced proposals is clearly better, since they achieve significant recall improvement at higher tIoU thresholds. As reference, our method obtains a 30% improvement in recall over APT when tIoU is fixed to 0.5 in Thumos14. Interestingly, BoFrag achieves a high recall at lower tIoU thresholds. However, our class-induced proposals outperform BoFrag by a large margin for tIoU thresholds greater than 0.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Proposal Ranking Quality</head><p>In this experiment, we study the performance of our method in terms of recall when only a limited number of proposals is retrieved. To do this, we select a set number of top-ranked proposals generated by each method and measure the average recall between tIoU 0.5 to 1. As such, the average recall measure summarizes proposal performance across tIoU thresholds and it correlates with detection performance as shown in <ref type="bibr" target="#b11">[12]</ref>. Notice that APT, BoFrag and FAP produce a fixed not scored number of proposals; therefore, we randomly select the number of retrieved proposals for these two methods. Results of this experiment are depicted in the recall curves in <ref type="figure" target="#fig_3">Figure 4</ref>. In these plots, we gradually increase the number of retrieved proposals and record the recall rate of each method. For example, in <ref type="figure" target="#fig_3">Figure 4</ref> (left), when retrieving only the top-50 proposals per video, our class-induced method reports a recall of 0.7 on MSR-II outperforming all other methods. This shows that our method ranks proposals better than competing methods, which results in higher recall when a small number of proposals is retrieved. Such behavior is important to guarantee an acceptable recall rate for time-sensitive detection tasks.</p><p>We also note that our class-induced proposal method performs better than the class-independent method. In this case, the class-induced proposals reach a high recall faster. Compared to the BPC baseline, we are able to obtain a higher recall rate, no matter how many proposals are retrieved. As shown in <ref type="figure" target="#fig_3">Figure 4</ref> (right), our method can achieve a recall of 0.5 with only 1000 proposals, as compared to 0.35 obtained by the BPC.</p><p>In comparison to state-of-the-art approaches, our classinduced method produces temporal proposals with higher specificity. Similar to the localization quality analysis, FAP shows low recall performance at a small number of proposals. Notice also that FAP tends to generate a relatively small number of proposals. For example, on MSR-II, it generates an average of ≈ 50 proposals. Additionally, we observe than BoFrag produces a modest performance when using a small number of proposals which is an indication of a poor ranking quality. On the other hand, APT shows an acceptable performance when using a small number of proposals. However, our method obtains higher recall with much smaller number of proposals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Efficiency Analysis</head><p>In this experiment, we measure the processing speed of our method and compare it against competing approaches on the Thumos14 dataset. As a reminder, proposal generation methods are used to reduce the computational burden of applying expensive action classifiers exhaustively in a sliding window fashion. Therefore, it is important for these methods to process video segments efficiently. <ref type="table">Table 1</ref> summarizes the running time of our proposed method in comparison with competing approaches. The reported time is the average time needed to generate the maximum number of proposals from a single video in the Thu-mos14 dataset. Note that the average length of this video is 180 seconds. Our method achieves the best recall performance while keeping an attractive computational time. In fact, when comparing our method against APT, we are able to generate the same number of proposals 15 times faster while obtaining higher recall. Our method significantly speeds up the required time for generating proposals, while leveraging visual features that can be computed faster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Application to Action Detection</head><p>The end goal of our temporal proposals is to improve the detection of human activities in long, untrimmed video sequences. To analyze the merits of our method towards this goal, we incorporate our method into an action detec-tion pipeline as follows. First, we train action classifiers using the trimmed action instances available in the training set of each dataset. At the testing stage, we process input video to generate temporal proposals with each method. We then apply the trained action classifiers to each temporal proposal. We evaluate the detection performance of this pipeline by measuring the mean Average Precision (mAP). We run the same pipeline with each proposal generation method to compare the final action detection performance.</p><p>For MSR-II, we first extract improved dense trajectories <ref type="bibr" target="#b32">[32]</ref>. Then, we encode the obtained descriptors using Fisher vectors <ref type="bibr" target="#b25">[26]</ref>. As in <ref type="bibr" target="#b31">[31]</ref>, we learn a Gaussian Mixture Model with 128 components. We also use PCA to reduce the descriptors dimensionality to the half. For training, we use a one-vs-all linear SVM classifier. As for the large-scale experiment in Thumos14, we use the features provided in the action detection challenge <ref type="bibr" target="#b13">[14]</ref>. Next, we learn a χ 2 kernel SVM within a Multi-Channel approach as in <ref type="bibr" target="#b17">[18]</ref>.</p><p>In <ref type="table">Table 2</ref>, we summarize the mAP detection results obtained by the various methods on both datasets. As stated earlier, we randomly retrieve proposals for APT and FAP from the initial subset of proposals they generate. This is because their lack of scored proposals. We demonstrate that our class-induced proposals provide a significant benefit to the action detection pipeline. In both datasets, our proposals obtain better performance compared to more computationally demanding approaches such as APT. Additionally, the results show that our method is able to obtain an acceptable mAP with only a small number of proposals. <ref type="table">Table 3</ref> compares our temporal action detection results to the state-of-the-art on the Thumos14 detection challenge <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b28">29]</ref>. Our Class-Induced proposals achieve a 13.5% mAP score at 0.5 overlap threshold, as compared to 14.3% obtained by the top performer in Thumos14 <ref type="bibr" target="#b22">[23]</ref>. Although the computation time of the latter method is not available for direct comparison, we estimate it to be higher than APT, as it uses a sliding window approach. Therefore, this result is encouraging considering that our method scans less temporal windows and provides a faster detection pipeline.</p><p>Method Sun et al. <ref type="bibr" target="#b28">[29]</ref> Wang et al. <ref type="bibr" target="#b33">[33]</ref> Oneata et al. <ref type="bibr" target="#b22">[23]</ref> Ours mAP 4.4% 8.3% 14.3% 13.5% <ref type="table">Table 3</ref>. Comparison to the state-of-the-art on the Thumos14 detection challenge. We report the mean Average Precision (mAP) at 0.5 threshold. Our Class-Induced proposal method achieves a competitive performance while keeping an attractive computational complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Qualitative Results</head><p>In <ref type="figure" target="#fig_4">Figure 5</ref>, we present a qualitative analysis of proposals generated by our class-induced method. We show the Top-5 highest ranked and the Bottom-5 worst ranked proposals.</p><p>Notice the ability of our method to highly score proposals that are related with a previously seen action. For ex- Top-5 best ranked proposals Bottom-5 worst ranked proposals  ample, all the five best ranked proposals are related with one of the 20 classes on Thumos14. As illustrated by the figure, our proposal method is able to tightly localize the actions. Additionally, our method ranks unseen actions with low scores, as exemplified in the proposal that contains frames from a penalty kick foul. Interestingly, we find an incomplete high jump action ranked in the bottom. This is evidence that our proposal method is able to discard low quality proposals.</p><p>In general, as showed in Section 4.2, our proposal method is able not only to retrieve proposals with good localization but also to rank them with high score. In <ref type="figure" target="#fig_5">Figure  6</ref>, we show two illustrative examples where the Top-3 best ranked proposals correctly match the ground truth, and one example where it fails to retrieve the actions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>We introduced a framework that generates temporal action proposals on untrimmed videos. We demonstrated that our method is able to generate high quality proposals in term of: localization and ranking. From the efficiency point of view, our proposal method was able to generate proposals 15 times faster than previous approaches, as it runs at 10FPS. We also showed that our proposals can serve an important role in an end-to-end action detection pipeline by improving its overall performance on a large-scale benchmark. For future work, we are interested in further improving the efficiency of our proposal method by interleaving or combining the feature extraction and proposal representation stages, which are currently done independently.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>Visualization of temporal action proposals for a sample long video of five minutes. Our method is not only able to retrieve the temporal locations of actions with high recall but also it generates proposals quickly.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Recall rate at different tIoU thresholds.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Average recall rate against average selected number of proposals. The recall is averaged over multiple tIoU thresholds from 0.5 to 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Left: Top-5 best ranked proposals from entire Thumos14 testing set. Right: Bottom-5 worst ranked proposals from entire Thumos14 testing set.False positive proposalGround truth True positive proposal</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Two illustrative examples where our proposal method correctly covers the ground truth, and one example where it fails.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head></head><label></label><figDesc>Table 2. Detection performance (mAP) for different approaches and datasets. Given the lack of available implementation, results for FAP in Thumos14 are not reported.</figDesc><table>Baselines 

Previous Work 
Our Method 
Uniform Sampling 
BPC 
FAP [35] 
APT [31] 
Class-Independent 
Class-Induced 
tIoU 
0.125 0.5 
0.8 0.125 0.5 
0.8 0.125 0.5 
0.8 0.125 0.5 
0.8 0.125 0.5 
0.8 0.125 0.5 
0.8 

# Proposals 
MSR-II 
10 
45.3 29.7 11.5 65.8 46.3 18.9 68.9 27.1 12.1 66.5 45.1 22.1 56.7 32.3 18.3 72.9 55.4 26.3 
50 
50.1 31.3 13.2 73.1 49.9 21.1 71.3 28.2 12.3 72.7 49.8 28.7 58.3 33.7 18.9 76.1 57.7 29.1 
100 
52.2 30.9 9.7 
75.1 55.2 21.3 
− 
− 
− 
74.1 54.5 33.7 58.6 34.1 19.2 80.1 60.3 33.9 
Thumos14 
10 
9.1 
2.4 
0.9 
22.7 
6.2 
3.5 
− 
− 
− 
22.2 
5.8 
3.2 
15.4 
3.5 
1.7 
25.7 
9.5 
4.1 
100 
12.1 
3.2 
1.3 
29.7 
8.7 
4.3 
− 
− 
− 
27.1 
7.9 
4.1 
19.1 
5.9 
2.9 
33.5 12.1 
6.9 
1000 
19.1 
3.1 
1.9 
32.1 
9.9 
5.1 
− 
− 
− 
30.7 
9.1 
4.8 
21.8 
6.7 
3.3 35.7 13.5 7.5 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Measuring the objectness of image windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Alexe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2189" to="2202" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multiscale combinatorial grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Trajectory-based fisher kernel representation for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Atmosukarto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Activitynet: A large-scale video benchmark for human activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Camera motion and surrounding scene appearance as context for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Actionness ranking with lattice conditional ordinal random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning to segment moving objects in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Felsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Local features are not lonely -laplacian sparse coding for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-T</forename><surname>Chia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Finding action tubes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning sparse representations for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Guha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ward</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1576" to="1588" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">What makes for effective detection proposals?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hosang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="814" to="830" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Action localization with tubelets from motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">V</forename><surname>Gemert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bouthemy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<ptr target="http://crcv.ucf.edu/THUMOS14/" />
		<title level="m">THUMOS challenge: Action recognition with a large number of classes</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Supervised learning and codebook optimization for bag-of-words models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Baskurt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Computation</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="409" to="419" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">On space-time interest points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="107" to="123" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning realistic human actions from movies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marszalek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rozenfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Supervised dictionary learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">R</forename><surname>Bach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Bag-of-fragments: Selecting and encoding video fragments for event detection and recounting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mettes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Van Gemert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cappallo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th ACM on International Conference on Multimedia Retrieval</title>
		<meeting>the 5th ACM on International Conference on Multimedia Retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="427" to="434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Unsupervised learning of human action categories using spatial-temporal words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="299" to="318" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Spatiotemporal object detection proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Oneata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">The LEAR submission at thumos 2014. THUMOS14 Action Recognition Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Oneata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Bag of visual words and fusion methods for action recognition: Comprehensive study and good practice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1405.4506</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">An efficient projection for l1,∞ regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Quattoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Carreras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Image classification with the fisher vector: Theory and practice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">105</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="222" to="245" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Recognizing human actions: a local svm approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schuldt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Locality-constrained and spatially regularized coding for scene categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shabou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Leborgne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Temporal localization of fine-grained actions in videos by domain transfer from web images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd</title>
		<meeting>the 23rd</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<title level="m">Annual ACM Conference on Multimedia Conference</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="371" to="380" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Selective search for object recognition. International journal of computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">E</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Smeulders</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="page" from="154" to="171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">APT: Action localization proposals from dense trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Van Gemert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G M</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Action recognition with improved trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Action recognition and detection by combining motion and appearance features. THU-MOS14 Action Recognition Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Linear spatial pyramid matching using sparse coding for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Fast action proposals for human action detection and search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Discriminative subvolume search for efficient action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Visual classification with multi-task joint sparse representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Robust visual tracking via structured multi-task sparse learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="367" to="383" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<title level="m">Structural Sparse Tracking. In CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Quasi real-time summarization for consumer videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Edge boxes: Locating object proposals from edges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
