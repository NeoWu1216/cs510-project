<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Sparse to Dense 3D Reconstruction from Rolling Shutter Images</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Saurer</surname></persName>
							<email>saurero@inf.ethz.ch</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Mechanical Engineering</orgName>
								<orgName type="institution">ETH Zürich</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Pollefeys</surname></persName>
							<email>marc.pollefeys@inf.ethz.ch</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Mechanical Engineering</orgName>
								<orgName type="institution">ETH Zürich</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gim</forename><forename type="middle">Hee</forename><surname>Lee</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Sparse to Dense 3D Reconstruction from Rolling Shutter Images</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:46+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>It is well known that the rolling shutter effect in images captured with a moving rolling shutter camera causes inaccuracies to 3D reconstructions. The problem is further aggravated with weak visual connectivity from wide baseline images captured with a fast moving camera. In this paper, we propose and implement a pipeline for sparse to dense 3D construction with wide baseline images captured from a fast moving rolling shutter camera. Specifically, we propose a cost function for Bundle Adjustment (BA) that models the rolling shutter effect, incorporates GPS/INS readings, and enforces pairwise smoothness between neighboring poses. We optimize over the 3D structures, camera poses and velocities. We also introduce a novel interpolation scheme for the rolling shutter plane sweep stereo algorithm that allows us to achieve a 7× speed up in the depth map computations for dense reconstruction without losing accuracy. We evaluate our proposed pipeline over a 2.6km image sequence captured with a rolling shutter camera mounted on a moving car.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The majority of image sensors on the market today (found in mobile phones, and compact cameras etc.) are CMOS sensors. In contrast to standard CCD sensors which have global sensor readout, classical CMOS sensors have sequential readout. This leads to sequential exposures of each image row or column -commonly known as rolling shutter. For example, typical readout times in todays mobile phones are around 10-40ms <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b27">28]</ref>. Given such a delay, significant deformations appear in the image when the camera is in motion. Assuming a camera with a readout time of 35ms and moving at 5km/h, objects which are closer than 25m will show deformations due to the rolling shutter <ref type="bibr" target="#b25">[26]</ref> effects.</p><p>In the last decade, Structure from Motion (SfM) techniques have been used to build 3D models from both ordered and unordered image sequences <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b15">16]</ref>. These SfM techniques rely on a global shutter camera model and become brittle when used on rolling shutter images <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref>. Hedbors <ref type="bibr" target="#b11">[12]</ref> introduced a BA method which provides accurate structure and motion from a rolling shutter video stream by using a continuous time parametrization of the camera pose. It is however limited to continuous image streams with small baselines and does not work on sparse images with wide baselines. Moreover, none of the existing works showed a full pipeline that does full sparse to dense 3D reconstruction from rolling shutter images with wide baselines.</p><p>In this paper, we propose and implement a pipeline for sparse to dense 3D construction with wide baseline images captured from a fast moving rolling shutter camera. <ref type="figure" target="#fig_0">Fig. 1</ref> shows an example of the sparse and dense 3D reconstructions obtained from our proposed pipeline. Wide baseline images are captured at a low frame rate of 4Hz by a rolling shutter camera mounted on a moving car, driven at 17km/h. Specifically, we propose a cost function for BA that models the rolling shutter effect, incorporates GPS/INS readings, and enforces pairwise smoothness between neighboring poses. We optimize over the 3D structures, camera poses and velocities. In contrast to <ref type="bibr" target="#b14">[15]</ref> that minimizes the absolute difference of the GPS/INS and camera poses, our smoothness term minimizes the difference between neighboring camera relative poses and their corresponding neighboring GPS/INS relative poses. As a result, our BA is able to compensate for the drifts that are accumulated in large scenes from the weak visual connectivity of wide baseline images without the risk of causing discontinuities in the poses.</p><p>The optimized camera poses are used to compute dense motion stereo. We adopt the plane sweep algorithm for rolling shutter camera proposed by <ref type="bibr" target="#b25">[26]</ref>. We show that we can achieve a 7× speed up in the depth map computation without losing accuracy with our novel interpolation scheme for the rolling shutter plane sweep stereo algorithm. We evaluate and show the feasibility of our approach on a 2.6km sequence, and compare our sparse and dense 3D reconstructions to those obtained from global shutter reconstructions.</p><p>Our contributions can be summarized as follow:</p><p>• Propose and show a working pipeline for full sparse to dense 3D reconstruction from large scale wide baseline rolling shutter images.</p><p>• New cost function for BA that models the rolling shutter effect, incorporates GPS/INS readings, and enforces pairwise smoothness between neighboring poses.</p><p>• Achieved 7× speed up in depth map computation without losing accuracy with our novel interpolation scheme on the 3D planes for the rolling shutter plane sweep stereo algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>In the recent years, an increasing number of 3D vision algorithms originally designed for global shutter cameras are reformulated to include the rolling shutter camera model. These algorithms cover many aspects of 3D vision from camera calibration, pose estimation, bundle adjustment to dense motion stereo etc. However, none of these existing works showed a full sparse to dense 3D reconstruction from wide baseline rolling shutter images.</p><p>The authors of <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b22">23]</ref> suggested algorithms for the calibration of rolling shutter timing. Others <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b4">5]</ref> have looked into rolling shutter wobble corrections using additional sensors such as gyroscopes or assuming that a rolling shutter wobble is induced by only a pure rotational motion. <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b3">4]</ref> addressed the rolling shutter pose estimation problem with the minimal solutions from different variants of rolling shutter camera model. Meilland et al. <ref type="bibr" target="#b21">[22]</ref> proposed a RGB-D SfM algorithm which simultaneously solves for motion blur and rolling shutter deformations. In <ref type="bibr" target="#b20">[21]</ref>, the authors proposed an iterative algorithm to solve for the camera pose and velocity.</p><p>A rolling shutter camera bundle adjustment for a continuous stream of small baseline images was shown in <ref type="bibr" target="#b11">[12]</ref>. They reformulate the bundle adjustment problem such that it requires 6 additional parameters compared to the global shutter version. These 6 additional parameters are due to the exploitation of the fact that the pose of the last scanline corresponds to the pose of the first scanline in the next frame. In <ref type="bibr" target="#b13">[14]</ref>, the authors modelled a rolling shutter camera rig as a generalized camera, where relative poses between cameras are obtained from a GPS/INS system. In the bundle adjustment, they optimized for one global camera rig pose, while using a rolling shutter aware reprojection error.</p><p>Ait-Aider et al. <ref type="bibr" target="#b2">[3]</ref> proposed a stereo algorithm for a rolling shutter stereo rig. Given a set of point correspondences, they recover the object pose and velocity by optimizing a non-linear system of equations. A challenge of finding pixel correspondences between image pairs arises in the monocular setup. While this is done by searching along the epipolar line for global shutter stereo, the search becomes difficult for the rolling shutter stereo where the pixel correspondence lies on an epipolar curve. The curvature of the epipolar curve depends on the motion and lens distortion of both cameras. In <ref type="bibr" target="#b25">[26]</ref>, the authors addressed this problem with a monocular rolling shutter plane sweep stereo algorithm. The main drawback of the algorithm is that it is computationally expensive to solve a high order polynomial for each pixel at various depths. We built upon their findings and proposed a novel algorithm which is 7× faster in speed and provides similar accuracy in depth estimation. <ref type="figure" target="#fig_1">Fig. 2</ref> illustrates our proposed pipeline for sparse to dense 3D reconstruction from rolling shutter images. Our proposed reconstruction pipeline follows the standard SfM approach <ref type="bibr" target="#b23">[24]</ref> with modifications made for wide baseline rolling shutter images. In detail:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Reconstruction Pipeline</head><p>Data Acquisition: Images are captured with a rolling shutter camera rig mounted on a car. The rig consists of 15 cameras arranged such that they cover over 80% of a sphere. In this work, we consider only 8 cameras which point sideways to the driving direction. Images from consecutive frames have wide baseline because our cameras are recording at a low frame rate of 4Hz. Each image is also synchronized with a GPS/INS position. Tracking: We start by extracting SIFT <ref type="bibr" target="#b19">[20]</ref> features using <ref type="bibr" target="#b29">[30]</ref> on the radially distorted images. Features are then matched between neighboring images using a GPU bruteforce matcher creating tracks between consecutive frames. Frames which show little parallax (&lt; 50 pixels) are removed from the tracks.</p><p>Loop closure: Loop closures are detected by comparing the GPS/INS poses. Images that are within a radius of 15m are considered as potential loop closures. Potential loop closures are verified geometrically using a rolling shutter pose estimation algorithm similar to <ref type="bibr" target="#b26">[27]</ref>. We consider a loop closure to valid if it is within the vicinity of their original GPS/INS pose (within 15m) and there is enough inliers (&gt; 100) from the geometric verification. Each detected loop closure is added to BA in the form of a pose graph as an additional constraint (see Section 4 for more details).</p><p>Rolling Shutter BA: Tracks are first radially undistorted with the standard radial/tangential distortion model proposed by Brown <ref type="bibr" target="#b5">[6]</ref>. Next, the undistorted keypoints are triangulated with the camera poses provided by the GPS/INS system. It is important to note that the camera poses provided by the GPS/INS system are not perfect due to systematic errors in calibration and multi-path problems in the urban environment. We use the GPS/INS system for a rough pose estimate of each scanline in the image. We then use our proposed rolling shutter aware BA to optimize over the 3D points, and camera extrinsics i.e. poses and velocities, while enforcing smoothness constraints between neighboring poses in the pose graph. The BA refinement process is further discussed in Section 4.</p><p>Rolling Shutter Stereo: The refined poses are then used to compute a dense 3D model using a multi-view and multiresolution rolling shutter plane sweeping stereo algorithm similar to <ref type="bibr" target="#b25">[26]</ref>. Here, we show that our novel interpolation scheme on the 3D planes for the rolling shutter plane sweep stereo algorithm allows us to achieve a 7× speed up in the depth map computations for dense reconstruction without losing accuracy. The depth maps are regularized using Semi-Global matching <ref type="bibr" target="#b12">[13]</ref>. More details are given in Section 5.</p><p>Depth map Fusion: Finally, all 3D models are merged into a single coordinate frame. Only depth values which obtained support from at least 3 or more views are kept and used to render a dense point cloud.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Rolling Shutter Bundle Adjustment (BA)</head><p>Traditional BA <ref type="bibr" target="#b28">[29]</ref> that minimizes the total reprojection errors is formulated as:</p><formula xml:id="formula_0">argmin R,t m n ||x m,n − K · D(π(P m , X n ))|| 2 ,<label>(1)</label></formula><p>where K is the camera intrinsics parameter, D is the radial distortion function, P = [R, −Rt] is the camera extrinsics, i.e. rotation and translation, x m,n is the feature point corresponding to the 3D point X n observed by the camera m, and π(.) : P 3 → P 2 denotes the projection function. This formulation assumes a global shutter camera model. For a moving rolling shutter camera, each scanline gets exposed at a different place in space along the motion trajectory. As a result, Eq. 1 no longer holds. Similar to <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b3">4]</ref> we propose to use a constant translational and rotational velocity parametrization for the camera pose:</p><formula xml:id="formula_1">t(τ ) = t 0 + vτ, (2a) R(τ ) = exp(Ωτ )R 0 ,<label>(2b)</label></formula><p>where v denotes the translational velocity, Ω denotes the angular velocity and τ the time of exposure (time delay between first an current scanline). R 0 and t 0 are the camera pose at the first scanline. The function exp(.) : so(3) → SO(3) denotes the exponential map that transforms the angle-axis rotation representation to a corresponding rotation matrix. The pose of a given scanline exposed at time τ can be linearly interpolated from Eq. 2, which yields the following parametrized transformation matrix:</p><formula xml:id="formula_2">P(τ ) = R(τ ) −R(τ )t(τ ) .<label>(3)</label></formula><p>Given the continuous time pose parametrization in Eq. 3, we can rewrite the global shutter reprojection error in Eq. 1 as:</p><formula xml:id="formula_3">argmin v,Ω,R0,t0,X m n ||x m,n − K · D(π(P m (τ ) · X n ))|| 2 ,<label>(4)</label></formula><p>where we optimize over the camera pose at the first scanline of each image (R 0 , t 0 ), velocities (v, Ω) and 3D structures X. Unfortunately, the optimization based on Eq. 4 often breaks for our wide baseline images with weak visual connectivity. This problem is made worst at the end of façades in the scene where feature tracks drop tremendously. To overcome this problem, we introduce an additional smoothness term that enforces pairwise smoothness between neighboring poses:</p><formula xml:id="formula_4">(i,j)∈G ||P −1 i (0) · P j (0) − M i,j || 2 .<label>(5)</label></formula><p>P i (0) is the i th camera extrinsics parameters at the first scanline we optimize over, and represented as a 6 dimen-  argmin v,Ω,R0,t0,X m n ||x m,n −K·D(π(P m (τ )·X n ))|| 2</p><formula xml:id="formula_5">sional vector [log(R 0 ) ⊤ , t ⊤ 0 ] where log(.) : SO(3) → so(3). (i, j) ∈ G</formula><formula xml:id="formula_6">M i,j =P −1 i (0)P j (0),<label>(6)</label></formula><formula xml:id="formula_7">+ λ (i,j)∈G ||P −1 i (0) · P j (0) − M i,j || 2 ,<label>(7)</label></formula><p>where λ is a weighting factor we estimated empirically and set to 1e6 in all our experiments. We proposed an alternating optimization approach to minimize the cost function in Eq. 7. First, we optimize over the parameters R 0 , t 0 , X while keeping the parameters v, Ω fixed. Next, we optimize over v, Ω while keeping R 0 , t 0 , X fixed. We alternate between the two configurations until we reach convergence. In all our experiments, the solution converged after a maximum of two iterations, and in most cases one iteration is enough for convergence. We use the Ceres <ref type="bibr" target="#b1">[2]</ref> optimization framework to solve for the non-linear least squares problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Time Continuous Rolling Shutter Stereo</head><p>We adopt and improve upon the plane sweep stereo algorithm proposed in <ref type="bibr" target="#b25">[26]</ref> for rolling shutter cameras. The main difference between the plane sweep algorithm for global and rolling shutter cameras is the way that a pixel gets warped from a target image into a reference image for the evaluation of the photo-consistency cost. This can be done easily with homography for the global shutter cameras. Unfortunately, simple homography is not applicable for the rolling shutter cameras since each scanline has a different pose and therefore the plane parameters vary according to the scanlines. <ref type="bibr" target="#b25">[26]</ref> incorporates the rolling shutter camera model into the plane sweep algorithm to achieve the warping of a pixel from a target to reference image. This results in the need to solve for the time of exposure τ that corresponds to the time (scanline) a given 3D point is imaged by the sensor. More precisely, the time of exposure τ is obtained by solving the following fix-point function:</p><formula xml:id="formula_8">s · K · D(π(P(τ ) · X)) = τ,<label>(8)</label></formula><p>where K denotes the camera intrinsics matrix, D is the lens distortion function, P(τ ) is the camera extrinsic at time τ and X is the 3D point and π(.) : P 3 → P 2 denotes the projection function. s is the scanline selection operator that selects either the top or bottom row of the left hand side of Eq. 8, i.e. we set s = [1, 0] for a shutter that moves horizontally, and s = [0, 1] for a shutter that moves vertically. Note that solving for τ in Eq. 8 requires solving for the roots of a high order polynomial, where the order depends on the lens distortion model and motion parameterization. Using the motion parametrization presented in Section 4, we get a 9 th order polynomial in τ , where τ is solved using the Gauss-Newton minimization. The need to solve a 9 th order polynomial for every pixel in the image quickly becomes computationally expensive. Authors of <ref type="bibr" target="#b25">[26]</ref> reported a processing time of 27ms per image with the Graphics Processing Unit (GPU). In addition, they proposed an alternative approach that first solves the τ values for a subset of pixels on the target image, and next obtain the τ values for all other pixels from interpolations of the subset of pixels with known τ that are warped onto the reference image. However, this technique comes with the cost of interpolation artefacts. Here we propose an alternative interpolation scheme where we process every pixel on the target image, but solve τ for a subset of the plane intersections (each plane corresponds to the depth that is searched for pixel correspondence in the plane sweep stereo algorithm) with each back-projected ray from every pixel on the target image. We solve for all the τ values by interpolating between the τ values solved from the subset of plane intersections projected onto the reference image.   4m to 9m. It should be noted that the closer the intersection point of the ray with the plane gets to the camera, the more rapidly (can be increasing or decreasing depending on the pixel location and camera motion) the time of exposure τ (scanline) changes with increasing velocity.</p><p>Finding a suitable parametrization for the interpolation of τ can be challenging since the curve τ (d) varies a lot with different pixel location, camera motion and radial distortion coefficients. We noted that the depth in which we search for pixel correspondences in the plane sweep stereo is bounded by the two planes, i.e. closest and furthest away from the camera. Exploiting this fact, we can evaluate the τ values for a subset of depths for each pixel and interpolate the missing intermediate τ values. We experimented with different interpolation schemes -quadratic, cubic, quartic, cubic spline and piecewise quadratic. <ref type="figure" target="#fig_5">Fig. 3</ref> shows the errors for the different interpolation schemes. We can see that the error obtained from piecewise quadratic interpolation is the lowest at ≤ 1e − 3 pixel. All the other interpolation schemes give errors that are &gt; 1 pixel, which means that the estimated scanline is off by at least one scanline. We compare the results from the interpolation schemes against a ground truth obtained by solving Eq. 8 with a Gauss-Newton scheme. Since the rolling shutter effect is dependent on the scene depth, the curvature of τ becomes much higher when the plane is closer to the camera, and becomes almost linear when the plane is located further away from the camera. This motivates us to use an adaptive interpolation range in which a quadratic function is fitted in dependence of the relative scene depth. Let c ( √ 3 in our experiments) be the exponential factor and b be the initial interpolation range. The adaptive depth range d i used for the piecewise depth interpolation is obtained using d i = c i b. Given the total number of planes to sweep w, the number of times τ needs to be fully evaluated for each pixel then is: i = log(w/b). In addition, we can sparsely evaluate τ in the image space and bi-linearly interpolate the in between values. Combining those two interpolation schemes gives us an overall speedup of 6.56×, i.e. ∼ 7×.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Results</head><p>We evaluate our proposed pipeline on both synthetic and real datasets. We use the "Castle" and "Old Town" datasets provided by <ref type="bibr" target="#b25">[26]</ref> for the synthetic experiments. The real dataset was captured with a car mounted with a rig of 15 rolling shutter cameras, covering 80% of a sphere. The trajectory has a total length of 2.6km with 17k images. Images have a resolution of 1944 × 2592 pixels and are sparsely recorded at 4Hz. The shutter scan time for each camera is 72ms. On average the car drives at 17km/h, which results in an average camera motion of 0.34m during image formation, meaning the distance between the first and last scanline is 0.34m apart. out BA is noisy. Although the 3D structures appear sharper after global shutter refinement, it is obvious that the method produces misaligned façades. In contrast, our proposed rolling shutter refinement produces a sharp and consistent 3D sparse model. Quantitatively, the histograms in <ref type="figure" target="#fig_10">Fig.  6</ref> shows the count of reprojection error for all 1, 371, 294 3D points. <ref type="figure" target="#fig_10">Fig. 6</ref> (center) shows that the global shutter BA has a very high count of reprojection errors ≥ 5 pixels, while the reprojection errors from our method is significantly lower. <ref type="figure" target="#fig_11">Fig. 7</ref> shows a comparison of the "zoomed-in view" of the camera poses from GPS/INS readings (blue), our rolling shutter BA with pairwise smoothness (green), and global shutter BA without pairwise smoothness (red). It can be clearly seen that the wide baselines between the camera poses cause breakages in the global shutter BA without pairwise smoothness (red). On the other hand, our method (green) produces a smooth trajectory of camera poses.     </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Bundle Adjustment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Stereo</head><p>For our experiments, we use our proposed plane sweep algorithm with a single reference plane normal. The reference plane is obtained by finding the dominant plane using RANSAC <ref type="bibr" target="#b6">[7]</ref> on the sparse point cloud obtained from SfM. A sweep through 3D space is performed within the distance of [d f ront , d back ] around the reference plane. We set d f ront = 5m and d back = 3m. Planes in between are sampled linearly in image space. A pixel transfer between two images is computed by first undistorting a pixel and intersecting the resulting ray with the corresponding plane. The intersection point is then back-projected onto the other view for texture lookup. The correlation between two images is computed using Normalized Cross-Correlation (NCC), with a window size of 5 × 5 pixels. We make use of a multi-resolution approach, where we aggregate the correlation cost over multiple pyramid levels (3 levels in our case) to be more robust towards textureless surfaces. Furthermore we use a multi-view approach to handle occlusions. At each depth (plane), we consider the k = 3 best views out of n = 6 that provide the highest correlation. Once the cost volume is obtained, we regularize it with Semi-Global-Matching <ref type="bibr" target="#b12">[13]</ref> using 16 different path directions. We fit a quadratic through the depth that provides the lowest cost to obtain the final depth. Lastly, all estimated 3D points undergo a geometric verification, where points that give a consistent depth within 3 or more views are considered to be valid. We use a threshold of 0.1m for our consistency check.</p><p>Synthetic Data: Tab. 1 shows the evaluation of our algorithm on synthetic data. It can be seen that the performance of our proposed piecewise quadratic interpolation has accuracies (median error and fill rate) that are comparable to the RS method from <ref type="bibr" target="#b25">[26]</ref> for both datasets, yet achieving 3.7× speedup. We can further see that by combining bilinear and piecewise quadratic interpolations, we achieved a speedup of ∼ 7× over RS without losing much accuracy. <ref type="figure">Fig. 8</ref> shows a visualization of the voxelwise errors from the stereo reconstruction compared to ground truth for RS, FA2 <ref type="bibr" target="#b25">[26]</ref> and ours. It is clear that FA2 has the highest errors (more red), and there is an insignificant reduction in accuracy of our method compared to RS. <ref type="figure">Figure 8</ref>. Visualization of the voxelwise errors from the stereo reconstruction compared to ground truth for RS <ref type="bibr" target="#b25">[26]</ref>, FA2 <ref type="bibr" target="#b25">[26]</ref> and ours on the synthetic "Castle" dataset.</p><p>LiDAR: We also compare our reconstruction to sparse Li-DAR data, which was captured alongside with the image data, in <ref type="figure" target="#fig_12">Fig. 9</ref> and Tab. 2. We re-project the LiDAR data into the estimated depth map and evaluate the respective error of the estimated depth maps computed with our rolling shutter pipeline (rolling shutter BA and stereo) and the global shutter pipeline (global shutter BA and stereo). It should be noted that there might be some inaccuracies in this experiment because the offset between our LiDAR and camera is physically measured. Nonetheless, we observe that in general the global shutter reconstructions have median errors that are ∼ 0.15m more than the rolling shutter reconstructions.</p><p>Global Shutter (m) Rolling Shutter (m) <ref type="figure" target="#fig_12">Fig.9(a)</ref> 0.42 0.23 <ref type="figure" target="#fig_12">Fig.9(b)</ref> 0.37 0.25 <ref type="table">Table 2</ref>. Median errors from global and rolling shutter depth maps, compared to sparse LiDAR data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Full Pipeline</head><p>We show results of sparse to dense reconstruction in <ref type="figure" target="#fig_0">Fig. 10</ref> with our proposed methods on a large scale dataset over 2.6km in length taken at San Francisco City Hall and its surroundings. First we use a global window BA with smoothness prior to refine the camera poses and 3D structure, as described in Section 4. A robust Huber loss function is used to handle outliers. Only tracks of size ≥ 3 are consider in the BA process. After BA, we remove points that have a reprojection error &gt; 1 pixel. In the second stage, we run our rolling shutter aware motion stereo algorithm, as mentioned in Section 5. We adaptively evaluate τ in sweep space at a rate of d i = c i b in all our experiments, where d i denotes the depth interval, c = 1.5 and b = 6. In addition, τ is evaluated sparsely in the image at every 5 th pixel and bilinearly interpolated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>We proposed a rolling shutter bundle adjustment, which optimizes over the 3D structure, the camera poses and velocities using an additional smoothness term to compensate for drift. In addition, we proposed a simple yet efficient interpolation scheme for rolling shutter stereo which speeds up the algorithm by 7×, while providing almost same accuracy as state-of-the art algorithms. We evaluated our pipeline on a camera trajectory of 2.6km length and show quantitative results of the sparse and dense reconstruction. In the future, we would like to fuse the terrestrial reconstruction with aerial reconstructions. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Sparse and dense 3D reconstructions from wide baseline images captured with a rolling shutter camera mounted on a moving car.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Overview of the reconstruction pipeline.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>denotes all pairs of neighboring poses in the pose graph G. Here, j = i + 1 denotes consecutive neighboring poses, and j = i + 1 denotes loop closure neighboring poses mentioned in Section 3. For consecutive neighboring poses, i.e. j = i + 1, M i,j is the corresponding relative pose obtained from the GPS/INS reading:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>whereP i ( 0 )</head><label>0</label><figDesc>andP j (0) are the absolute poses from the GPS/INS at the first image scanline. It becomes obvious now that Eq. 6 penalizes deviation from the GPS/INS poses and preserves smoothness between neighboring poses. For non-consecutive neighboring poses, i.e. j = i + 1, M i,j is the relative pose computed from the pose estimation during the geometric verification in the loop closure mentioned in Section 3. Consequently, our BA formulation is also capable of closing large loop closure errors.Our final objective function then writes as follows:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Fig. 4shows how τ (d) changes with increasing plane depth d over a range of</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 .</head><label>3</label><figDesc>Evaluation of τ interpolation error for four different pixels. The first row shows the different approximations of τ in dependence of the depth d, i.e., τ (d). The second row shows the error to the ground truth. Only piecewise quadratic interpolation gives an error below 1e-3 pixel, while all other interpolation -quadratic, cubic, quartic and cubic spline give at least 1.0 pixel error.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 .</head><label>4</label><figDesc>Representation of τ (d) at different pixel locations over a plane depth range of 9m, d ∈ [4m, 13m].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 5 Figure 5 .</head><label>55</label><figDesc>shows a comparison of the sparse 3D reconstructions (a) without optimization, (b) with global shutter model and (c) with our proposed BA cost. It can be seen that the 3D structures reconstructed from GPS/INS readings with-Sparse 3D reconstructions: (a) Initial model obtained using GPS/INS poses, where the 3D points are very noisy with typical misalignment of 0.2-0.3m. (b) After global shutter BA with misaligned façades (arrows). (c) After rolling shutter BA (proposed method) with sharp and well aligned façades. Note that only points with ≤ 5 pixels reprojection error are shown in (b) and (c). The respective reprojection error distribution for the reconstructions are shown inFigure 6.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 6 .</head><label>6</label><figDesc>Reprojection error distributions for all 1, 371, 294 3D points from the reconstruction inFigure 5. Note that the last bin is extended to infinity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 7 .</head><label>7</label><figDesc>A close-up comparison of camera poses from initial GPS/INS readings (blue), our Rolling Shutter BA with pairwise smoothness (green), and Global Shutter BA (red).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 9 .</head><label>9</label><figDesc>Comparison of rolling shutter and global shutter stereo to LiDAR data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 10 .</head><label>10</label><figDesc>Sample reconstructions of the San Francisco city hall and its surroundings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>Castle Old Town Method speed / warp [ms] median [m] MAD [m] fill rate median [m] MAD [m] fill rate Table 1. Comparison of our methods -Piecewise Quadratic Interpolation (PQI) and Bilinear Interpolation with PQI with RS and FA2 on the "Castle" and "Old Town" synthetic datasets.</figDesc><table>FA2 [26] 
2.2 
1.02 
1.02 
52.8% 
0.26 
0.22 
58.6% 
Bilinear + PQI 
4.2 
0.05 
0.049 
75.6% 
0.099 
0.096 
57.1% 
Piecewise Quadratic 
7.4 
0.049 
0.041 
75.6% 
0.098 
0.096 
57.2% 
RS [26] 
27.7 
0.041 
0.032 
76.3% 
0.085 
0.077 
62.0% 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>We thank the reviewers for their constructive comments. This work is partially supported by a Google award. The last author is funded by a start-up grant #R-265-000-548-133 from the Faculty of Engineering at the National University of Singapore.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Building rome in a day</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Furukawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="105" to="112" />
			<date type="published" when="2001" />
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mierle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Others</forename><surname>Ceres</surname></persName>
		</author>
		<ptr target="http://ceres-solver.org.4" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Structure and kinematics triangulation with a rolling shutter stereo rig</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ait-Aider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Berry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1835" to="1840" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">R6p -rolling shutter absolute camera pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Albl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kukelova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015-06" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Digital video stabilization and rolling shutter correction using gyroscopes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Karpenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">E</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Levoy</surname></persName>
		</author>
		<idno>CTSR 2011-03</idno>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
		<respStmt>
			<orgName>Stanford University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Close-range camera calibration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Photogrammetric Engineering</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="855" to="866" />
			<date type="published" when="1971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Random sample consensus: A paradigm for model fitting with applications to image analysis and automated cartography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Fischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Bolles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="381" to="395" />
			<date type="published" when="1981-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Rectifying rolling shutter video from hand-held devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-E</forename><surname>Forssén</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ringaby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Building rome on a cloudless day</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-M</forename><surname>Frahm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fite-Georgel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gallup</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Raguram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Jen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Dunn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Clipp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th European Conference on Computer Vision: Part IV, ECCV&apos;10</title>
		<meeting>the 11th European Conference on Computer Vision: Part IV, ECCV&apos;10<address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="368" to="381" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Geometric models of rolling-shutter cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Geyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Meingast</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sastry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of OMNIVIS</title>
		<meeting>OMNIVIS</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Stabilizing cell phone video using inertial measurement sensors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hanning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Forslöw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-E</forename><surname>Forssén</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ringaby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Törnqvist</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Callmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Second IEEE International Workshop on Mobile Vision</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Rolling shutter bundle adjustment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hedborg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-E</forename><surname>Forssén</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ringaby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Stereo processing by semiglobal matching and mutual information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hirschmuller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2008-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Street view motion-from-structure-from-motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Klingner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Roseborough</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision</title>
		<meeting>the International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Fusion of gps and structure-from-motion using constrained bundle adjustments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lhuillier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="3025" to="3032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Modeling and recognition of landmark image collections using iconic scene graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-M</forename><surname>Frahm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th European Conference on Computer Vision: Part I, ECCV &apos;08</title>
		<meeting>the 10th European Conference on Computer Vision: Part I, ECCV &apos;08<address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="427" to="440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Analysis and compensation of rolling shutter effect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-K</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1323" to="1330" />
			<date type="published" when="2002-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Subspace video stabilization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gleicher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="4" to="5" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Sba: A software package for generic sparse bundle adjustment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I A</forename><surname>Lourakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Argyros</surname></persName>
		</author>
		<idno>2:1-2:30</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Math. Softw</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In International Journal on Computer</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Global optimization of object pose and motion from a single rolling shutter image with automatic 2d-3d matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Magerand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bartoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ait-Aider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pizarro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th European conference on Computer Vision -Volume Part I, ECCV&apos;12</title>
		<meeting>the 12th European conference on Computer Vision -Volume Part I, ECCV&apos;12<address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="456" to="469" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A unified rolling shutter and motion blur model for 3d visual registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Meilland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Drummond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">I</forename><surname>Comport</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2013-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Rolling shutter camera calibration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Oth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Furgale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kneip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Siegwart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Detailed real-time urban 3d reconstruction from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Nistér</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-M</forename><surname>Frahm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Akbarzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mordohai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Clipp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Engels</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gallup</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Merrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="143" to="167" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Efficient video rectification and stabilisation for cell-phones</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ringaby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-E</forename><surname>Forssén</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="335" to="352" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Rolling shutter stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Saurer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Koser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Bouguet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A minimal solution to the rolling shutter pose estimation problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Saurer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE/RSJ International Conference on</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note>Intelligent Robots and Systems</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Camera rolling shutter amounts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Thalin</surname></persName>
		</author>
		<ptr target="http://www.guthspot.se/video/deshaker.htm.1" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Bundle adjustment a modern synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mclauchlan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fitzgibbon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Workshop on Vision Algorithms: Theory and Practice, ICCV</title>
		<meeting>the International Workshop on Vision Algorithms: Theory and Practice, ICCV</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2000" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">VLFeat: An open and portable library of computer vision algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fulkerson</surname></persName>
		</author>
		<ptr target="http://www.vlfeat.org/" />
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
