<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Temporal Epipolar Regions</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mor</forename><surname>Dar</surname></persName>
							<email>mor.dar@post.idc.ac.il</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">The Interdisciplinary Center</orgName>
								<address>
									<postCode>46150</postCode>
									<settlement>Herzliya</settlement>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yael</forename><surname>Moses</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">The Interdisciplinary Center</orgName>
								<address>
									<postCode>46150</postCode>
									<settlement>Herzliya</settlement>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Efi</forename><surname>Arazi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">The Interdisciplinary Center</orgName>
								<address>
									<postCode>46150</postCode>
									<settlement>Herzliya</settlement>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Temporal Epipolar Regions</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:47+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Dynamic events are often photographed by a number of people from different viewpoints at different times, resulting in an unconstrained set of images. Finding the corresponding moving features in each of the images allows us to extract information about objects of interest in the scene. Computing correspondence of moving features in such a set of images is considerably more challenging than computing correspondence in video due to possible significant differences in viewpoints and inconsistent timing between image captures. The prediction methods used in video for improving robustness and efficiency are not applicable to a set of still images. In this paper we propose a novel method to predict locations of an approximately linear moving feature point, given a small subset of correspondences and the temporal order of image captures. Our method extends the use of epipolar geometry to divide images into valid and invalid regions, termed Temporal Epipolar Regions (TERs). We formally prove that the location of a feature in a new image is restricted to valid TERs. We demonstrate the effectiveness of our method in reducing the search space for correspondence on both synthetic and challenging real world data, and show the improved matching.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>While most moving object analysis is based on video data, videos of dynamic scenes are not always available. Instead of videos, still images captured by observers of a dynamic event may be considered for analyzing a moving object. Such a set of images, termed CrowdCam ( <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b9">10]</ref>), is taken from multiple viewpoints at different times. As in videos, feature matching may serve as a basic component for analysis of moving objects. A common strategy to improve efficiency and robustness of feature matching in a video sequence is to limit the search space of a feature location using prediction. Such methods are not applicable to CrowdCam data, as video based approaches assume short, consistent time intervals between frames as well as <ref type="figure" target="#fig_3">Figure 1</ref>: An example (dataset h1) of limiting the search space for correspondence using TERs. Epipolar lines calculated from three correspondences (yellow stars) are used in conjunction with a known temporal order to define valid regions in a fourth image, greatly limiting the search space for correspondence. (Best viewed on a computer screen.) no significant viewpoint change. In this paper, we propose a method to predict the possible location of a candidate match in CrowdCam images.</p><p>To predict the location of moving features, assumptions must be made relating to the movement of the object, the positions of the cameras, or the timing of the frames. Our work assumes the following: (i) The features to be matched are moving in an approximately linear 3D trajectory. This is a common assumption in many video-based tracking works (e.g., <ref type="bibr" target="#b29">[30]</ref>). (ii) The epipolar geometry between pairs of cameras can be computed using static features from a common background. Our method does not require epipolar geometry for every pair, but at least three fundamental matrices for each image in which we search for correspondence. (iii) The temporal order of the image captures is given or can be computed directly from the data. When reliable camera clocks are unavailable, the temporal order of CrowdCam images can be computed using photo sequencing <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>. These studies use a similar setup and the two aforementioned assumptions. (iv) The feature location in at least three images is given. If not, we can use standard matching techniques as an initialization in order to find them. Note that prediction methods in videos also assume that some initial correspondence can be computed.</p><p>In order to predict the possible locations of a moving feature in a new image, I, we utilize assumptions (i-iv) to define valid and invalid regions for correspondence in I. Assumptions (ii) and (iv) are used to compute epipolar lines in I from the known correspondences. In contrast to static points, a moving point is not restricted to lie on the epipolar line. However, using the approximate linear motion and known temporal order, we show how to use epipolar lines to restrict the possible locations of the point. To that end, we define regions in I which are bounded by the epipolar lines and their parallels. We prove that the given temporal order of the set of images determines whether each region is valid or invalid for the feature location. As such, we name these regions Temporal Epipolar Regions (TERs). The validity of each TER depends only on the temporal order, and therefore for any set of images and features, the same lookup table may be used to assign validity to TERs (e.g., <ref type="table">Table 1</ref>). We discuss relaxing our assumptions in Sec. 4.</p><p>We demonstrate, through experimental results on a variety of datasets, that TERs considerably restrict the possible location of features within the images. We further show that additional matched features increase the set of known corresponding points and, in turn, decrease the size of the valid TERs in the remainder of the images. As TERs reduce the search space for a corresponding feature, they may increase the efficiency and accuracy of any feature matching algorithm. The overhead required for our method consists of preprocessing to calculate the fundamental matrices and computing the temporal order, if not available. The calculation of the valid regions is computationally inexpensive as it involves computing epipolar lines and utilizing a lookup table.</p><p>The main contributions of our work are (i) the introduction of the novel problem of predicting the location of moving image points in a CrowdCam setting and (ii) the proposed extension of epipolar geometry to constrain the location of moving feature points in a set of images, as opposed to the classic use of epipolar lines for restricting the location of static features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Prediction of feature (or object) location in successive frames is a basic building block in video based tracking methods (see review by <ref type="bibr" target="#b29">[30]</ref>). A na√Øve prediction is a window around the location of the feature (or object) in a previous frame (e.g., <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b32">33]</ref>). Another approach is to utilize a motion model learned from previous frames. This approach often utilizes Kalman or Particle filters (e.g., <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b24">25]</ref>). As these strategies for prediction assume short, consistent, time intervals between frames as well as no significant viewpoint change between frames, they do not apply to CrowdCam image sets.</p><p>The epipolar constraints between a pair of images are used to restrict the possible set of correspondences for static scenes. For example, they can be used to find dense correspondence for pairs of images (e.g., <ref type="bibr" target="#b27">[28]</ref>) or rejecting incorrect matches when searching for sparse correspondence (e.g., <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b13">14]</ref>. A recent work by Shah et al. <ref type="bibr" target="#b28">[29]</ref> uses epipolar geometry between wide-baseline images to predict correspondence to improve matching in the presence of repetitive structures. When a set of images is available, Structure From Motion can also be used to improve correspondences ( <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b30">31]</ref>). However, these studies, which use geometric constraints, only apply to static features.</p><p>Finding correspondence between still images has been very well studied and varies in its approaches according to the goal of the work. Direct comparison of descriptors <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b6">7]</ref> may be the most common approach for matching. Our proposed method is complementary to direct feature matching, as TERs are used to limit the search space. Therefore, matching strategies may be used in conjunction with our method for improved matching accuracy.</p><p>The CrowdCam is becoming increasingly popular as various devices such as smartphones are used for capturing dynamic events. Therefore, novel problems are being addressed in a number of recent studies. Some focus on visualization of CrowdCam video sequences (e.g., <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b0">1]</ref>). Others order the images in time <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b15">16]</ref> or space (e.g., <ref type="bibr" target="#b1">[2]</ref>). Our work is another step forward in extracting information available in CrowdCam images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Spatial-Temporal Consistent Regions</head><p>In this section, we define TERs, the validity of regions, and describe how to determine which of the TERs are valid. Assume a 3D point Q travels along a unidirectional linear trajectory and is projected to the set of images I = {I j } at a set of unknown times T = {t(I j )}. Let,≈ú q = {q j } be the unknown projection of Q onto the set I such that q j is the projection of Q onto image I j at time t(I j ). Given a known corresponding subset S q ‚äÇ≈ú q , our goal is to find the remainder of the set. Finding q u in I u requires overcoming possible ambiguities and may be computationally intensive. To narrow the search, we propose a method for defining valid and invalid image regions where q u ‚àà I u can or cannot be located. To do so, we use the fundamental matrices F u,j between images I u and I j , where q j ‚àà I j and q j ‚àà S q .</p><p>In order to define the said regions, we use the temporal order of the set of images given by the permutation œÉ of the indices of I. As such, œÉ : {1 . . . N } ‚Üí {1 . . . N }, such that t(I œÉ(1) ) &lt; t(I œÉ(2) ) . . . &lt; t(I œÉ(N ) ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Consistency Definitions</head><p>To best describe our method, we begin by defining the spatial-temporal consistency of a set of points. We then propose a method to determine valid and invalid regions using spatial-temporal consistency.</p><p>Definition 1: The set of points S q is spatial-temporally consistent (STC) with a linear motion and œÉ iff the following conditions hold:</p><p>1. There exists a set of capturing times T = {t(I j )} which is consistent with the temporal order, œÉ.</p><p>2. There exists a set of 3D points S Q = {Q j } along a 3D line, L, such that q j is the projection of Q j onto image I j at time t(I j ).</p><p>3. The relative spatial locations of each point in S Q along L correspond to the temporal order, œÉ.</p><p>Note that if t(I i ) &lt; t(I j ) &lt; t(I k ), then Q j ‚àà L Q i , Q k . That is, Q j is located on the interval of L between Q i and Q k . Therefore, (1) can be verified using the relative spatial locations of S Q along L.</p><p>Assume that we are given S q and we search for its unknown correspondence q u in an additional image I u . A point Œ± u ‚àà I u is a candidate location for q u only if S q ‚à™ {Œ± u } is a STC set. Such a Œ± u is termed a valid spatialtemporally consistent point with respect to S q and œÉ or, for short, a valid point.</p><p>It is possible to compute L through trajectory reconstruction, when at least five correspondences are known, and a full calibration of all cameras is available (e.g., <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b14">15]</ref>). We would like to consider the validity of a point while avoiding direct computation of the 3D set S Q (and therefore L) and the timing set T . Our method requires a weaker calibration (only fundamental matrices between partial set of images) than those required for L recovery, and only three corresponding points as initialization. Furthermore, our method is less sensitive to deviation from linear motion.</p><p>Consider the set of 2D points, S u = {p j }, the projections of S Q onto a single image, I u . That is, p j is the projection of Q j onto I u . Note that the spatial order of S Q along L is identical to the spatial order of the corresponding set S u along ‚Ñì u , the projection of L onto I u . Therefore, the temporal consistency of S q can be verified by the spatial order of S u along ‚Ñì u . However, S u and ‚Ñì u are both unknown. That being said, as we know that p j ‚àà I u corresponds to the given q j ‚àà S q , we can limit the location of p j ‚àà I u to the epipolar line ‚Ñì j on I u , given byl j = F ujqj (wher·∫Ω k are the homogeneous coordinates of k). As such, let us consider the order of the intersections of a line ‚Ñì, passing through Œ± u , with the epipolar lines defined by S q and the point Œ± u . If the order of the crossings matches œÉ, we say that ‚Ñì conserves œÉ. The point Œ± u is valid if there exists a line ‚Ñì which conserves œÉ. Conversely, Œ± u is invalid if no such line exists.</p><p>Definition 2: (i) A valid temporal epipolar region is the set of all valid points in an image I u with respect to S q and œÉ. (ii) An invalid temporal epipolar region is the set of all invalid points in an image I u with respect to S q and œÉ.</p><p>In order to demonstrate how TERs and their validity are calculated efficiently, we take a closer look at the epipolar lines corresponding to S q on I u .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Two Epipolar Lines</head><p>First, let us discuss the degenerate case where we are given two parallel epipolar lines, ‚Ñì j and ‚Ñì k , on I u and œÉ = (j, u, k). Consider a point Œ± u in the area between ‚Ñì j and ‚Ñì k and any line ‚Ñì, not parallel to ‚Ñì j , passing through it. Let Œ± j and Œ± k be the intersections of ‚Ñì with the lines ‚Ñì j and ‚Ñì k . Clearly, Œ± u is on the interval between Œ± j and Œ± k on ‚Ñì; hence the order of œÉ is preserved by ‚Ñì and therefore Œ± u is a valid point <ref type="figure" target="#fig_0">(Fig. 2a)</ref>. Note that all points within this area are similarly valid, while all points outside the area are invalid. As such this image is split into one valid and two invalid temporal epipolar regions.</p><p>In the more general case, in which the epipolar lines are not parallel, all candidate points in the image are valid <ref type="figure" target="#fig_0">(Fig. 2b)</ref>. We next show that, given more than 2 epipolar lines, it is possible to limit the valid regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Three Epipolar Lines</head><p>Given three epipolar lines, ‚Ñì i , ‚Ñì j , and ‚Ñì k , and a temporal order defined by œÉ, we can split the image plane of I u into sixteen distinct TERs of five types <ref type="figure" target="#fig_1">(Fig. 3</ref>). TERs are defined by ‚Ñì i , ‚Ñì j , and ‚Ñì k , and their parallels. We define the linel j as a line parallel to ‚Ñì j and passing through the intersection of ‚Ñì i and ‚Ñì k (l i andl k are defined similarly). The following claims are proved in the supplementary material:  <ref type="table">Table 1</ref> summarizes the classifications of TERs as valid or invalid given œÉ, defined without loss of generality as having the suborder œÉ ‚Ä≤ = (i, j, k) and four possible locations of u. In practice, much of the efficiency of our method is derived from this classification scheme. As we know which TERs will be valid given a certain œÉ, there is no need to search for ‚Ñì for every point Œ± u in each region. Instead, we can simply label regions as valid or invalid using <ref type="table">Table 1</ref>.</p><formula xml:id="formula_0">(R1) (R2) (R3) (R4) (R5)</formula><p>We next outline the proof for the above classifications. Inspired by <ref type="bibr" target="#b8">[9]</ref>, we define for a point, Œ± u , six sections. Each section defines a unique order in which an ‚Ñì passes through Œ± u and the set of epipolar lines. The sections are defined using two types of critical lines. The first is the line, c ij , connecting Œ± u with the intersections of a pair of epipolar lines. Formally, let Œ≥ ij =l j √ól i and c ij =Œ± u √óŒ≥ ij . The second type of critical line is parallel to the epipolar lines and passes through Œ± u (i.e., c i is parallel to ‚Ñì i and passes through Œ± u ). <ref type="figure" target="#fig_4">Fig. 4</ref> gives an example of sections for a point in R2(i, j). Note that Œ± i =l √ól i (in homogenoues coordinates) and that Œ± j and Œ± k are similarly defined.</p><p>The following observations can be easily verified geometrically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A1</head><p>The order of Œ± i and Œ± j is swapped in neighboring sections which share the border c ij .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A2</head><p>The location of Œ± i in the order moves from first to last (or vice versa) in neighboring sections which share the border c i .</p><p>A3 Within each section, the order of intersections of all lines is preserved.</p><p>Using these observations about sections and the order of critical lines, we use one valid order to calculate the rest.</p><p>Let us consider as an example the region R2. Without loss of generality, two non-parallel lines divide a 2D space into four areas. In the general case, a third line will pass through three of these areas. The remaining area is defined to be R2.  Proof Outline of Claim 1: A formal proof can be found in the supplementary material. Consider a line ‚Ñì in the section bordered by the pair (c ij , c ik ) (see <ref type="figure" target="#fig_4">Fig. 4</ref>). By definition it passes through Œ± u and R1 (the triangular region defined by three non-parallel epipolar lines). It is easy to visually verify that the intersection order is given by œÉ = (u, j, i, k). The intuition is as follows. As c ij and c ik both intersect ‚Ñì i on the border of R1, ‚Ñì must also intersect ‚Ñì i in this interval. As this intersection is outside of R2(i, j), ‚Ñì must intersect ‚Ñì j between Œ± u and ‚Ñì i . As R1 is a closed convex shape (triangle), any line passing through it must cross two of its borders. As such, ‚Ñì k and ‚Ñì i are adjacent in the intersection order. Finally, as ‚Ñì k does not pass through R2(i, j), it cannot be between ‚Ñì j and ‚Ñì i in the order. Therefore, we conclude that the order of intersections of ‚Ñì must be œÉ = (u, j, i, k).</p><p>To find the remainder of the valid orders of R2(i, j), we utilize observations A1-A2 and the order of the critical lines, given by (c jk , c ij , c ik , c i , c k , c j ) (a formal proof is given in the supplementary material). Critical line c ik is on the border of neighboring sections (c ij , c ik ) and (c ik , c i ). To find the order in section (c ik , c i ), A1 dictates that a swap be made between i and k in the order, such that œÉ = (u, j, k, i). Using A2, the order in section (c i , c k ), which shares the border c i with (c ik , c i ), is given by moving i to the opposite end of the order. As such, for this section we have that œÉ = (i, u, j, k). The remaining valid orders, œÉ = (k, i, u, j), œÉ = (j, k, i, u), and œÉ = (u, i, j, k), can be computed similarly. These six orders represent the 6 distinct valid orders of R2(i, j) out of the 12 possible orders. Hence, the remaining 6 are invalid. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Beyond Three Epipolar Lines</head><p>When |S q | &gt; 3 (more than three images with known correspondences), we calculate valid TERs on I u for each subset of three images with known correspondences, then find the intersection of all the valid TERs. For each subset we use a subpermutation of œÉ that conserves the relative order between S q and u. The intersection of all the valid TERs defines an overall valid region. This region does not guarantee that there exists a line which conserves œÉ. Instead, it guarantees the correctness of the invalid regions. Even though this method is not optimal, a larger |S q | can only further limit valid TERs, and therefore the search space. For an optimal computation of the valid region, it is necessary to define additional region types for each size of S q . This is left for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">TERs and matching</head><p>Now that we have defined the validity of TERs, we focus on how to utilize them to improve feature matching algorithms, and to propose correspondence verification. As a first step of our method, features are extracted from all the images (we use SIFT features <ref type="bibr" target="#b17">[18]</ref>) and the fundamental matrices F ij between required pairs of images I i , I j ‚àà I are computed (we use the BEEM algorithm <ref type="bibr" target="#b10">[11]</ref>). If the temporal order of the images is unknown, it is possible to utilize photosequencing <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b8">9]</ref> as an additional preprocessing step to calculate it. A set S q of at least three correspondences is either given or computed by any standard matching algorithm.</p><p>Matching using prediction: Given S q and the set of fundamental matrices F ij , we define TERs in each image for which correspondence remains unknown. For |S q | = 3, the valid regions in each image are defined using <ref type="table">Table 1</ref>. For larger |S q |, the method described in Sec. 3.4 is used. Only the features in the valid regions are considered as candi-œÉ</p><p>Valid TERs u, i, j, k R 2 (i, j), R 2 (i, k), R 3 (i,ƒµ,k), R 4 (√Æ,k), R 4 (ƒµ,k), R 5 (i,√Æ,k), R 5 (j,ƒµ,k), R 5 (k,k,ƒµ) i, u, j, k R 1 (i, j, k), R 2 (i, j), R 3 (j,√Æ,k), R 3 (k,√Æ,ƒµ), R 4 (√Æ,ƒµ), R 5 (i,√Æ,ƒµ), R 5 (i,√Æ,k), R 5 (j,ƒµ,√Æ) i, j, u, k R 1 (i, j, k), R 2 (j, k), R 3 (j,√Æ,k), R 3 (i,ƒµ,k), R 4 (ƒµ,k), R 5 (j,ƒµ,k), R 5 (k,k,√Æ), R 5 (k,k,ƒµ) i, j, k, u R 2 (i, k), R 2 (j, k), R 3 (k,√Æ,ƒµ), R 4 (√Æ,ƒµ), R 4 (√Æ,k), R 5 (i,√Æ,ƒµ), R 5 (j,ƒµ,√Æ), R 5 (k,k,√Æ) <ref type="table">Table 1</ref>: This table defines, without loss of generality, the valid regions given a suborder of œÉ, œÉ ‚Ä≤ = (i, j, k), and the location of u ‚àà œÉ; all other regions are invalid.</p><p>dates for correspondence. From these features, the nearest neighbor to the features in S q is chosen as the corresponding point, Œ± u (note that other matching criteria may be used). Then, S q is updated to S q = S q ‚à™ {Œ± u }. The additional constraints defined by Œ± u are used to update and reduce the size of the valid regions in the remainder of the images. This process is repeated until all the images are assigned correspondence. We present an example of matching using prediction in <ref type="figure">Fig. 5</ref>. An alternative approach which may be more efficient, is to compute features only in valid regions.</p><p>Relaxed TERs: The valid regions may be unreliable when the motion deviates from linearity or when the fundamental matrices are inaccurate. To compensate for this, a simple forgiveness parameter may be used. The borders of the valid TERs are extended by this parameter. Note that larger forgiveness parameters make our method more robust in handling deviations from our assumptions, but also increase the valid set of candidate correspondences.</p><p>Unreliable TERs: As there may be noise in the computation of epipolar lines, TERs may be unreliable when epipolar lines or their intersections are too close together. In these cases we may declare the TERs unreliable, and thus match without them (by any standard matching algorithm). Alternatively, in cases in which we have n &gt; 3 epipolar lines, we propose the following workaround. Instead of building TERs using epipolar lines which are close together, we find TERs using every subset of epipolar lines which are sufficiently far apart. Then we find the union of the valid TERs to determine an overall valid area. This allows us to still utilize the constraints given by each known correspondence while avoiding errors from close epipolar lines.</p><p>Correspondence verification: In some cases no valid regions exist in the image. This can be regarded as a dead end, as S q cannot be extended. If we encounter such a dead end, we deduce that either the linear motion assumption does not Image 4</p><p>Image 8 Image 7 Image 6 Image 3 <ref type="figure">Figure 5</ref>: Given correspondences in the first, second, and fifth images of a scene (dataset c5), we run our method and find corresponding points in the remaining five images of the set using matching with TERs (green stars) and without TERs (red circles).Note: images are labeled based on their order in time but presented in the order, from left to right, in which best nearest neighboring matches were selected by the TER algorithm. As matching with TERs is run independently of matching without TERs and each finds matches based on previously found correspondences, incorrect selections by standard matching may still fall within valid regions (as in image 7).</p><p>hold or that there are errors in the computed fundamental matrices, temporal order, or set S q . In this case, we can do no better than matching without TERs. This is demonstrated in tests 3 and 6 in Sec. 5. If we assign points to all images without hitting a dead end, it is likely that points were selected correctly. Note that the more images we have, the higher the confidence that the matching is correct.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>To study how our method performs in practice, we implemented it in MATLAB and tested it on synthetic and real data. Quantitative results were obtained by evaluating the reduction in search space. We also compared a direct nearest neighbor matching method with and without the use of TERs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Simulated Data</head><p>The simulated scenario consisted of a set of 4-8 cameras capturing a moving point. The cameras were positioned randomly along a semisphere, all pointed approximately at the origin, through which a randomly generated line passes. Images were created by projecting onto each image a randomly selected 3D point along the line, thus generating≈ú q . The FOV of each camera was set to 30 ‚Ä¢ in both x and y and the distance to the origin was approximately 550 units. Each image was 512 by 512 pixels.</p><p>Test 1: We tested the effectiveness of our method in reducing the percent of the image which is valid (PV) for different values of initial correspondences. This test is required as we have no closed analytic computation of the size of the valid region; it depends on the camera configurations and the location of the moving point. We ran 50,000 simulations, split into five groups of 10,000. Each group had a different number of cameras generated (between 4 and 8), all but one of which was given initial correspondences, such that |S q | = |≈ú q | ‚àí 1. The image I u , which was not assigned a correspondence, was selected at random. In each simulation we computed valid TERs in I u using S q and calculated the PV.</p><p>The results are summarized in <ref type="figure" target="#fig_5">Fig. 6</ref>(a) as cumulative histograms of PV, one for each number of initial correspondences, |S q |. Ideally, a large number of experiments (the y axis) should have as small as possible percent of the image valid (the x axis). Our data shows that for |S q | = 3, approximately 27% of the 10000 simulations have images of which up to 20% is valid. When considering |S q | = 4, approximately 42% of the simulations fall into this category. When |S q | = 7, over 65% of our simulations restrict the valid region to 20% of the image or less. Thus, as expected, for a larger |S q |, it is more likely that the search space for an additional correspondence will be smaller. Indeed, when more information about the moving point is available through a larger number of correspondences, the size of the valid regions decreases, as desired. We do not present the cases in which |S q | &gt;&gt; 7, as each additional image added to S q can only further restrict the search space, and therefore the trend is expected to continue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Test 2:</head><p>We tested the robustness of our method to noise in pixel locations, which may be caused by deviation from linear movement. We ran 5,000 simulations, using 6 generated cameras, three of which were given initial correspondences. In (c), S q is given by ground-truth points, while in (d) S q was initialized using nearest neighbor matching. We see that, with the exception of two datasets in which initialization failed, TERs improve the percent correct matching. Note that in (c) and (d) datasets are grouped by location, and each location is assigned a letter ID (a-i).</p><p>Each dataset in each location is also assigned a number, such that a1-a4 are presented in the first leftmost bars of each chart.</p><p>We built TERs in the remaining images and selected one of them at random. If the point projected onto this image was in a valid region, we added it to S q for the next iteration.</p><p>Otherwise, if all the images had points in invalid regions, we stopped, as no match could be used for further iterations. Similarly, we stop if an image has reached a dead end.</p><p>In each simulation we tested 4 noise levels per point on each image for which correspondence was unknown. The original projected location and the original projection shifted in x and y by a small random amount selected from a normal distribution with zero mean and standard deviations of 1, 3, or 5. We set an upper bound on the noise such that no shift could be more than twice the standard deviation.</p><p>For each noise level used, we find the number of simulations in which: (i) a point in a valid region exists in each image; (ii) no point was in a valid region in at least one of the images; and (iii) a dead end occurred. For STD of 0 and 1, all simulations fell into category (i). When the STD rose to 3, 0.54% of simulations were of category (ii) and the rest were category (i). Finally, when the STD was 5, 3.34% of the simulations were category (ii), 2 simulations were category (iii) and the remainder were category (i).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Test 3:</head><p>We tested whether dead ends can be used to identify incorrect correspondence. We ran 30,000 simulations, using between 4 and 6 cameras (10,000 simulations each) in which we initialized S q with projections from the generated line (as in test 2). However, in each of the remaining images a random point was selected. For this test we used a forgiveness parameter (as described in Sec. 4) of 2 pixels. We proceeded through the images as in Test 2. The results are as follows: using four cameras, no points were found in valid regions in 46.2% of the simulations, which reflects the probability of having a random chosen point within an invalid region (no dead ends can occur with only 4 cameras). With 5 cameras 74.81% of the simulations had no matches, and of the remaining simulations 26.38% were dead ends.</p><p>With 6 cameras the results were 70.98% with no matches, and 66.37% were dead ends. As such, given enough images, it is increasingly likely that incorrect correspondences will yield a dead end case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Real Data</head><p>We evaluated our method on novel datasets captured at eight locations, by up to six smartphone cameras (e.g., Samsung Galaxy S4, Apple iPhone 5S). At each location up to 7 datasets were captured from different viewpoints and at different times. Each dataset consists of between 5 and 15 images. In each dataset, we searched for correspondence for between 1 and 9 dynamic points, visible in all images. The locations, datasets, and points varied greatly in our experiments. Scenes were captured indoors and outdoors; some had many moving objects, while others had just one. The features considered for correspondence were on rigid (e.g., cars or soda cans) and non-rigid (e.g., people or dogs) objects. In addition to these novel datasets, we include in our results the rock climbing dataset supplied by <ref type="bibr" target="#b25">[26]</ref>. From this dataset, we selected a partial set in which we consider only the unidirectional movement of the climber. Examples taken from datasets we considered are presented in <ref type="figure">Fig. 7</ref> and additional examples may be found in the supplementary materials. The selection of examples was in part based on the size of the valid regions, as in many cases, valid TERs are are too small to be easily viewed in a figure. In all datasets, the ground-truth correspondences (≈ú q ) and the time order among the images were identified manually. Test 4: We repeated Test 1 in order to examine the percent of valid regions in natural still images. The ground-truth correspondence was used to initialize a set of size |S q | = 3. We ran our method and for each experiment selected at random a fourth image for which we calculated the valid TERs and PV. We followed a similar procedure given an initial set of |S q | ‚àà {4, 5, 6, 7}. We selected at random 1000 sam-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset a2</head><p>Dataset g1</p><p>Dataset e1 <ref type="figure">Figure 7</ref>: The application of our method to a variety of scenes. Correspondences found using TERs (green stars) are correct and those without TERs (red circles) are incorrect. Additional examples may be found in the supplementary materials.</p><p>ples of the set of experiments for each size of |S q |. This allows to compare the PV for different values of |S q |. The results ( <ref type="figure" target="#fig_5">Fig. 6(b)</ref>) show that on average, as in the simulated data, we see smaller valid regions when the number of correspondences increases. However, in the real data, for more than four correspondences, we found that over 85% of our samples had a PV of less than 40%. In general, natural image datasets restricted the search space better than simulated data, with the exception of when |S q | = 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Test 5:</head><p>We tested the robustness of our algorithm in finding correspondence and compared the results to the same matching algorithm without prediction by TERs. Matching was done by finding the nearest neighboring SIFT to those in S q , using the cosine similarity between feature vectors. When dead ends occurred, or in the case of unreliable TERs, our method could not be applied; it can do no better than standard matching. We only present cases in which no dead-ends or unreliable TERs were detected, as those cases offer a fair comparison. Statistics relating to dead ends and unreliable TERs in the supplementary materials.</p><p>As TERs are dependent on epipolar geometry, their effectiveness in restricting the search space dipends on the geometry between the images captured. Thus, we present the results of this test per dataset. <ref type="figure" target="#fig_5">Fig. 6</ref>(c) shows the average percent of correct matchings using standard matching with and without our prediction. There are between 9 and 221 experiments in each dataset. The number of experiments is defined by the number of points to be matched and the number of combinations of initial correspondences. In all datasets, we see that our TER-assisted matching equals or outperforms standard matching on average, with an average improvement of 7.1% over all datasets.</p><p>Test 6: We tested the effectiveness of our method similarly to test 5 when S q is not given. Given a point, S q was initialized by finding its two nearest-neighbors in the image set. As expected, the results show errors in the initial set. In 294 of the 496 experiments, at least one point in S q was incorrectly matched. Experiments using these incorrect initializations yielded dead ends more often in datasets containing more images. Datasets with 5 images, yielded 21.1% dead ends, datasets with 7 images yielded 80.0% dead ends, and datasets having 10 or more images always resulted in dead ends. Additional details are provided in the supplementary materials. This trend demonstrates that with high probability errors in correspondence, in particular with the initial correspondence, can be detected using only 10 images of the moving feature.</p><p>In <ref type="figure" target="#fig_5">Fig. 6</ref>(d) we present the percent correct matching when experiments did not reach a dead end. For each dataset this figure, between 1 and 24 experiments were performed. We see that with or without TERs, the percent of correct matchings drops significantly due to the lack of correct initial matches. However, this test highlights that given any initialization, TERs may still be used to improve matching. The average improvement over all the datasets is 2.7%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>In this paper, we introduced a method for utilizing epipolar geometry to predict correspondence of moving points through CrowdCam images. We demonstrated on both simulated and real world data that this prediction reduces the possible locations of a moving feature within an image. As a result, it may improve the efficiency and accuracy of matching algorithms. Additionally, we showed how our method may be used to verify the accuracy of our assumptions and correspondence using dead ends. Developing algorithms that utilize dead end detection in order to improve correspondence, is left for future research. Another possible extension is to directly determine the optimal TERs for |S q | &gt; 3, rather than using intersections of valid regions. We expect that any state-of-the-art algorithm for matching dynamic features can be improved using our method.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>(a) As ‚Ñì is unknown, candidate locations of Œ± u are limited to a region between the parallel lines. (b) With two nonparallel lines, there always exists an ‚Ñì crossing Œ± u , which preserves any œÉ.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Three epipolar lines (‚Ñì i , ‚Ñì j , ‚Ñì k ) and their parallels (l i ,l j ,l k ) split an image plane into sixteen distinct regions (in gray) of five types.(i) In each region, all points are valid or all points are invalid. This classification is dependent not on the epipolar lines but on œÉ. (ii) There are 4!/2 possible orders up to direction that must be considered. For each region, 6 of these orders are valid while the remaining 6 are invalid. (iii) For a given œÉ, 8 of the 16 regions are valid TERs, while the other 8 are invalid.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Definition 3 :</head><label>3</label><figDesc>R2(i, j) is defined as the area comprised of ‚Ñì i and ‚Ñì j through which ‚Ñì k does not pass.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Claim 1 :</head><label>1</label><figDesc>R2(i, j) is a valid TER for all orders in which u is not adjacent to k in œÉ.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>The different sections defined by critical lines, given Œ± u ‚àà R2(i, j) and the order along the line ‚Ñì in the section bordered by (c ij , c ik ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>(a) and (b) show, given different numbers of initial correspondences, the cumulative percentages of the area of valid TERs in each image, for simulated and real data, respectively. (c) and (d) show the average percent correct matching per dataset with and without TERs.</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was partially supported by the Israel Science Foundation grant no. 930/12 and by the Israeli Ministry of Science, Grant 3-8744.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Crowdcam: Instantaneous navigation of crowd images using angled graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arpa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ballan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Taubin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Raskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on 3D Vision</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Ringit: Ring-ordering casual photos of a temporal event</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Averbuch-Elor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Trajectory triangulation: 3d reconstruction of moving points from a monocular image sequence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Avidan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shashua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="348" to="357" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Lucas-kanade 20 years on: A unifying framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="221" to="255" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Unstructured video-based rendering: Interactive exploration of casually captured videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ballan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Puwein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">87</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Template matching using fast normalized cross correlation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Briechle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">D</forename><surname>Hanebeck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Aerospace/Defense Sensing, Simulation, and Controls. International Society for Optics and Photonics</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Brief: Binary robust independent elementary features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Calonder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Strecha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">An adaptive coupled-layer visual model for robust visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cehovin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kristan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Leonardis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Space-time tradeoffs in photo sequencing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dekel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Moses</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Avidan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Photo sequencing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Moses</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Avidan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Balanced exploration and exploitation model search for efficient epipolar geometry estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Goshen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Shimshoni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1230" to="1242" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Computer matching of areas in stereo images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Hannah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">DTIC Document</title>
		<imprint>
			<date type="published" when="1974" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Struck: Structured output tracking with kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saffari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Multiple view geometry in computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A general framework for trajectory triangulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Kaminski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Teicher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematical Imaging and Vision</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="27" to="41" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Who shot the picture and when?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kanojia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Malireddi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Gullapally</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Raman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Visual Computing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="438" to="447" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Brisk: Binary robust invariant scalable keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Leutenegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">Y</forename><surname>Siegwart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A global solution to sparse correspondence problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Maciel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Costeira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="187" to="199" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Robust visual tracking using ‚Ñì1 minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Minimum error bounded efficient ‚Ñì1 tracker with occlusion detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Blasch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Scalable nearest neighbour algorithms for high dimensional data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Muja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2227" to="2240" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Fast approximate nearest neighbors with automatic algorithm configuration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Muja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision Theory and Applications</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Fast occluded object tracking by a robust appearance filter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1099" to="1104" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Locally orderless tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Oron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bar-Hillel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Levi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Avidan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">3d reconstruction of a moving point from a series of 2d projections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Shiratori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Orb: an efficient alternative to sift or surf</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rublee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Rabaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Konolige</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bradski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A taxonomy and evaluation of dense two-frame stereo correspondence algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Scharstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="7" to="42" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Geometry-aware feature matching for structure from motion applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conference on Applications of Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="278" to="285" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Visual tracking: An experimental survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Smeulders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cucchiara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Calderara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dehghan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1442" to="1468" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Modeling the world from internet photo collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="189" to="210" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Computer vision: algorithms and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Superpixel tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Towards linear-time incremental structure from motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on 3D Vision</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A robust technique for matching two uncalibrated images through the recovery of the unknown epipolar geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Deriche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Faugeras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q.-T</forename><surname>Luong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="87" to="119" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
