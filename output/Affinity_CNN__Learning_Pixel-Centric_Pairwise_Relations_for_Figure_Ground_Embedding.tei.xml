<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Affinity CNN: Learning Pixel-Centric Pairwise Relations for Figure/Ground Embedding</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
							<email>mmaire@ttic.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Takuya Narihira Sony Corp</orgName>
								<address>
									<settlement>Berkeley / ICSI</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tti</forename><surname>Chicago</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Takuya Narihira Sony Corp</orgName>
								<address>
									<settlement>Berkeley / ICSI</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
							<email>stellayu@berkeley.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Takuya Narihira Sony Corp</orgName>
								<address>
									<settlement>Berkeley / ICSI</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Affinity CNN: Learning Pixel-Centric Pairwise Relations for Figure/Ground Embedding</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:37+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Spectral embedding provides a framework for solving perceptual organization problems, including image segmentation and figure/ground organization. From an affinity matrix describing pairwise relationships between pixels, it clusters pixels into regions, and, using a complex-valued extension, orders pixels according to layer. We train a convolutional neural network (CNN) to directly predict the pairwise relationships that define this affinity matrix. Spectral embedding then resolves these predictions into a globallyconsistent segmentation and figure/ground organization of the scene. Experiments demonstrate significant benefit to this direct coupling compared to prior works which use explicit intermediate stages, such as edge detection, on the pathway from image to affinities. Our results suggest spectral embedding as a powerful alternative to the conditional random field (CRF)-based globalization schemes typically coupled to deep neural networks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Systems for perceptual organization of scenes are commonly architected around a pipeline of intermediate stages.</p><p>For example, image segmentation follows from edge detection <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b3">4]</ref>; figure/ground, occlusion, or depth layering follows from reasoning over discrete contours or regions <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b17">18]</ref> with some systems also reliant on motion cues <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b30">31]</ref>. This trend holds even in light of rapid advancements from designs centered on convolutional neural networks (CNNs). Rather than directly focus on image segmentation, recent CNN architectures <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b3">4]</ref> target edge detection. Turaga et al. <ref type="bibr" target="#b32">[33]</ref> make the connection between affinity learning and segmentation, yet restrict affinities to be precisely local edge strengths. Pure CNN approaches for depth from a single image do focus on directly constructing the desired output <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b7">8]</ref>. However, these works do not address the problem of perceptual grouping without fixed semantic classes. We engineer a system for simultaneous segmentation and figure/ground organization by directly connecting a CNN to  <ref type="figure" target="#fig_3">Figure 1</ref>. System architecture. We send an image through a CNN which is trained to predict the grouping and ordering relations between each of the n pixels and its neighbors at k displacements laid out in a fixed stencil pattern. We assemble these n×2k pixelcentric relations into a sparse n×n complex affinity matrix between pixels, each row indicating a pixel's affinity with others. Shown above is the row for the pixel at the center of a log-polar sampling pattern; its positive/negative relations with neighbors are marked by red/cyan squares overlaid on the image. We feed the pairwise affinity matrix into Angular Embedding for global integration, producing an eigenvector representation that reveals figureground organization: we know not only which pixels go together, but also which pixels go in front. an inference algorithm which produces a globally consistent scene interpretation. Training the CNN with a target appropriate for the inference procedure eliminates the need for hand-designed intermediate stages such as edge detection. Our strategy parallels recent work connecting CNNs and conditional random fields (CRFs) for semantic segmentation <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b34">35]</ref>. A crucial difference, however, is that we handle the generic, or class independent, image partitioning problem. In this context, spectral embedding, and specifically Angular Embedding (AE) <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b37">38]</ref>, is a more natural inference algorithm. <ref type="figure" target="#fig_3">Figure 1</ref> illustrates our architecture. Angular Embedding, an extension of the spectral relaxation of Normalized Cuts <ref type="bibr" target="#b28">[29]</ref> to complex-valued affinities, provides a mathematical framework for solving joint group-ing and ranking problems. Previous works established this framework as a basis for segmentation and figure/ground organization <ref type="bibr" target="#b21">[22]</ref> as well as object-part grouping and segmentation <ref type="bibr" target="#b23">[24]</ref>. We follow the spirit of <ref type="bibr" target="#b21">[22]</ref>, but employ major changes to achieve high-quality figure/ground results:</p><p>• We reformulate segmentation and figure/ground layering in terms of an energy model with pairwise forces between pixels. Pixels either bind together (group) or differentially repel (layer separation), with strength of interaction modulated by confidence in the prediction.</p><p>• We train a CNN to directly predict all data-dependent terms in the model.</p><p>• We predict interactions across multiple distance scales and use an efficient solver <ref type="bibr" target="#b22">[23]</ref> for spectral embedding.</p><p>Our new energy model replaces the ad-hoc boundarycentric interactions employed by <ref type="bibr" target="#b21">[22]</ref>. Our CNN replaces hand-designed features. Together they facilitate learning of pairwise interactions across a regular stencil pattern. Choosing a sparse stencil pattern, yet including both shortand long-range connections, allows us to incorporate multiscale cues while remaining computationally efficient.</p><p>Section 2 develops our model for segmentation and figure/ground while providing the necessary background on Angular Embedding. Section 3 details the structure of our CNN for predicting pairwise interaction terms in the model.</p><p>As our model is fully learned, it could be trained according to different notions of segmentation and figure/ground. For example, consistent definitions for figure/ground include true depth ordering as in <ref type="bibr" target="#b8">[9]</ref>, object class-specific foreground/background separation as in <ref type="bibr" target="#b23">[24]</ref>, and boundary ownership or occlusion as in <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b21">22]</ref>. We focus on the latter and define segmentation as a region partition and figure/ground as an ordering of regions by occlusion layer. The Berkeley segmentation dataset (BSDS) provides ground-truth annotation of this form <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b12">13]</ref>. We demonstrate segmentation results competitive with the state-ofthe-art on the BSDS benchmark <ref type="bibr" target="#b10">[11]</ref>, while simultaneously generating high-quality figure/ground output.</p><p>The occlusion layering interpretation of figure/ground is the one most likely to be portable across datasets; it corresponds to a mid-level perceptual task. We find this to be precisely the case for our learned model. Trained on BSDS, it generates quite reasonable output when tested on other image sources, including the PASCAL VOC dataset <ref type="bibr" target="#b9">[10]</ref>. We believe this to be a significant advance in fully automatic perceptual organization. Section 4 presents experimental results across all datasets, while Section 5 concludes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Spectral Embedding &amp; Generalized Affinity</head><p>We abstract the figure/ground problem to that of assigning each pixel p a rank θ(p), such that θ(·) orders pixels by Given Pairwise: <ref type="figure">Figure 2</ref>. Angular Embedding <ref type="bibr" target="#b37">[38]</ref>. Given (C, Θ) capturing pairwise relationships between nodes, the Angular Embedding task is to map those nodes onto the unit semicircle, such that their resulting absolute positions respect confidence-weighted relative pairwise ordering (Equation 1). Relative ordering is identified with rotation in the complex plane. For node p, θ(p) = arg(z(p)) recovers its global rank order from its embedding z(p).</p><formula xml:id="formula_0">Ordering Θ(·, ·) Confidence C(·, ·) Recover: Global ordering θ(p) p → z(p) = e iθ(p) i 0 1 −1 z(p) z(q) z(r) z(r)e iΘ(p,r) z(q)e iΘ(p,q) C( p, r) C ( p , q ) Θ ( p , r ) Θ ( p , q ) z(p)</formula><p>occlusion layer. Assume we are given estimates of the relative order Θ(p, q) between many pairs of pixels p and q.</p><p>The task is then to find θ(·) that agrees as best as possible with these pairwise estimates. Angular Embedding <ref type="bibr" target="#b37">[38]</ref> addresses this optimization problem by minimizing error ε:</p><formula xml:id="formula_1">ε = p q C(p, q) p,q C(p, q) · |z(p) −z(p)| 2<label>(1)</label></formula><p>where C(p, q) accounts for possibly differing confidences in the pairwise estimates and θ(p) is replaced by z(p) = e iθ(p) . As <ref type="figure">Figure 2</ref> shows, this mathematical convenience permits interpretation of z(·) as an embedding into the complex plane, with desired ordering θ(·) corresponding to absolute angle.z(p) is defined as the consensus embedding location for p according to its neighbors and Θ:</p><formula xml:id="formula_2">z(p) = qC (p, q) · e iΘ(p,q) · z(q) (2) C(p, q) = C(p, q) q C(p, q)<label>(3)</label></formula><p>Relaxing the unit norm constraint on z(·) yields a generalized eigenproblem:</p><formula xml:id="formula_3">W z = λDz<label>(4)</label></formula><p>with D and W defined in terms of C and Θ by:</p><formula xml:id="formula_4">D = Diag(C1 n ) (5) W = C • e iΘ<label>(6)</label></formula><p>where n is the number of pixels, 1 n is a column vector of ones, Diag(·) is a matrix with its vector argument on the main diagonal, and • denotes the matrix Hadamard product. For Θ everywhere zero (W = C), this eigenproblem is identical to the spectral relaxation of Normalized Cuts <ref type="bibr" target="#b28">[29]</ref>, in which the second and higher eigenvectors encode grouping <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b1">2]</ref>. With nonzero entries in Θ, the first of the now <ref type="figure">Figure 3</ref>. Complex affinities for grouping and figure/ground. An angular displacement, corresponding to relative figure/ground or depth ordering, along with a confidence on that displacement, specify pairwise local grouping relationships between pixels. A single complex number encodes confidence as magnitude and displacement as angle from the positive real axis. Four basic interaction types span the space of possible pairwise pixel relationships. Contiguous region: Pixels p and q lie in the same region. A vector along the positive real axis represents high confidence on zero relative displacement. Ambiguous boundary: Pixels p and q lie in different regions whose interface admits no cues for discriminating displacement. The shared boundary could be a surface marking or depth discontinuity with either of p or q in front. The origin of the complex plane represents zero confidence on the correct relationship. <ref type="figure">Figure transition:</ref> As boundary convexity tends to indicate foreground, moving from p to q likely transitions from ground to figure. We have high confidence on positive angular displacement. Ground transition: In the reverse case, q is ground with respect to p, and the complex representation has negative angle.</p><formula xml:id="formula_5">❚ ② ♣ ❡ | ❡ ♣ r ❡ s ❡ ♥ t t ✁ ♦ ♥ ✂ q ❈ ✄ ☎ ✆ ✐ ❣ ✉ ✄ ✉ ✝ ✞ ✟ ❣ ✐ ✄ ☎ ✶ ✠ ✶ ✡ ✠ ✡ ✂ q ❆ ♠ ☛ ✐ ❣ ✉ ✄ ✉ ✝ ❇ ✄ ✉ ☎ ❞ ❛ ☞ ✌ ✶ ✠ ✶ ✡ ✠ ✡ ✂ q • ☞ ✄ ✉ ☎ ❞ ✦ ❋ ✐ ❣ ✉ ☞ ✟ ✶ ✠ ✶ ✡ ✠ ✡ ✂ q ❋ ✐ ❣ ✉ ☞ ✟ ✦ • ☞ ✄ ✉ ☎ ❞ ✶ ✠ ✶ ✡ ✠ ✡</formula><p>complex-valued eigenvectors is nontrivial and its angle encodes rank ordering while the subsequent eigenvectors still encode grouping <ref type="bibr" target="#b21">[22]</ref>. We use the same decoding procedure as <ref type="bibr" target="#b21">[22]</ref> to read off this information. Specifically, given eigenvectors, {z 0 , z 1 , ..., z m−1 }, and corresponding eigenvalues, λ 0 ≤ λ 1 ≤ ... ≤ λ m−1 , solving Equation 4, θ(p) = arg(z 0 (p)) recovers figure/ground ordering. Treating the eigenvectors as an embedding of pixels into C m , distance in this embedding space reveals perceptual grouping. We follow <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b21">22]</ref> to recover both boundaries and segmentation from the embedding by taking the (spatial) gradient of eigenvectors and applying the watershed transform. This is equivalent to a form of agglomerative clustering in the embedding space, with merging constrained to be between neighbors in the image domain.</p><p>A remaining issue, solved by <ref type="bibr" target="#b23">[24]</ref>, is to avoid circular wrap-around in angular span by guaranteeing that the solution fits within a wedge of the complex plane. It suffices to rescale Θ by π 2 (1 T n |Θ|1 n ) −1 prior to embedding. Having chosen Angular Embedding as our inference procedure, it remains to define the pairwise pixel relationships C(p, q) and Θ(p, q). In the special case of Normalized Cuts, C(p, q) represents a clustering affinity, or confidence on zero separation (in both clustering and figure/ground). For Let us develop the model in terms of probabilities: e(p) = P r(p lies on a boundary) (7) b(p, q) = P r(seg(p) = seg(q)) (8) f (p, q) = P r(figural(p, q) | seg(p) = seg(q))) (9) g(p, q) = P r(figural(q, p) | seg(p) = seg(q)))</p><p>where seg(p) is the region (segment) containing pixel p and figural(p, q) means that q is figure with respect to p, according to the true segmentation and figure/ground ordering. b(p, q) is the probability that some boundary separates p and q. f (p, q) and g(p, q) are conditional probabilities of figure and ground, respectively. Note g(p, q) = 1 − f (p, q).</p><p>There are three possible transitions between p and q: none (same region), ground → figure, and figure → ground. Selecting the most likely, the probabilities of erroneously binding p and q into the same region, transitioning to figure, or transitioning to ground are respectively:</p><formula xml:id="formula_7">E B (p, q) = b(p, q) (11) E F (p, q) = 1 − (1 − e(p))b(p, q)(1 − e(q))f (p, q) (12)</formula><p>E G (p, q) = 1 − (1 − e(p))b(p, q)(1 − e(q))g(p, q) <ref type="bibr" target="#b12">(13)</ref> where (1 − e(p))b(p, q)(1 − e(q)) is the probability that there is a boundary between p and q, but that neither p nor Image Ground-truth F/G Spectral F/G Maire <ref type="bibr" target="#b21">[22]</ref> Spectral F/G Spectral Boundaries Segmentation + F/G Our System <ref type="figure">Figure 7</ref>. Image segmentation and figure/ground results. We compare our system to ground-truth and the results of Maire <ref type="bibr" target="#b21">[22]</ref>. Spectral F/G shows per-pixel figure/ground ordering according to the result of Angular Embedding. The colormap matches <ref type="figure">Figure 2</ref>, with red denoting figure and blue denoting background. Spectral boundaries show soft boundary strength encoded by the eigenvectors. These boundaries generate a hierarchical segmentation <ref type="bibr" target="#b1">[2]</ref>, one level of which we display in the final column with per-pixel figure/ground averaged over regions. Note the drastic improvement in results over <ref type="bibr" target="#b21">[22]</ref>. While <ref type="bibr" target="#b21">[22]</ref> reflects a strong lower-region bias for figure, our system learns to use image content and extracts foreground objects. All examples are from our resplit figure/ground test subset of BSDS.  <ref type="figure">Figure 7</ref>) onto the ground-truth segmentation by taking the median value over each region. For boundaries separating regions with different ground-truth figure/ground layer assignments, we check whether the predicted owner (more figural region) matches the owner according to the ground-truth. The rightmost two columns mark correct boundary ownership predictions in green and errors in red. Note how we correctly predict ownership of object lower boundaries. <ref type="table">Table 1</ref> gives quantitative benchmarks.</p><p>Segmentation: <ref type="figure">Figure</ref>  <ref type="table">(lower table)</ref> segmentations, we quantify accuracy of local relative region and boundary relationships. Our system dramatically outperforms <ref type="bibr" target="#b21">[22]</ref> across all metrics.</p><p>tection performance, we develop a simpler alternative. Given a per-pixel figure/ground ordering assignment, and a segmentation partitioning an image into regions, we can easily order the regions according to figure/ground layering. Simply assign each region a rank order equal to the median figure/ground order of its member pixels.</p><p>This transfer procedure serves as a basis for comparing different figure/ground orderings. We transfer them both onto the same segmentation. This yields two orderings of the same regions, which we compare according to:</p><p>• Region accuracy (R-ACC): Over all pairs of neighboring regions, how often does predicted relative ordering match ground-truth relative ordering?</p><p>• Boundary ownership accuracy (B-ACC): Define the front region as owning the pixels on the common boundary of the region pair and measure the average per-pixel accuracy of predicted boundary ownership.</p><p>• B-ACC-50, B-ACC-25: Identical to B-ACC, except consider only boundaries which belong to the foreground-most 50% or 25% of regions according to ground-truth figure/ground ordering. These metrics emphasize the importance of correctly organizing foreground objects while ignoring more distant objects. <ref type="table">Table 1</ref> quantitatively compares our figure/ground predictions and those of <ref type="bibr" target="#b21">[22]</ref> against ground-truth figure/ground on our 50 image test subset of BSDS <ref type="bibr" target="#b24">[25]</ref>. We consider both projection onto ground-truth segmentation and onto our own system's segmentation output. For the latter, as our system produces hierarchical segmentation, we use the region partition at a fixed level of the hierarchy, calibrated for optimal boundary F-measure. <ref type="figure" target="#fig_2">Figure 8</ref> and the supplementary material provide visual comparisons.</p><p>Across all metrics, our system significantly outperforms <ref type="bibr" target="#b21">[22]</ref>. We achieve 69% and 70% boundary ownership accuracy on ground-truth and automatic segmentation, respectively, compared to 58% and 62% for <ref type="bibr" target="#b21">[22]</ref>. <ref type="figure">Figure 9</ref> demonstrates that our BSDS-trained system captures generally-applicable notions of both segmentation and figure/ground. On both PASCAL VOC <ref type="bibr" target="#b9">[10]</ref> and the Weizmann Horse database <ref type="bibr" target="#b4">[5]</ref>, it generates figure/ground layering that respects scene organization. On the Weizmann examples, though having only been trained for perceptual organization, it behaves like an object detector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Additional Datasets</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We demonstrate that Angular Embedding, acting on CNN predictions about pairwise pixel relationships, provides a powerful framework for segmentation and figure/ground organization. Our work is the first to formulate a robust interface between these two components. Our results are a dramatic improvement over prior attempts to use spectral methods for figure/ground organization.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>figure-ground</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>Generalized affinity. Combining the base cases in Figure 3, we express generalized affinity W as the sum of a binding force acting along the positive real axis, and figure and ground displacement forces acting at angles. In absence of any strong boundary, the binding force dominates, linking pixels together. In presence of a strong and discriminative boundary, either the figure or ground force dominates, triggering displacement. Under conditions of uncertainty, all forces are weak. Left: The plot for |W | illustrates total force strength, while the plot for ∠W shows the dominant force. Right: Complex-valued W varies smoothly across its configuration space, yet exhibits four distinct modes (binding, figure, ground, uncertain). Smooth transitions occur in the region of uncertainty at the origin. the more general case, we must also predict non-zero figure/ground separation values and assign them confidences.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 8 .</head><label>8</label><figDesc>Figure/ground prediction accuracy measured on ground-truth segmentation. We transfer per-pixel figure/ground predictions (columns 2 through 4 of</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Table 1 .</head><label>1</label><figDesc>Figure/ground benchmark results. After transferring figure/ground predictions onto either ground-truth (upper table and Figure 8) or our own</figDesc></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Boundary extraction in natural images using ultrametric contour maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>POCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Contour detection and hierarchical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">DeepEdge: A multiscale bifurcated deep network for top-down contour detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">High-for-low and lowfor-high: Efficient boundary detection from deep object features and its applications to high-level vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Combined top-down/bottomup segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Borenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ullman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Semantic image segmentation with deep convolutional nets and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.7062</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Fast edge detection using structured forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Depth map prediction from a single image using a multi-scale deep network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">The PASCAL Visual Object Classes (VOC) challenge. IJCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">The Berkeley Segmentation Dataset and Benchmark (BSDB)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Learning affinity functions for image segmentation: Combining patch-based and gradient-based approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Local figure/ground cues are valid for natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Vision</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">S</forename><surname>Lempitsky</surname></persName>
		</author>
		<title level="m">N 4 -fields: Neural network nearest neighbor fields for image transforms. ACCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Occlusion boundary detection using pseudo-depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Recovering occlusion boundaries from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Caffe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<title level="m">Convolutional architecture for fast feature embedding</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">A learning based framework for depth ordering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Efficient piecewise training of deep structured models for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<title level="m">Salient object detection using concavity context. ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Simultaneous segmentation and figure/ground organization using angular embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Progressive multigrid eigensolvers for multiscale spectral segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Object detection and segmentation from joint embedding of parts and pixels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Direct intrinsics: Learning albedo-shading decomposition by convolutional regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Narihira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<title level="m">Figure/ground assignment in natural images. ECCV</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Deep-Contour: A deep convolutional feature learned by positivesharing loss for contour detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Normalized cuts and image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Occlusion boundaries from motion: Low-level detection and mid-level reasoning. IJCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Local layering for joint motion estimation and occlusion detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pfister</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Occlusion boundary detection and figure/ground assignment from optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sundberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Convolutional networks can learn to generate affinity graphs for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Turaga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Helmstaedter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Briggman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Denk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Seung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Precision-recall-classification evaluation framework: Application to depth estimation on single images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">P</forename><surname>Visa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Salembier</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Joint object and part segmentation using deep learned potentials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Layered object models for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hallman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Angular embedding: from jarring intensity differences to perceived luminance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Angular embedding: A robust quadratic criterion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Semantic amodal segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mexatas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.01329</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
