<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Virtual Worlds as Proxy for Multi-Object Tracking Analysis</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Gaidon</surname></persName>
							<email>adrien.gaidon@xrce.xerox.com</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Vision group</orgName>
								<orgName type="institution">Xerox Research Center Europe</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiao</forename><surname>Wang</surname></persName>
							<email>qiao.wang@asu.edueleonora.vig@dlr.de</email>
							<affiliation key="aff1">
								<orgName type="department">School of Electrical, Computer, and Energy Engineering and School of Arts, Media, and Engineering</orgName>
								<orgName type="institution">Arizona State University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yohann</forename><surname>Cabon</surname></persName>
							<email>yohann.cabon@xrce.xerox.com</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Vision group</orgName>
								<orgName type="institution">Xerox Research Center Europe</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleonora</forename><surname>Vig</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Vision group</orgName>
								<orgName type="institution">Xerox Research Center Europe</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Virtual Worlds as Proxy for Multi-Object Tracking Analysis</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Research-Development/Computer-Vision/Proxy-Virtual-Worlds</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:48+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Modern computer vision algorithms typically require expensive data acquisition and accurate manual labeling. In this work, we instead leverage the recent progress in computer graphics to generate fully labeled, dynamic, and photo-realistic proxy virtual worlds. We propose an efficient real-to-virtual world cloning method, and validate our approach by building and publicly releasing a new video dataset, called "Virtual KITTI" 1 , automatically labeled with accurate ground truth for object detection, tracking, scene and instance segmentation, depth, and optical flow. We provide quantitative experimental evidence suggesting that (i) modern deep learning algorithms pre-trained on real data behave similarly in real and virtual worlds, and (ii) pre-training on virtual data improves performance. As the gap between real and virtual worlds is small, virtual worlds enable measuring the impact of various weather and imaging conditions on recognition performance, all other things being equal. We show these factors may affect drastically otherwise high-performing deep models for tracking.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Although cheap or even no annotations might be used at training time via weakly-supervised (resp. unsupervised) learning, experimentally evaluating the generalization performance and robustness of a visual recognition model requires accurate full labeling of large representative datasets. This is, however, challenging in practice for video understanding tasks like multi-object tracking (MOT), because of the high data acquisition and labeling costs that limit the quantity and variety of existing video benchmarks. For instance, the KITTI <ref type="bibr" target="#b0">[1]</ref> multi-object tracking benchmark contains only 29 test sequences captured in similar good con- * AG and QW have contributed equally † EV is currently at the German Aerospace Center 1 http://www.xrce.xerox.com/ Research-Development/Computer-Vision/ Proxy-Virtual-Worlds <ref type="figure">Figure 1</ref>: Top: a frame of a video from the KITTI multi-object tracking benchmark <ref type="bibr" target="#b0">[1]</ref>. Middle: the corresponding rendered frame of the synthetic clone from our Virtual KITTI dataset with automatic tracking ground truth bounding boxes. Bottom: automatically generated ground truth for optical flow (left), scene-and instance-level segmentation (middle), and depth (right). ditions and from a single source. To the best of our knowledge, none of the existing benchmarks in computer vision contain the minimum variety required to properly assess the performance of video analysis algorithms: varying conditions (day, night, sun, rain, . . . ), multiple detailed object class annotations (persons, cars, license plates, . . . ), and different camera settings, among many others factors.</p><p>Using synthetic data should in theory enable full control of the data generation pipeline, hence ensuring lower costs, greater flexibility, and limitless variety and quantity. In this work, we leverage the recent progress in computer graphics (especially off-the-shelf tools like game engines) and commodity hardware (especially GPUs) to generate photorealistic virtual worlds used as proxies to assess the performance of video analysis algorithms.</p><p>Our first contribution is a method to generate large, photo-realistic, varied datasets of synthetic videos, automatically and densely labeled for various video understanding tasks. Our main novel idea consists in creating virtual worlds not from scratch, but by cloning a few seed realworld video sequences. Using this method, our second and main contribution is the creation of the new Virtual KITTI dataset (cf. <ref type="figure">Figure 1)</ref>, which at the time of publication contains 35 photo-realistic synthetic videos (5 cloned from the original real-world KITTI tracking benchmark <ref type="bibr" target="#b0">[1]</ref>, coupled with 7 variations each) for a total of approximately 17,000 high resolution frames, all with automatic accurate ground truth for object detection, tracking, depth, optical flow, as well as scene and instance segmentation at the pixel level.</p><p>Our third contribution consists in quantitatively measuring the usefulness of these virtual worlds as proxies for multi-object tracking. We first propose a practical definition of transferability of experimental observations across real and virtual worlds. Our protocol rests on the comparison of real-world seed sequences with their corresponding synthetic clones using real-world pre-trained deep models (in particular Fast-RCNN <ref type="bibr" target="#b1">[2]</ref>), hyper-parameter calibration via Bayesian optimization <ref type="bibr" target="#b2">[3]</ref>, and the analysis of taskspecific performance metrics <ref type="bibr" target="#b3">[4]</ref>. Second, we validate the usefulness of our virtual worlds for learning deep models by showing that virtual pre-training followed by real-world fine-tuning outperforms training only on real world data. Our experiments, therefore, suggest that the recent progress in computer graphics technology allows one to easily build virtual worlds that are indeed effective proxies of the real world from a computer vision perspective.</p><p>Our fourth contribution builds upon this small virtualto-real gap to measure the potential impact on recognition performance of varied weather conditions (like fog), lighting conditions, and camera angles, all other things being equal, something impractical or even impossible in realworld conditions. Our experiments show that these variations may significantly deteriorate the performance of normally high-performing models trained on large real-world datasets. This lack of generalization highlights the importance of open research problems like unsupervised domain adaptation and building more varied training sets, to move further towards applying computer vision in the wild.</p><p>The paper is organized as follows. Section 2 reviews related works on using synthetic data for computer vision. Section 3 describes our approach to build virtual worlds in general and Virtual KITTI in particular. Section 4 reports our multi-object tracking experiments using strong deep learning baselines (Section 4.1) to assess the transferability of observations across the real-to-virtual gap (Section 4.2), the benefits of virtual pre-training (Section 4.3), and the impact of various weather and imaging conditions on recognition performance (Section 4.4). We conclude in section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Several works investigate the use of 3D synthetic data to tackle standard 2D computer vision problems such as object detection <ref type="bibr" target="#b4">[5]</ref>, face recognition, scene understanding <ref type="bibr" target="#b5">[6]</ref>, and optical flow estimation <ref type="bibr" target="#b6">[7]</ref>. From early on, computer vision researchers leveraged 3D computer simulations to model articulated objects including human shape <ref type="bibr" target="#b7">[8]</ref>, face, and hand appearance <ref type="bibr" target="#b8">[9]</ref>, or even for scene interpretation and vision as inverse graphics <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12]</ref>. However, these methods typically require controlled virtual environments, are tuned to constrained settings, and require the development of taskspecific graphics tools. In addition, the lack of photorealism creates a significant domain gap between synthetic and real world images, which in turn might render synthetic data too simplistic to tune or analyze vision algorithms <ref type="bibr" target="#b12">[13]</ref>.</p><p>The degree of photorealism allowed by the recent progress in computer graphics and modern high-level generic graphics platforms enables a more widespread use of synthetic data generated under less constrained settings. First attempts to use synthetic data for training are mainly limited to using rough synthetic models or synthesized real examples (e.g., of pedestrians <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref>). In contrast, Marín et al. <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18]</ref> went further and positively answer the intriguing question whether one can learn appearance models of pedestrians in a virtual world and use the learned models for detection in the real world. A related approach is described in <ref type="bibr" target="#b18">[19]</ref>, but for scene-and scene-location specific detectors with fixed calibrated surveillance cameras and a priori known scene geometry. In the context of video surveillance too, <ref type="bibr" target="#b19">[20]</ref> proposes a virtual simulation test bed for system design and evaluation. Several other works use 3D CAD models for more general object pose estimation <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref> and detection <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref>.</p><p>Only few works use photo-realistic imagery for evaluation purposes, and in most cases these works focus on lowlevel image and video processing tasks. Kaneva et al. <ref type="bibr" target="#b24">[25]</ref> evaluate low-level image features, while Butler et al. <ref type="bibr" target="#b25">[26]</ref> propose a synthetic benchmark for optical flow estimation: the popular MPI Sintel Flow Dataset. The recent work of Chen et al. <ref type="bibr" target="#b26">[27]</ref> is another example for basic building blocks of autonomous driving. These approaches view photo-realistic imagery as a way of obtaining ground truth that cannot be easily obtained otherwise (e.g., optical flow). When ground-truth can be collected, for instance via crowdsourcing, real-world imagery is often preferred over synthetic data because of the artifacts the latter might introduce.</p><p>In this paper, we show that such issues can be partially circumvented using our approach, in particular for highlevel video understanding tasks for which ground-truth data is tedious to collect. We believe current approaches face two major limitations that prevent broadening the scope of virtual data. First, the data generation is itself costly and timeconsuming, as it often requires creating animation movies from scratch. This also limits the quantity of data that can be generated. An alternative consists in recording scenes from humans playing video games <ref type="bibr" target="#b15">[16]</ref>, but this faces similar time costs, and further restricts the variety of the generated scenes. The second limitation lies in the usefulness of synthetic data as a proxy to assess real-world performance on high-level computer vision tasks, including object detection and tracking. It is indeed difficult to evaluate how conclusions obtained from virtual data could be applied to the real world in general.</p><p>Due to these limitations, only few of the previous works have so far exploited the full potential of virtual worlds: the possibility to generate endless quantities of varied video sequences on-the-fly. This would be especially useful in order to assess model performance, which is crucial for real-world deployment of computer vision applications. In this paper, we propose steps towards achieving this goal by addressing two main challenges: (i) automatic generation of arbitrary photo-realistic video sequences with ground-truth by scripting modern game engines, and (ii) assessing the degree of transferability of experimental conclusions from synthetic data to the real world.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Generating Proxy Virtual Worlds</head><p>Our approach consists in five steps detailed in the following sections: (i) the acquisition of a small amount of realworld data as a starting point for calibration (Section 3.1), (ii) the "cloning" of this real-world data into a virtual world (Section 3.2), (iii) the automatic generation of modified synthetic sequences with different weather or imaging conditions (Section 3.3), (iv) the automatic generation of detailed ground truth annotations (Section 3.4), and (v) the quantitative evaluation of the "usefulness" of the synthetic data (Section 3.5). We describe both the method and the particular choices made to generate our Virtual KITTI dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Acquiring real-world (sensor) data</head><p>The first step of our approach consists in the acquisition of a limited amount of seed data from the real world for the purpose of calibration. Two types of data need to be collected: videos of real-world scenes and physical measurements of important objects in the scene including the camera itself. The quantity of data required by our approach is much smaller than what is typically needed for training or validating current computer vision models, as we do not require a reasonable coverage of all possible scenarios of interest. Instead, we use a small fixed set of core real-world video sequences to initialize our virtual worlds, which in turn allows one to generate many varied synthetic videos. Furthermore, this initial seed real-world data results in higher quality virtual worlds (i.e. closer to real-world conditions) and to quantify their usefulness to derive conclusions that are likely to transfer to real-world settings. In our experiments, we use the KITTI dataset <ref type="bibr" target="#b0">[1]</ref> to initialize our virtual worlds. This standard public benchmark was captured from a car driving in the German city of Karlsruhe, mostly under sunny conditions. The sensors used to capture data include gray-scale and color cameras, a 3D laser scanner, and an inertial and GPS navigation system. From the point clouds captured by the 3D laser scanner, human annotators labeled 3D and 2D bounding boxes of several types of objects including cars and pedestrians. In our experiments we only consider cars as objects of interest for simplicity and because they are the main category of KITTI. The annotation data include the positions and sizes of cars, and their rotation angles about the vertical axis (yaw rotation). The movement of the camera itself was recorded via GPS (latitude, longitude, altitude) and its orientation (roll, pitch, yaw) via a GPS/IMU sensor, which has a fixed spatial relationship with the cameras.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Generating synthetic clones</head><p>The next step of our approach consists in semiautomatically creating photo-realistic dynamic 3D virtual worlds in which virtual camera paths follow those of the real world seed sequences to generate outputs we call synthetic video clones, which closely resemble the real-world data. To build Virtual KITTI, we select five training videos from the original KITTI MOT benchmark as "real-world seeds" to create our virtual worlds (cf. <ref type="figure" target="#fig_0">Figure 2</ref>): 0001 (crowded urban area), 0002 (road in urban area then busy intersection), 0006 (stationary camera at a busy intersection), 0018 (long road in the forest with challenging imaging conditions and shadows), and 0020 (highway driving scene).</p><p>We decompose a scene into different visual components, with which off-the-shelf computer graphics engines (e.g., game engines) and graphic assets (e.g., geometric and material models) can be scripted to reconstruct the scene. We use the commercial computer graphics engine Unity 2 to create virtual worlds that closely resemble the original ones in KITTI. This engine has a strong community that has developed many "assets" publicly available on Unity's Asset Store. These assets include realistic 3D models and materials of objects. This allows for efficient crowd-sourcing of most of the manual labor in the initial setup of our virtual worlds, making the creation of each virtual world efficient (approximately one-person-day in our experiments).</p><p>The positions and orientations of the objects of interest in the 3D virtual world are calculated based on their positions and orientations relative to the camera and the position and orientation of the camera itself, both available from acquired real-world data in the case of KITTI. The main roads are also placed according to the camera position, with minor manual adjustment in special cases (e.g., the road changing width). To build the Virtual KITTI dataset, we manually place secondary roads and other background objects such as trees and buildings in the virtual world, both for simplicity and because of the lack of position data for them. Note that this could be automated using Visual SLAM or semantic segmentation. A directional light source together with ambient light simulates the sun. Its direction and intensity are set manually by comparing the brightness and the shadows in the virtual and real-world scenes, a simple process that only takes a few minutes per world in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Changing conditions in synthetic videos</head><p>After the 3D virtual world is created, we can automatically generate not only the clone synthetic video, but also videos with changed components. This allows for the quantitative study of the impact of single factors ("ceteris paribus analysis"), including rare events or difficult to observe conditions that might occur in practice ("what-if analysis").</p><p>The conditions that can be changed to generate new synthetic videos include (but are not limited to): (i) the number, trajectories, or speeds of cars, (ii) their sizes, colors, or models, (iv) the camera position, orientation, and path, (v) the lighting and weather conditions. All components can be randomized or modified "on demand" by changing parameters in the scripts, or by manually adding, modifying, or removing elements in the scene.</p><p>To illustrate some of the vast possibilities, Virtual KITTI includes some simple changes to the virtual world that translate in complex visual changes that would otherwise require the costly process of re-acquiring and re-annotating data in the real-world. First, we turned the camera to the right and then to the left, which lead to some considerable change of appearances of the cars. Second, we changed lighting conditions to simulate different time of the day: 2 http://unity3d.com early morning and before sunset. Third, we used special effects and a particle system together with changed lighting conditions to simulate different weather: overcast, fog and heavy rain. See <ref type="figure" target="#fig_1">Figure 3</ref> for an illustration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Generating ground-truth annotations</head><p>As stated above, ground-truth annotations are essential for computer vision algorithms. In the KITTI dataset, the 2D bounding boxes used for evaluation were obtained from human annotators by drawing rectangular boxes on the video frames and manually labeling the truncation and occlusion states of objects. This common practice is however costly, does not scale to large volumes of videos and pixellevel ground-truth, and incorporates varying degrees of subjectiveness and inconsistency. For example, the bounding boxes are usually slightly larger than the cars and the margins differ from one car to another and from one annotator to another. The occlusion state ("fully visible", "partly occluded", or "largely occluded") is also subjective and the underlying criterion may differ from case to case, yielding many important edge cases (occluded and truncated cars) with inconsistent labels.</p><p>In contrast, our approach can automatically generate accurate and consistent ground-truth annotations accompanying synthetic video outputs, and the algorithm-based approach allows richer (e.g., pixel-level) and more consistent results than those from human annotators. We render each moment of the scene four times. First, we do the photo-realistic rendering of the clone scene by leveraging the modern rendering engine of Unity. Second, the depth map is rendered by using the information stored in the depth buffer. Third, the per-pixel category-and instance-level ground-truth is efficiently and directly generated by using unlit shaders on the materials of the objects. These modified shaders output a color which is not affected by the lighting <ref type="figure" target="#fig_3">Figure 4</ref>: Rendered frame (left) and automatically generated scene and instance-level segmentation ground-truth (right) for two modified conditions: camera horizontally rotated to the left (top), rain (bottom). and shading conditions. A unique color ID is assigned for each object of interest (cf. <ref type="figure" target="#fig_3">Figure 4)</ref>. Fourth, we compute the dense optical flow between the previous and the current frames by sending all Model, View, and Projection matrices for each object to a vertex shader, and interpolate the flow of each pixel using a fragment shader. Note that these multiple renderings are an efficient strategy to generate pixel-level ground truth, as it effectively leverages shaders offloading parallel computations to GPUs (most of the computation time is used to swap materials). For Virtual KITTI, with a resolution of around 1242 × 375, the full rendering and ground truth generation pipeline for segmentation, depth, and optical flow runs at 5-8 FPS on a single desktop with commodity hardware.</p><p>We generate 2D multi-object tracking ground truth by (i) doing the perspective projection of the 3D object bounding boxes from the world coordinates to the camera plane (clipping to image boundaries in the case of truncated objects), (ii) associating the bounding boxes with their corresponding object IDs to differentiate object instances, and (iii) adding truncation and occlusion meta-data as described below. The truncation rate is approximated by dividing the volume of an object's 3D bounding box by the volume of the 3D bounding box of the visible part (computed by intersecting the original bounding box with the camera frustum planes). We also estimate the 2D occupancy rate of an object by dividing the number of ground-truth pixels in its segmentation mask by the area of the projected 2D bounding box, which includes the occluder, as it results from the perspective projection of the full 3D bounding box of the object. In the special case of fog, we additionally compute the visibility of each object from the fog formula used to generate the effect. To have comparable experimental protocols and reproducible ground truth criteria across real and virtual KITTI, we remove manually annotated "DontCare" areas from the original KITTI training ground truth (i.e. they may can count as false alarms), and ignore all cars smaller than 25 pixels or heavily truncated / occluded during evaluation (as described in <ref type="bibr" target="#b0">[1]</ref>). We set per sequence global thresholds on occupancy and truncation rates of virtual objects to be as close as possible to original KITTI annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Assessing the usefulness of virtual worlds</head><p>In addition to our data generation and annotation methods, a key novel aspect of our approach consists in the assessment of the usefulness of the generated virtual worlds for computer vision tasks. It is a priori unclear whether and when using photo-realistic synthetic videos is indeed a valid alternative to real-world data for computer vision algorithms. The transferability of conclusions obtained on synthetic data is likely to depend on many factors, including the tools used (especially graphics and physics engines), the quality of implementation (e.g., the degree of photo-realism and details of environments and object designs or animations), and the target video analysis tasks. Although using synthetic training data is common practice in computer vision, we are not aware of related works that systematically study the reverse, i.e. using real-world training data, which can be noisy or weakly labeled, and synthetic test data, which must be accurately labeled and where, therefore, synthetic data has obvious benefits.</p><p>To assess robustly whether the behavior of a recognition algorithm is similar in real and virtual worlds, we propose to compare its performance on the initial "seed" real-world videos and their corresponding virtual world clones. We compare multiple metrics of interest (depending on the target recognition task) obtained with fixed hyper-parameters that maximize recognition performance on both the real and virtual videos, while simultaneously minimizing the performance gap. In the case of MOT, we use Bayesian hyperparameter optimization <ref type="bibr" target="#b2">[3]</ref> to find fixed tracker hyperparameters for each pair of real and clone videos. We use as objective function the sum of the multi-object tracking accuracies (MOTA <ref type="bibr" target="#b3">[4]</ref>) over original real-world videos and their corresponding virtual clones, minus their absolute differences, normalized by the mean absolute deviations of all other normalized CLEAR MOT metrics <ref type="bibr" target="#b3">[4]</ref>.</p><p>This allows us to quantitatively and objectively measure the impact of the virtual world design, the degree of photorealism, and the quality of other rendering parameters on the algorithm performance metrics of interest. Note that this simple technique is a direct benefit of our virtual world generation scheme based on synthetically cloning a small set of real-world sensor data. Although the comparisons depend on the tasks of interest, it is also possible to complement task-specific metrics with more general measures of discrepancy and domain mismatch measures <ref type="bibr" target="#b27">[28]</ref>.</p><p>Finally, note that our protocol is complementary to the more standard approach consisting of using synthetic training data and real-world test data. Therefore, in our experiments with Virtual KITTI we investigate both methods to assess the usefulness of virtual data, both for learning virtual models applied in the real world and for evaluating realworld pre-trained models in both virtual and real worlds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we first describe the MOT models used in our experiments. We then report results regarding the differences between the original real-world KITTI videos and our virtual KITTI clones. We then report our experiments on learning in virtual worlds models applied on real-world data. Finally, we conclude with experiments to measure the impact of camera, lighting, and weather on recognition performance of real-world pre-trained MOT algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Strong Deep Learning Baselines for MOT</head><p>Thanks to the recent progress on object detection, association-based tracking-by-detection in monocular video streams is particularly successful and widely used for MOT <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38]</ref> (see <ref type="bibr" target="#b38">[39]</ref> for a recent review). These methods consist in building tracks by linking object detections through time.</p><p>In our experiments, the detector we use is the recent Fast-R-CNN object detector from Girshick <ref type="bibr" target="#b1">[2]</ref> combined with the efficient Edge Boxes proposals <ref type="bibr" target="#b39">[40]</ref>. In all experiments (except for the virtual training ones), we follow the experimental protocol of <ref type="bibr" target="#b1">[2]</ref> to learn a powerful VGG16-based Fast-RCNN car detector by fine-tuning successively from ImageNet, to Pascal VOC 2007 cars, to the KITTI object detection benchmark training images 3 .</p><p>To use this detector for association-based MOT, we consider two trackers. The first is based on the principled network flow algorithm of <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b29">30]</ref>, which does not require video training data. The maximum a posteriori (MAP) data association problem can indeed be elegantly formalized as a special integer linear program (ILP) whose global optimum can be found efficiently using max-flow min-cost network flow algorithms <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b29">30]</ref>. In particular, the dynamic programming min-cost flow (DP-MCF) algorithm of Pirsiavash et al. <ref type="bibr" target="#b29">[30]</ref> is well-founded and particularly efficient. Although it obtains poor results on the KITTI MOT benchmark <ref type="bibr" target="#b41">[42]</ref>, it can be vastly improved by (i) using a better detector, (ii) replacing the binary pairwise costs in the network by using the intersection-overunion, and (iii) allowing for multiple time-skip connections in the network to better handle missed detections. Our DP MCF RCNN tracker reaches 57% MOTA on the KITTI MOT evaluation server <ref type="bibr" target="#b41">[42]</ref>, improving by +20% w.r.t. the original DP MCF <ref type="bibr" target="#b29">[30]</ref>. Note that this baseline tracker could be further improved, as shown recently by Wang and Fowlkes <ref type="bibr" target="#b35">[36]</ref>. Their method indeed obtains 77% MOTA with a related algorithm thanks to better appearance and motion modeling coupled with structured SVMs to learn hyper-parameters on training videos.</p><p>The second tracker we consider is the recent state-ofthe-art Markov Decision Process (MDP) method of Xiang et al. <ref type="bibr" target="#b37">[38]</ref>. It relies on reinforcement learning to learn a policy for data association from ground truth training tracks. This method reaches 76% MOTA on the KITTI MOT test set using ConvNet-based detections. In our experiments requiring a pre-trained tracker, we learned the MDP parameters on the following 12 real-world KITTI training videos: 0000, 0003, 0004, 0005, 0007, 0008, 0009, 0010, 0011, 0012, 0014, 0015. (The remaining videos are either the seed sequences used to create the virtual worlds, or sequences containing no or very few cars.) <ref type="table">Table 1</ref> contains the multi-object tracking performance of our DP-MCF and MDP trackers on the virtual KITTI clone videos and their original KITTI counterparts following the protocol described in Section 3.5. See <ref type="figure" target="#fig_2">Figure 5</ref> for some tracking visualizations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Transferability across the Real-to-Virtual Gap</head><p>According to the MOTA metric which summarizes all aspects of MOT, the real-to-virtual performance gap is minimal for all real sequences and their respective virtual clones and for all trackers, and &lt; 0.5% on average for both trackers. All other metrics show also a limited gap. Consequently, the visual similarity of the sequences and the comparable performance and behavior of the tracker across realworld videos and their virtual worlds counterpart suggest that similar causes in the real and virtual worlds are likely to cause similar effects in terms of recognition performance. The amount of expected "transferability of conclusions" from real to virtual and back can be quantified by the difference in the metrics reported in table 1.   <ref type="table">Table 1</ref>: DP-MCF (left) and MDP (right) MOT results on original real-world KITTI train videos and virtual world video "clones" (prefixed by a "v"). AVG (resp. v-AVG) is the average over real (resp. virtual) sequences. We report the CLEAR MOT metrics The most different metrics are the MOTP (average intersection-over-union of correct tracks with the matching ground truth), and the fraction of Mostly Tracked (MT) objects (fraction of ground truth objects tracked at least 80% of the time), which are both generally lower in the virtual world. The main factor explaining this gap lies in the inaccurate and inconsistent manual annotations of the frequent "corner cases" in the real world (heavy truncation or occlusion, which in the original KITTI benchmark is sometimes labeled as "DontCare", ignored, or considered as true positives, depending on the annotator). In contrast, our Virtual KITTI ground truth is not subjective, but automatically determined by thresholding the aforementioned computed occupancy and truncation rates. This discrepancy is illustrated in <ref type="figure" target="#fig_2">Figure 5</ref>, and explains the small drop in recall for sequences 0001, 0018, and 0020 (which contain many occluded and truncated objects). Note, however, that the Fast-RCNN detector achieves similar F1 performance between real and virtual worlds, so this drop in recall is generally compensated by an increase in precision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Virtual Pre-Training</head><p>As mentioned previously, our method to quantify the gap between real and virtual worlds from the perspective of computer vision algorithms is complementary to the more widely-used approach of leveraging synthetic data to train models applied in real-world settings. Therefore, we additionally conduct experiments to measure the usefulness of Virtual KITTI to train MOT algorithms.</p><p>We evaluated three different scenarios: (i) training only on the 5 real KITTI seed sequences (configuration 'r'), (ii) training only on the corresponding 5 virtual KITTI clones (configuration 'v'), and (iii) training first on the Virtual KITTI clones, then fine-tuning on the real KITTI sequences, a special form of virtual data augmentation we call virtual pre-training (configuration 'v→r'). We split the set of real KITTI sequences not used during training in two: (i) a test set of 7 long diverse videos <ref type="bibr" target="#b3">(4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b14">15)</ref>   <ref type="table">Table 2</ref>: DP-MCF and MDP MOT results on seven held-out original real-world KITTI train videos <ref type="bibr" target="#b3">(4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b14">15)</ref> by learning the models on (r) the five real seed KITTI videos <ref type="bibr" target="#b0">(1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b19">20)</ref>, (v) the corresponding five Virtual KITTI clones, and (v→r) by successively training on the virtual clones then the real sequences (virtual pre-training). See <ref type="table">Table 1</ref> for details about the metrics.</p><p>RCNN detector was always pre-trained on ImageNet. The MDP association model is trained from scratch using reinforcement learning as described in <ref type="bibr" target="#b37">[38]</ref>. <ref type="table">Table 2</ref> reports the average MOT metrics on the aforementioned real test sequences for all trackers trained with all configurations. Although training only on virtual data is not enough, we can see that the best results are obtained with configuration v→r. Therefore, virtual pre-training improves performance, which further confirms the usefulness of virtual worlds for high-level computer vision tasks. The improvement is particularly significant for the DP-MCF tracker, less for the MDP tracker. MDP can indeed better handle missed detections and works in the high-precision regime of the detector (the best minimum detector score threshold found on the validation set is around 95%), which is not strongly improved by the virtual pre-training. On the other hand, DP-MCF is more robust to false positives but requires more recall (validation score threshold around 60%), which is significantly improved by virtual pre-training. In all cases, we found that validating an early stopping criterion (maximum number of SGD iterations) of the second fine-tuning stage of the v→r configuration is critical to avoid overfitting on the small real training set after pretraining on the virtual one. <ref type="table">Table 3</ref> contains the performance of our real-world pretrained trackers (Section 4.1) in altered conditions generated either by modifying the camera position, or by using special effects to simulate different lighting and weather conditions. As the trackers are trained on consistent ideal sunny conditions, all modifications negatively affect all metrics and all trackers. In particular, bad weather (e.g., fog) causes the strongest degradation of performance. This is expected, but difficult to quantify in practice without re-acquiring data in different conditions. This also suggests that the empirical generalization performance estimated on the limited set of KITTI test videos is an optimistic upper bound at best. Note that the MDP tracker is suffering from stronger overfitting than DP-MCF, as suggested by the bigger performance degradation under all conditions.  <ref type="table">Table 3</ref>: Impact of variations on MOT performance in virtual KITTI for the DP-MCF (top) and MDP (bottom) trackers. We report the average performance on the virtual clones and the difference caused by the modified conditions in order to measure the impact of several phenomena, all other things being equal. "+15deg" (resp. "-15deg") corresponds to a camera rotation of 15 degrees to the right (resp. left). "morning" corresponds to typical lighting conditions after dawn on a sunny day. "sunset" corresponds to slightly before night time. "overcast" corresponds to lighting conditions in overcast weather, which causes diffuse shadows and strong ambient lighting. "fog" is implemented via a volumetric formula, and "rain" is a simple particle effect ignoring the refraction of water drops on the camera.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Impact of Weather and Imaging Conditions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work we introduce a new fully annotated photorealistic synthetic video dataset called Virtual KITTI, built using modern computer graphics technology and a novel real-to-virtual cloning method. We provide quantitative experimental evidence suggesting that the gap between real and virtual worlds is small from the perspective of highlevel computer vision algorithms, in particular deep learning models for multi-object tracking. We also show that these state-of-the-art models suffer from over-fitting, which causes performance degradation in simulated modified conditions (camera angle, lighting, weather). Our approach is, to the best of our knowledge, the only one that enables to scientifically measure the potential impact of these important phenomena on the recognition performance of a statistical computer vision model.</p><p>In future works, we plan to expand Virtual KITTI by adding more worlds, and by also including pedestrians, which are harder to animate. We also plan to explore and evaluate domain adaptation methods and larger scale virtual pre-training or data augmentation to build more robust models for a variety of video understanding tasks, including multi-object tracking and scene understanding.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Frames from 5 real KITTI videos (left, sequences 1, 2, 6, 18, 20 from top to bottom) and rendered virtual clones (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Simulated conditions. From top left to bottom right: clone, camera rotated to the right by 15 • , to the left by 15 • , "morning" and "sunset" times of day, overcast weather, fog, and rain.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Predicted tracks on matching frames of two original videos (top) and their synthetic clones (bottom) for both DP-MCF (left) and MDP (right). Note the visual similarity of both the scenes and the tracks. Most differences are on occluded, small, or truncated objects.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>[ 4 ]</head><label>4</label><figDesc>-including MOT Accuracy (MOTA), MOT Precision (MOTP), ID Switches (I), and Fragmentation (F) -complemented by the Mostly Tracked (MT) and Mostly Lost (ML) ratios, as well as our detector's precision (P) and recall (R).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>to evaluate performance, and (ii) a validation set of 5 short videos (0,3,10,12,14) used for hyper-parameter tuning. The Fast-</figDesc><table>MOTA↑ MOTP↑ 

MT↑ 
ML↑ I↓ F↓ 
P↑ 
R↑ 

DP-MCF v 
64.3% 
75.3% 35.9% 31.5% 
0 15 96.6% 71.0% 
DP-MCF r 
71.9% 
79.2% 45.0% 24.4% 
5 17 98.0% 76.5% 
DP-MCF v→r 
76.7% 
80.9% 53.2% 12.3% 
7 27 98.3% 81.1% 

MDP v 
63.7% 
75.5% 35.9% 36.9% 
5 12 96.0% 70.6% 
MDP r 
78.1% 
79.2% 60.7% 22.0% 
3 
9 97.3% 82.5% 
MDP v→r 
78.7% 
80.0% 51.7% 19.4% 
5 10 98.3% 82.6% 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">http://www.cvlibs.net/datasets/kitti/eval_ object.php</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? The KITTI vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Urtasun</surname></persName>
		</author>
		<ptr target="http://www.cvlibs.net/datasets/kitti/eval_tracking.php.1" />
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fast R-Cnn</surname></persName>
		</author>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Making a science of model search: Hyperparameter optimization in hundreds of dimensions for vision architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yamins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Evaluating Multiple Object Tracking Performance: The CLEAR MOT Metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keni</forename><surname>Bernardin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rainer</forename><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EURASIP Journal on Image and Video Processing</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Teaching 3d geometry to deformable part models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bojan</forename><surname>Pepik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Data-driven scene understanding from 3d models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Satkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A naturalistic open source movie for optical flow evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Butler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Inferring 3d structure with a statistical image-based shape model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Motion capture of hands in action using discriminative salient points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Ballan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aparna</forename><surname>Taneja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Simulation as an engine of physical scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><forename type="middle">B</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Hamrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tenenbaum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>PNAS</publisher>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="page" from="18327" to="18332" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Approximate Bayesian Image Interpretation using Generative Probabilistic Graphics Programs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mansinghka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Td Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Perov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Picture: A Probabilistic Programming Language for Scene Perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Td Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mansinghka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Differences between stereo and motion behaviour on synthetic and real-world stereo sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobi</forename><surname>Vaudrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clemens</forename><surname>Rabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reinhard</forename><surname>Klette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Milburn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IVCNZ</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Model-based validation approaches and matching techniques for automotive vision based pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Broggi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fascioli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Grisleri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Graf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Meinecke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Back to the future: Learning shape models from 3d cad data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Goesele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning appearance in virtual scenarios for pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Marin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Geronimo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Lopez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation of virtual and real worlds for pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vázquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>López</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ponsa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Virtual and real world adaptation for pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Marin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ponsa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Geronimo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning scene-specific pedestrian detectors without real data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiro</forename><surname>Hattori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vishnu</forename><surname>Bodetti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kris</forename><forename type="middle">M</forename><surname>Kitani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeo</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Ovvv: Using virtual worlds to design and evaluate surveillance systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Geoffrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul C</forename><surname>Chosak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brewer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Seeing 3D chairs: exemplar part-based 2D-3D alignment using a large dataset of CAD models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aubry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Maturana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Adaptation of Synthetic Data for Coarse-to-Fine Viewpoint Refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pp Busto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liebelt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">From Virtual to Reality: Fast Adaptation of Virtual Object Detectors to Real Domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning Deep Object Detectors from 3D Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Evaluation of image features using a photorealistic virtual world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kaneva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A naturalistic open source movie for optical flow evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">J</forename><surname>Butler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garrett</forename><forename type="middle">B</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">DeepDriving: Learning affordance for direct perception in autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Seff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alain</forename><forename type="middle">L</forename><surname>Kornhauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.00256.2</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A kernel two-sample test</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mj Rasch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Online Multi-Person Tracking-by-Detection from a Single, Uncalibrated Camera. PAMI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Md Breitenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Reichlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Koller-Meier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Globally-optimal greedy algorithms for tracking a variable number of objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schindler</surname></persName>
		</author>
		<title level="m">Continuous Energy Minimization for Multi-Target Tracking. PAMI</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wojek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Urtasun</surname></persName>
		</author>
		<title level="m">3D Traffic Scene Understanding from Movable Platforms. PAMI</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Online, Real-Time Tracking Using a Category-to-Individual Detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Hybrid Stochastic/Deterministic Optimization for Tracking Sports Players and Pedestrians</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Carr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Online Domain Adaptation for Multi-Object Tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gaidon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning Optimal Parameters For Multi-target Tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaofei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Near-Online Multi-target Tracking with Aggregated Local Flow Descriptor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wongun</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning to Track : Online Multi-Object Tracking by Decision Making</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Multiple object tracking: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.7618</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Edge Boxes: Locating Object Proposals from Edges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Global data association for multi-object tracking using network flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakant</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kitti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Results</surname></persName>
		</author>
		<ptr target="http://www.cvlibs.net/datasets/kitti/eval_tracking.php" />
		<imprint/>
	</monogr>
	<note>Online; accessed 2016-04-08</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
