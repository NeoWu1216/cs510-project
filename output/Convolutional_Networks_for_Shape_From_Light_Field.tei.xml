<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Convolutional Networks for Shape from Light Field</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Heber</surname></persName>
							<email>stefan.heber@icg.tugraz.at</email>
							<affiliation key="aff0">
								<orgName type="institution">Graz University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Pock</surname></persName>
							<email>pock@icg.tugraz.at</email>
							<affiliation key="aff0">
								<orgName type="institution">Graz University of Technology</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Austrian Institute of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Convolutional Networks for Shape from Light Field</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:38+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Convolutional Neural Networks (CNNs) have recently been successfully applied to various Computer Vision (CV) applications. In this paper we utilize CNNs to predict depth information for given Light Field (LF) data. The proposed method learns an end-to-end mapping between the 4D light field and a representation of the corresponding 4D depth field in terms of 2D hyperplane orientations. The obtained prediction is then further refined in a post processing step by applying a higher-order regularization.</p><p>Existing LF datasets are not sufficient for the purpose of the training scheme tackled in this paper. This is mainly due to the fact that the ground truth depth of existing datasets is inaccurate and/or the datasets are limited to a small number of LFs. This made it necessary to generate a new synthetic LF dataset, which is based on the raytracing software POV-Ray. This new dataset provides floating point accurate ground truth depth fields, and due to a random scene generator the dataset can be scaled as required.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>A 4D light field <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b13">14]</ref> provides information of all light rays, that are emitted from a scene and hit a predefined surface. Contrary to a traditional image, a LF contains not only intensity information, but also directional information. This additional directional information inherent in the LF implicitly defines the geometry of the observed scene.</p><p>It is common practice to use the so-called two-plane or light slab parametrization to describe the LF. This type of parametrization defines a ray by its intersection points with two planes, that are usually referred to as image plane Ω ⊆ R 2 and lens plane Π ⊆ R 2 . Hence the LF can be defined in mathematical terms as the mapping</p><formula xml:id="formula_0">L : Ω × Π → R, (p, q) → L(p, q) ,<label>(1)</label></formula><p>where p = (x, y) ⊤ ∈ Ω and q = (ξ, η) ⊤ ∈ Π represent the spatial and directional coordinates. There are different ways to visualize the 4D LF. One way of visualizing the LF is as a flat 2D array of 2D ar- rays, which can be arranged position major or direction major. The direction major representation can be interpreted as a set of pinhole views, where the viewpoints are arranged on a regular grid parallel to a common image plane (c.f . <ref type="figure" target="#fig_1">Figure 2</ref>). Those pinhole views are called subaperture images, and they clearly show that the LF provides information about the scene geometry. When considering Equation (1) a sub-aperture image is obtained by holding q fixed and by varying over all spatial coordinates p. Another visualization of LF data is called Epipolar Plane Image (EPI) representation, which is a more abstract visualization, where one spatial coordinate and one directional coordinate is held constant. For example if we fix y and η, then we restricts the LF to a 2D function Σ y,η : R 2 → R, (x, ξ) → L(x, y, ξ, η) ,</p><p>that defines a 2D EPI. The EPI represents a 2D slice through the LF and it also shows that the LF space is largely linear, i.e. that a 3D scene point always maps to a 2D hyperplane in the LF space (c.f . <ref type="figure" target="#fig_0">Figure 1</ref>). There are basically two ways of capturing a dynamic LF. First, there are camera arrays <ref type="bibr" target="#b33">[34]</ref>, that are bulky and ex-pensive, but allow to capture high resolution LFs. Second, more recent efforts in this field focus on plenoptic cameras <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25]</ref>, that are able to capture the LF in a single shot. Although LFs describe a powerful concept that is well-established in CV, commercially available LF capturing devices (e.g. Lytro, Raytrix, or Pelican) currently only fill a market niche, and are by far outweighed by traditional 2D cameras.</p><p>LF image processing is highly interlinked with the development of efficient and reliable shape extraction methods. Those methods are the foundation of all kinds of applications, like digital refocusing <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b23">24]</ref>, image segmentation <ref type="bibr" target="#b32">[33]</ref>, or super-resolution <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b30">31]</ref>, to name but a few. In the context of Shape from Light Field (SfLF) authors mainly focus on robustness w.r.t. depth discontinuities or occlusion boundaries. These occlusion effects occur when near objects hide parts of objects that are further away from the observer. In the case of binocular stereo occlusion handling is a tough problem, because it is basically impossible to establish correspondences between points that are observed in one image but occluded in the other image. In this case only prior knowledge about the scene can be used to resolve those problems. This prior knowledge is usually added in terms of a regularizer. In the case of multi-view stereo those occlusion ambiguities can be addressed by using the different viewpoints. This somehow suggests to select for each image position a subset of viewpoints that, when used for shape estimation, reduce the occlusion artifacts in the final result. In this paper we propose a novel method that implicitly learns the pixelwise viewpoint selection and thus allows to reduce occlusion artifacts in the final reconstruction.</p><p>Contribution. The contribution of the presented work is twofold. First, we propose a novel method to estimate the shape of a given LF by utilizing deep learning strategies. More specifically, we propose a method that predicts for each imaged scene point the orientation of the corresponding 2D hyperplane in the domain of the LF. After the pointwise prediction, we use a 4D regularization step to overcome prediction errors in textureless or uniform regions, where we use a confidence measure to gauge the reliability of the estimate. For this purpose we formulated a convex optimization problem with higher-order regularization, that also uses a 4D anisotropic diffusion tensor to guide the regularization.</p><p>Second, we present a dataset of synthetic LFs, that provides highly accurate ground-truth depth fields, and where scenes can be randomly generated. On the one hand, the generated LFs are used to train a CNN for the hyperplane prediction. On the other hand, we use the generated data to analyze the results and compare to other SfLF algorithms. Our experiments show that the proposed method works for synthetic and real-world LF data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Extracting geometric information from LF data is one of the most important problems in LF image processing. We briefly review publications most relevant in this field. As already mentioned in the introduction, LF imaging can be seen as an extreme case of a multi-view stereo system, where a large amount of highly overlapping views are available. Hence, it is hardly surprising, that the increasing popularity of LFs renewed the interest on specialized multi-view reconstruction methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b7">8]</ref>. For instance, in <ref type="bibr" target="#b1">[2]</ref> Bishop and Favaro proposed a multi-view stereo method, that theoretically utilizes the information of all possible combinations of sub-aperture images. Anyhow, the paper mainly focuses on anti-aliasing filters, that are used as a pre-possessing step for the actual depth estimation. In a further work <ref type="bibr" target="#b2">[3]</ref> they propose a method that performs the matching directly on the raw image of a plenoptic camera by using a specifically designed photoconsistency constraint. Heber et al. <ref type="bibr" target="#b16">[17]</ref> proposed a variational multiview stereo method, where they use a circular sampling scheme that is inspired by a technique called Active Wavefront Sampling (AWS) <ref type="bibr" target="#b10">[11]</ref>. In <ref type="bibr" target="#b7">[8]</ref> Chen et al. introduced a bilateral consistency metric on the surface camera to indicate the probability of occlusions. This occlusion probability is then used for LF stereo matching.</p><p>Another, more classical way of extracting the depth information from LF data is to analyze the line directions in the EPIs <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b15">16]</ref>. Wanner and Goldluecke <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b12">13]</ref> for example applied the 2D structure tensor to measure the direction of each position in the EPIs. The estimated line directions are then fused using variational methods, where they incorporate additional global visibility constraints. Heber and Pock <ref type="bibr" target="#b15">[16]</ref> recently proposed a method for SfLF, that shears the 4D light field by applying a lowrank assumption. The amount of shearing then allows to estimate the depth map of a predefined sub-aperture image.</p><p>Unlike all the above mentioned methods, we suggest to train a CNN that allows to predict for each imaged 3D scene point the corresponding 2D hyperplane orientation in the LF domain. This is achieved by extracting information from vertical and horizontal EPIs around a given position in the LF domain, and feeding this information to a CNN. In order to handle textureless regions a 4D regularization is applied to obtain the final result, where a confidence measure is used to gauge the reliability of the CNN prediction. Our approach incorporates higher-order regularization, which avoids surface flattening. Moreover, we also make use of a 4D anisotropic diffusion tensor, that is calculated based on the intensity information in the LF. This tensor weights and orients the gradient during the optimization process. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>In this section we describe the methodology of the proposed approach, that can be divided into three main areas:</p><p>(1) Utilizing deep learning to predict 2D hyperplane orientations in the LF space (c.f . Section 3.1), (2) formulating a convex energy functional to refine the predicted orientations (c.f . Section 3.2), and (3) solving the resulting optimization problem using a first-order primal-dual algorithm (c.f . Section 3.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Hyperplane Prediction</head><p>The popularity of CNNs trained in a supervised manner via backpropagation <ref type="bibr" target="#b21">[22]</ref> increased drastically after Krizhevsky et al. <ref type="bibr" target="#b20">[21]</ref> utilized them effectively for the task of large-scale image classification. Inspired by the good performance on the image classification task, authors proposed numerous works, that apply CNNs to different CV problems including depth prediction <ref type="bibr" target="#b9">[10]</ref>, keypoint localization <ref type="bibr" target="#b14">[15]</ref>, edge detection <ref type="bibr" target="#b11">[12]</ref>, and image matching <ref type="bibr" target="#b34">[35]</ref>. Zbontar and LeCun <ref type="bibr" target="#b34">[35]</ref> for example proposed to train a CNN on pairs of small image patches, to predict stereo matching costs. Those costs were then refined using crossbased cost aggregation and semiglobal matching.</p><p>In the case of LF data the depth information of an imaged scene point is encoded in the orientation of the corresponding 2D hyperplane in the LF domain. In order to be able to predict this orientation we extract information from a predefined neighborhood of a given point (p, q) ∈ Ω × Π. More specifically, a training example comprises two image patches of size 31 × 11 centered at (p, q), where the first patch P v (p, q) ⊆ Σ x,ξ is extracted from the vertical EPI, and the second patch P h (p, q) ⊆ Σ y,η is extracted from the horizontal EPI. Note, that values outside the domain of the LF are set to zero. <ref type="figure" target="#fig_1">Figure 2</ref> illustrates this patch extraction step. The figure shows a pair of horizontal and vertical patches, where the orientation of the line that intersects the center of the patch defines the orientation of the 2D hyperplane.  Network Architecture. The used network architecture is depicted in <ref type="figure" target="#fig_2">Figure 3</ref>. The network consists of five layers, denoted as L i , i ∈ [5] 1 . The first four layers are convolutional layers, followed by one fully-connected layer. Each convolutional layer is followed by a Rectified Linear Unit (ReLU) nonlinearity. The first and third layer is padded such that the width and height between input and output is not changing. The kernel size of the convolutional layers decreases towards deeper layers. More precisely, we us kernels of size 7 × 7 for the first two layers, and kernels of size 5 × 5 for the layers three and four. The number of feature maps also increases towards deeper layers, i.e. we use 64 feature maps for the first two layers and double them for the following two layers. Note, that there is no pooling involved in the used network architecture, and the inputs are two RGB image patches of size 31 × 11.</p><p>POV-Ray Dataset. Despite the success of CNNs, there are also some drawbacks. One main drawback is the need of huge labeled datasets, that can be used for the supervised training. In order to fulfill this requirement we generated a synthetic LF dataset using POV-Ray <ref type="bibr" target="#b27">[28]</ref>. Compared to the widely used Light Field Benchmark Dataset (LFBD) <ref type="bibr" target="#b31">[32]</ref>, which is generated with Blender <ref type="bibr" target="#b4">[5]</ref>, POV-Ray allows to calculate floating point accurate ground truth depth maps without discretization artifacts. In order to be able to increase the dataset as required, we also implemented a random scene generator. This scene generator divides the entire scene in foreground, midground, and background, as illustrated in <ref type="figure" target="#fig_3">Figure 4</ref>(a). The foreground and midground regions are randomly filled with comparatively small and large objects, respectively. Those objects are heavily occluding each other. The resulting occlusion and disocclusion effects lead to a high degree of hyperplane intersections in the LF domain. The used 3D objects for the foreground and midground are obtained from the Stanford 3D scanning repository <ref type="bibr" target="#b8">[9]</ref>, and from the Oyonale dataset <ref type="bibr" target="#b25">[26]</ref>. We use around 20 different 3D objects, where about half of them come with random textures from categories like for instance stone, wood, or metal. Moreover, we also use random finish properties. Among other things those finish properties define the non-Lambertian reflectance characteristics of the different surfaces. The backgrounds of the scenes are represented by images downloaded from Google image search, that are labeled for reuse. We use background images with various resolutions from the categories city, landscape, mountain, and street.</p><p>After creating a random scene we render it from various viewpoints, where those viewpoints are placed on a regular grid (c.f . <ref type="figure" target="#fig_1">Figure 2 left</ref>). All rendered images use the same image plane, and the optical axes converge at a predefined point, that is chosen at random somewhere between the image plane and the background. Note, that due to the nonparallel viewing directions this results in non-perpendicular camera vectors, which is intended. Using this procedure we generate 25 LFs, where we use 20 to extract patches for training and 5 LFs are used for testing. The spatial resolution of the rendered LFs is set to 640 × 480, and the directional resolution is set to 11 × 11, which results in 121 sub-aperture images per LF.</p><p>Data Augmentation. Data augmentation is a widely used strategy to generalize neural networks <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b9">10]</ref>. Although we could simply increase the dataset, augmentation during training seems to be important to avoid overfitting. The used augmentation includes changes in brightness, and color, as well as additive Gaussian noise. More specifically, the additive Gaussian noise has a sigma of 0.05. The multiplicative and additive color changes for each RGB channel are randomly sampled from the interval [0.5, 2] and [−0.15, 0.15], respectively. <ref type="figure" target="#fig_4">Figure 5</ref> provides some augmentation examples.</p><p>Network Training. In order to train the CNN we make use of the caffe framework <ref type="bibr" target="#b18">[19]</ref>, where we use Adam <ref type="bibr" target="#b19">[20]</ref> as the optimization method to minimize the Euclidean loss. From the 20 LFs rendered for training we extract 8e6 training examples, which are doubled using data augmentation. We pre-process each patch by subtracting the mean and dividing by the standard deviation of the pixel intensities. In order to monitor overfitting we use a test set of 2e6 examples. The results presented in this paper are obtained after 150k iterations of backpropagation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Refinement Model</head><p>In order to refine the predicted orientations we utilize variational techniques and formulate the following optimization problem</p><formula xml:id="formula_2">minimize u µ D(u, f ) + R(u) ,<label>(3)</label></formula><p>where f and u denote tensors of order four. The objective function in Equation <ref type="formula" target="#formula_2">(3)</ref> is a combination of the data term D(u, f ), that measures the fidelity of the argument u to the predicted measurements f , and the regularization term R(u) that incorporates prior-knowledge about the solution. The scalar µ is used to balance the influence of the data term w.r.t. the regularization term. The data term in our model ensures that the final solution is close to the predicted measurements of the CNN and is thus defined as</p><formula xml:id="formula_3">D(u, f ) = 1 2 c ⊙ (u − f ) 2 2 ,<label>(4)</label></formula><p>where c is a confidence measure (c.f . (8)), and ⊙ denotes the Hadamard product. The regularization term has to meet the challenges of removing artifacts and noise and simultaneously preserving sharp discontinuities in the sub-aperture images and in the EPIs as well. Common regularization terms are based on the first-order smoothness assumption. A famous example is the Total Variation (TV) semi-norm <ref type="bibr" target="#b28">[29]</ref> given as TV(u) = ∇u 1 . This type of regularization favors piecewise constant solutions an is thus well suited for intensity image denoising. However, when used for range data this property of the solution to be piecewise constant results in piecewise fronto-parallel depth reconstructions, which is not desirable. In order to avoid this effect in the spatial domain of the reconstruction we use a generalization of TV called Total Generalized Variation (TGV) introduced by Bredies et al. <ref type="bibr" target="#b5">[6]</ref>. TGV of order k introduces higher order derivatives to incorporate smoothness from the first up to the k th derivative. In other words, TGV of order k favors piecewise polynomial solutions of order k − 1. For our purpose TGV of second order is sufficient, since most objects can be well approximated by piecewise affine surfaces. The primal form of TGV of second order is given as</p><formula xml:id="formula_4">TGV 2 α (z) = min w α 1 ∇z − w 1 + α 0 Ew 1 , (5)</formula><p>where Ew is the distributional symmetrized derivative of w, and α i , i ∈ {0, 1}, are weighting factors. The objective function in Equation <ref type="formula">(5)</ref> has the following intuitive interpretation. Before the TV of z is measured a vector field w of low variation is subtracted from the gradient. We choose to apply the TGV regularization w.r.t. the spatial coordinates p of the LF, and use a TV regularization w.r.t. to the directional coordinates q of the LF, which results in the following regularization term</p><formula xml:id="formula_5">R(u) = TGV 2 α (u| x,y ) + β TV(u| ξ,η ) ,<label>(6)</label></formula><p>where β is a scalar that allows to weight the TV component, and u| x,y and u| ξ,η denote the restrictions of u to the coordinates (x, y) and (ξ, η), respectively. Assuming that intensity discontinuities in the LF correspond to depth discontinuities, we will make use of the intensity information to guide the regularization. More specifically, we will include an anisotropic diffusion tensor Γ, that is calculated by analyzing the 4D structure tensor at each point (p, q) in the discrete domain of the LF. Therefore we will first calculate the eigenvalues λ i and eigenvectors v i , i ∈ <ref type="bibr" target="#b3">[4]</ref>, of the 4D structure tensor at position (p, q). Assuming that the eigenvalues are given in ascending order, λ 1 . . . λ 4 , the anisotropic diffusion tensor Γ(p, q) is given as</p><formula xml:id="formula_6">i∈[2] v i v ⊤ i + j∈[4]\[2] exp(−γ ∇L(p, q) δ ) v j v ⊤ j ,<label>(7)</label></formula><p>where γ and δ adjust the magnitude and the sharpness of the tensor. Γ will orientate and weight the gradient direction during the optimization process, which leads to sharp depth transition at regions with high intensity differences. Note that a similar strategy was used in <ref type="bibr" target="#b16">[17]</ref> to regularize the depth map of the 2D center view of the LF. The confidence measure c is also calculated based on the information derived from the structure tensor, i.e. the confidence at position (p, q) is calculated as</p><formula xml:id="formula_7">c(p, q) = i∈[3] j∈[4]\[i] (λ i − λ j ) 2 .<label>(8)</label></formula><p>The final energy term combines the data term (4), the confidence measure (8), the regularization term <ref type="bibr" target="#b5">(6)</ref>, and the anisotropic diffusion tensor <ref type="bibr" target="#b6">(7)</ref>, and is given as</p><formula xml:id="formula_8">min u,w µ 2 c ⊙ (u − f ) 2 2 (9) + Γ α 1 (∇u| x,y − w) β ∇u| ξ,η 1 + α 0 Ew 1 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Optimization</head><p>The optimization problem in Equation <ref type="formula">(9)</ref> is convex but non-smooth. In order to find a global optimal solution we will utilize the primal-dual algorithm proposed by Chambolle and Pock <ref type="bibr" target="#b6">[7]</ref>. Therefore we reformulate (9) as the following convex-concave saddle-point problem</p><formula xml:id="formula_9">min u,v max du,dw µ 2 c ⊙ (u − f ) 2 2 (10) + Γ α 1 (∇u| x,y − w) β ∇u| ξ,η , d u − χ B∞(0,1) (d u | x,y ) − χ B∞(0,1) (d u | ξ,η ) + α 0 Ew, d w − χ B∞(0,1) (d w ) ,</formula><p>where we introduced the dual variables d u and d w . Moreover, we denote by B ∞ (0, 1) the ℓ 2,∞ norm ball centered at zero with radius one, and χ A denotes the characteristic function of a set A. The saddle-point formulation in Equation <ref type="formula" target="#formula_0">(10)</ref> allows to directly apply the primal-dual algorithm. Moreover, using adequate symmetric and positive definite preconditioning matrices as suggested in <ref type="bibr" target="#b26">[27]</ref> the convergence speed of the algorithm can be further improved. Note however that the diagonal preconditioning results in dimension-dependent step lengths, instead of global step lengths, i.e. the global complexity of the algorithm does not change. The final algorithm is iterated for a fixed number of iterations or till a suitable convergence criterion is fulfilled. The involved gradient and divergence operators are approximated using forward/backward differences with Neumann and Dirichlet boundary conditions, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section we will evaluate the proposed method on synthetic and real world LF data. For the synthetic evaluation we will use the test set of the generated POV-Ray dataset. For the real world evaluation we use the Stanford Light Field Archive (SLFA), that includes LFs captured with a multi-camera array <ref type="bibr" target="#b33">[34]</ref>, where each LF contains 289 sub-aperture images on a 17 × 17 grid.</p><p>Synthetic Evaluation. We start with the synthetic evaluation. When considering <ref type="figure" target="#fig_5">Figure 6</ref>, that provides network predictions and refinement results for two examples of the POV-Ray test set, we see that the CNN is able to predict reasonable 2D hyperplane orientations. The predicted orientations are accurate in well textured regions, but degrade in regions with less texture. Note that predicted orientations are barely effected by depth discontinuities. When comparing the network predictions with the refinement results, we see that the additional refinement model allows to reduce the errors in textureless regions and simultaneously preserves sharp depth discontinuities, as expected. In <ref type="figure">Figure 7</ref> we compare our method to the works of Wanner and Goldluecke <ref type="bibr" target="#b29">[30]</ref> and Heber and Pock <ref type="bibr" target="#b15">[16]</ref>. The method by Wanner and Goldluecke <ref type="bibr" target="#b29">[30]</ref> makes use of the EPI representation of the LF and calculates a globally consistent depth labeling. Heber and Pock <ref type="bibr" target="#b15">[16]</ref> proposed a variational multi-view stereo model based on low rank minimization, where they use ideas from Robust Principal Component Analysis (RPCA), to define an all vs. all matching term. Compared to the method by Wanner and Goldluecke <ref type="bibr" target="#b29">[30]</ref> we observe that the proposed method provides a more accurate reconstruction. This is mainly due to the fact that the proposed method provides continuous estimates and the method of Wanner and Goldluecke only provides a discrete depth labeling. Also note that the method by Wanner and Goldluecke fails if the hyperplane orientations are too close to the orientation of the xy plane. In this case the lines in the EPIs disconnect and the 2D structure tensor fails to estimate the correct orientation of the line. This is for example the reason for the large reconstruction errors of this method in scene1 (c.f . <ref type="figure">Figure 7)</ref>. Compared to the variational model by Heber and Pock <ref type="bibr" target="#b15">[16]</ref> the proposed method provides more details and sharper depth discontinuities for objects close to the camera. Hence the proposed method is especially useful in areas with severe occlusion effects. The <ref type="table">Table 1</ref>. Quantitative results for the POV-Ray test set. The table shows the RMSE scaled by a factor of 100 for the different synthetic scenes shown in <ref type="figure" target="#fig_5">Figure 6</ref> and <ref type="figure">Figure 7</ref>. Note, that the results for the methods proposed by Wanner and Goldluecke <ref type="bibr" target="#b29">[30]</ref> and Heber and Pock <ref type="bibr" target="#b15">[16]</ref> are obtained by running the source code provided by the authors. <ref type="bibr">#</ref> Wanner and Goldluecke <ref type="bibr" target="#b29">[30]</ref> Heber and Pock <ref type="bibr" target="#b15">[16]</ref> CNN proposed  <ref type="bibr" target="#b15">[16]</ref> suffers from a lose of detail mainly due to the required coarse to fine warping scheme. Quantitative results in terms of the root mean squared error (RMSE) are presented in <ref type="table">Table 1</ref> for the entire test set. When considering the results of the individual scenes we see that the proposed method is able to outperform the scene1 scene2 scene3 LF ground truth Wanner et al. <ref type="bibr" target="#b29">[30]</ref> Heber and Pock <ref type="bibr" target="#b15">[16]</ref> proposed <ref type="figure">Figure 7</ref>. Comparison to state-of-the-art methods on the synthetic POV-Ray dataset. The figure shows, from left to right, the center view of the LF, the color-coded ground truth, the results for two state-of-the-art SfLF methods <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b15">16]</ref>, followed by the refinement result of the proposed method.</p><p>Amethyst center view Wanner et al. <ref type="bibr" target="#b29">[30]</ref> Heber and Pock <ref type="bibr" target="#b15">[16]</ref> proposed <ref type="figure">Figure 8</ref>. Qualitative comparison for a LF from the SLFA. The figure shows, from left to right, the center view of the LF, the results for two state-of-the-art SfLF methods <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b15">16]</ref>, and the refinement result of the proposed method, where we also show the network prediction in the purple sub-window.</p><p>competing methods on all but one scene. Furthermore, on average the proposed method is able to clearly outperform the other state-of-the-art methods. However, it should be emphasized that a main drawback of the presented method is to apply the CNN in a sliding window fashion, which results in considerable high computational costs.</p><p>Real World Evaluation. We continue with a short real world evaluation. <ref type="figure">Figure 8</ref> provides a qualitative comparison of different SfLF methods, where a LF from the SLFA is used as input. Note that the scene is quite challenging because of the high degree of specularity. The result of the proposed method basically shows that the trained model can be applied to reconstruct real world LF data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper we proposed a novel method for SfLF. Our method is a combination of deep learning and variational techniques. We trained a CNN to predict 2D hyperplane orientations in the LF domain. Knowing these orientations allows to reconstruct the geometry of the scene. In addition to the learning approach we formulated a global energy optimization problem with a higher-order regularization to refine the network predictions. For numerical optimization of the variational model we use a first-order primal-dual algorithm. Overall the presented method demonstrates the possibility to use deep learning strategies for the task of shape estimation in the LF setting.</p><p>In order to provide enough data to train the network we generated a synthetic dataset by using the raytracing software POV-Ray. To generate an arbitrary amount of scenes we also implemented a random scene generator. The generated data was not just used to train the CNN, but also to provide quantitative and qualitative comparisons to existing SfLF methods. The qualitative evaluation of reconstruction results of synthetic and real world LF data showed that the proposed method is able to provide accurate reconstructions with sharp depth discontinuities. Moreover, our quantitative experiments showed that the proposed method is able to outperform existing methods on the POV-Ray test set in terms of the RMSE.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Illustration of LF data. (a) shows a sub-aperture image with vertical and horizontal EPIs. The EPIs correspond to the positions indicated with dashed lines in the sub-aperture image. (b) shows the corresponding depth field, where red regions are close to the camera and blue regions are further away. In the EPIs a set of 2D hyperplanes is indicated with yellow lines, where corresponding scene points are highlighted with the same color in the sub-aperture representation in (b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Illustration of the patch extraction. The figure to the left illustrates the LF as a direction major 2D array of 2D arrays, where the coordinate (p0, q0) is marked. The corresponding vertical and horizontal patches at that location are shown to the right.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Illustration of the network architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Illustration of a rendered LF. (a) provides a 3D view of a randomly generated scene, where foreground, midground, and image plane are highlighted in green, blue and purple. (b) shows a sub-aperture image of the obtained LF.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Illustration of data augmentation. The figure shows the original patches to the left and three different augmentation results to the right.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Illustration of reconstruction results for example scenes from the POV-ray dataset. The figure shows, from left to right, the LF data, the color-coded ground truth, the CNN prediction, and the refinement result.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Notation: [N ] := {1, . . . , N }</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgment. This work was supported by the FWF-START project Bilevel optimization for Computer Vision, No. Y729 and the Vision+ project Integrating visual information with independent knowledge, No. 836630.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Single lens stereo with a plenoptic camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Adelson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y A</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="99" to="106" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Plenoptic depth estimation from multiple aliased views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th International Conference on Computer Vision Workshops</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1622" to="1629" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Full-resolution depth map estimation from an aliased plenoptic light field</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">E</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th Asian Conference on Computer Vision -Volume Part II</title>
		<meeting>the 10th Asian Conference on Computer Vision -Volume Part II<address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="186" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The light field camera: Extended depth of field, aliasing, and superresolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">E</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="972" to="986" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blender</surname></persName>
		</author>
		<ptr target="https://www.blender.org.3" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Total generalized variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bredies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kunisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Imaging Sciences</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A first-order primal-dual algorithm for convex problems with applications to imaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chambolle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematical Imaging and Vision</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="120" to="145" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Light field stereo matching using bilateral statistics of surface cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Bing</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A volumetric method for building complex models from range images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Levoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd Annual Conference on Computer Graphics and Interactive Techniques, SIGGRAPH &apos;96</title>
		<meeting>the 23rd Annual Conference on Computer Graphics and Interactive Techniques, SIGGRAPH &apos;96<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1996" />
			<biblScope unit="page" from="303" to="312" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Depth map prediction from a single image using a multi-scale deep network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and K. Weinberger</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">3-dimensional Surface Imaging Using Active Wavefront Sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Frigerio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
		<respStmt>
			<orgName>Massachusetts Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">4 -fields: Neural network nearest neighbor fields for image transforms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ACCV 2014</title>
		<editor>D. Cremers, I. Reid, H. Saito, and M.-H. Yang</editor>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">9004</biblScope>
			<biblScope unit="page" from="536" to="551" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The variational structure of disparity and regularization of 4d light fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Goldluecke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wanner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Gortler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Grzeszczuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Cohen</surname></persName>
		</author>
		<title level="m">The lumigraph. In SIGGRAPH</title>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="page" from="43" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Hypercolumns for object segmentation and fine-grained localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Shape from light field meets robust PCA</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Heber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th European Conference on Computer Vision</title>
		<meeting>the 13th European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Variational Shape from Light Field</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Heber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranftl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Energy Minimization Methods in Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Dynamically reparameterized light fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Isaksen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mcmillan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Gortler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="297" to="306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Caffe: Convolutional architecture for fast feature embedding. CoRR, abs/1408</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">5093</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization. CoRR, abs/1412</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">6980</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">; F</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Burges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="541" to="551" />
			<date type="published" when="1989-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Light field rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Levoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hanrahan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd Annual Conference on Computer Graphics and Interactive Techniques, SIGGRAPH &apos;96</title>
		<meeting>the 23rd Annual Conference on Computer Graphics and Interactive Techniques, SIGGRAPH &apos;96<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1996" />
			<biblScope unit="page" from="31" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Digital Light Field Photography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
		<respStmt>
			<orgName>Stanford University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Phd thesis</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Light field photography with a hand-held plenoptic camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Levoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brédif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Duval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Horowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hanrahan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
		<respStmt>
			<orgName>Stanford University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Oyonale</surname></persName>
		</author>
		<ptr target="http://www.oyonale.co.4" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Diagonal preconditioning for first order primal-dual algorithms in convex optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chambolle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV 2011)</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pov-Ray</surname></persName>
		</author>
		<ptr target="http://www.povray.org.3" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Nonlinear total variation based noise removal algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">I</forename><surname>Rudin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Fatemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physica D: Nonlinear Phenomena</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">1-4</biblScope>
			<biblScope unit="page" from="259" to="268" />
			<date type="published" when="1992-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Globally consistent depth labeling of 4D lightfields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wanner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Goldluecke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Spatial and angular variational super-resolution of 4d light fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wanner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Goldluecke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Datasets and benchmarks for densely sampled 4d light fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wanner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Meister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Goldluecke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Vision, Modelling and Visualization (VMV)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Globally consistent multi-label assignment on the ray space of 4d light fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wanner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Straehle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Goldluecke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">High performance imaging using large camera arrays</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wilburn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vaish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E.-V</forename><surname>Talvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Antunez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Barth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Horowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Levoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2005-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Computing the stereo matching cost with a convolutional neural network. CoRR, abs/1409</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zbontar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">4326</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
