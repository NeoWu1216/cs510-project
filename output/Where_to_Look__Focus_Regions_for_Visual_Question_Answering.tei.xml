<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Where To Look: Focus Regions for Visual Question Answering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">J</forename><surname>Shih</surname></persName>
							<email>kjshih2@illinois.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Singh</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
							<email>dhoiem@illinois.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Where To Look: Focus Regions for Visual Question Answering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:48+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>We present a method that learns to answer visual questions by selecting image regions relevant to the text-based query. Our method maps textual queries and visual features from various regions into a shared space where they are compared for relevance with an inner product. Our method exhibits significant improvements in answering questions such as "what color," where it is necessary to evaluate a specific location, and "what room," where it selectively identifies informative image regions. Our model is tested on the recently released VQA [1] dataset, which features free-form human-annotated questions and answers.</head></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Visual question answering (VQA) is the task of answering a natural language question about an image. VQA includes many challenges in language representation and grounding, recognition, common sense reasoning, and specialized tasks like counting and reading. In this paper, we focus on a key problem for VQA and other visual reasoning tasks: knowing where to look. Consider <ref type="figure">Figure 1</ref>. It's easy to answer "What color is the walk light?" if the light bulb is localized, while answering whether it's raining may be dealt with by identifying umbrellas, puddles, or cloudy skies. We want to learn where to look to answer questions supervised by only images and question/answer pairs. For example, if we have several training examples for "What time of day is it?" or similar questions, the system should learn what kind of answer is expected and where in the image it should base its response.</p><p>Learning where to look from question-image pairs has many challenges. Questions such as "What sport is this?" might be best answered using the full image. Other questions such as "What is on the sofa?" or "What color is the woman's shirt?" require focusing on particular regions. Still others such as "What does the sign say?" or "Are the Is it raining?</p><p>What color is the walk light? <ref type="figure">Figure 1</ref>. Our goal is to identify the correct answer for a natural language question, such as "What color is the walk light?" or "Is it raining?" We particularly focus on the problem of learning where to look. This is a challenging problem as it requires grounding language with vision and learning to recognize objects, use relations, and determine relevance. For example, whether it is raining may be determined by detecting the presence of puddles, gray skies, or umbrellas in the scene, whereas the color of the walk light requires focused attention on the light alone. The above figure shows example attention regions produced by our proposed model. man and woman dating?" require specialized knowledge or reasoning that we do not expect to achieve. The system needs to learn to recognize objects, infer spatial relations, determine relevance, and find correspondence between natural language and visual features. Our key idea is to learn a non-linear mapping of language and visual region features into a common latent space to determine relevance. The relevant regions are then used to score a specific questionanswer pairing. The latent embedding and the scoring function are learned jointly using a margin-based loss supervised solely by question-answer pairings. We perform ex-Do children like this object? What color are the dots on the handle of the utensil?</p><p>Is it raining? <ref type="figure">Figure 2</ref>. Examples from VQA <ref type="bibr" target="#b2">[1]</ref>. From left to right, the above examples require focused region information to pinpoint the dots, whole image information to determine the weather, and abstract knowledge regarding relationships between children and stuffed animals.</p><p>periments on the VQA dataset <ref type="bibr" target="#b2">[1]</ref> because it features openended language, with a wide variety of questions (see <ref type="bibr">Figure 2)</ref>. We focus on its multiple-choice format because its evaluation is much less ambiguous than open-ended answer verification.</p><p>We focus on learning where to look and provide useful baselines and analysis for the task as a whole. Our contributions are as follows:</p><p>• We present an image-region selection mechanism that learns to identify image regions relevant to questions.</p><p>• We present a learning framework for solving multiplechoice visual QA with a margin-based loss that significantly outperforms provided baselines from <ref type="bibr" target="#b2">[1]</ref>.</p><p>• We provide a detailed comparison with various baselines to highlight exactly when our region selection model improves VQA performance</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>Many recent works in tying text to images have explored the task of automated image captioning <ref type="bibr" target="#b11">[10,</ref><ref type="bibr" target="#b8">7,</ref><ref type="bibr" target="#b23">22,</ref><ref type="bibr" target="#b13">12,</ref><ref type="bibr" target="#b14">13,</ref><ref type="bibr" target="#b15">14,</ref><ref type="bibr" target="#b7">6,</ref><ref type="bibr" target="#b5">4,</ref><ref type="bibr" target="#b22">21]</ref>. While VQA can be considered as a type of directed captioning task, our work relates to some <ref type="bibr" target="#b23">[22,</ref><ref type="bibr" target="#b8">7]</ref> in that we learn to employ an attention mechanism for region focus, though our formulation makes determining region relevance a more explicit part of the learning process. In Fang et al. <ref type="bibr" target="#b8">[7]</ref>, words are detected in various portions of the image and combined together with a language model to generate captions. Similarly, Xu et al. <ref type="bibr" target="#b23">[22]</ref> uses a recurrent network model to detect salient objects and generate caption words one by one. Our model works in the opposite direction of these caption models at test time by determining the relevant image region given a textual query as input. This allows our model to determine whether a question-answer pair is a good match given evidence from the image.</p><p>Partly due to the difficulty of evaluating image captioning, several visual question answering datasets have been proposed along with applied approaches. We choose to experiment on VQA <ref type="bibr" target="#b2">[1]</ref> due to the open ended nature of its question and answer annotations. Questions are collected by asking annotators to pose a difficult question for a smart robot, and multiple answers are collected for each question. We experiment on the multiple-choice setting as its evaluation is less ambiguous than that of open-ended response evaluation. Most other visual question answering datasets <ref type="bibr" target="#b18">[17,</ref><ref type="bibr" target="#b24">23]</ref> are based on reformulating existing object annotations into questions, which provides an interesting visual task but limits the scope of visual and abstract knowledge required.</p><p>Our model is inspired by End-to-End Memory Networks <ref type="bibr" target="#b20">[19]</ref> proposed for answering questions based on a series of sentences. The regions in our model are analogous to the sentences in theirs, and, similarly to them, we learn an embedding to project question and potential features into a shared subspace to determine relevance with an inner product. Our method differs in many details such as the language model and more broadly in that we are answering questions based on an image, rather than a text document. Ba et al. <ref type="bibr" target="#b3">[2]</ref> also uses a similar architecture, but in a zero-shot learning framework to predict classifiers for novel categories. They project language and vision features into a shared subspace to perform similarity computations with inner products like us, though the score is used to guide the generation of object classifiers rather than to rank image regions.</p><p>Approaches in VQA tend to use recurrent networks to model language and predict answers <ref type="bibr" target="#b18">[17,</ref><ref type="bibr" target="#b2">1,</ref><ref type="bibr" target="#b24">23,</ref><ref type="bibr" target="#b16">15]</ref>, though simpler Bag-Of-Words (BOW) and averaging models have been shown to perform roughly as well if not better than sequence-based LSTM <ref type="bibr" target="#b18">[17,</ref><ref type="bibr" target="#b2">1]</ref>. Yu et al. <ref type="bibr" target="#b24">[23]</ref>, which proposes a Visual Madlibs dataset for fill-in-the-blank and question answering, focuses their approach on learning latent embeddings and finds normalized CCA on averaged word2vec representations <ref type="bibr" target="#b11">[10,</ref><ref type="bibr" target="#b17">16]</ref> to outperform recurrent networks for embedding. Similarly, we find a fixed-length averaged representation of word2vec vectors for language to be highly effective and much simpler to train, and our approach differs at a high level in our focus on learning where to look.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach</head><p>Our method learns to embed the textual question and the set of visual image regions into a latent space where the inner product yields a relevance weighting for each region. See <ref type="figure" target="#fig_0">Figure 3</ref> for an overview. The input is a question, potential answer, and image features from a set of automatically selected candidate regions. We encode the parsed question and answer using word2vec <ref type="bibr" target="#b17">[16]</ref> and a three-layer network. Visual features for each region are encoded using the top two layers (including the output layer) of a CNN trained on ImageNet <ref type="bibr" target="#b19">[18]</ref>. The language and vision features are then embedded and compared with a dot product, which is softmaxed to produce a per-region relevance weighting. Using these weights, a weighted average of concatenated vision  and language features is the input to a two-layer network that outputs a confidence for the answer candidate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">QA Objective</head><p>Our model is trained for the multiple choice task of the VQA dateset. For a given question and its corresponding choices, the objective of our network aims to maximize a margin between correct and incorrect choices in a structured-learning fashion. We achieve this by using a hinge loss over predicted confidences y.</p><p>In our setting, multiple answers could be acceptable to varying degrees, as correctness is determined by the consensus of 10 annotators. For example, most may say that the color of a scarf is "blue" while a few others say "purple". To take this into account, we scale the margin by the gap in number of annotators returning the specific answer:</p><formula xml:id="formula_0">L(y) = max ∀n =p (0, y n + (a p − a n ) − y p ).<label>(1)</label></formula><p>The above objective requires that the score of the correct answer (y p ) is at least some margin above the score of the highest-scoring incorrect answer (y n ) selected from the set of incorrect choices (n = p). For example, if <ref type="bibr" target="#b7">6</ref> 10 of the annotators answer p (a p = 0.6) and 2 annotators answer n (a n = 0.2), then y p should outscore y n by a margin of at least 0.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Region Selection Layer</head><p>Our region selection layer selectively combines incoming text features with image features from relevant regions of the image. To determine relevance, the layer first projects the image features and the text features into a shared Ndimensional space, after which an inner product is computed between each question-answer pair and all available regions.</p><p>Let V = ( v 1 , v 2 , ... v K ) be a collection of visual features extracted from K image regions and q be the feature representation of the question and candidate answer pair. The forward pass to compute the relevance weighting of the jth region is computed as follows:</p><formula xml:id="formula_1">g j =(A v j + b A ) ⊤ (B q + b B ) (2) s j = e gj k e g k<label>(3)</label></formula><p>Here, vectors b represent bias vectors for each affine projection. The inner product forces the model to compute region-question relevance (g j ) in a vector similarity fashion. Using softmax-normalization across 100 regions per image (K = 100) gives us a 100-dimensional vector s of normalized relevance weights. The vector s is then used to compute a weighted average across all region features. We first construct a languagevision feature representation for each region by defining d j as the concatenation of v j with q. Each feature vector is then projected with W and b W before computing the weighted average feature vector z.</p><formula xml:id="formula_2">z = j W d j + b W s j<label>(4)</label></formula><p>We also tried learning to predict a relevance score directly from concatenated vision and language features, rather than computing the dot product of the features in a latent embedded space. However, the resulting model appeared to learn a salient region weighting scheme that varied little with the language component. The inner-product based relevance was the only formulation we tried that successfully varies with different queries given the same image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Language Representation</head><p>We represent our words with 300-dimensional Google News dataset pre-trained word2vec vectors <ref type="bibr" target="#b17">[16]</ref> for their simplicity and compact representation. We are also motivated by the ability of vector-based language representations to encode similar words with similar vectors, which may aid answering open-ended questions. Using means of word2vec vectors, we construct fixed-length vectors for each question-answer pair, which our model then learns to score. In our results section, we show that our vectoraveraging language model noticeably outperforms a more complex LSTM-based model from <ref type="bibr" target="#b2">[1]</ref>, demonstrating that BOW-like models provide very effective and simple language representations for VQA tasks.</p><p>We first tried separately averaging vectors for each word with the question and answer, concatenating them to yield a 600-dimensional vector, but since the word2vec representation is not sparse, averaging several words may muddle the representation. We improve the representation using the Stanford Parser <ref type="bibr" target="#b6">[5]</ref> to bin the question into additional separate semantic bins. The bins are defined as follows: Bin 1 captures the type of question by averaging the word2vec representation of the first two words. For example, "How many" tends to require a numerical answer, while "Is there" requires a yes or no answer. Bin 2 contains the nominal subject to encode subject of question. Bin 3 contains the average of all other noun words. Bin 4 contains the average of all remaining words, excluding determiners such as "a," "the," and "few."</p><p>Each bin then contains a 300-dimensional representation, which are concatenated with a bin for the words in the candidate answer to yield a 1500-dimensional question/answer representation. <ref type="figure" target="#fig_2">Figure 4</ref> shows examples of binning for the parsed question. This representation separates out important components of a variable-length question while maintaining a fixed-length representation that simplifies the network architecture.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Image Features</head><p>The image features from 100 rectangular regions are fed directly into the region-selection layer from a pre-trained network. We first select candidate regions by extracting the top-ranked 99 Edge Boxes <ref type="bibr" target="#b26">[25]</ref> from the image after performing non-maximum suppression with a 0.2 intersection over union overlap threshold. We found this aggressive thresholding to be important for selecting smaller regions that may be important for some questions, as the top-ranked regions tend to be highly overlapping large regions. Finally, a whole-image region is also added to ensure that the model at least has the spatial support of the full frame if necessary, bringing the total number of candidate regions to 100 per image. While we have not experimented with the number of regions, it is possible that the improved recall from additional regions may improve performance.</p><p>We extract features using the VGG-s network <ref type="bibr" target="#b4">[3]</ref>, concatenating the output from the last hidden layer (4096 dimensions) and the pre-softmax layer (1000 dimensions). The pre-softmax classification layer was included to provide a more direct signal for objects from the Imagenet <ref type="bibr" target="#b19">[18]</ref> classification task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Training</head><p>Our network architecture is a multi-layer network as seen in <ref type="figure" target="#fig_0">Figure 3</ref>, implemented in MatConvNet <ref type="bibr" target="#b21">[20]</ref>. Our fully connected layers are initialized with Xavier initialization <ref type="bibr" target="#b9">[8]</ref> and separated with a batch-normalization <ref type="bibr" target="#b12">[11]</ref> and ReLU layer <ref type="bibr" target="#b10">[9]</ref>. The word2vec text features are fed into the network's input layer, whereas the image region features feed in through the region selection layer.</p><p>Our network sizes are set as follows. The 1500 dimensional language features first pass through 3 fully connected layers with output dimensions 2048, 1500, and 1024 respectively. The embedded language features are then passed through the region selection layer to be combined with the vision features. Inside the region selection layer, projections A and B project both vision and language representations down to 900 dimensions before computing their inner product. The exiting feature representation passes through W with an output dimension of 2048. then finally through two more fully connected layers with output dimensions of 900 and 1 where the output scalar is the question-answer pair score.</p><p>The training was especially sensitive to the initialization of the region-selection layer. The magnitude of the projection matrices A, B and W are initialized to 0.001 times the standard normal distribution. We found that low initial values were important to prevent the softmax in selection from spiking too early and to prevent the higher-dimensional vision component from dominating early in the training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We evaluate the effects of our region-selection layer on the multiple-choice format of the MS COCO Visual Question Answering (VQA) dataset <ref type="bibr" target="#b2">[1]</ref>. This dataset contains 82,783 images for training, 40,504 for validation, and 81,434 for testing. Each image has 3 corresponding questions with recorded free-response answers from 10 annotators. Any response that comes from at least 3 annotators is considered correct. We evaluate on multiple choice task because its evaluation is much less ambiguous than the openended response task, though our method could be applied to the latter by treating the most common or likely M responses as a large M -way multiple choice task. We perform detailed baseline comparisons on the validation set and report final scores on the test set. We evaluate and analyze how much our regionweighting improves accuracy compared to using the whole image or only language (Tables 1, 2, 3) and show examples in <ref type="figure" target="#fig_5">Figure 8</ref>. We also perform a simple evaluation on a subset of images showing that relevant regions tend to have higher than average weights ( <ref type="figure" target="#fig_3">Figure 6</ref>). We also show the advantage of our language model over other schemes <ref type="table">(Table 4</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Comparisons between region, image, and language-only models</head><p>We compare our region selection model with several baseline methods, described below. All models use a 10% held-out from train for model selection. Word-only: We train a network to score each answer purely from the language representation. This provides a baseline to demonstrate improvement due to image features, rather than just good guesses. Word+Whole image: We concatenate CNN features computed over the entire image with the language features and score them using a three-layer neural network, essentially replacing the region-selection layer with features computed over the whole image. Word+Uniform averaged region features: To test that region weighting is important, we also try uniformly averaging features across all regions as the image representation and train as above. Word+Salient region weighting: We include a baseline where each region's weight is computed independently of the language component. We replace the inner product computation between vision and language features with an affine transformation that projects just the vision features down to a scalar, followed by a softmax over all regions. The layer's output is the weighted combination of concatenated vision and language features as before, but using the salient weights. <ref type="table">Table 1</ref> shows the comparison of overall accuracy on the validation set, where it is clear our proposed model performs best. The salient weighting baseline alone showed noticeable improvement over the simpler whole image and averaging baselines. We noticed it performed similarly to the whole image baseline on localization dependent categories such as "what color" due to its inability localize on mentioned subjects, but performed similarly to the proposed model in scene and sport recognition questions due to its ability to highlight discriminative regions. We also include the best-performing LSTM question+image model on val from the authors of <ref type="bibr" target="#b2">[1]</ref>. This model significantly underperforms even our much simpler baselines, which could be partly because the model was designed for open-ended answering and adapted for multiple choice. We evaluate our model on the test-dev and test-standard partitions in order to compare with additional models from <ref type="bibr" target="#b2">[1]</ref>. In <ref type="table">Table 2</ref>, we include comparisons to the bestperforming question+image based models from the VQA dataset paper <ref type="bibr" target="#b2">[1]</ref>, as well as a competitive implementation of the whole image+language baseline from Zhou et al. <ref type="bibr" target="#b25">[24]</ref>. Our model was retrained on train+val data using the same held-out set as before for model selection. Our model significantly outperforms the baselines in the "others" category, which contains the majority of the question types that our model excels at. <ref type="table">Table 3</ref> offers a more detailed performance summary across various question types, with discussion in the caption. <ref type="figure" target="#fig_5">Figure 8</ref> shows a qualitative comparison of results, highlighting some of the strengths and remaining problems of our approach. These visualizations are created by soft masking the image with a mask created by summing the weights of each region and normalizing to a max of one. A small blurring filter is applied to remove distracting artifacts that occur from multiple overlapping rectangles. On color questions, localization of the mentioned object tends to be very good, which leads to more accurate answers. On questions such as "How many birds are in the sky?" the system cannot produce the correct answer but does focus on the relevant objects. The third row shows examples of how different questions lead to different focus regions. Notice how the model identifies the room as a bathroom in the third row by focusing on the toilet, and, when confirming that "kite" is the answer to "What is the woman flying over the beach?" focuses on the kite, not the woman or the beach.</p><p>In <ref type="figure">Figure 5</ref>, we show additional qualitative examples of how the region selection varies with question-answer pairs. In the first row, we see the model does more than simply match answer choices to regions. While it does find a matching green region, the corresponding confidence is still low. In addition, we see that irrelevant answer choices tend to have less-focused attention weightings. For example, the kitchen recognition question has most of its weighting on  <ref type="figure">Figure 5</ref>. Comparison of attention regions generated by various question-answer pairings for the same question. Each visualization is labeled with its corresponding answer choice and returned confidence. We show the highlighted regions for the top multiple choice answers and some unrelated ones. Notice that in the first example, while the model clearly identified a green region within the image to match the "green" option, the corresponding confidence was significantly lower than that of the correct options, showing that the model does more than just match answer choices with image regions.  what appears to be a discriminative kitchen patch for the correct choice, whereas the "blue" choice appears to have a more evenly spread out weighting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Region Evaluation</head><p>We set up an informal experiment to evaluate the consistency of our region weightings with respect to various types Pixel weights are normalized by the maximum pixel weight. Often much more weight is assigned to the relevant region and very rarely much less.</p><p>of questions. We manually annotated 205 images from the validation set with bounding boxes considered relevant to answering the corresponding question. An example of the annotation and predicted weights can be seen in <ref type="figure" target="#fig_3">Figure 6</ref>.</p><p>To evaluate, we compare the average pixel weighting within the annotated boxes with the average across all pixels. Pixel weighting was determined by cumulatively adding each region's selection weight to each of its constituent pixels. We observe that the the mean weighting within the annotated regions was greater than the global average in 148 of the instances (72.2%), often much greater and rarely much smaller <ref type="figure" target="#fig_4">(Figure 7)</ref>. We further investigate the effectiveness of ranking by our region scores in <ref type="figure" target="#fig_6">Figure 9</ref>    <ref type="table">Table 3</ref>. Accuracies by type of question on the validation set. Percent accuracy is shown for each subset for our region-based approach, classification using the whole image and question/answer text, and classification based only on text. We also show the frequency of each question type. Since there are 121,512 questions used for testing, there are hundreds or thousands of examples of even the rarest question types, so small gains are statistically meaningful. Overall, our region selection scheme outperforms use of whole images by 2% and text-only features by 5%. There is substantial improvement in particular types of questions. For example, questions such as "What is the woman holding?" are answered correctly 70% of the time vs. 67% for whole image and only 57% for text. "What color," "What room," and "What sport" also benefit greatly from use of image features and further from region weighting. Question types that have yes/no answers tend not to improve, in part because the prior is so reliable. E.g., someone is unlikely to ask "Does the girl have a lollipop?" if she is not so endowed. So "no" answers are unlikely and also more difficult to verify. We also note that reading questions ("What does the sign say?") and counting questions ("How many sheep?") are not greatly improved by visual features in our system because they require specialized processes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Accuracy (%) Q+A (2-bin) 51.87 parsed(Q)+A (5-bin) 53.98 <ref type="table">Table 4</ref>. Language model comparison. The 2-bin model is the concatenation of the question and answer averages. The parsed model uses the Stanford dependency parser to further split the question into 4 bins.</p><p>weighted regions (retained weights are L1 normalized) or only the Kth (1-hot weighting of Kth region). We observe that performance on color-type questions does not improve significantly beyond the first 10 regions, and that perfor- mance drops off sharply in the Kth-only experiment. This provides further evidence that our model is able to score relevant regions above the rest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Language Model</head><p>We also compare our parsed and binned language model with a simple two-binned model (one bin averages word2vec of question words; the other averages answer words) to justify our more complex representation. Each model is trained on the train set and evaluated on the validation set of the VQA real-images subset. The comparison results are shown in <ref type="table">Table 4</ref> and depict a significant performance improvement using the parsing.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 .</head><label>3</label><figDesc>Overview of our network for the example question-answer pairing: "What color is the fire hydrant? Yellow." Question and answer representations are concatenated, fed through the network, then combined with selectively weighted image region features to produce a score.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>How many birds are in the photoIs there a cat on the car | Is there | cat | car | on | | How many | birds | photo | are in | What animal is in the picture | What animal | animal | picture | is in |</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Example parse-based binning of questions. Each bin is represented with the average of the word2vec vectors of its members. Empty bins are represented with a zero-vector.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 .</head><label>6</label><figDesc>Example image with corresponding region weighting. Red boxes correspond to manual annotation of regions relevant to the question: "Are the people real?"</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 .</head><label>7</label><figDesc>Histogram of differences between mean pixel weight within (µ in ) annotated regions and across the whole image (µ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 .</head><label>8</label><figDesc>by retaining only the top K Comparison of qualitative results from Val. The larger image shows the selection weights overlaid on the original image (smaller). L: Word only model; I: Word+Whole Image; R: Region Selection. The scores shown are ground truth confidence -top incorrect. Note that the first row shows successful examples in which tight region localization allowed for an accurate color detection. In the third row, we show examples of how weighting varies on the same image due to differing language components.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 .</head><label>9</label><figDesc>Plot of color-based question accuracy with varying number of regions sampled at every 10. The experiment was run on a 10% held-out set on train. We look at using the weighted average of only the top K scoring regions, as well as only the Kth. We include the whole image baseline's accuracy in this category for comparison.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head></head><label></label><figDesc>Table 2. Accuracy comparison on VQA test sets.</figDesc><table>Model 
All 
Y/N 
Num. Others 
test-dev 
LSTM Q+I [1] 
57.17 78.95 35.80 43.41 
Q+I [1] 
58.97 75.97 34.35 50.33 
iBOWIMG [24] 
61.68 76.68 37.05 54.44 
Word+Region Sel. 62.44 77.62 34.28 55.84 
test-standard 
iBOWIMG [24] 
61.97 76.86 37.30 54.60 
Word+Region Sel. 62.43 77.18 33.52 56.09 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We presented a model that learns to select regions from the image to solve visual question answering problems. Our model outperforms all baselines and existing work on the MS COCO VQA multiple choice task <ref type="bibr" target="#b2">[1]</ref>, with substantial gains for some questions such as identifying object colors that require focusing on particular regions. One direction for future work is to learn to perform specialized tasks such as counting or reading. Other directions are to incorporate and adapt pre-trained models for object and attribute detectors or geometric reasoning, or to use outside knowledge sources to help learn what is relevant to answer difficult questions. We are also interested in learning where to look to find small objects and to recognize activities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledgements</head><p>This work is supported by NSF CAREER award 1053768, NSF Award IIS-1029035, and ONR MURI Awards N00014-10-1-0934 and N00014-16-1-2007. We gratefully acknowledge the support of NVIDIA Corporation with the donation of the Tesla K40 GPUs used for this research.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">7) I: gray (-0.6) R: white (0.1) Ans: white! What animal is that?! L: red (1.0) I: red (0.3) R: red (1.7) Ans: red! L: 1 (-0.7) I: several (-0.1) R: 9600 (-0.2) Ans: 5! What is the woman flying over the beach?! L: goose (-1.1) I: kite (1.4) R: kite (5.3) Ans: kite! How many birds are in the sky?</title>
	</analytic>
	<monogr>
		<title level="m">! L: gray (-0.2) I: gray (-0.4) R: yellow (0.4) Ans: yellow! What color is the fence?! L: black</title>
		<imprint/>
	</monogr>
	<note>! L: red (-0.3) I: red (-0.3) R: green (1.1) Ans: green! What color is the walk light?! How many people?! What is on the</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">1) I: bathroom (2.6) R: bathroom (6.8) Ans: bathroom! L: no(3.6) I: no (3.1) R: no (5.1) Ans: no! What is the man doing?</title>
	</analytic>
	<monogr>
		<title level="m">! L: surfing (2.5) I: blue (3.7) R: surfing (9.7) Ans: surfing! Is there a lot of pigeons in the picture?! L: on shelf (-1.4) I: on shelf (-0.7) R: on tub</title>
		<imprint/>
	</monogr>
	<note>L: airplane(-0.9) I: snow (2.9) R: snow (3.7) Ans: snow! What room is this?! Is the faucet turned on?! L: bathroom(0.. 0.1) Ans: windowsill! Where is the shampoo?! L: yes (1.5) I: yes (0.5) R: yes (1.0) Ans: yes!</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Vqa: Visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Predicting deep zero-shot convolutional neural networks using textual descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Return of the devil in the details: Delving deep into convolutional nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chatfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Mind&apos;s eye: A recurrent visual representation for image caption generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Generating typed dependency parses from phrase structure parses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-C</forename><surname>De Marneffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Maccartney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of LREC</title>
		<meeting>LREC</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="449" to="454" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.4389</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Platt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.4952</idno>
		<title level="m">From captions to visual concepts and back</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on artificial intelligence and statistics</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep sparse rectifier neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="315" to="323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Improving image-sentence embeddings using large weakly annotated photo collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hodosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hockenmaier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2014</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="529" to="545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep visual-semantic alignments for generating image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep captioning with multimodal recurrent neural networks (m-rnn)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Explain images with multimodal recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Deep Learning Workshop</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Ask your neurons: A neural-based approach to answering questions about images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Mateusz Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Exploring models and data for image question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.02074v3</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2015-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Weakly supervised memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<idno>abs/1503.08895</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Matconvnet -convolutional neural networks for matlab</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lenc</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Show and tell: A neural image caption generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03044</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.00278</idno>
		<title level="m">Visual madlibs: Fill in the blank image generation and question answering</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.02167</idno>
		<title level="m">Simple baseline for visual question answering</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Edge boxes: Locating object proposals from edges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2014</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="391" to="405" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
