<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Shortlist Selection with Residual-Aware Distance Estimator for K-Nearest Neighbor Search</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jae-Pil</forename><surname>Heo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Adobe Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Adobe Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Adobe Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Brandt</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Adobe Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sung-Eui</forename><surname>Yoon</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Adobe Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Shortlist Selection with Residual-Aware Distance Estimator for K-Nearest Neighbor Search</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:45+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we introduce a novel shortlist computation algorithm for approximate, high-dimensional nearest neighbor search. Our method relies on a novel distance estimator: the residual-aware distance estimator, that accounts for the residual distances of data points to their respective quantized centroids, and uses it for accurate shortlist computation. Furthermore, we perform the residualaware distance estimation with little additional memory and computational cost through simple pre-computation methods for inverted index and multi-index schemes. Because it modifies the initial shortlist collection phase, our new algorithm is applicable to most inverted indexing methods that use vector quantization. We have tested the proposed method with the inverted index and multi-index on a diverse set of benchmarks including up to one billion data points with varying dimensions, and found that our method robustly improves the accuracy of shortlists (up to 127% relatively higher) over the state-of-the-art techniques with a comparable or even faster computational cost.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Approximate K-nearest neighbor (ANN) search is a fundamental problem in computer science, which has many practical applications, especially in many computer vision tasks such as image retrieval, feature matching, tracking, object recognition, etc. Conventional ANN techniques can be inefficient in both speed and memory, when the size of the database is large and the dimensionality of the feature space is high, as is the case for large-scale image retrieval using holistic descriptors.</p><p>In order to achieve high scalability, recent search methods typically adopt an inverted index-based representation with a compact data representation to perform large-scale retrieval in two steps: candidate retrieval and candidate reranking. These approaches first collect candidates for Knearest neighbors called a shortlist by quantized indices, and then reorder them by exhaustive distance computations with more accurate distance approximations. Accu-rate shortlist retrieval is a crucial first step for large-scale retrieval systems as it determines the upper-bound performance for the K-nearest neighbor search in such two-step search process.</p><p>Previous methods have attempted to introduce better quantization models (e.g., product quantization <ref type="bibr" target="#b13">[14]</ref>) and inverted indexing schemes (e.g., the inverted index and inverted multi-index <ref type="bibr" target="#b0">[1]</ref>). These approaches identify inverted lists whose centroids are close to the query, and include all the data points in those inverted lists to the shortlist. While these approaches are very efficient for collecting shortlists, they do not consider fine-grained positions of those data points, and thus the computed shortlist may still contain many data points that are too far away from the query, and close neighbors could be missed in the shortlist due to the quantization error.</p><p>Our contributions. In this paper, we introduce a novel shortlist computation algorithm based on the inverted lists for high-dimensional, approximate K-nearest neighbor search. We first propose a novel distance estimator, residual-aware distance estimator, between a query and data points by considering the residual distances to the quantized centroids (Sec. 4.1). We also propose effective precomputation methods of using our distance estimator for runtime queries with minor memory and computation costs with the inverted index (Sec. 4.2) and multi-index (Sec. 4.3). We have extensively evaluated our method on a diverse set of large-scale benchmarks consisting of up to one billion data with SIFT, GIST, VLAD, and CNN features. We have found that our method significantly improves the accuracy of shortlists over the state-of-the-art techniques with a comparable or even faster computational performance (Sec. 5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>There have been many tree-based techniques for ANN search, since those hierarchical structures provide a logarithmic search cost. Notable approaches include KDtree <ref type="bibr" target="#b4">[5]</ref>, randomized KD-tree forests <ref type="bibr" target="#b23">[24]</ref>, HKM (Hierarchical K-means tree) <ref type="bibr" target="#b20">[21]</ref>, etc. Unfortunately, those tree-based methods provide less effective indexing for largescale high-dimensional data.</p><p>Designing inverted indexing structures based on vector quantization is a popular alternative to the tree-based approaches. In such methods, the index for a data point is defined by its cluster centroid in high-dimensional data, and the data point is assigned to the nearest cluster according to the distance to the centroid. JÃ©gou et al. <ref type="bibr" target="#b13">[14]</ref> have applied vector quantization to the approximate nearest neighbor search problem. Inverted multi-index <ref type="bibr" target="#b0">[1]</ref> has been proposed to use product quantization <ref type="bibr" target="#b13">[14]</ref> to generate the index. The technique can acquire a large number of clusters without incurring a high computational overhead in indexing and search. Ge et al. <ref type="bibr" target="#b6">[7]</ref> have optimized the inverted multi-index technique by reducing the quantization error based on their prior optimization framework <ref type="bibr" target="#b5">[6]</ref>, and they mostly used two dimensional index using two subspaces. Iwamura et al. <ref type="bibr" target="#b12">[13]</ref> have proposed a bucket distance hashing scheme that uses higher-dimensional multi-index to increase the number of indices to cover the database size, and a shortlist retrieval method specialized to their indexing method. Xia et al. <ref type="bibr" target="#b26">[27]</ref> have proposed the joint inverted index that defines multiple sets of centroids for higher accuracy.</p><p>At a high level, the aforementioned methods based on vector quantization have been mostly focused on reducing the quantization error. In other words, they have designed more accurate vector quantization methods by increasing the number of centroids or optimizing the subspaces. While these prior techniques show high accuracy, they are mainly designed and evaluated for one nearest neighbor search, i.e., 1-NN. In contrast, our goal is to develop an accurate shortlist retrieval method for K-nearest neighbor search, where K can be large (e.g. 100, and 1000), which is useful for large-scale visual search in practice. Furthermore, these prior works are mostly evaluated on SIFT <ref type="bibr" target="#b19">[20]</ref> and GIST <ref type="bibr" target="#b22">[23]</ref> descriptors, but are not evaluated against very high-dimensional (e.g., 8K) and recent image descriptors such as VLAD <ref type="bibr" target="#b14">[15]</ref> or deep convolutional neural network (CNN) features <ref type="bibr" target="#b17">[18]</ref>.</p><p>Once a shortlist is selected, the data in the shortlist is re-ranked based on exhaustive distance computations. It is impractical to use raw vectors of the data due to the consequent high computational and memory cost. Hence there have been a lot of techniques to represent data as compact codes. Those compact data representations provide benefits to both of computational and memory costs. There are two popular approaches, hashing and product quantization. Examples of hashing techniques include LSH <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b18">19]</ref>, spectral hashing <ref type="bibr" target="#b25">[26]</ref>, ITQ <ref type="bibr" target="#b7">[8]</ref>, and etc. <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b9">10]</ref>. Examples of quantization-based methods include PQ <ref type="bibr" target="#b13">[14]</ref>, transform coding <ref type="bibr" target="#b1">[2]</ref>, OPQ <ref type="bibr" target="#b5">[6]</ref>, and etc. <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b21">22]</ref>. Regardless of distance computation methods used in these techniques, the performance of overall retrieval systems is highly dependent on the accuracy of the shortlist computed by indexing schemes. In this paper, we propose a shortlist method that can be used with different indexing schemes to improve the overall accuracy without incurring a high computational overhead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Background</head><p>We explain the background of computing shortlists with an inverted indexing scheme.</p><p>Suppose that an inverted file consists of M inverted lists, L 1 , ..., L M . Each inverted list L i has its corresponding centroid c i â R D . In general, the centroids are computed by the k-means clustering algorithm <ref type="bibr" target="#b14">[15]</ref>. Given a database X = {x 1 , x 2 , ..., x N }, each item x â X is assigned to an inverted list based on the nearest centroid index computed by a vector quantizer q(x):</p><formula xml:id="formula_0">q(x) = argmin ci d(x, c i ),</formula><p>where d(Â·, Â·) is the Euclidean distance between two vectors. Each inverted list L i contains data points whose nearest centroid is c i :</p><formula xml:id="formula_1">L i = {x|q(x) = c i , x â X} = {x i 1 , ..., x i ni }.</formula><p>When processing a query y, a shortlist S is first identified to be a set of candidate search results, whose size is T . To collect T data items from the inverted file, inverted lists are traversed in the order of increasing distance to the centroids d(y, c i ). Once the shortlist S is prepared, the items in S are re-ranked by exhaustive distance evaluations with either the original data or their compact codes. The problem that we address in this paper is identifying an optimal shortlist S â X, which maximizes the recall rate for retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Our Approach</head><p>In this section, we first explain our distance estimator, followed by its applications to the inverted index and multiindex schemes for handling large-scale search problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Residual-Aware Distance Estimator</head><p>In the conventional approach, the residual distance from the data point x to its corresponding centroid q(x) is omitted. In this paper, we propose a more accurate distance estimator by taking the residual distance into account. We denote this residual distance as r x :</p><formula xml:id="formula_2">r x = d(x, q(x)).</formula><p>Similarly, we denote the distance between a query y and the quantized data q(x) as h y,x : h y,x = d(y, q(x)).</p><p>The exact squared distance between a query y and a data item x can be written as the following according to the law of cosines:</p><formula xml:id="formula_3">d(y, x) 2 = h 2 y,x + r 2 x â 2h y,x r x cos Î¸ = h 2 y,x + r 2 x (1 â 2h y,x r x cos Î¸),<label>(1)</label></formula><p>where Î¸ is the angle between two vectors of y â q(x) and</p><p>x â q(x).</p><p>While the term 1 â 2hy,x rx cos Î¸ depends on specific x and y, we approximate the exact distance by treating this term as a constant Î± K . The reason is to constrain the distance estimator to have a factorized representation in terms of h 2 y,x depending on y, and r 2</p><p>x , which is independent from y, for efficiency. This results in our residual-aware distance estimator:d</p><formula xml:id="formula_4">(y, x) 2 = h 2 y,x + Î± K r 2 x ,<label>(2)</label></formula><p>where Î± K is a constant value within the range [0, 1]. Shortlists computed by the residual-aware distance estimator (Eq. 2) with Î± K = 0 is identical to those of the conventional approach. Note that two random vectors are highly likely to be orthogonal or near-orthogonal in a high-dimensional space <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b10">11]</ref> and the orthogonality holds better with increasing dimensionality. As a result, we use 1 as the default value of Î± K instead of zero. The distance estimator with Î± K = 1, however, is likely to overestimate distances, when two vectors of y â q(x) and x â q(x) are not perfectly orthogonal.</p><p>To mitigate the overestimation problem of our distance estimator, we train Î± K depending on the target number of true neighbors, K, that we aim to search for. For our training process, we first randomly choose N s data {s 1 , ..., s Ns } from the database X, and compute K-nearest neighbors for each sample, s i . Let us denote n i j as the j th nearest neighbor of the training sample s i . We could compute an average Î± K from this set of nearest neighbor data, but it can result in over-fitting. To avoid the over-fitting issue, we also randomly select another K(=the target number of true neighbors) different data points for each s i , denoted by {m i 1 , ..., m i K }. We then train Î± K value with a simple equation that computes the average value from those two different data sets:</p><formula xml:id="formula_5">Î± K = 1 2KN s Ns i=1 ( K j=1 f (s i , n i j ) + K j=1 f (s i , m i j )),<label>(3)</label></formula><p>where</p><formula xml:id="formula_6">f (y, x) = 1 â 2h y,x r x cos Î¸ = d(y, x) 2 â h 2 y,x r 2 x .</formula><p>While training Î± K values, we ignore any sample that is the cluster centroid itself (i.e., x = q(x)), to avoid the zero denominator. Since limited numbers of K are commonly used such as K = 1, 50, 100, or 1000 in practice, we can pre-compute Î± K for a discrete set of K parameters. When we need to use a new K value that is untrained, we can simply use the default value 1 for Î± K or linearly interpolated Î± K based on precomputed neighboring parameters. In practice, using Î± K values computed by this training process shows up to 20% higher accuracy over the default value Î± K = 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Inverted Index</head><p>We first explain our method with the inverted index scheme. We introduce a simple lookup table precomputation method that enables an effective and efficient way of our distance estimator for accurate shortlist computation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Lookup Table Precomputation</head><p>In order to compute a shortlist according to our distance estimator (Eq. 2), we need to have the distances from data points to their corresponding cluster centroids, e.g., r x = d(x, q(x)) and h y,x = d(y, q(x)) in Eq. 2, in runtime. Unfortunately, computing such distances on-the-fly is impractical due to its computational cost and memory overhead. Furthermore, the data points are encoded into compact codes so we cannot even access the original values of those data.</p><p>To overcome these issues, we propose an efficient lookup table-based method. Our distance estimator (Eq. 2) consists of two decoupled variables h y,x and r x . Since r x is independent from a query, we can precompute those values and retrieve them in run-time. However, storing those values requires an additional memory overhead, i.e., 4 bytes for each item. Instead, we propose to use a lookup table that only contains the number of data items whose r x belongs to a certain range, which yield a negligible storage/memory overhead. In the following, we explain the details of the look-up table construction method.</p><p>During the inverted file construction stage, we first prepare data points, x i j , for each inverted list, L i , and then sort them in the non-decreasing way according to the distance between x i j and its centroid, c i . Next, we compute the global minimal and maximal squared distances in the database as the following:</p><formula xml:id="formula_7">R m = min d(x, q(x)) 2 , R M = max d(x, q(x)) 2 .</formula><p>We then uniformly partition the range [R m , R M ] of those squared distances into Z intervals, each of which has a âR span: âR = (R M â R m )/Z. We denote the j-th boundary value of Z different intervals to be R j , i.e., R j = R m + jâR.</p><p>We finally define each entry of a lookup table, W (i, j), to memorize the number of data points in the inverted list L i , whose squared distances to the centroid are less than R j as follows: where | Â· | is the cardinality of the given set. <ref type="figure" target="#fig_0">Fig. 1(a)</ref> shows an example lookup table computed by our method. The lookup table W has O(M Z) memory complexity, where M and Z are independent of the dataset size. As a result, its memory overhead can be set to be much smaller than the size of the database, while providing high performance improvement over the on-the-fly computation. For example, the overhead of the lookup table takes 64 MB for our tested benchmark consisting of 1 billion data, when we use M = 2 14 and Z = 1024.</p><formula xml:id="formula_8">W (i, j) = |{x|d(x, c i ) 2 &lt; R j , x â L i }|,<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Shortlist Computation</head><p>We precomputed the query independent term r 2</p><p>x of our residual-aware distance estimator (Eq. 2 in the lookup table W . The distances h 2 y,x between the query y and centroids q(x), the query-dependent term of our distance estimator, can only be computed during the runtime. For simplicity, we introduce h 2 i to denote the squared distance between a query y and c i , i.e., h 2 i = d(y, c i ) 2 . The key idea of our new shortlist computation method is to consider all the inverted list jointly by aligning them with respect to the estimated distances to the query. Before presenting our shortlist computation method, we introduce a new function, w(y, i, t), which counts the number of data points in the inverted list L i whose estimated distance from the query y is less than t, as the following:</p><formula xml:id="formula_9">w(y, i, t) = |{x|d(y, x) 2 &lt; t, x â L i }| = |{x|h 2 y,x + Î± K r 2 x &lt; t, x â L i }|(âµ Eq. 2) = |{x|h 2 i + Î± K r 2 x &lt; t, x â L i }|(âµ q(x) = c i ) = |{x|Î± K r 2 x &lt; t â h 2 i , x â L i }| = |{x|r 2 x &lt; (t â h 2 i )/Î± K , x â L i }|.<label>(5)</label></formula><p>Note that Eq. 5 has the same form as Eq. 4, when (t â h 2 i )/Î± K is replaced with R j . To utilize the lookup table W (i, j), we need to compute the index j, and we can approximate w(y, i, t) as follows:</p><formula xml:id="formula_10">w(y, i, t) = W (i, (t â h 2 i )/Î± K â R m âR ).<label>(6)</label></formula><p>Based on this equation, we can compute the number of data within a particular distance t by considering r 2 x and h 2 i terms jointly through the lookup table W . The total number of data points in all the inverted lists that are within the distance t can be computed by K i=1 w(y, i, t). For example, in <ref type="figure" target="#fig_0">Fig. 1</ref>(b) the number of data points which are within the distance t 1 is computed by summing numbers on the red line.</p><p>When a query y is given at runtime, we first compute and store h 2 i for all the c i . We then estimate the optimal threshold t of the estimated distance that meets a given shortlist size T . Since elements in each row of W (i, Â·) are arranged in the non-decreasing order, the column-wise sum in the table W has also the non-decreasing order. Therefore, w(y, i, t) is also non-decreasing as we increase the value of t. Thanks to this simple property, we can use binary search to find an appropriate threshold t efficiently. The binary search for t is performed within the range of</p><formula xml:id="formula_11">[min h 2 i + Î± K R m , max h 2 i + Î± K R M ].</formula><p>We stop the search when we found the largest t value that satisfies the inequality:</p><formula xml:id="formula_12">T â¤ K i=1 w(y, i, t).</formula><p>The final shortlist is constructed by collecting w(y, i, t) data points from an inverted list L i that have smaller estimated distances than the threshold found by the binary search <ref type="figure" target="#fig_0">(Fig. 1)</ref>.</p><p>Our method performs a column-wise sum on the lookup table W and the binary search among Z intervals. As a result, our cost is O(M log Z). In practice Z = 1024 provides a good balance between the speed and accuracy (Sec. 5.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Inverted Multi-Index</head><p>The inverted multi-index (IMI) <ref type="bibr" target="#b0">[1]</ref> supports exponentially increasing number of inverted lists. Although we can directly use the method described in Sec. 4.2 for the IMI, it can become inefficient in terms of memory and computation cost due to the large number of inverted lists. Thus, we propose a shortlist selection method tailored to the IMI with our residual-aware distance estimator. Traverse Order , , , : . , , , : . , , , : . , , , : . , , , : . â¦.. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-Index Distance</head><formula xml:id="formula_13">(x k ) = argmin c k i d(x k , c k i ), r x,k = d(x k , q k (x k )), and h k,i = d(y k , c k i )</formula><p>. For our method built on top of IMI, we partition i th cluster in the k th subspace, X k i = {x k |q k (x k ) = c k i }, according to the residual distances r x,k . Each cluster X k i is then decomposed into P partitions X k i,1 , ... , X k i,P with residual distance boundaries, R k i,j :</p><formula xml:id="formula_14">X k i,j = {x k |R k i,jâ1 â¤ r k x,i &lt; R k i,j , x k â X k i }.</formula><p>The residual distance boundaries R k i,j are determined to divide the data equally into P partitions. Note that R k i,0 and R k i,P are set to the min and max values in their subspaces, respectively. We use a pair of a cluster ID i and a distance ID j, (i, j), as the index of a subspace.</p><p>An inverted list in our multi-index is then defined by two indices:</p><formula xml:id="formula_15">L[(i 1 , j 1 ), (i 2 , j 2 )] = {x|x 1 â X 1 i1,j1 , x 2 â X 2 i2,j2 },<label>(7)</label></formula><p>where, i 1 and i 2 are cluster IDs in 1 st and 2 nd subspaces, respectively. Also, j 1 and j 2 are the quantized distance IDs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Shortlist Computation</head><p>Our residual-aware distance estimator Eq. 2 is extended to the inverted multi-index as the following: d(y, x) 2 = d(y 1 , q 1 (x 1 )) 2 + Î± K,1 r 2</p><p>x,1 +d(y 2 , q 2 (x 2 )) 2 + Î± K,2 r 2</p><p>x,2 ,</p><p>where, Î± K,k is computed in the k th subspace, in the same manner described in Sec. 4.1.</p><p>Storing r 2 x,k values or computing them in the querying stage is impractical, as the case for the inverted list. We could also use our lookup table method for the multi-index. Nonetheless, this approach might generate a scalability issue for the multi-index, since our search space with the lookup table grows exponentially as we have more subspaces. Instead, we propose to a representative residual distance for each partition for the multi-index. A representative residual distance,r k,i,j , for an index (i, j) in the k th subspace regarding to a partition X k i,j is defined by the average of residual distances of data within X k i,j as the following:</p><formula xml:id="formula_17">r k,i,j = r x,k |X k i,j | , for x k â X k i,j .<label>(9)</label></formula><p>We then derive a residual-aware distance estimator for our multi-index scheme for a query y and a data point x â L[(i 1 , j 1 ), (i 2 , j 2 )] based on Eq. 8 and Eq. 9:</p><formula xml:id="formula_18">d(y, x) 2 = h 2 1,i1 + Î± K,1r 2 1,i1,j1 d2 1,i 1 ,j 1 + h 2 2,i2 + Î± K,2r 2 2,i2,j2 d2 2,i 2 ,j 2 .<label>(10)</label></formula><p>The squared distance between y k and an index (i, j) in k th subspace is denoted byd 2 k,i,j as indicated in the above equation.</p><p>When a query y is given at runtime, we first compute squared distances between the query y and cluster centroids h 2 1,i and h 2 2,i . We then fetch precomputed values of Î± K,k andr 2 k,i,j values, and compute the distances to all the indices as:d 2 k,i,j = h 2 k,i + Î± K,kr 2 k,i,j in each subspace with a minor computational cost; i.e. 2M addition operation, where M is the number of indices in each subspace. We then sort the indices in each subspace separately according to the computed distancesd 2 k,i,j . Once the indices are sorted, we traverse the multi-index structure in the non-decreasing order of the estimated distances to select T search result candidates as the shortlist. We utilize the multi-sequence algorithm <ref type="bibr" target="#b0">[1]</ref> based on a priority queue. Our multi-index structure and shortlist computation method is illustrated in <ref type="figure" target="#fig_1">Fig. 2</ref>.</p><p>Note that the representative residual distance method can also be applied to the inverted index (Sec. 4.2). Nonetheless, for the inverted index case, the lookup table with the binary search algorithm works better, even when we need to compute a large shortlist for the inverted index.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Evaluation</head><p>We use the following datasets:</p><p>â¢ SIFT-1M and SIFT-1B: BigANN dataset <ref type="bibr" target="#b13">[14]</ref>. 1 million and 1 billion of 128-dimensional SIFT features. â¢ GIST384-1M and GIST384-80M: Tiny Images <ref type="bibr" target="#b24">[25]</ref>.</p><p>1 and 80 million of 384-dimensional GIST descriptors. For all the datasets, we have 1000 queries disjoint from the retrieval databases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Protocol</head><p>We evaluate the performance of different methods based on the accuracy of the shortlist as a function of shortlist size T . The accuracy is measured by recall, i.e. how many true neighbors are included in the shortlist.</p><p>We compare our method against the conventional shortlist computation method (Sec. 3) that is commonly used in <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b6">7]</ref>. We represent our proposed shortlist selection method as Ours. Each shortlist selection method is combined with three different inverted indexing schemes as follows:</p><p>â¢ II: Inverted Index <ref type="bibr" target="#b13">[14]</ref>.</p><p>â¢ IMI: Inverted Multi-Index <ref type="bibr" target="#b0">[1]</ref>.</p><p>â¢ OIMI: Optimized Inverted Multi-Index <ref type="bibr" target="#b6">[7]</ref>.</p><p>â¢ BDH: Bucket Distance Hashing <ref type="bibr" target="#b12">[13]</ref> â¢ II+Ours: Our method combined with II.</p><p>â¢ IMI+Ours: Our method combined with IMI.</p><p>â¢ OIMI+Ours: Our method combined with OIMI.</p><p>We use 100K and 1M randomly sampled training data to train indexing methods for 1M and 1B dataset respectively. We use 500 randomly chosen samples (N s in Sec. 4.1) from each dataset to train Î± K values. For example, the trained Î± 100 values for GIST960-1M, VLAD2K-1M, and VLAD8K-1M are 0.58, 0.80, and 0.92 respectively. We also use Z=1024 for II+Ours for all the experiments. For IMI+Ours and OIMI+Ours, we use the number of partitions of each cluster to be two, i.e., P =2, and M =M imi /2, where M imi is the number of clusters used for IMI and OIMI, to set ours to have the same number of inverted lists with IMI and OIMI for a fair comparison. We also tested other settings such as (P =4, M =M imi /4), and found that these two settings show slightly varying performance across different benchmarks. For all the tests shown in this paper, we report results with (P =2, M =M imi /2). Note that when a parameter M is specified, II and II+Ours have M inverted lists, while IMI, OIMI, IMI+Ours, and OIMI+Ours have M 2 inverted lists.</p><p>All the methods are implemented in the C++ (Intel Compiler and its MKL is used for the faster performance). We conduct the experiments on a machine that consists of 2 Xeon E5-2690 CPUs and 256GB main memory. We use a single thread when measuring the computational time. <ref type="figure" target="#fig_4">Fig. 3</ref> shows the accuracy of shortlists retrieved by the tested methods on six different benchmarks consisting of 1 million high-dimensional data, when the number of true neighbors K is 100. We use P =4 and C = 2 24 for BDH, and M =1024 for all the other methods. Our shortlist selection method Ours improved all the baselines II, IMI, and OIMI on GIST384-1M, GIST960-1M, VLAD2K-1M, VLAD8K-1M, and CNN-1M benchmarks, and provided comparable accuracy on SIFT-1M. For instance, our method collected 103%, 126%, and 90% more true neighbors compared to II, IMI, and OIMI on VLAD8K-1M, respectively, when T =12800(=1.28% of the benchmark size). Moreover, the performance gain by combining Ours becomes larger with higher dimensional data (GIST384-1M vs GIST960-1M, and VLAD2K-1M vs VLAD8K-1M). This result is achieved, mainly because our distance estimator works well even with high-dimensional cases, while employed quantization methods deteriorate with those cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Results</head><p>While our method shows higher accuracy over other techniques, we also analyze the computational time for   different methods. <ref type="table">Table 1</ref> reports the computation time to collect shortlists. II+Ours shows similar computation times with II. On the other hand, IMI+Ours are faster, 173%, 119%, 28%, and 72%, over IMI in GIST960-1M, VLAD2K-1M, VLAD8K-1M, and CNN-11M, respectively. Similarly, OIMI+Ours shows 118%, 6%, 32%, and 17% higher performance over OIMI in those datasets. The speedup comes mainly from the smaller number of centroids of our method combined with the multi-index. We report the results with respect to the number of true neighbors K, since the desired number of neighbors can vary depending on applications. <ref type="figure" target="#fig_5">Fig. 4</ref> shows the accuracy of shortlists on GIST960-1M and VLAD8K-1M dataset, when K=1 and K=1000. The accuracy improvement over the baseline is larger with larger K. This trend confirms the merits of our shortlist collection method based on the novel distance estimator against the baseline, which relies only on the distance between the query and the centroids. <ref type="figure" target="#fig_6">Fig. 5(a)</ref> shows the results of II and II+Ours on CNN-11M with three different numbers of inverted lists M = 1024, 2048, and 8192, when K = 100. II+Ours consistently provides performance improvements over II for all the tested M . Moreover, Ours+II using 1024 inverted lists provides almost identical recall rates with II using 8192 inverted lists.</p><p>We now report results with large-scale datasets: GIST384-80M and two one billion descriptors of SIFT-1B and S-VLAD2K-1B. Unfortunately, we were unable to collect one billion images for the high-dimensional VLAD dataset. Instead, we construct the synthetic one, S-VLAD2K-1B, as described earlier. <ref type="table">Table 2</ref> show results with different methods. In those large-scale experiments, we used M =2 12 , M =2 14 , and M =2 14 for GIST384-80M, SIFT-1B, and S-VLAD2K-1B, respectively. The results on GIST384-80M and SIFT-1B have similar trends to their corresponding 1M dataset. II+Ours identified more accurate shortlists over II, and IMI+Ours provided a higher or comparable accuracy with a faster performance compared to IMI.</p><p>Note that our method works better with higher dimensional data as shown in <ref type="figure" target="#fig_4">Fig. 3</ref>. In a similar trend, our method tested with S-VLAD2K-1B outperformed the baselines, but its improvement looks marginal compared to results on VLAD2K-1M. We would like to point out that this  <ref type="table">Table 2</ref>. Experimental results on GIST384-80M, SIFT-1B, and S-VLAD2K-1B, when K = 100. In each cell, the first and second values are a recall and computation time (in ms), respectively. is mainly due to the nature of the synthetic dataset, not a scalability issue of our method. In order to explain this, we provide an experimental support that shows the difference between real and synthetic data ( <ref type="figure" target="#fig_6">Fig. 5(b)</ref>). As can be seen, the performance improvement using our method with a real 1 M dataset is much higher than that with a synthetic 1 M dataset. Based on this support, we expect that our method can provide higher improvements on a real dataset compared with those observed with S-VLAD2K-1B.</p><p>Results with re-ranking: We report the results with reranking in <ref type="figure" target="#fig_7">Fig. 6</ref>. In the experiments with re-ranking, we utilized 64 bits OPQ codes <ref type="bibr" target="#b5">[6]</ref> for all tested methods except IVFADC <ref type="bibr" target="#b13">[14]</ref> that uses PQ. Three methods based on the II (II, II+Ours, IVFADC) use residual vectors. The results confirm that for fixed encoding and shortlist size, shortlist accuracy is pivotal to end-to-end retrieval accuracy. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We have presented a novel shortlist selection algorithm for large-scale, high-dimensional approximate K-nearest neighbor search. The proposed method utilizes the residualaware distance estimator that considers the residual distances of the data to their corresponding quantized centroids. In order to efficiently select a given size of the shortlist, we proposed effective pre-computation schemes for the inverted index and multi-index with a minor memory overhead. We have tested the proposed algorithm combined with the inverted index and inverted multi-index on largescale benchmarks, and found that our method significantly improves the accuracy of shortlists over the prior methods with comparable or lower computational costs.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>An example of our lookup table W and shortlist computation. (a) The number of data points in L2 whose distances to the centroid c2 are less than R5 is W (2, 5) = 24. (b) The number of data whose estimated distances are less than t1 (red boxes) is approximately the sum of values on the red line. When T = 50, the binary search is performed in an order of t1 â t2 â t3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>An example of our multi-index structure and shortlist computation. (a) An index in a subspace is a pair of a cluster ID and a quantized residual distance ID. The representative residual distances are pre-computed since they are independent from the query. (b) The distances from a query to centroids h 2 k,i are computed. Based on h 2 k,i the distances to indices in k th subspaced 2 k,i,j = h 2 k,i + Î± K,kr2 k,i,j computed (blue values). In this example, Î± K,k is 1 for simplicity. The indices in each subspace are sorted. We traverse the table in order of estimated distance by using the multi-sequence algorithm [1]. 4.3.1 Indexing Structure IMI decomposes a vector x to x = [x 1 x 2 ], where x 1 , x 2 â R D/2 are in the 1 st and 2 nd subspaces. Cluster centroids c 1 i and c 2 i are defined in the 1 st and 2 nd subspaces (i = 1, ..., M ), respectively. Let us redefine terms used for the inverted index (Sec. 4.2) to the k th subspace: q k</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>â¢ GIST960-1M: BigANN dataset<ref type="bibr" target="#b13">[14]</ref>. 1 million of 960-dimensional GIST descriptors.â¢ VLAD2K-1M and VLAD8K-1M: 1 million of 2048and 8192-dimensional VLAD descriptors<ref type="bibr" target="#b14">[15]</ref>. â¢ CNN-1M and CNN-11M: 1 and 11 million of 4096dimensional image features from the last fully connected layer (fc7) in the CNN<ref type="bibr" target="#b17">[18]</ref>. â¢ S-VLAD2K-1B: 1 billion of 2048-dimensional synthetic VLAD descriptors. Each VLAD descriptor is synthesized with 1000 randomly sampled SIFT features from SIFT-1B.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 .</head><label>3</label><figDesc>Experimental results on SIFT-1M, GIST384-1M, GIST960-1M, VLAD2K-1M, VLAD8K-1M, and CNN-1M when the number of true neighbors K = 100.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>Experimental results on GIST960-1M and VLAD8K-1M, and CNN-1M when the number of true neighbors K = 1 and 1000.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 .</head><label>5</label><figDesc>(a) This figure shows an experimental result on CNN-11M dataset with three different numbers of inverted lists, when K = 100. The number in brackets indicates the number of inverted lists M . Note that 1K is equal to 1024. (b) This figure shows the difference between real VLAD (VLAD2K-1M) and synthetic VLAD (S-VLAD2K-1M).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 .</head><label>6</label><figDesc>Results with re-ranking when K = 100 and T = 12800.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head></head><label></label><figDesc>GIST384-80M S.List.Size T</figDesc><table>1K 
5K 
10K 
50K 
100K 
500K 
1M 

II+Ours 
0.053 / 0.91 0.134 / 0.93 0.195 / 0.96 0.412 / 1.15 0.534 / 1.29 0.805 / 2.71 0.890 / 4.48 
II 
0.009 / 0.76 0.043 / 0.77 0.081 / 0.80 0.276 / 0.92 0.397 / 1.12 0.721 / 2.56 0.837 / 4.23 
IMI+Ours 
0.087 / 2.42 0.210 / 2.28 0.307 / 2.39 0.571 / 2.79 0.705 / 3.01 0.928 / 7.01 0.976 / 11.7 
IMI 
0.066 / 2.77 0.184 / 2.81 0.270 / 2.89 0.533 / 3.33 0.658 / 3.90 0.890 / 9.12 0.949 / 15.0 
SIFT-1B 
S.List.Size T 
1K 
10K 
50K 
100K 
500K 
1M 
10M 

II+Ours 
0.025 / 2.54 0.123 / 2.61 0.320 / 2.84 0.461 / 3.06 0.798 / 4.44 0.891 / 6.43 0.998 / 32.3 
II 
0.005 / 2.20 0.048 / 2.23 0.234 / 2.35 0.388 / 2.51 0.763 / 3.52 0.872 / 5.70 0.996 / 28.2 
IMI+Ours 
0.159 / 28.1 0.509 / 28.1 0.788 / 28.2 0.894 / 29.3 0.969 / 35.9 0.992 / 42.4 0.999 / 319 
IMI 
0.167 / 29.7 0.517 / 31.3 0.792 / 31.4 0.900 / 39.0 0.971 / 42.7 0.994 / 69.2 0.999 / 365 
S-VLAD2K-1B 
S.List.Size T 
1K 
10K 
50K 
100K 
500K 
1M 
10M 

II+Ours 
0.001 / 15.4 0.004 / 15.7 0.013 / 16.0 0.019 / 16.6 0.054 / 17.4 0.082 / 18.8 0.298 / 43.4 
II 
0.000 / 15.1 0.001 / 15.2 0.004 / 15.5 0.009 / 16.3 0.034 / 16.9 0.057 / 17.5 0.242 / 40.3 
IMI+Ours 
0.002 / 35.3 0.010 / 35.7 0.031 / 36.9 0.045 / 42.4 0.129 / 50.6 0.182 / 58.7 0.506 / 288 
IMI 
0.001 / 45.2 0.009 / 46.9 0.027 / 47.1 0.040 / 51.5 0.109 / 60.6 0.158 / 63.8 0.461 / 304 
</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We would like to thank anonymous reviewers for constructive comments. S.-E. Yoon is a corresponding author of the paper. This work was supported by MSIP/IITP [R0126- ], (MSIP) (No. 2013-067321, R0101-16-0176).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The inverted multi-index</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Babenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Transform coding for fast approximate nearest neighbor search in high dimensions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Brandt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Distributions of angles in random packing on spheres</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Similarity estimation techniques from rounding algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Charikar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">STOC</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">An algorithm for finding best matches in logarithmic expected time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Bentley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Finkel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM TOMS</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="209" to="226" />
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Optimized product quantization for approximate nearest neighbor search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Optimized product quantization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Iterative quantization: a procrustean approach to learning binary codes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Compact hashing with joint optimization of search accuracy and time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Radhakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bauer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Spherical hashing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-P</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Yoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Distance encoded product quantization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-P</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Yoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Approximate nearest neighbors: toward removing the curse of dimensionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Indyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Motwani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">STOC</title>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">What is the most efficient way to select nearest neighbor candidates for fast approximate nearest neighbor search? In ICCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iwamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kise</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Product quantization for nearest neighbor search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>JÃ©gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Aggregating local descriptors into a compact image representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>JÃ©gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>PÃ©rez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Random maximum margin hashing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Buisson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Locally optimized product quantization for approximate nearest neighbor search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Avrithis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Kernelized locality-sensitive hashing for scalable image search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Scalable recognition with a vocabulary tree</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>NistÃ©r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>StewÃ©nius</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Cartesian k-means</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Modeling the shape of the scene: a holistic representation of the spatial envelope</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Optimised kd-trees for fast image descriptor matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Silpa-Anan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Machines</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Canberra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Small codes and large image databases for recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<title level="m">Spectral hashing</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note>NIPS</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Joint inverted indexing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
