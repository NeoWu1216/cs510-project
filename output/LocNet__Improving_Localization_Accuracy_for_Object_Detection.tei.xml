<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">LocNet: Improving Localization Accuracy for Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
							<email>gidariss@imagine.enpc.fr</email>
							<affiliation key="aff0">
								<orgName type="institution">Universite Paris Est</orgName>
								<address>
									<settlement>Ecole des Ponts ParisTech</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
							<email>nikos.komodakis@enpc.fr</email>
							<affiliation key="aff1">
								<orgName type="institution">Universite Paris Est</orgName>
								<address>
									<addrLine>Ecole des Ponts ParisTech</addrLine>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">LocNet: Improving Localization Accuracy for Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:43+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a novel object localization methodology with the purpose of boosting the localization accuracy of stateof-the-art object detection systems. Our model, given a search region, aims at returning the bounding box of an object of interest inside this region. To accomplish its goal, it relies on assigning conditional probabilities to each row and column of this region, where these probabilities provide useful information regarding the location of the boundaries of the object inside the search region and allow the accurate inference of the object bounding box under a simple probabilistic framework.</p><p>For implementing our localization model, we make use of a convolutional neural network architecture that is properly adapted for this task, called LocNet. We show experimentally that LocNet achieves a very significant improvement on the mAP for high IoU thresholds on PASCAL VOC2007 test set and that it can be very easily coupled with recent stateof-the-art object detection systems, helping them to boost their performance. Finally, we demonstrate that our detection approach can achieve high detection accuracy even when it is given as input a set of sliding windows, thus proving that it is independent of box proposal methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Object detection is a computer vision problem that has attracted an immense amount of attention over the last years. The localization accuracy by which a detection system is able to predict the bounding boxes of the objects of interest is typically judged based on the Intersection over Union (IoU) between the predicted and the ground truth bounding box. Although in challenges such as PASCAL VOC an IoU detection threshold of 0.5 is used for deciding whether an object has been successfully detected, in real life applications a higher localization accuracy (e.g. IoU 0.7) is normally required (e.g., consider the task of a This work was supported by the ANR SEMAPOLIS project. Code and trained models are available on: https://github.com/gidariss/LocNet <ref type="figure">Figure 1</ref>: Illustration of the basic work-flow of our localization module. Left column: our model given a candidate box B (yellow box) it "looks" on a search region R (red box), which is obtained by enlarging box B by a constant factor, in order to localize the bounding box of an object of interest. Right column: To localize a bounding box the model assigns one or more probabilities on each row and independently on each column of region R. Those probabilities can be either the probability of an element (row or column) to be one of the four object borders (see top-right image), or the probability for being on the inside of an objects bounding box (see bottom-right image). In either case the predicted bounding box is drawn with blue color. robotic arm that must grasp an object). Such a need is also reflected in the very recently introduced COCO detection challenge <ref type="bibr" target="#b27">[23]</ref>, which uses as evaluation metric the traditional average precision (AP) measurement but averaged over multiple IoU thresholds between 0.5 (loosely localized object) and 1.0 (perfectly localized object) so as to reward detectors that exhibit good localization accuracy.</p><p>Therefore, proposing detectors that exhibit highly accurate (and not loose) localization of the ground truth objects should be one of the major future challenges in object detection. The aim of this work is to take a further step towards addressing this challenge. In practical terms, our goal is to boost the bounding box detection AP performance across a wide range of IoU thresholds (i.e., not just for IoU thresh-old of 0.5 but also for values well above that). To that end, a main technical contribution of this work is to propose a novel object localization model that, given a loosely localized search region inside an image, aims to return the accurate location of an object in this region (see <ref type="figure">Figure 1)</ref>.</p><p>A crucial component of this new model is that it does not rely on the commonly used bounding box regression paradigm, which uses a regression function to directly predict the object bounding box coordinates. Indeed, the motivation behind our work stems from the belief that trying to directly regress to the target bounding box coordinates, constitutes a difficult learning task that cannot yield accurate enough bounding boxes. We argue that it is far more effective to attempt to localize a bounding box by first assigning a probability to each row and independently to each column of the search region for being the left, right, top, or bottom borders of the bounding box (see <ref type="figure">Fig. 1</ref> top) or for being on the inside of an object's bounding box (see <ref type="figure">Fig. 1</ref> bottom). In addition, this type of probabilities can provide a measure of confidence for placing the bounding box on each location and they can also handle instances that exhibit multi-modal distributions for the border locations. They thus yield far more detailed and useful information than the regression models that just predict 4 real values that correspond to estimations of the bounding box coordinates. Furthermore, as a result of this, we argue that the task of learning to predict these probabilities is an easier one to accomplish.</p><p>To implement the proposed localization model, we rely on a convolutional neural network model, which we call LocNet, whose architecture is properly adapted such that the amount of parameters needed on the top fully connected layers is significantly reduced, thus making our LocNet model scalable with respect to the number of object categories.</p><p>Importantly, such a localization module can be easily incorporated into many of the current state-of-the-art object detection systems <ref type="bibr" target="#b11">[9,</ref><ref type="bibr" target="#b13">11,</ref><ref type="bibr" target="#b32">28]</ref>, helping them to significantly improve their localization performance. Here we use it in an iterative manner as part of a detection pipeline that utilizes a recognition model for scoring candidate bounding boxes provided by the aforementioned localization module, and show that such an approach significantly boosts AP performance across a broad range of IoU thresholds.</p><p>Related work. Most of the recent literature on object detection, treats the object localization problem at prerecognition level by incorporating category-agnostic object proposal algorithms <ref type="bibr" target="#b39">[35,</ref><ref type="bibr" target="#b44">40,</ref><ref type="bibr" target="#b30">26,</ref><ref type="bibr" target="#b2">1,</ref><ref type="bibr" target="#b21">18,</ref><ref type="bibr" target="#b22">19,</ref><ref type="bibr" target="#b3">2,</ref><ref type="bibr">34,</ref><ref type="bibr" target="#b37">33]</ref> that given an image, try to generate candidate boxes with high recall of the ground truth objects that they cover. Those proposals are later classified from a category-specific recognition model in order to create the final list of detections <ref type="bibr" target="#b14">[12]</ref>. Instead, in our work we focus on boosting the localization accuracy at post-recognition time, at which the improve-ments can be complementary to those obtained by improving the pre-recognition localization. Till now, the work on this level has been limited to the bounding box regression paradigm that was first introduced from Felzenszwalb et al. <ref type="bibr" target="#b10">[8]</ref> and ever-since it has been used with success on most of the recent detection systems <ref type="bibr" target="#b14">[12,</ref><ref type="bibr" target="#b13">11,</ref><ref type="bibr" target="#b32">28,</ref><ref type="bibr" target="#b34">30,</ref><ref type="bibr" target="#b18">15,</ref><ref type="bibr" target="#b41">37,</ref><ref type="bibr" target="#b43">39,</ref><ref type="bibr">29,</ref><ref type="bibr" target="#b28">24]</ref>. A regression model, given an initial candidate box that is loosely localized around an object, it tries to predict the coordinates of its ground truth bounding box. Lately this model is enhanced by high capacity convolutional neural networks to further improve its localization capability <ref type="bibr" target="#b11">[9,</ref><ref type="bibr" target="#b13">11,</ref><ref type="bibr" target="#b34">30,</ref><ref type="bibr" target="#b32">28]</ref>.</p><p>Contributions. In summary, we make the following contributions: (1) We cast the problem of localizing an object's bounding box as that of assigning probabilities on each row and column of a search region. Those probabilities represent either the likelihood of each element (row or column) to belong on the inside of the bounding box or the likelihood to be one of the four borders of the object. Both of those cases is studied and compared with the bounding box regression model. <ref type="bibr" target="#b3">(2)</ref> To implement the above model, we propose a properly adapted convolutional neural network architecture that has a reduced number of parameters and results in an efficient and accurate object localization network (LocNet). (3) We extensively evaluate our approach on VOC2007 <ref type="bibr" target="#b7">[5]</ref> and we show that it achieves a very significant improvement over the bounding box regression with respect to the mAP for IoU threshold of 0.7 and the COCO style of measuring the mAP. It also offers an improvement with respect to the traditional way of measuring the mAP (i.e., for IoU 0.5), achieving in this case 78.4% and 74.78% mAP on VOC2007 <ref type="bibr" target="#b7">[5]</ref> and VOC2012 <ref type="bibr" target="#b8">[6]</ref> test sets, which are the state-of-the-art at the time of writing this paper. Given those results we believe that our localization approach could very well replace the existing bounding box regression paradigm in future object detection systems. (4) Finally we demonstrate that the detection accuracy of our system remains high even when it is given as input a set of sliding windows, which proves that it is independent of bounding box proposal methods if the extra computational cost is ignored.</p><p>The remainder of the paper is structured as follows: We describe our object detection methodology in §2. We present our localization model in §3. We show experimental results in §4 and conclude in §5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Object Detection Methodology</head><p>Our detection pipeline includes two basic components, the recognition and the localization models, integrated into an iterative scheme (see algorithm 1). This scheme starts from an initial set of candidate boxes B 1 (which could be, e.g., either dense sliding windows <ref type="bibr" target="#b34">[30,</ref><ref type="bibr" target="#b29">25,</ref><ref type="bibr" target="#b31">27,</ref><ref type="bibr" target="#b26">22]</ref> or category-agnostic bounding box proposals <ref type="bibr" target="#b44">[40,</ref><ref type="bibr" target="#b39">35,</ref><ref type="bibr" target="#b32">28]</ref>) and</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1: Object detection pipeline</head><p>Input : Image I, initial set of candidate boxes B 1 Output:</p><formula xml:id="formula_0">Final list of detections Y for t ← 1 to T do S t ← Recognition(B t |I) if t &lt; T then B t+1 ← Localization(B t |I) end end D ← ∪ T t=1 {S t , B t } Y ← PostProcess(D)</formula><p>on each iteration t it uses the two basic components in the following way:</p><p>Recognition model: Given the current set of candidate</p><formula xml:id="formula_1">boxes B t = {B t i } Nt i=1</formula><p>, it assigns a confidence score to each of them {s t i } Nt i=1 that represents how likely it is for those boxes to be localized on an object of interest.</p><p>Localization model: Given the current set of candidate</p><formula xml:id="formula_2">boxes B t = {B t i } Nt i=1 , it generates a new set of candi- date boxes B t+1 = {B t+1 i } Nt+1 i=1</formula><p>such that those boxes they will be "closer" (i.e., better localized) on the objects of interest (so that they are probably scored higher from the recognition model).</p><p>In the end, the candidate boxes that were generated on each iteration from the localization model along with the confidences scores that were assigned to them from the recognition model are merged together and a postprocessing step of non-max-suppression <ref type="bibr" target="#b10">[8]</ref> followed from bounding box voting <ref type="bibr" target="#b11">[9]</ref> is applied to them. The output of this post-processing step consists the detections set produced from our pipeline. Both the recognition and the localization models are implemented as convolutional neural networks <ref type="bibr" target="#b25">[21]</ref> that lately have been empirically proven quite successful on computers vision tasks and especially those related to object recognition problems <ref type="bibr" target="#b35">[31,</ref><ref type="bibr" target="#b24">20,</ref><ref type="bibr" target="#b17">14,</ref><ref type="bibr" target="#b20">17,</ref><ref type="bibr" target="#b36">32]</ref>. More details about our detection pipeline are provided in appendix E of technical report <ref type="bibr" target="#b12">[10]</ref>.</p><p>Iterative object localization has also been explored before <ref type="bibr" target="#b4">[3,</ref><ref type="bibr" target="#b11">9,</ref><ref type="bibr" target="#b15">13,</ref><ref type="bibr" target="#b40">36]</ref>. Notably, Gidaris and Komodakis <ref type="bibr" target="#b11">[9]</ref> combine CNN-based regression with iterative localization while Caicedo et al. <ref type="bibr" target="#b4">[3]</ref> and Yoo et al. <ref type="bibr" target="#b40">[36]</ref> attempt to localize an object by sequentially choosing one among a few possible actions that either transform the bounding box or stop the searching procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Localization model</head><p>In this paper we focus on improving the localization model of this pipeline. The abstract work-flow that we use for this model is that it gets as input a candidate box B in the image, it enlarges it by a factor γ 1 to create a search region R and then it returns a new candidate box that ideally will tightly enclose an object of interest in this region (see right column of <ref type="figure">Figure 1</ref>).</p><p>The crucial question is, of course, what is the most effective approach for constructing a model that is able to generate a good box prediction. One choice could be, for instance, to learn a regression function that directly predicts the 4 bounding box coordinates. However, we argue that this is not the most effective solution. Instead, we opt for a different approach, which is detailed in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Model predictions</head><p>Given a search region R and object category c, our object localization model considers a division of R in M equal horizontal regions (rows) as well as a division of R in M equal vertical regions (columns), and outputs for each of them one or more conditional probabilities. Each of these conditional probabilities is essentially a vector of the form</p><formula xml:id="formula_3">p R,c = {p(i|R, c)} M i=1</formula><p>(hereafter we drop the R and c conditioned variables so as to reduce notational clutter). Two types of conditional probabilities are considered here:</p><p>In-Out probabilities: These are vectors p</p><formula xml:id="formula_4">x = {p x (i)} M i=1 and p y = {p y (i)} M i=1</formula><p>that represent respectively the conditional probabilities of each column and row of R to be inside the bounding box of an object of category c (see left part of <ref type="figure" target="#fig_0">Figure 2)</ref>. A row or column is considered to be inside a bounding box if at least part of the region corresponding to this row or column is inside this box. For example, if B gt is a ground truth bounding box with top-left coordinates (B gt l , B gt t ) and bottom-right coordinates (B gt r , B gt b ), 2 then the In-Out probabilities p = {p x , p y } from the localization model should ideally equal to the following target probabilities T = {T x , T y }:</p><formula xml:id="formula_5">∀i ∈ {1, . . . , M }, T x (i) = 1, if B gt l ≤ i ≤ B gt r 0, otherwise , ∀i ∈ {1, . . . , M }, T y (i) = 1, if B gt t ≤ i ≤ B gt b 0, otherwise .</formula><p>Border probabilities: These are vectors</p><formula xml:id="formula_6">p l = {p l (i)} M i=1 , p r = {p r (i)} M i=1 , p t = {p t (i)} M i=1 and p b = {p b (i)} M i=1</formula><p>that represent respectively the conditional probability of each column or row to be the left (l), right (r), top (t) and bottom (b) border of the bounding box of an object of category c (see right part of <ref type="figure" target="#fig_0">Figure 2</ref>). In this case, the target probabilities T = {T l , T r , T t , T b } that should ideally be predicted by the localization model for a ground truth</p><formula xml:id="formula_7">bounding box B gt = (B gt l , B gt t , B gt r , B gt b ) are given by ∀i ∈ {1, . . . , M }, T s (i) = 1, if i = B gt s 0, otherwise ,</formula><p>where s ∈ {l, r, t, b}. Note that we assume that the left and right border probabilities are independent and similarly for the top and bottom cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Bounding box inference</head><p>Given the above output conditional probabilities, we model the inference of the bounding box location B = (B l , B t , B r , B b ) using one of the following probabilistic models:</p><p>In-Out ML: Maximizes the likelihood of the in-out ele-</p><formula xml:id="formula_8">ments of B L in-out (B) = i∈{B l ,...,Br} p x (i) i∈{Bt,...,B b } p y (i) i / ∈{B l ,...,Br}p x (i) i / ∈{Bt,...,B b }p y (i),<label>(1)</label></formula><p>wherep x (i) = 1−p x (i) andp y (i) = 1−p y (i). The first two terms in the right hand of the equation represent the likelihood of the rows and columns of box B (in-elements) to be inside a ground truth bounding box and the last two terms the likelihood of the rows and columns that are not part of B (out-elements) to be outside a ground truth bounding box. Borders ML: Maximizes the likelihood of the borders of box B:</p><formula xml:id="formula_9">L borders (B) = p l (B l ) · p t (B t ) · p r (B r ) · p b (B b ).<label>(2)</label></formula><p>Combined ML: It uses both types of probability distributions by maximizing the likelihood for both the borders and the in-out elements of B:</p><formula xml:id="formula_10">L combined (B) = L borders (B) · L in-out (B).<label>(3)</label></formula><p>target probabilities. <ref type="figure">Figure 3</ref>: We show the evolution during training. In the left image the green squares indicate the two highest modes of the left border probabilities predicted by a network trained only for a few iterations (5k). Despite the fact that the highest one is erroneous, the network also maintains information for the correct mode. As training progresses (50k), this helps the network to correct its mistake and recover a correct left border(right image).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Discussion</head><p>The reason we consider that the proposed formulation of the problem of localizing an object's bounding box is superior is because the In-Out or Border probabilities provide much more detailed and useful information regarding the location of a bounding box compared to the typical bounding box regression paradigm <ref type="bibr" target="#b10">[8]</ref>. In particular, in the later case the model simply directly predicts real values that corresponds to estimated bounding box coordinates but it does not provide, e.g., any confidence measure for these predictions. On the contrary, our model provides a conditional probability for placing the four borders or the inside of an object's bounding box on each column and row of a search region R. As a result, it is perfectly capable of handling also instances that exhibit multi-modal conditional distributions (both during training and testing). During training, we argue that this makes the per row and per column probabilities much easier to be learned from a convolutional neural network that implements the model, than the bounding box regression task (e.g., see <ref type="figure">Figure 3</ref>), thus helping the model to converge to a better training solution. Furthermore, during testing, these conditional distributions as we saw can be exploited in order to form probabilistic models for the inference of the bounding box coordinates. In addition, they can indicate the presence of a second instance inside the region R and thus facilitate the localization of multiple adjacent instances, which is a difficult problem on object detection. In fact, when visualizing, e.g., the border probabilities, we observed that this could have been possible in several cases (e.g., see <ref type="figure" target="#fig_1">Figure 5</ref>). Although in this work we did not explore the possibility of utilizing a more advanced probabilistic model that predicts K &gt; 1 boxes per region R, this can certainly be an interesting future addition to our method.</p><p>Alternatively to our approach, we could predict the probability of each pixel to belong on the foreground of an ob-  <ref type="figure">Figure 4</ref>: Visualization of the LocNet network architecture. In the input image, with yellow is drawn the candidate box B and with red the search region R. In its output, the LocNet network yields probabilities for each of the C object categories. The parameter M that controls the output resolution is set to the value 28 in our experiments. The convolutional layers of the VGG16-Net <ref type="bibr" target="#b35">[31]</ref> that are being used in order to extract the image activations AI are those from conv1 1 till conv5 3. ject, as Pinheiro et al. <ref type="bibr" target="#b30">[26]</ref> does. However, in order to learn such a type of model, pixel-wise instance segmentation masks are required during training, which in general is a rather tedious task to collect. In contrast, for our model to learn those per row and per column probabilities, only bounding box annotations are required. Even more, this independence is exploited in the design of the convolutional neural network that implements our model in order to keep the number of parameters of the prediction layers small (see § 3.2). This is significant for the scalability of our model with respect to the number of object categories since we favour category-specific object localization that has been shown to exhibit better localization accuracy <ref type="bibr" target="#b35">[31]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">LocNet network architecture</head><p>Our localization model is implemented through the convolutional neural network that is visualized in <ref type="figure">Figure 4</ref> and which is called LocNet. The processing starts by forwarding the entire image I (of size w I × h I ), through a sequence of convolutional layers (conv. layers of VGG16 <ref type="bibr" target="#b35">[31]</ref>) that outputs the A I activation maps (of size w I 16 × h I 16 × 512). Then, the region R is projected on A I and the activations that lay inside it are cropped and pooled with a spatially adaptive max-pooling layer <ref type="bibr" target="#b18">[15]</ref>. The resulting fixed size activation maps (14 × 14 × 512) are forwarded through two convolutional layers (of kernel size 3 × 3 × 512), each followed by ReLU non-linearities, that yield the localization-aware activation maps A R of region R (with dimensions size 14 × 14 × 512).</p><p>At this point, given the activations A R the network yields the probabilities that were described in section §3.1. Specifically, the network is split into two branches, the X and Y, with each being dedicated for the predictions that correspond to the dimension (x or y respectively) that is assigned to it. Both start with a max-pool layer that aggregates the A R activation maps across the dimension perpendicular to the one dedicated to them, i.e.,</p><formula xml:id="formula_11">A x R (i, f ) = max j A R (i, j, f ),<label>(4)</label></formula><formula xml:id="formula_12">A y R (j, f ) = max i A R (i, j, f ),<label>(5)</label></formula><p>where i,j,and f are the indices that span over the width, height, and feature channels of A R respectively. The resulted activations A x R and A y R (both of size 14 × 512) efficiently encode the object location only across the dimension that their branch handles. This aggregation process could also be described as marginalizing-out localization cues irrelevant for the dimension of interest. Finally, each of those aggregated features is fed into the final fully connected layer that is followed from sigmoid units in order to output the conditional probabilities of its assigned dimension. Specifically, the X branch outputs the p x and/or the (p l , p r ) probability vectors whereas the Y branch outputs the p y and/or the (p t , p b ) probability vectors. Despite the fact that the last fully connected layers output category-specific predictions, their number of parameters remains relatively small due to the facts that: 1) they are applied on features of which the dimensionality has been previously drastically reduced due to the max-pooling layers of equations 4 and 5, and 2) that each branch yields predictions only for a single dimension.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Training</head><p>During training, the network learns to map a search regions R to the target probabilities T that are conditioned on the object category c. Given a set of N training samples {(R k , T k , c k )} N k=1 the loss function that is minimised is</p><formula xml:id="formula_13">L(θ) = 1 N N k=1 l(θ|R k , T k , c k ),<label>(6)</label></formula><p>where θ are the network parameters that are learned and l(θ|R, T, c) is the loss for one training sample. Both for the In-Out and the Borders probabilities we use the sum of binary logistic regression losses per row and column. Specifically, the per sample loss of the In-Out case is:</p><formula xml:id="formula_14">a∈{x,y} M i=1</formula><p>T a (i) log(p a (i)) +T a (i) log(p a (i)) , <ref type="bibr" target="#b9">(7)</ref> and for the Borders case is:</p><formula xml:id="formula_15">s∈{l,r,u,b} M i=1 λ + T s (i) log(p s (i)) + λ −T s (i) log(p s (i)) , (8) wherep = 1 − p.</formula><p>In objective function <ref type="formula">(8)</ref>, λ + and λ − represent the weightings of the losses for misclassifying a border and a non-border element respectively. These are set as</p><formula xml:id="formula_16">λ − = 0.5 · M M − 1 , λ + = (M − 1) · λ − ,</formula><p>so as to balance the contribution on the loss of those two cases (note thatT s (i) will be non-zero M − 1 times more than T s (i)). We observed that this leads to a model that yields more "confident" probabilities for the borders elements. Implementation details about the training procedure are provided in section 4 of technical report <ref type="bibr" target="#b12">[10]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental results</head><p>We empirically evaluate our localization models on PAS-CAL VOC detection challenge <ref type="bibr" target="#b9">[7]</ref>. Specifically, we train all the recognition and localization models on VOC2007+2012 trainval sets and we test them on the VOC2007 test set. As baseline we use a CNN-based bounding box regression model <ref type="bibr" target="#b11">[9]</ref> (see appendices A, B, and C of technical report <ref type="bibr" target="#b12">[10]</ref>). The remaining components of the detection pipeline include Initial set of candidate boxes: We examine three alternatives for generating the initial set of candidate boxes: the Edge Box algorithm <ref type="bibr" target="#b44">[40]</ref> (EB), the Selective Search algorithm (SS), and a sliding windows scheme. In <ref type="table" target="#tab_2">Table 1</ref> we provide the recall statistics of those box proposal methods. Recognition model: For the recognition part of the detection system we use either the Fast-RCNN <ref type="bibr" target="#b13">[11]</ref> or the MR-CNN <ref type="bibr" target="#b11">[9]</ref> recognition models. During implementing the latter one, we performed several simplifications on its architecture and thus we call the resulting model Reduced-MR-CNN (those modifications are detailed in appendix D of technical report <ref type="bibr" target="#b12">[10]</ref>). The Fast-RCNN and Reduced-MR-CNN models are trained using both selective search and edge box proposals and as top layer they have classspecific linear SVMs <ref type="bibr" target="#b14">[12]</ref>.</p><p>Apart from on PASCAL, we also provide preliminary results of our approach on COCO detection challenge <ref type="bibr" target="#b27">[23]</ref>.   <ref type="table">Table 2</ref>: mAP results on VOC2007 test set. The hyphen symbol <ref type="bibr">(-)</ref> indicates that the localization model was not used at all and that the pipeline ran only for T = 1 iteration. The rest entries are obtained after running the detection pipeline for T = 4 iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Localization performance</head><p>We first evaluate merely the localization performance of our models, thus ignoring in this case the recognition aspect of the detection problem. For that purpose we report the recall that the examined models achieve. Specifically, in <ref type="figure" target="#fig_3">Figure 6a</ref> we provide the recall as a function of the IoU threshold for the candidate boxes generated on the first iteration and the last iteration of our detection pipeline. Also, in the legends of these figures we report the average recall (AR) <ref type="bibr" target="#b19">[16]</ref> that each model achieves. Note that, given the set of initial candidate boxes and the recognition model, the input to the iterative localization mechanism is exactly the same and thus any difference on the recall is solely due to the localization capabilities of the models. We observe that for IoU thresholds above 0.65, the proposed models achieve higher recall than bounding box regression and that this improvement is actually increased with more iterations of the localization module. Also, the AR of our proposed models is on average 6 points higher than bounding box regression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Detection performance</head><p>Here we evaluate the detection performance of the examined localization models when plugged into the detec-   <ref type="table">Table 2</ref> we report the mAP on VOC2007 test set for IoU thresholds of 0.5 and 0.7 as well as the COCO style of mAP that averages the traditional mAP over various IoU thresholds between 0.5 and 1.0. The results that are reported are obtained after running the detection pipeline for T = 4 iterations. We observe that the proposed InOut ML, Borders ML, and Combined ML localization models offer a significant boost on the mAP for IoU ≥ 0.7 and the COCO style mAP, relative to the bounding box regression model (Bbox reg.) under all the tested cases. The improvement on both of them is on average 7 points. Our models also improve for the mAP with IoU≥ 0.5 case but with a smaller amount (around 0.7 points). In <ref type="figure" target="#fig_3">Figure 6b</ref> we plot the mAP as a function of the IoU threshold. We can observe that the improvement on the detection performance thanks to the proposed localization models starts to clearly appear on the 0.65 IoU threshold and then grows wider till the 0.9. In <ref type="table" target="#tab_3">Table 3</ref> we provide the per class AP results on VOC2007 for the best approach on each metric. In the same table we also report the AP results on VOC2012 test set but only for the IoU ≥ 0.5 case since this is the only metric that the evaluation server provides. In this dataset we achieve mAP of 74.8% which is the state-of-the-art at the time of writing this paper (6/11/2015). Finally, in <ref type="figure" target="#fig_4">Figure 7</ref> we examine the detection performance behaviour with respect to the number of iterations used by our pipeline. We observe that as we increase the number of iterations, the mAP for high IoU thresholds (e.g. IoU ≥ 0.8) continues to improve while for lower thresholds the improvements stop on the first two iterations.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Sliding windows as initial set of candidate boxes</head><p>In <ref type="table" target="#tab_4">Table 4</ref> we provide the detection accuracy of our pipeline when, for generating the initial set of candidate boxes, we use a simple sliding windows scheme (of 10k windows per image). We observe that:</p><p>• Even in this case, our pipeline achieves very high mAP results that are close to the ones obtained with selective search or edge box proposals. We emphasize that this is true even for the IoU≥ 0.7 or the COCO style of mAP that favour better localized detections, despite the fact that in the case of sliding windows the initial set of candidate boxes is considerably less accurately localized than in the edge box or in the selective search cases (see <ref type="table" target="#tab_2">Table 1</ref>). • Just scoring the sliding window proposals with the recognition model (hyphen (-) case) yields much worse mAP results than in the selective search or edge box cases. However, when we use the full detection pipeline that includes localization models and rescoring of the new better localized candidate boxes, then this gap is significantly reduced. • The difference in mAP between the proposed localization models (In-Out ML, Borders ML, and Combined ML) and the bounding box regression model (Bbox reg.) is even greater in the case of sliding windows. To the best of our knowledge, the above mAP results are considerably higher than those of any other detection method when only sliding windows are used for the initial bounding box proposals (similar experiments are reported in <ref type="bibr" target="#b13">[11,</ref><ref type="bibr" target="#b19">16]</ref>). We also note that we had not experimented with increasing the number of sliding windows. Furthermore, the tested recognition model and localization models were not re-trained with sliding windows in the training set. As a  <ref type="table">Table 5</ref> -Preliminary results on COCO. In those experiments the Fast R-CNN recognition model uses a softmax classifier <ref type="bibr" target="#b13">[11]</ref> instead of class-specific linear SVMs <ref type="bibr" target="#b14">[12]</ref> that are being used for the PASCAL experiments.</p><p>result, we foresee that by exploring those two factors one might be able to further boost the detection performance for the sliding windows case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Preliminary results on COCO</head><p>To obtain some preliminary results on COCO, we applied our training procedure on COCO train set. The only modification was to use 320k iterations (no other parameter was tuned). Therefore, LocNet results can still be significantly improved but the main goal was to show the relative difference in performance between the Combined ML localization model and the box regression model. Results are shown in <ref type="table">Table 5</ref>, where it is observed that the proposed model boosts the mAP by 5 points in the COCO-style evaluation, 8 points in the IoU ≥ 0.75 case and 1.4 points in the IoU ≥ 0.5 case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We proposed a novel object localization methodology that is based on assigning probabilities related to the localization task on each row and column of the region in which it searches the object. Those probabilities provide useful information regarding the location of the object inside the search region and they can be exploited in order to infer its boundaries with high accuracy. We implemented our model via using a convolutional neural network architecture properly adapted for this task, called LocNet, and we extensively evaluated it on PAS-CAL VOC2007 test set. We demonstrate that it outperforms CNN-based bounding box regression on all the evaluation metrics and it leads to a significant improvement on those metrics that reward good localization. Importantly, LocNet can be easily plugged into existing state-of-the-art object detection methods, in which case we show that it contributes to significantly boosting their performance. Finally, we demonstrate that our object detection methodology can achieve very high mAP results even when the initial set of bounding boxes is generated by a simple sliding windows scheme.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>The posterior probabilities that our localization model yields given a region R. Left Image: the in-out conditional probabilities that are assigned on each row (py) and column (px) of R. They are drawn with the blues curves on the right and on the bottom side of the search region. Right Image: the conditional probabilities p l , pr, pt, and p b of each column or row to be the left (l), right (r), top (t) and bottom (b) border of an object's bounding box. They are drawn with blue and red curves on the bottom and on the right side of the search region.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 5 :</head><label>5</label><figDesc>We depict the probabilities for the left (blue) and right (red) borders that a trained model yields for a region with two instances of the same class (cow). The probability modes in this case can clearly indicate the presence of two instances.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Recall as a function of IoU. (b) mAP as a function of IoU.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 (</head><label>6</label><figDesc>a) Recall of ground truth bounding boxes as a function of the IoU threshold on VOC2007 test set. Note that, because we perform class-specific localization the recall that those plots report is obtained after averaging the per class recalls.Top-Left: Recalls for the Reduced MR-CNN model after one iteration of the detection pipeline. Top-Middle: Recalls for the Reduced MR-CNN model after four iterations of the detection pipeline.Bottom-Left: Recalls for the Fast-RCNN model after one iteration of the detection pipeline. Bottom-Middle: Recalls for the Fast-RCNN model after four iterations of the detection pipeline. (b) mAP as a function of the IoU threshold on VOC2007 test set. Top-Right: mAP plots for the configurations with the Reduced-MR-CNN recognition model. Bottom-Right: mAP plots for the configurations with the Fast-RCNN recognition model. tion pipeline that was described in section §2. In</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 :</head><label>7</label><figDesc>Plot of the mAP as a function of the iterations number of our detection pipeline on VOC2007 test set. To generate this plot we used the Reduced-MR-CNN recognition model with the In-Out ML localization model and Edge Box proposals.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>and/or M in-out probabilities px M border probabilities p t , p b and/or M in-out probabilities py</figDesc><table>VGG16 
Conv. Layers 
R 

w I 
16 
× 
h I 
16 
×512 

w I ×h I ×3 
14×14×512 

activations AR 

14×14×512 

output size 
14×14×512 

14×1×512 

activations AR y 

14×1×512 

activations A R 

x 

M border probabilities p l , p r 
Conv. Layer + ReLU 
Kernel: 3 x 3 x 512 

Conv. Layer + ReLU 
Kernel: 3 x 3 x 512 

Region Adaptive 
Max Pooling 

Fully Connected 
Layer + Sigmoid 

Max Pooling 
over X Axis 

Max Pooling 
over Y Axis 

Fully Connected 
Layer + Sigmoid 

Y Branch 

X Branch 

output size 

activations AI 

Projecting region R 

Image I 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 1 :</head><label>1</label><figDesc>Recall statistics on VOC2007 test set of the bounding box proposals methods that are being used in our works.</figDesc><table>Detection Pipeline 
mAP 
Localization 
Initial Boxes 
IoU ≥ 0.5 IoU ≥ 0.7 COCO style 

Reduced-MR-CNN 

-
2k Edge Box 
0.747 
0.434 
0.362 
InOut ML 
2k Edge Box 
0.783 
0.654 
0.522 
Borders ML 
2k Edge Box 
0.780 
0.644 
0.525 
Combined ML 2k Edge Box 
0.784 
0.650 
0.530 
Bbox reg. 
2k Edge Box 
0.777 
0.570 
0.452 
-
2k Sel. Search 
0.719 
0.456 
0.368 
InOut ML 
2k Sel. Search 
0.782 
0.654 
0.529 
Borders ML 
2k Sel. Search 
0.777 
0.648 
0.530 
Combined ML 2k Sel. Search 
0.781 
0.653 
0.535 
Bbox reg. 
2k Sel. Search 
0.774 
0.584 
0.460 

Fast-RCNN 

-
2k Edge Box 
0.729 
0.427 
0.356 
InOut ML 
2k Edge Box 
0.779 
0.651 
0.522 
Borders ML 
2k Edge Box 
0.774 
0.641 
0.522 
Combined ML 2k Edge Box 
0.780 
0.648 
0.530 
Bbox reg. 
2k Edge Box 
0.773 
0.570 
0.453 
-
2k Sel. Search 
0.710 
0.446 
0.362 
InOut ML 
2k Sel. Search 
0.777 
0.645 
0.526 
Borders ML 
2k Sel. Search 
0.772 
0.640 
0.526 
Combined ML 2k Sel. Search 
0.775 
0.645 
0.532 
Bbox reg. 
2k Sel. Search 
0.769 
0.579 
0.458 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="true"><head>Table 3 :</head><label>3</label><figDesc>Per class AP results on VOC2007 and VO2012 test sets. Initial Boxes: 10k Sliding Windows</figDesc><table>Localization Model 
mAP 
IoU ≥ 0.5 IoU ≥ 0.7 COCO style 
-
0.617 
0.174 
0.227 
InOut ML 
0.770 
0.633 
0.513 
Borders ML 
0.764 
0.626 
0.513 
Combined ML 
0.773 
0.639 
0.521 
Bbox reg. 
0.761 
0.550 
0.436 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4 :</head><label>4</label><figDesc>mAP results on VOC2007 test set when using 10k sliding windows as initial set of candidate boxes. In order to generate the sliding windows we use the publicly available code that accompanies the work of Hosang et al.<ref type="bibr" target="#b19">[16]</ref> and includes a sliding window implementation inspired by BING[4,<ref type="bibr" target="#b42">38]</ref>. All the entries in this table use the Reduced-MR-CNN recognition model.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head></head><label></label><figDesc>IoU ≥ 0.75 COCO style Combined ML Fast R-CNN Sel. Search</figDesc><table>Detection Pipeline 
mAP 
Localization 
Recognition Proposals 
Dataset 
IoU ≥ 0.5 5K mini-val set 
0.424 
0.282 
0.264 
Bbox reg. 
Fast R-CNN Sel. Search 
5K mini-val set 
0.407 
0.202 
0.214 
Combined ML Fast R-CNN Sel. Search 
test-dev set 
0.429 
0.279 
0.263 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We use γ = 1.8 in all of the experiments.<ref type="bibr" target="#b3">2</ref> We actually assume that the ground truth bounding box is projected on the output domain of our model where the coordinates take integer values in the range {1, . . . , M }. This is a necessary step for the definition of the</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-Mr-Cnn &amp;amp;</forename><surname>Reduced</surname></persName>
		</author>
		<idno>EB 0.863 0.830 0.761 0.608 0.546 0.799 0.790 0.906 0.543 0.816 0.620 0.890 0.857 0.855 0.828 0.497 0.766 0.675 0.832 0.674 0.748</idno>
		<title level="m">Out ML &amp;</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IoU ≥</title>
		<imprint>
			<biblScope unit="volume">0</biblScope>
			<biblScope unit="issue">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Measuring the objectness of image windows. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Alexe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multiscale combinatorial grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Active object localization with deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Caicedo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Bing: Binarized normed gradients for objectness estimation at 300fps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE Conference on</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">The pascal visual object classes challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">The pascal visual object classes challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained partbased models. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Object detection via a multiregion &amp; semantic segmentation-aware cnn model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07763</idno>
		<title level="m">Technical report -locnet: Improving localization accuracy for object detection</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">An active search strategy for efficient object class detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gonzalez-Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vezhnevets</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE Conference on</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">What makes for effective detection proposals?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hosang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.05082</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Geodesic object proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2014</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning to propose objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
			</analytic>
	<monogr>
		<title level="m">IEEE Conference on</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename></persName>
		</author>
		<imprint>
			<date type="published" when="1989" />
		</imprint>
	</monogr>
	<note>Neural computation</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">R-cnn minus r</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lenc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference (BMVC)</title>
		<meeting>the British Machine Vision Conference (BMVC)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.3505</idno>
		<title level="m">Deepid-net: multi-stage and deformable deep convolutional neural networks for object detection</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Modeling local and global deformations in deep learning: Epitomic convolution, multiple instance learning, and sliding window detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Savalle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning to segment object candidates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">O</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.02640</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.06066</idno>
		<title level="m">Object detection networks on convolutional feature maps</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Overfeat: Integrated recognition, localization and detection using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6229</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Scalable</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.1441</idno>
		<title level="m">high-quality object detection</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deep neural networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Segmentation as selective search for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">E</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV), 2011 IEEE International Conference on</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Attentionnet: Aggregating weak directions for accurate object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Paek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>So Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Improving object detection with deep convolutional networks via bayesian optimization and structured prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Cracking bing and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference</title>
		<meeting>the British Machine Vision Conference</meeting>
		<imprint>
			<publisher>BMVA Press</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">segdeepm: Exploiting segmentation and context in deep neural networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Edge boxes: Locating object proposals from edges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
