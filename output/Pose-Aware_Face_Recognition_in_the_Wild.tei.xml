<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Pose-Aware Face Recognition in the Wild</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iacopo</forename><surname>Masi</surname></persName>
							<email>iacopo.masi@usc.edu</email>
							<affiliation key="aff0">
								<orgName type="department">USC Institute for Robotics and Intelligent Systems (IRIS)</orgName>
								<address>
									<settlement>Los Angeles</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Rawls</surname></persName>
							<email>srawls@isi.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">USC Information Sciences Institute (ISI)</orgName>
								<address>
									<addrLine>Marina Del Rey</addrLine>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">GÃ©rard</forename><surname>Medioni</surname></persName>
							<email>medioni@usc.edu</email>
							<affiliation key="aff0">
								<orgName type="department">USC Institute for Robotics and Intelligent Systems (IRIS)</orgName>
								<address>
									<settlement>Los Angeles</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prem</forename><surname>Natarajan</surname></persName>
							<email>pnataraj@isi.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">USC Information Sciences Institute (ISI)</orgName>
								<address>
									<addrLine>Marina Del Rey</addrLine>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Pose-Aware Face Recognition in the Wild</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:44+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a method to push the frontiers of unconstrained face recognition in the wild, focusing on the problem of extreme pose variations. As opposed to current techniques which either expect a single model to learn pose invariance through massive amounts of training data, or which normalize images to a single frontal pose, our method explicitly tackles pose variation by using multiple posespecific models and rendered face images. We leverage deep Convolutional Neural Networks (CNNs) to learn discriminative representations we call Pose-Aware Models (PAMs) using 500K images from the CASIA WebFace dataset. We present a comparative evaluation on the new IARPA Janus Benchmark A (IJB-A) and PIPA datasets. On these datasets PAMs achieve remarkably better performance than commercial products and surprisingly also outperform methods that are specifically fine-tuned on the target dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>There has been a flurry of advances in face recognition in recent years, with some techniques claiming to have met <ref type="bibr" target="#b28">[29]</ref> or even surpassed <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b24">25]</ref> human face verification performance. It is common to see saturated accuracy under certain conditions on the standard LFW benchmark.</p><p>Recognizing that current face verification systems still have shortcomings under real-world conditions, a new benchmark (IJB-A) has been proposed in <ref type="bibr" target="#b12">[13]</ref>. This is a publicly available benchmark to encourage researchers to focus on novel issues in face recognition in the wild. The IJB-A dataset focuses especially on variations in pose and represents a more challenging benchmark compared to LFW. <ref type="figure" target="#fig_0">Fig. 1</ref> shows the distribution of yaw angles in LFW and in the newly released IJB-A dataset. As can be seen, the IJB-A dataset encompasses a wider variety of face poses than LFW and for this reason in this work we propose a method that is entirely designed to overcome variations in pose. In addition, the IJB-A dataset introduces a new testing protocol which more closely matches real-world use cases. Instead of evaluating image pairs, the IJB-A protocol evaluates template pairs, where a template is a set of one or more images. This could represent the more realistic use-case where end-users have multiple images of a single subject. Since templates can contain images of a subject over multiple poses, it becomes important to consider how to handle template matching and how the pose variation takes part in the matching process. To the best of our knowledge, as also pointed out in <ref type="bibr" target="#b22">[23]</ref>, none have addressed pose variation in IJB-A.</p><p>The contribution of this paper is that we propose to take into account pose variability by training multiple posespecific models, and exploiting those models when matching images of varying poses. While most previous approaches rely only on a single frontal-pose model <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b4">5]</ref>, possibly normalizing images via frontalization <ref type="bibr" target="#b28">[29]</ref>, we propose to handle pose variability by learning Pose-Aware Models (PAMs) for frontal, half-profile and full-profile poses. We partition and augment the training dataset considering the training pose distribution and then co-train deep Convolutional Neural Networks (CNNs) to learn Pose-Aware Models. PAMs are then used to perform poseaware face recognition in the wild. Moreover, differently from <ref type="bibr" target="#b32">[33]</ref> that uses multi-task learning and from the multiview perceptron (MVP) method <ref type="bibr" target="#b35">[36]</ref>, we use rendering technique to generate a synthetic view, instead of training the network to interpolate between views. PAMs outperform a single pose-agnostic model and yield state-of-theart results on IJB-A. Moreover PAMs outperform Deep-Face <ref type="bibr" target="#b28">[29]</ref> on the new PIPA dataset despite having significantly less training data.</p><p>The paper is organized as follows: in Sect. 2 we review papers that address pose-invariance and face recognition in the wild. In Sect. 3 we present the overview about PAMs and in Sect. 4 we discuss how to learn Pose-Aware Models. In Sect. 5 we show how to use PAMs for face recognition. Sect. 6 presents experimental results, while we draw conclusions and discuss future research in Sect. 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Researchers have long acknowledged that matching techniques struggle to handle variations in pose. According to this, we review techniques to handle pose variation regarding state-of-the-art face recognition. Seminal papers in the past introduced the idea of training face classifiers at different poses, for example, the effort in <ref type="bibr" target="#b20">[21]</ref> that extends the basic Eigenface approach to multiple poses. Recently, methods for pose-invariance have focused primarily in dealing with controlled datasets, such as Multi-PIE <ref type="bibr" target="#b8">[9]</ref>. The authors in <ref type="bibr" target="#b16">[17]</ref> are the first to introduce a 3D average face model rather than relying on 3D cylindrical or ellipsoid model. Prabhu et al. in <ref type="bibr" target="#b21">[22]</ref> proposed an efficient way to estimate a 3D model from a single frontal image using a Generic Elastic Model (GEM). The method has been further improved considering diverse average values of depth per ethnic group <ref type="bibr" target="#b10">[11]</ref>. It was noteworthy because they were one of the first to match frontal versus profile images using rendering. Moreover, 3D data have been used for matching rendered images with 2D face imagery in the wild, accounting for small pose variations <ref type="bibr" target="#b19">[20]</ref> and, additionally, other researchers introduced the idea of rotating a face to get different training poses <ref type="bibr" target="#b6">[7]</ref>. Recent papers focus on normalizing the pose from a profile face to a canonical frontal view. The paper in <ref type="bibr" target="#b1">[2]</ref> is the first paper that reports the improvement in face recognition by rendering a profile face to frontal and others have also followed this approach, proposing different methods for the same underlying idea of "frontalization" <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b7">8]</ref>. In contrast to these methods, Sharma et al. <ref type="bibr" target="#b25">[26]</ref> proposed the Discriminant Multiple Coupled Latent Subspace framework to address pose variations. Regarding methods that trained a CNN to recognize faces, one approach to obtain pose-invariance is to train a single CNN with a large enough dataset covering a diverse set of poses so that the CNN in principle could learn some degree of pose invariance automatically: FaceNet <ref type="bibr" target="#b24">[25]</ref> shows that it is possible to learn a compact embedding for faces with an end-to-end learning system trained on 260 million images. DeepID <ref type="bibr" target="#b26">[27]</ref> uses a large ensemble of networks trained on different patches of the face along with Joint Bayesian metric learning, showing remarkable performance. This work has been extended in <ref type="bibr" target="#b27">[28]</ref> to show how the CNN is learning sparse features that implicitly encode attribute informations such as the gender.</p><p>Another approach is to apply pose-normalization to a frontal view as a preprocessing step <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b4">5]</ref>. For instance, DeepFace <ref type="bibr" target="#b28">[29]</ref> learns a CNN on 4 million face images using frontalization technique to reduce pose variability. Wang et al. <ref type="bibr" target="#b29">[30]</ref> showed that is possible to perform accurate face identification on a gallery of 80 million images and on the IJB-A benchmark. Moreover, Chen et al. in <ref type="bibr" target="#b4">[5]</ref> showed that is possible to get compelling results on IJB-A by using a single CNN trained from scratch on a frontal view, fine-tuning it and learning the metric on the target dataset. Lately, Zhu et al. <ref type="bibr" target="#b36">[37]</ref> showed that a CNN can be used, not only for classification, but to recover and normalize a near-frontal face to a frontal view. A similar frontalization idea is developed in <ref type="bibr" target="#b9">[10]</ref> using a generic 3D model and a rendering framework with a soft-symmetry technique to compensate for self-occlusion, without the use of a neural network.</p><p>Researches also tried to let the network disentangle the identity and the view by either performing multi-task learning <ref type="bibr" target="#b32">[33]</ref> or multi-view perceptron <ref type="bibr" target="#b35">[36]</ref>. The drawback of these latter methods is that are only trained on constrained images on the Multi-PIE dataset in which the pose is manually specified, without reporting performance on benchmarks in the wild such as IJB-A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Pose-Aware Face Models for Recognition</head><p>Our method assumes that in general the face pose distribution p(p|I), given one image I, is not dominated by nearfrontal faces and thus we propose to learn multiple posespecific CNN models as opposed to a single CNN. Assuming detected landmarks on an image, we observe that it is easy to compensate for roll when the face is near-frontal, and for pitch, when the face is near profile by just using inplane alignment. Thus we focus our models to compensate mainly for yaw variations, by assuming p(p|I) â p(Ï|I), where Ï represents the face yaw angle. Another observation is that compensating for out-of-plane variations using frontalization <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b9">10]</ref> could be a noisy process that gets harder as input images move closer to profile. For this reason, we propose a method that extends the concept of frontalization to multiple modes of the pose distribution. In  addition, our method considers different ways of aligning a face since the latter is very important in face recognition. Our motivation is that in presence of images in the wild, complex methods of alignment such as those compensating out-of-plane rotation could produce weak results when dense landmarks are difficult to localize; in this case, a safer way of alignment (in-plane alignment) could work better, requiring only few anchor points. We have observed that poor quality landmarks can seriously effect the quality of rendered images, and thus cause poor recognition performance. Currently, besides the use of in-plane alignment, we do not do anything to mitigate this, but we plan to investigate methods of incorporating landmark confidence into our approach in the future.</p><p>For each model a particular alignment process is applied specifically to the pose we consider. In particular we apply the concept of multi-alignment using the following:</p><p>2D in-plane alignment: Image are aligned in plane with a 2D non-reflective similarity that compensates scale, inplane rotation and translation. In these images important properties of face geometry are preserved along with discriminative part of the head such as hair or ears. The drawback is that these images contain high variability in the pose. We use one model to align images to a frontal reference, while the other to a profile one.</p><p>3D out-of-plane alignment: Out-of-plane rotation is explicitly compensated by rendering images at a specific yaw value, in order to adjust the pose and remove pose variations. Unlike <ref type="bibr" target="#b28">[29]</ref> that uses an adaptive 3D face shape for face modeling, we use an unmodified 3D generic face model, following the idea in <ref type="bibr" target="#b9">[10]</ref>. Nevertheless, differently from both <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b9">10]</ref> that use only frontalization (0 â¢ ), we also render images to half-profile (40 â¢ ) and full-profile (75 â¢ ) views in order to cope for extreme yaw variations. Considering these two types of alignment, we trained an ensemble of five CNN models, each of which is "aware" of the face viewpoint by learning specific features for each view. Our models are called Pose-Aware CNN Models (PAMs) and are learned using the CASIA WebFace dataset <ref type="bibr" target="#b31">[32]</ref>, which is currently the largest publicly available dataset, containing roughly 500K face images. <ref type="figure" target="#fig_1">Fig. 2</ref> shows an overview of our approach along with different kinds of alignment used in our method. In the next section we show how PAMs can be learned automatically taking into account the pose distribution of the training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Learning Pose-Aware CNN Models (PAMs)</head><p>Differently from approaches that use just a single, frontal face reference to train a CNN <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b4">5]</ref>, our idea is to learn Pose-Aware CNN Models (PAMs). Firstly, one issue is that we do not have access to millions of data covering all the possible poses: the CASIA WebFace training pose distribution is still biased towards frontal, nevertheless there are still some profile images that could be exploited, as can be seen from <ref type="figure" target="#fig_2">Fig. 3</ref>. Secondly, it is necessary to consider that CNN generalization power is usually proportional to the training data size, thus we need to trade-off between data partitioning and clustering when processing the training dataset. In contrast to approaches relying on multi-task learning <ref type="bibr" target="#b32">[33]</ref> or <ref type="bibr" target="#b35">[36]</ref>, that models the identity and face view with the same network, we treat each type of alignment and data independently, that is, we learn a specific model for each type of alignment and mode of the pose distribution. The main motivation for this is that having multiple networks permits cotraining them in order to improve transferability of learned features. We have found this particularly important for generalization over other datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Discovering the training pose distribution</head><p>We use the approach in <ref type="bibr" target="#b2">[3]</ref> to detect landmarks using a fixed bounding box in CASIA WebFace since most of the faces are centered on the image. From the detected landmarks on CASIA, we estimate the pose of the face in the image putting in correspondence 2D detected landmarks l â R 2Ã|J| with 3D labeled landmarks L . = M(J) â R 3Ã|J| on a 3D generic model M, where J is a set of indices that selects the corresponding landmarks on the 3D model. We can then estimate a perspective camera model mapping the generic 3D model M on the image such as:</p><formula xml:id="formula_0">l = p L (1) where p = K [R t] .<label>(2)</label></formula><p>We use the PnP method to estimate external camera parameters, assuming the principal point in the image center and then refine the focal length by minimizing landmark reprojection error.</p><p>From p, we extract the rotation matrix R â R 3Ã3 that contains the 3D rotation parameters of the model with respect to the image. By decomposing R we obtain the yaw values Ï of the face across all the dataset w.r.t to the 3D generic model. We accumulate all the {Ï i } N i=1 values in order to estimate the training pose distribution p(Ï|I).</p><p>Instead of treating all the images as belonging to the same frontal model, irrespective of the yaw distribution, we express the yaw variability. In order to get the main modes of the training yaw distribution p(Ï|I) we run K-means on {Ï i } N i=1 to find the main T modes:</p><formula xml:id="formula_1">Î¨ = {Âµ Ït } T t=1 ,<label>(3)</label></formula><p>along with the hard-assignment of each image to a certain mode. We can interpret the latter as a function that maps each image to a specific mode as:</p><formula xml:id="formula_2">Î´(I) = t where t â [1 . . . T ]<label>(4)</label></formula><p>and T represents the five models we want to learn. Each Âµ Ït represents a mode in the yaw distribution and Î´(Â·) gives the assignments to each t-th mode, for each image. We found the modes pretty balanced each other and to be centered roughly in {0 â¢ , Â±15 â¢ , Â±30 â¢ }; moreover most of the images are concentrated into the two frontal modes, confirming that this dataset is still biased towards frontal faces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">PAMs for in-plane alignment</head><p>Starting from Eq. <ref type="formula" target="#formula_1">(3)</ref>, we exploit face symmetry property to simplify our representation. By flipping one direction of yaw distribution as Âµ Ït â |Âµ Ït |, we flip the corresponding images along the vertical axes and modify the assignments in Eq. (4) accordingly. In this way we can consider only one side of the distribution p(Ï|I), for example left side, reducing the number of models we need to train. We finally have a new set of modes Î¨ â² = {Âµ frontal , Âµ near-frontal , Âµ profile }, corresponding to yaw values centered in {0 â¢ , +15 â¢ , +30 â¢ }. This allows us to express p(Ï|I) as a bi-modal distribution by partitioning the dataset in two classes: near frontal faces with small variability and profile faces with high variability in pose. In particular we partition the images using the image assignments in Eq. (4), classifying one image as profile if belongs to the third mode Âµ profile and frontal otherwise. In this way, we are able to partition the CASIA dataset in two different new datasets, that are used to learn two CNN models with in plane alignment namely PAM in-f and PAM in-p . Since we divided frontal images from profile ones, we are able to perform different types of 2D in-plane alignment for each set: the frontal set uses nine most reliable landmarks, while the profile set is aligned using the tip of the nose and the center of the two eyes. For both the alignments we use a non-reflective similarity transformation S(s, Î¸, t x , t y ). The parameters for scale, in-plane rotation an translation are recovered by solving a linear system of equations using detected and reference landmarks, specific for each alignment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">PAMs for out-of-plane alignment</head><p>There are still unresolved issues about the process presented in Sect. 4.2. The first one is that the images associated to each mode discovered in Eq. (3) have still intra-pose variability within the same mode that could be captured by the network and it is well known that pose variability drastically affects face recognition performance. Another issue is that is very hard to find publicly available datasets containing a large amount of full profile faces in order to learn a discriminative CNN model for a full-profile view. For all these reasons, in this section we learn also other models that compensate for out-of-plane rotation in order to minimize pose variability and tackle the lack of training data for profile faces. As in Sect. 4.2, in this case p(Ï|I) is expressed as multi-modal distribution with three prominent modes. We can exploit again Eq. (3) and face symmetry as done in</p><formula xml:id="formula_3">0Â°+ 40Â°+75Â°0Â°+ 15Â°+30Â°+75Â°+40Â°+ 40Â°0Â°( a) (b)</formula><p>Input.Images <ref type="figure">Figure 4</ref>: (a) The directed graph used to map each mode of CASIA yaw distribution to the desired mode (b) The process helps to render properly one image: if the face is frontal we can render to a frontal and half-profile view. If an image is far from being frontal we avoid frontalizing it.</p><p>Sect. 4.2 but here we can apply the concept of pose adjustment to a certain yaw value, as done before for frontalization for near-frontal faces. We would like our models to represent frontal, half-profile and full profile faces but our data are mostly centered on near-frontal faces. We propose to unbias the source, yaw distribution and its modes by transforming them to a new target distribution: we use the 3D generic model M and the estimated pose p in Eq.</p><p>(2) to render a specific face to a new mode of the desired distribution using dense facial landmarks. This new, target distribution is decided a priori to have frontal (0 â¢ ), half-profile (40 â¢ ) and a full profile views (75 â¢ ) in order to be able to learn a discriminative classifier for full-profile faces as well. These values are set in order to trade-off between having good rendered faces and generating side views. In particular we employ the directed graph in <ref type="figure">Fig. 4</ref> to decide how to render a target face; each edge in the graph represents the rendering process from a certain mode to a target one. The face rendering technique is derived from <ref type="bibr" target="#b9">[10]</ref> with soft-symmetry. Besides reducing pose variability for each mode, a benefit that we get also by rendering images is that, for each new target pose, we can increase the number of samples to train our models. The increasing factor is function of the number of images assigned to each source mode and the number of edges entering the target node. Considering this process, we are able learn three additional networks, one for each mode of the new desired pose distribution, namely PAM out-0 , PAM out-40 , PAM out-75 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Fine-tuning PAMs</head><p>For each pose-specific dataset created in previous Sections, we train a Pose-Aware CNN. Since training from scratch a CNN could require millions of annotated images, we learn our Pose-Aware CNNs by fine-tuning state-of-theart CNN models trained on ImageNet. In our approach, we experiment with a CNN with 8 layers (AlexNet) <ref type="bibr" target="#b14">[15]</ref> and one with 19 layers (VGGNet) <ref type="bibr" target="#b3">[4]</ref>. We experiment with different network types since we can show that our method is agnostic to the CNN model used and by fusing across pose, we can get improvement, irrespective of the architecture used. All these CNN models end with fully connected layers fc7 and fc8. The output of fc8 is fed to a C-way SoftMax which gives a distribution over the subject labels C. Denoting x i (I) the i-th output of the network on a given image I, the probability assigned to the i-th class is the output of the SoftMax function p i (I) = e x i (I) C l=1 e x l (I) . The fine-tuning is performed through stochastic gradient descent (SGD) and standard back-propagation <ref type="bibr" target="#b15">[16]</ref>, minimizing the cross-entropy loss using the SoftMax function and the onehot vector of ground-truth class c over the entire training set of images. We start from pre-learned weights on ImageNet, initialize from scratch fc8 layer with parameters drawn from a Gaussian distribution with zero mean and standard deviation 0.01. The initial learning rate is set to Î± = 0.001. We fine-tune all the layers with this learning rate but the new fc8 layer which has a learning rate of an order of magnitude greater than Î±. We learn the the biases two times faster than the weights. Moreover we decrease Î± of an order of magnitude when a plateau is reached in the validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Co-training PAMs to improve transferability</head><p>The analysis provided in <ref type="bibr" target="#b33">[34]</ref> motivates our approach to fine-tune our models as opposed to training them from scratch, since million images are not available in our case. To improve transferability, we propose to co-train our models in order to get a better optimization point in the loss minimization. Differently from <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b28">29]</ref>, that use a single, trained-at-once model, we have different CNN models to address a specific view point of the face and a specific alignment. By exploiting multiple models we opti- mize the same objective function but from different viewpoints and data. Considering in-plane aligned models, we only fine-tune the frontal network from ImageNet prelearned weights (PAM in-f ); we found experimentally that the PAM in-p fine-tuned from ImageNet was performing poorly since we have few profile images, thus we co-train it resuming the optimization from PAM in-f weights. Regarding out-of-plane alignment, we start to fine-tune PAM out-0 from ImageNet. Then we fine-tune PAM out-40 , starting from PAM out-0 weights. We keep iterating by alternating the co-training until the validation accuracy saturates in both models. <ref type="figure" target="#fig_3">Fig. 5</ref> shows the steep increase in the validation accuracy provided by the co-training using AlexNet. After these models are trained, we finally proceed to co-train PAM out-75 from PAM out-40 . We performed the same process for VGGNet, but this deeper model requires less steps of co-training to get the validation accuracy saturated. To perform recognition, we use the fc7 layer response x â R D .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Pose-Aware Face Recognition</head><p>The Pose-Aware CNN models learned in Sect. 4 provide a way to perform pose-aware face recognition. We can interpret each CNN as a discriminative classifier explicitly trained at a certain mode of the pose distribution. In the following we describe the general recognition procedure which is applied for each of the ensemble PAMs defined by { PAM in-f , PAM in-p , PAM out-0 , PAM out-40 , PAM out-75 } Main Matching Process: In the matching process, we exploit the face symmetry in the same way we did in the training set to align or render a face to one side and then flip it back, if needed, to be aligned to the corresponding PAM. The score between a feature pair (x 1 , x 2 ) is given by the correlation as:</p><formula xml:id="formula_4">s(x 1 , x 2 ) = (x 1 âx 1 )(x 2 âx 2 ) T ||x 1 âx 1 || ||x 2 âx 2 || wherex = 1 D d x.</formula><p>In case of multiple images per template, each single PAM performs template score pooling using a weighted average of the pair-wise scores, where each weight is function of the score using an exponential function as exp Î³ s(x 1 , x 2 ) . Finally, we average again all the the responses â Î³ â [0..20].</p><p>Matching with in-plane alignment: Given a testing image I, we firstly detect landmarks and classify the pose using the Âµ profile corresponding to the mode found for the profile faces in Sect. 4.2. In particular as frontal if |Ï| â¤ Âµ profile otherwise as profile. In a general case, if a template contains multiple images (set-to-set matching), we proceed to align and forward each face image either to PAM in-f or PAM in-p , accordingly to the classified pose.</p><p>Matching with out-of-plane alignment: We extend the concept of frontalization by rendering faces to the modes {0 â¢ , 40 â¢ , 75 â¢ } we have defined in Sect. 4.3. We render each image always to the half-profile view (40 â¢ ) and then if the image is classified as near-frontal we frontalize to 0 â¢ otherwise we render the image to the profile view (75 â¢ ). In this case we use the PAM out-0 , PAM out-40 , PAM out-75 accordingly.</p><p>Pose Fusion: Assuming a template pair (or an image pair) to be recognized, then we have at most five scores, each of which is produced by a specific PAM. In our current approach we simply pool the available scores together by average. We found this to provide a good baseline in all our experiments. <ref type="figure" target="#fig_4">Fig. 6</ref> shows the performance in terms of ROC and CMC for each single PAM as described in Sect. 5 and the pose fusion (magenta curve) on JANUS CS2 using VGGNet. In our experiments we got improvement as well, by fusing across poses using AlexNet. Note that since we classify the pose, some models could not trigger on some images, that is why in general the performance for profile is low in <ref type="figure" target="#fig_4">Fig. 6</ref>. We have also tried simply to forward all the images to all the PAMs but we found the proposed pose fusion the more stable result in all our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experimental Results</head><p>In this section we present the experimental results on three datasets in the wild. We are interested in testing our method on imagery containing extreme pose variations, so we report the performance on the new IARPA JANUS Benchmark-A (IJB-A) <ref type="bibr" target="#b12">[13]</ref> and People In Photo Albums (PIPA) <ref type="bibr" target="#b34">[35]</ref> dataset. We apply PCA and Power Transformation <ref type="bibr" target="#b23">[24]</ref> (PT) to the features on the training split of each dataset used in this paper. In all the experiments, no supervised method is applied except for the learning of PAMs on CASIA WebFace. That is, differently from <ref type="bibr" target="#b4">[5]</ref>, we do not re-train our models using the training splits of each dataset. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">IARPA Janus Benchmark A (IJB-A)</head><p>IJB-A is a new publicly available challenge proposed by IARPA and spread by NIST 1 to push frontiers of face recognition in the wild since lately LFW <ref type="bibr" target="#b11">[12]</ref> performance saturated. Both IJB-A and JANUS CS2 share the same 500 subjects under extreme conditions regarding pose, expression and illuminations. IJB-A considers the more difficult pairs compare to the JANUS CS2 splits. The IJB-A evaluation protocol mainly consists of face verification (1:1) and face identification <ref type="figure" target="#fig_0">(1:N)</ref>. The interesting thing about this dataset is that each subject is described by a template containing a set of images or frames extracted from videos. In order to have a fair comparison with the other methods, we removed the overlapping subjects of CASIA WebFace with JANUS CS2 and IJB-A while learning PAMs.</p><p>Component analysis: In Tab. 1 we report the improvement for each component of PAMs for two types of CNN models that we used (AlexNet and VGGNet). We show the performance on both JANUS CS2 and IJB-A splits reporting the TAR at FAR=0.01 for the verification protocol and the Recognition Rate at Rank-10 for the identification protocol. The table shows that significant improvement in performance is given by the co-training. The improvement given by the co-training is much bigger with AlexNet model respect to the VGGNet but still, even in this latter case, helps performance. Further improvement is obtained by applying PCA and Power Transformation.</p><p>Comparison to a single, frontal model: In <ref type="figure" target="#fig_6">Fig. 7</ref> we show the improvement of PAMs respect to learning a single CNN trained starting from CASIA WebFace with standard inplane alignment (corresponding to PAM in-f ). In this experiment we use VGGNet for both the methods. We show that pose-aware face recognition greatly improves over a single network trained, as previously done before <ref type="bibr" target="#b31">[32]</ref>, on imagery which pose distribution is dominated by near-frontal faces. Moreover, in order to better factor the effect of pose on performance, we designed an experiment using only imageto-image comparisons of IJB-A data, classifying the pose of each image using our method. In <ref type="figure" target="#fig_5">Fig. 8</ref> we report how image-to-image TAR varies by pose. In particular, <ref type="figure" target="#fig_5">Fig. 8</ref> shows that while a large part of our gain comes from improved matching of same-pose images, we also make a useful improvement to comparisons of different-pose images. Thus our PAM approach does improve pose-invariance over a single, frontal model. Comparison with state-of-the-art:</p><p>In Tab. 2 we report a comparison with the state-of-the-art. Pose-aware face recognition reports better performance compared to the COTS and GOTS (Commercial and Government Off-the-Shelf) systems and Fisher Vector encoding method using frontalization <ref type="bibr" target="#b5">[6]</ref>. If we compare with methods that use deep-learned features, PAMs show better performance than the method in <ref type="bibr" target="#b29">[30]</ref> which exploits seven networks and fuses the result with a commercial system. It is worth to mention that PAMs improve in IJB-A verification over <ref type="bibr" target="#b29">[30]</ref> of about 9% TAR at FAR=0.01 and 14% TAR at FAR=0.001, showing also better recognition rate at rank-1. Surprisingly, PAMs show even a better ROC with respect to methods that explicitly fine-tuned the network on the IJB-A training set and performed metric-learning on these sets. In particular we improve over <ref type="bibr" target="#b4">[5]</ref> of about 2% TAR at FAR=0.01 and 6% TAR at FAR=0.001 for JANUS CS2 splits and 4% TAR at FAR=0.01 for IJB-A. Overall, we sensibly improve over <ref type="bibr" target="#b4">[5]</ref> on all the metrics except for the recognition rate in IJB-A identification, in which our method is less effective to rank the gallery. Finally, in Tab. 2 we show also a comparison with the popular method of frontalization, which corresponds to using the frontalized PAM out-0 on all the images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">People In Photo Albums (PIPA)</head><p>The authors in <ref type="bibr" target="#b34">[35]</ref> recently introduced People In Photo Albums (PIPA) dataset, which is composed of public photo albums uploaded to Flickr. One of the characteristics of the dataset is that it contains extreme pose variations. They use the data to perform pose invariant person recognition using multiples cues (face, body, poselets) but they also measure face recognition performance using a subset of the data. We follow the same protocol as in <ref type="bibr" target="#b34">[35]</ref>, which uses  <ref type="table">Table 2</ref>: Comparative performance analysis on IJB-A benchmark and CS2 for verification (ROC) and identification (CMC). Symbol "-" indicates that the metric is not available for that protocol. Standard deviation is not available for all the methods. two-fold cross-validation on the face recognition subset of PIPA 2 . Differently from <ref type="bibr" target="#b34">[35]</ref>, we do not train a classifier for each subject but we classify each subject over the 581 identities as described in Sect. 5. We first run an experiment using only a single-pose frontal model, which matches the reported DeepFace performance of 47.97%. Next we run PAMs which achieved 57.65%. Furthermore, while Deep-Face trained SVM classifiers on the PIPA training set, our method made no supervised use of PIPA data. It is interesting to show that PAMs improve also over AlexNet trained on ImageNet but evaluated on body bounding box in the PIPA dataset (56.07%). We are aware that by using multiple cues (face, body, poselets) is possible to achieve higher recognition: we are interested to show that with PAMs there is still useful information to exploit in profile faces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion and Discussion</head><p>In this paper we proposed a pose-aware method to perform face recognition with imagery containing extreme pose variation. Our approach shows how we can rely not only on a single, frontal model but also on half-profile and full profile models to perform face recognition in the wild. Our approach is agnostic to the underlying CNN used. The main direction for the future is in mitigating the landmark detection errors and to assess pose-invariance in a more controlled dataset such as the Multi-PIE dataset. In particular</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Top: images with increasing yaw angle from left to right in LFW (blue) and IJB-A (red). Bottom: Pose distributions (yaw angle) in the two datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Given a template pair to verify, pose classification is used to forward each image to the corresponding Pose-Aware CNN Model. Given multiple images, each model extracts features, matches them and pools scores at a template level independently. Finally, the contribution of each model is pooled into a single, final score. Note how in our approach we use rendering technique to adjust the pose to a frontal (0 â¢ ), half-profile (40 â¢ ) and full-profile view (75 â¢ ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>(a) Average image obtained over the training set considering all the face imagery to learn a single model. Frontal pose absorbs the other poses. (b) Average images on the training set when pose is considered.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Steep increase in validation accuracy in function of iterations in the fine-tuning process. Each curve represents a step of co-training. Iterations are shown in log scale.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Fusing across pose improves the ROC in the verification protocol and the CMC in the identification protocol on JANUS CS2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 :</head><label>8</label><figDesc>Image-to-Image TAR across poses: PAMs (right) show better pose invariance than the single model (left).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Improvement in the ROC (a) and in the CMC (b) comparing a single CNN and the proposed PAMs on the IJB-A challenge and JANUS CS2. Horizontal axes are shown in log scale. Each curve shows also the standard deviation.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">IJB-A is available under request at http://www.nist.gov/ itl/iad/ig/ijba_request.cfm</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Data and splits are available at http://www.cs.berkeley. edu/Ënzhang/piper.html in order to make the approach more robust to landmark detector failures, we plan to either extend the method by using a confidence value for detected landmarks or having a secondary mechanism to assess the quality of rendered images. Additional future work consists in learning a better pose fusion and developing a single, multi-pose CNN in a unified framework. More details using different landmark detectors and various deep feature responses can be found in<ref type="bibr" target="#b0">[1]</ref>.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research is based upon work supported in part by the Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA), via IARPA 2014-14071600010. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of ODNI, IARPA, or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purpose notwithstanding any copyright annotation thereon. Moreover, we gratefully acknowledge USC HPC for hyper-computing and the support of NVIDIA Corporation with the donation of a NVIDIA Titan X. We would like to thank all the other authors of <ref type="bibr" target="#b0">[1]</ref> for their effort on the project. Moreover, thank A. D. Bagdanov, L. Ballan and the three anonymous reviewers for their fruitful comments that improved the manuscript.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Face recognition using deep multi-pose representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Abdalmageed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rawls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Masi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leksut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Natarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Medioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Fully automatic pose-invariant face recognition via 3D pose normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Asthana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Marks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Constrained local neural fields for robust facial landmark detection in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Baltrusaitis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-P</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshops</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Return of the devil in the details: Delving deep into convolutional nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chatfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Unconstrained face verification using deep CNN features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.01722v1</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Unconstrained face verification using fisher vectors computed from frontalized faces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BTAS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Dictionary-based face recognition from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Dictionary learning based 3D morphable model construction for face recognition with varying expression and pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lisanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Berretti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Del</forename><surname>Bimbo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
		<title level="m">Multi-PIE. Image Vision Computing</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Effective face frontalization in unconstrained images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Paz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Enbar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Gender and ethnicity specific generic elastic models from a single 2D image for novel 2D pose face synthesis and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savvides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2341" to="2350" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Labeled faces in the wild: A database for studying face recognition in unconstrained environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007-10" />
			<pubPlace>Amherst</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Massachusetts</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report 07-49</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Pushing the frontiers of unconstrained face detection and recognition: IARPA Janus Benchmark-A</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">F</forename><surname>Klare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Taborsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blanton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Grother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Burge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Open source biometric recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Klontz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Klare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Klum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Taborsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Burge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BTAS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Gradientbased learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Maximizing intraindividual correlations for face recognition across pose differences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Surpassing human-level face verification performance on LFW with GaussianFace</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Pose independent face recognition by localizing local binary patterns via deformation components</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Masi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Del</forename><surname>Bimbo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Medioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Using 3D models to recognize 2D faces in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Masi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lisanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bagdanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Del</forename><surname>Bimbo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">View-based and modular eigenspaces for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pentland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Moghaddam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Starner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Unconstrained poseinvariant face recognition using 3D generic elastic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Prabhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savvides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1952" to="1961" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learned-Miller. Face identification with bilinear CNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roychowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Image classification with the fisher vector: Theory and practice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>SÃ¡nchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">IJCV</biblScope>
			<biblScope unit="page" from="222" to="245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">FaceNet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Jacobs. Robust pose invariant face recognition using coupled latent space discriminant analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Haj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVIU</title>
		<imprint>
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1095" to="1110" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep learning face representation from predicting 10,000 classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deeply learned face representations are sparse, selective, and robust</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">DeepFace: Closing the gap to human-level performance in face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Face search at scale: 80 million gallery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Otto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.07242</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Towards pose robust face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3539" to="3545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Learning face representation from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.7923</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Rotating your face using multi-task deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">How transferable are features in deep neural networks? In NIPS</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lipson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Beyond frontal faces: Improving person recognition using multiple cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Multi-view perceptron: a deep model for learning face identity and view representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1404.3543</idno>
		<title level="m">Recover canonicalview faces in the wild with deep neural networks</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
