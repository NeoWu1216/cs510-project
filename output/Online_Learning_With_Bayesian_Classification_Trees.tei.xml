<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Online Learning with Bayesian Classification Trees</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">Rota</forename><surname>Bulò</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">FBK-irst Trento</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Kontschieder</surname></persName>
							<email>pkontschieder@gmail.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Cambridge</orgName>
								<address>
									<settlement>Mapillary Graz</settlement>
									<country>UK, Austria</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Online Learning with Bayesian Classification Trees</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:44+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Randomized classification trees are among the most popular machine learning tools and found successful applications in many areas. Although this classifier was originally designed as offline learning algorithm, there has been an increased interest in the last years to provide an online variant. In this paper, we propose an online learning algorithm for classification trees that adheres to Bayesian principles. In contrast to state-of-the-art approaches that produce large forests with complex trees, we aim at constructing small ensembles consisting of shallow trees with high generalization capabilities. Experiments on benchmark machine learning and body part recognition datasets show superior performance over state-of-the-art approaches.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Random classification forests are often the preferred machine learning tool due to their efficiency, scalability, and robustness. They found successful application in many areas such as computer vision <ref type="bibr" target="#b23">[21]</ref>, bio-informatics <ref type="bibr" target="#b28">[26]</ref>, medical data analysis <ref type="bibr" target="#b11">[9]</ref>, data-mining <ref type="bibr" target="#b26">[24]</ref>, etc.</p><p>Random forests are usually trained in batch (or offline) modality, i.e. all training data is expected to be given in advance and once a forest is trained it cannot be updated with new, labelled samples. However, there are application domains where the training data is not available in advance, but is collected over time, and inference might be required at any moment. Online learning approaches can serve such purposes and have a number of advantages over batch methods: they are more flexible as they can work in the presence of online data generation processes, can adapt to distributions varying over time, do not need to store the training set during the learning phase, to name just a few.</p><p>Joining the online learning paradigm with classification trees is particularly difficult due to the recursive nature of this classifier. Moreover, the presence of (typically) deterministic split decisions within the tree complicates using new data samples to correct decisions that were taken at an earlier level. Consequently, the number of works addressing online learning within decision trees in the literature is rather small and we review the most related ones next. The Hoeffding tree algorithm <ref type="bibr" target="#b13">[11]</ref> maintains a set of candidate splits in the leaves and tracks their quality as new data arrives. The Hoeffding bound is used to control the amount of data that should be collected before a probably-optimal split selection can be ensured. In <ref type="bibr" target="#b22">[20]</ref> a similar idea is pursued, but the leaf splitting condition changes and an online bagging <ref type="bibr" target="#b21">[19]</ref> strategy is adopted. A modification over <ref type="bibr" target="#b22">[20]</ref> was presented in <ref type="bibr" target="#b25">[23]</ref>, which uses reservoir sampling to keep track of a fixed-length, unbiased set of training samples for updating the trees. The work in <ref type="bibr" target="#b4">[2]</ref> also extends the Hoeffding tree algorithm by introducing an adaptivesize version, allowing to mix differently-sized trees. Their approach imposes restrictions on the number of split nodes such that shallower trees can adapt more quickly to changes in the distribution of the incoming data stream while deeper trees adapt more slowly and therefore maintain a longerterm memory. The work in <ref type="bibr" target="#b12">[10]</ref> also maintains sets of candidate splits in the leaf nodes. There, the authors provide a decision forest construction algorithm, which dynamically partitions the data into structure and estimation samples, the former being used to influence the tree structure and the latter being used to estimate the leaf predictions. The work of <ref type="bibr" target="#b15">[13]</ref> presents an alternative approach by governing the tree growing phase by a Mondrian process. There, label distributions are kept at each node and controlled by a hierarchy of normalized, stable processes.</p><p>A remaining limitation of state-of-the-art online algorithms for random forests is a tendency to producing oversized trees, as the split selection process is unaware of the tree complexity and split functions are very simple. Moreover, trees tend to overfit, thus requiring large forests to achieve good performance, which however implies slowdown of the inference process and increased memory requirements. In particular, the algorithms that keep candidate splits in the leaves during training suffer from a prohibitive memory cost as trees get deep <ref type="bibr" target="#b12">[10]</ref> or require multiple passes over the training data <ref type="bibr" target="#b12">[10,</ref><ref type="bibr" target="#b22">20]</ref>.</p><p>Contributions. In this paper, we introduce a novel online learning algorithm for Bayesian classification trees that tries to overcome the aforementioned limitations, by taking a different perspective. Our goal is to learn tree classifiers with a shallow structure and high generalization capability. We trade shallow tree structures for more complex split decision functions that jointly take into account multiple feature dimensions and their correlations. Our online learning procedure is a Bayesian approach that iteratively replaces a posterior distribution over trees, obtained after observing a new training sample, with a simpler, parametric distribution. This surrogate posterior is determined within the parametric family in a way to fit the updated posterior distribution with the minimum loss of information. Our algorithm is characterized by update rules for the tree hyper-parameters that are free from cumbersome learning rate selection and allow us to naturally absorb the information carried by each new sample. Due to the Bayesian learning principle, which takes the uncertainty about the tree's parameters into account, we can obtain tree classifiers that do not overfit. In summary, the proposed method matches our initial intention of obtaining ensembles containing few, shallow and well-generalizing trees. The provided experimental evaluation confirms our model choice and its benefits are shown with quantitative experiments on standard benchmark machine learning and more complex body part recognition datasets.</p><p>Relationship to Bayesian tree models. Bayesian models for decision trees have appeared in the literature for offline learning -namely the Bayesian hierarchical mixture of experts (BHMEs) <ref type="bibr" target="#b5">[3]</ref>, Bayesian CART models <ref type="bibr" target="#b7">[5,</ref><ref type="bibr" target="#b8">6,</ref><ref type="bibr" target="#b10">8,</ref><ref type="bibr" target="#b27">25]</ref>, Bayesian BART models <ref type="bibr" target="#b9">[7]</ref> -and for the online learning modality with dynamic tree models <ref type="bibr" target="#b24">[22,</ref><ref type="bibr" target="#b3">1]</ref>. Our model differentiates from these approaches since we are proposing a parametric, Bayesian model for decision trees that is trained in an online fashion, i.e. we update over time a posterior distribution over the space of decision trees, whereas other similar approaches like BHMEs (despite sharing structural similarities) work in an offline fashion and consider sigmoid-gated hierarchical mixture of experts in the underlying model hypothesis space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Classification Trees</head><p>In this section we recap classification trees to provide all the necessary notation and definitions that are subsequently used for introducing our Bayesian online learning approach.</p><p>Consider a classification problem with input space X and a finite set of categorical labels Y. A classification tree is a classifier consisting of decision nodes and prediction nodes, arranged into a tree-structure. Decision nodes correspond to the tree's internal nodes N and are responsible for routing data samples to an appropriate prediction node (i.e. leaf) in L. Each decision node n ∈ N takes a routing decision for a data sample x ∈ X via a routing function b n : X → {0, 1}. If b n (x) = 1, then x is routed to the left sub-tree, otherwise it goes to the right one. Akin to conventional decision trees, we consider binary decision functions of the following type:</p><formula xml:id="formula_0">b n (x) = 1 θ ⊤ n ξn(x)≥0 ,<label>(1)</label></formula><p>where 1 P is an indicator function for the truth value of P . The decision function b n depends on a node-specific feature map ξ n : X → R dn , and a d n -dimensional parameter vector θ n ∈ R dn (see also <ref type="bibr" target="#b17">[15]</ref>). Starting from the root node and after visiting a number of decision nodes, x ends up in a prediction node, where the actual class assignment takes place. Indeed, each prediction node ℓ ∈ L holds a probability distribution π ℓ = (π ℓy ) y∈Y over labels in Y that will be used to deliver the final prediction for the data sample reaching it. A classification tree, denoted by t, is identified by its structure S and its parameters, i.e. t = (θ, π, S), where θ = {θ n } n∈N holds the parameters of all decision nodes and π = {π ℓ } ℓ∈L contains the class distributions of all prediction nodes. The structure of the tree comprises the set of nodes N , leaves L and their relations. Moreover, it includes the node-specific feature maps ξ n .</p><p>Given a tree t = (θ, π, S), the predictive distribution p (y|t; x) for data sample x is defined as</p><formula xml:id="formula_1">p (y|t; x) = ℓ∈L r(ℓ|t; x)π ℓy ,<label>(2)</label></formula><p>where π ℓy denotes the probability of a sample ending in leaf ℓ to take on class y, and r(ℓ|x) is regarded as the routing function, which is 1 for the leaf ℓ where sample x is actually routed to, and 0 elsewhere. To give an explicit form to the routing function, we introduce the following binary relations that depend on the tree structure: ℓ ւ n is true if ℓ belongs to the left subtree of node n, and n ց ℓ is true if ℓ belongs to the right subtree of n. By exploiting these relations we can factorize the routing function as</p><formula xml:id="formula_2">r(ℓ|t; x) = n∈N b n (x) 1 ℓւn (1 − b n (x)) 1 nցℓ .<label>(3)</label></formula><p>Although the product in (3) runs over all nodes, only the decision nodes along the path from the root node to the leaf ℓ are affected. 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Bayesian Online Classification Trees</head><p>In this section we present our novel online learning algorithm for classification trees, adhering to Bayesian principles. Our approach falls within the theoretical framework of Bayesian Online Learning (BOL) <ref type="bibr" target="#b20">[18]</ref>, also known as assumed density filtering in the control literature <ref type="bibr" target="#b16">[14]</ref>, and is related to expectation propagation <ref type="bibr" target="#b19">[17]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Overview</head><p>Let D i = {(x 1 , y 1 ), . . . , (x i , y i )} ⊂ X × Y denote a collection of i labelled examples, and assume to have a prior distribution p(t) defined over the set of decision trees. In standard Bayesian inference it is possible to compute the posterior distribution of t given the collection of data points D i by recursively employing the Bayes rule in the following way:</p><formula xml:id="formula_3">p (t; D i ) ∝ p (y i |t; x i ) p (t; D i−1 ) ,<label>(4)</label></formula><p>where p (t; D 0 ) = p(t). This formula captures the change in the posterior distribution due to an added training sample (x i , y i ), when the posterior distribution after i − 1 samples is treated as the new prior for the incoming data point. Although the rule in (4) seems to be structurally suitable for an online scenario, it cannot instantly be used for online learning, because in general it requires knowledge about the entire training set. However, if we could store the posterior from the previous (i − 1) samples, and compute the normalizing constant, we would obtain an online learning approach by repeatedly applying (4), without the need of revisiting past samples. The BOL framework implements this idea by recursively building a surrogate distribution for the true posterior p (t; D i ). The surrogate distribution is confined to a pre-defined, parametric family of distributions Q, which can be compactly stored. In the rest of the paper we will denote by q(t; h i ), or more compactly q i (t), the surrogate distribution of the true posterior of t given i samples, h i being the parametrization of the distribution.</p><p>The recursive construction of this surrogate distribution alternates a Bayes update step, integrating information conveyed by new training data akin to (4), with a projection step, which re-maps the obtained distribution in the parametric family Q.</p><p>Update step. Let q i (t) be the surrogate posterior distribution of t from i samples, which is taken as prior for the update step of a new data sample (x i+1 , y i+1 ). By application of the Bayes rule we obtain the following updated posterior distribution:</p><formula xml:id="formula_4">q i+1 (t) ∝ p (y i+1 |t; x i+1 ) q i (t) .<label>(5)</label></formula><p>The family Q, where q i belongs to, is typically selected in a way to make the computation of (5) tractable. This property however is not necessarily preserved by the distribution q i+1 in case we use it for a subsequent update. For this reason, a projection step is required.</p><p>Projection step. The projection step finds the best approximation ofq i+1 within the parametric family Q, in a way to minimize the loss of information. In this regard, the sought distribution q i+1 ∈ Q is the one that minimizes the following Kullback-Leibler (KL) divergence:</p><formula xml:id="formula_5">q i+1 ∈ arg min q∈Q D KL q i+1 q .<label>(6)</label></formula><p>After performing the projection, q i+1 can be regarded as a surrogate of the true posterior distribution p(t; D i+1 ) of t given i + 1 samples, which can be used as a new prior distribution for subsequent updates (see, <ref type="figure" target="#fig_0">Fig. 1</ref>).</p><p>Inference. At any time, we can use the current surrogate posterior distribution, say q i (t), to compute the posterior predictive distribution for a new data sample x. The posterior predictive distribution, denoted by p(y; x, h i ), provides the expected class distribution that we obtain for x with a decision tree t sampled from the surrogate posterior distribution q i :</p><formula xml:id="formula_6">p(y; h i , x) = E q i [p(y|t; x)]<label>(7)</label></formula><p>where E q i [·] denotes expectation with respect to q i . In the rest of the paper we will refer to (7) as the posterior predictive distribution for convenience, but the reader should keep in mind that it is actually an approximation of the true posterior predictive distribution p (y; x, D i ) that one obtains in standard (offline) Bayesian inference from the observed set of labelled samples D i . Indeed, we replace the true posterior p (t; D i ) with the surrogate posterior q i (t), which has been sequentially estimated from D i as previously detailed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Surrogate Posterior for Classification Trees</head><p>The key component of the online learning algorithm described above is the surrogate posterior. On one hand, we <ref type="figure" target="#fig_0">Figure 1</ref>. Example of the Bayesian online learning process: we start with a prior distribution q 0 (t) over the set of decision trees, and apply the update rule in <ref type="bibr" target="#b7">(5)</ref> to incorporate the information from the first sample (x1, y1). The obtained posteriorq 1 (t) is then reprojected in the parametric family of distributions Q using <ref type="bibr" target="#b8">(6)</ref>. The resulting distribution q 1 (t) is a surrogate distribution of the true posterior p(t; D1), which can be used as a new prior for the next update step. We keep iterating this process as new training samples arrive. At any moment, the most recent surrogate posterior q i can be used for inference on unlabelled data samples. would like to keep it simple such that the projection step in <ref type="bibr" target="#b8">(6)</ref> and the computation of the posterior predictive distribution in <ref type="formula" target="#formula_6">(7)</ref> remain tractable. On the other hand, we would like to have it complex and multi-modal to best possibly capture the true posterior distribution. The solution we propose is a compromise between these two contradicting requirements.</p><formula xml:id="formula_7">Q q 0 q 1 q 2 q 3q 1 q 2 q 3 ( x 1 , y 1 ) ( x 2 , y 2 ) ( x 3 , y 3 )</formula><p>Unimodal with fixed structure. Assume in first place to have a pre-defined tree structureŜ. The surrogate posterior over trees t = (θ, π, S) can be defined as a factorization of independent distributions over the tree's parameters, which takes the following parametric form:</p><formula xml:id="formula_8">q(t; h) = δŜ(S) n∈N q n (θ n ; Σ n , µ n ) ℓ∈L q ℓ (π ℓ ; α ℓ ) . (8)</formula><p>The first term in <ref type="formula">(8)</ref> is a Dirac measure that supports only trees with structureŜ. Each decision function's parameter θ n follows a multivariate Gaussian with mean µ n and covariance Σ n , i.e. q n ∼ Gauss(µ n , Σ n ), while each prediction node's class distribution π ℓ follows a Dirichlet distribution with parameter α ℓ , i.e. q ℓ ∼ Dir(α ℓ ) (see <ref type="figure">Fig. 2</ref>). The argument h = (Ŝ, Σ, µ, α) of q holds all the parameters of the distribution (a.k.a. hyperparameters of the tree).</p><p>The distribution in <ref type="formula">(8)</ref> is unimodal for most parametrizations and, under this modelling choice, the projection step in (6) turns into the following, independent minimizations over the different parameters of q i (see, Subsection A.1 of the supplementary material):</p><formula xml:id="formula_9">(Σ i+1 n , µ i+1 n ) ∈ arg min Σ,µ Eqi+1 [− log q n (θ n ; Σ, µ)] , (9) α i+1 ℓ ∈ arg min α Eqi+1 [− log q ℓ (π ℓ ; α)] ,<label>(10)</label></formula><p>where the expectations are with respect to the updated posteriorq i+1 . Moreover, the structure of the tree is preserved by the update in <ref type="formula" target="#formula_5">(6)</ref>, i.e.Ŝ i+1 =Ŝ i . In Sec. 4, we describe how to solve <ref type="formula">(9)</ref> and <ref type="formula" target="#formula_0">(10)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-modal.</head><p>A simple way to better capture the true posterior distribution consists in modeling the surrogate posteθn ∼ Gauss(µn, Σn) π ℓ ∼ Dir(α ℓ ) <ref type="figure">Figure 2</ref>. The parameters of the tree are given by θn for each decision node n and π ℓ for each prediction node ℓ. Each decision node's parameter θn is a multivariate Gaussian with mean µ n and covariance Σn. Each prediction node's parameter π ℓ is a Dirichletvariate with concentration vector α ℓ .</p><p>rior as a uniform mixture of distributions</p><formula xml:id="formula_10">q(t; h 1 , . . . , h m ) = 1 m m j=1 q(t; h j ) ,<label>(11)</label></formula><p>where q(t; h j ) is defined as in <ref type="formula">(8)</ref>. In such a way, each of the mixture components might consider a different tree structure (including possibly different feature maps ξ n per node). Accordingly, the tree structures are still assumed to be given, but multiple structures can now be integrated into a single, multi-modal distribution. The projection step in <ref type="formula" target="#formula_5">(6)</ref> for the multi-modal posterior can be approximated by independent projections of each single surrogate posterior forming the mixture in (11) (see, Subsection A.2 in the supplementary material). Finally, the posterior predictive distribution under the multi-modal posterior has the simple closed-form</p><formula xml:id="formula_11">p(y; h 1 i , . . . , h m i , x) = 1 m m j=1 p(y; h j i , x) ,<label>(12)</label></formula><p>where h j i is the parameter of the surrogate posterior distribution of t obtained from i samples for the jth mixture component in <ref type="bibr" target="#b13">(11)</ref>. Please note that in <ref type="formula" target="#formula_0">(12)</ref>, we average the posterior predictive distribution over m online-trained Bayesian trees, as known from conventional forests <ref type="bibr" target="#b6">[4,</ref><ref type="bibr" target="#b11">9]</ref>. Next, we will focus on algorithmic details (e.g. parameter updates, implementation notes, etc.) of individual trees.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Algorithmic Details</head><p>In this section we provide further details about the form of the posterior predictive distribution in <ref type="formula" target="#formula_6">(7)</ref> as well as the update formulae for the surrogate posterior's parameters, providing the solutions for <ref type="formula">(9)</ref> and <ref type="bibr" target="#b12">(10)</ref>. Due to a lack of space we omit the full derivations and a detailed discussion about the computational complexity of our model, which are however available in the supplementary material.</p><p>Posterior predictive distribution. The posterior predictive distribution p (y; h i , x) as given in <ref type="formula" target="#formula_6">(7)</ref> can be computed in closed-form as follows:</p><formula xml:id="formula_12">p (y; h i , x) = ℓ∈L α i ℓy |α i ℓ | 1 ρ(ℓ; h i , x) .<label>(13)</label></formula><p>This distribution is the counterpart of (2), which we obtain by marginalizing out the tree using the surrogate posterior distribution q(t; h i ). The first term in the summation is the expectation of π ℓy under the Dirichlet distribution with parameter α i ℓ , where | · | 1 is the ℓ 1 norm. The second term is a stochastic routing function, which takes the form:</p><formula xml:id="formula_13">ρ(ℓ; h i , x) = n∈N β i n (x) 1 ℓւn (1 − β i n (x)) 1 nցℓ ,<label>(14)</label></formula><p>where β i n (x) represents the following probability of sample x to be routed to the left child at node n in a random tree t distributed as q(t; h i ):</p><formula xml:id="formula_14">β i n (x) = Φ(µ i⊤ nξ i n (x)) .</formula><p>Function Φ is the cumulative distribution function of the standard normal distribution, andξ i n is a Σ i n -normalized version of the node-specific feature map ξ n , given bỹ</p><formula xml:id="formula_15">ξ i n (x) = ξ n (x) ξ n (x) ⊤ Σ i n ξ n (x) −1/2 .<label>(15)</label></formula><p>Inuitively, β i n represent a softening of the hard decision rule b n in (1), induced by the uncertainty about the decision node's parametrization. <ref type="formula" target="#formula_0">(14)</ref> is still valid if we replace ℓ with any other node m in the tree, and in that case ρ(m; h i , x) provides the probability of reaching node m. We will use this later in this section. Moreover, we will also use a variant of the posterior predictive distribution, denoted by p(y|m; h i , x), which conditions on a node m of the tree. This provides the posterior predictive distribution if we took m as the starting node for the prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Remark 1 Function</head><p>Update rule for µ and Σ. The update rules for µ and Σ can be obtained by solving the cross-entropy minimization problem in <ref type="bibr" target="#b11">(9)</ref>. Since the Gaussian distribution is in the exponential family, we can determine a solution to the aforementioned minimization problem by momentmatching. The resulting update rules are given by</p><formula xml:id="formula_16">µ i+1 n = µ i n + κ n Σ i nξ i n ,<label>(16)</label></formula><formula xml:id="formula_17">Σ i+1 n = Σ i n − κ 2 n + κ n µ i⊤ nξ i n (Σ i nξ i n ) Σ i nξ i n ⊤ ,<label>(17)</label></formula><p>where we wroteξ i n as a shortcut forξ i n (x i+1 ) and</p><formula xml:id="formula_18">κ n = φ µ i⊤ nξ i n (u n L − u n R ) a n ,<label>(18)</label></formula><p>where φ(·) is the probability density function of the standard normal distribution, n L and n R denote the left and right child of node n, respectively, a n = ρ(n;hi,xi+1) p(yi+1;hi,xi+1) , and u n = p(y i+1 |n; h i , x i+1 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Remark 2 (Scale invariance)</head><p>The posterior predictive distribution will not change if we scale each mean µ i n by c n and each covariance Σ i n by c 2 n for any c n &gt; 0 (e.g. c n = 1/ µ i n 2 ). Moreover, the update rules in <ref type="bibr" target="#b18">(16)</ref> and <ref type="bibr" target="#b19">(17)</ref> are consistent under the same transformation, i.e. we obtain c n µ i+1 n and c 2 n Σ i+1 n if we transform µ i n and Σ i n in the same way. Hence, we can safely scale µ i+1 n and Σ i+1 n in this sense after each update round, without affecting the outcome of the algorithm. This helps to avoid numerical instabilities.</p><p>Update rule for α. The update rule for α can be determined by solving <ref type="bibr" target="#b12">(10)</ref>. Since π ℓ follows a Dirichlet distribution with parameter α ℓ , we obtain the following momentmatching equations:</p><formula xml:id="formula_19">E q i+1 ℓ [log(π ℓ )] = Eqi+1 [log(π ℓ )] .<label>(19)</label></formula><p>With some manipulations of the two expectation terms, we end up with the following system of equalities for z ∈ Y:</p><formula xml:id="formula_20">ψ(α i+1 ℓz ) − ψ(|α i+1 ℓ |) = ψ(α i ℓz ) − ψ(|α i ℓ |) + a ℓ (1 z=yi+1 − u ℓ ) |α i ℓ | 1<label>(20)</label></formula><p>where a ℓ and u ℓ are defined as in <ref type="formula" target="#formula_0">(18)</ref>, and ψ(·) is the digamma function. We solve the system using Newton-Raphson iterations, where few iterations (typically 5-10) are necessary to achieve a good accuracy. This provides us with a solution to (10) as required. We refer to <ref type="bibr" target="#b18">[16]</ref> for the derivation of the fixed-point iterations.</p><p>Prior distribution. We select a prior distribution from the same family Q as the surrogate posterior given in <ref type="bibr" target="#b10">(8)</ref> and it is identified by the timestamp i = 0, i.e. p (t) = q 0 (t).</p><p>We instantiate a prior distribution by providing a tree struc-tureŜ 0 with some pre-defined depth and with randomized feature map functions ξ n in each node having the form, ξ n (x) = [P n x; 1], where P n is a projection matrix and 1 accounts for a bias term. The prior terms for the decision nodes' parameters are improper, flat priors with mean µ 0 = 0 and Σ 0 → ∞I, which induces the following update once the first sample is observed: µ 1 n = ξ n (x 1 )/ ξ n (x 1 ) 2 and Σ 1 n = I/κ 2 n − µ 1 n µ 1⊤ n , where κ n is computed as per (18) withξ 0 n = 0. The prior parameters for the prediction nodes are uniformly sampled in the range (0, ǫ], i.e. 0 &lt; α 0 ℓy ≤ ǫ, where 0 &lt; ǫ ≪ 1 is a small, non-negative constant, with the exception of having one peaked preference per class uniformly distributed across the leaves.</p><p>Implementation notes The update of the surrogate tree posterior distribution after having seen a new training sample (x i+1 , y i+1 ) can be carried out by traversing the tree twice. The first traversal is top-down and computes ρ n = ρ(n|h i , x i+1 ) for each node n, which is required in the definition of κ n , in <ref type="bibr" target="#b20">(18)</ref> and in <ref type="bibr" target="#b22">(20)</ref>. This is done by initializing ρ ⊤ = 1, where ⊤ ∈ N is the root node, and by computing for each decision node n ∈ N visited in breadth-first order ρ n L = β i n (x i+1 )ρ n and ρ n R = ρ n − ρ n L , where n L and n R are the left and right child of node n. It is then possible to run over the leaves ℓ ∈ L to compute u ℓ = α i ℓyi+1 |α i ℓ | 1 and finally obtain the posterior predictive probability p(y i+1 ; h i , x i+1 ) = ℓ∈L ρ ℓ u ℓ . We have then all the required quantities to compute α i+1 ℓ for each initialize α 0 , µ 0 and Σ 0 (see, Prior distribution) ⊲ Forward pass over tree computes ρ n = ρ(n|h i , x i+1 ) 3: ρ ⊤ ← 1 4: for all n ∈ N in top-down, breadth-first order do 5:</p><formula xml:id="formula_21">ρ n L ← β i n (x i+1 )ρ n ⊲ n L : left child of n 6: ρ n R ← ρ n − ρ n L ⊲ n R : right child of n 7: u ℓ ← α i ℓy i+1 |α i ℓ |1 , ∀ℓ ∈ L 8: p (y i+1 ; h i , x i+1 ) ← ℓ∈L ρ ℓ u ℓ 9: compute α i+1</formula><p>ℓ by solving <ref type="bibr" target="#b22">(20)</ref>, ∀ℓ ∈ L ⊲ Backward pass over tree: u n = p (y i+1 |n; h i , x i+1 ) 10: for all n in bottom-up, breadth-first order do 11:</p><formula xml:id="formula_22">u n ← u n R + (u n L − u n R )β i n (x i+1 ) 12:</formula><p>compute κ n as per <ref type="formula" target="#formula_0">(18)</ref> 13:</p><p>if i = 0 then ⊲ Prior initialization <ref type="bibr" target="#b16">14</ref>:</p><formula xml:id="formula_23">µ 1 n ← ξn(xi+1) ξn(xi+1) 2</formula><p>15: compute µ i+1 n as per <ref type="formula" target="#formula_0">(16)</ref> 18:</p><formula xml:id="formula_24">Σ 1 n ← I/κ 2 n − µ 1 n µ 1⊤</formula><p>compute Σ i+1 n as per <ref type="formula" target="#formula_0">(17)</ref> 19:</p><p>rescale µ i+1 n and Σ i+1 n as per Remark 2 return h i+1 : new surrogate posterior parameters prediction node ℓ by solving the system <ref type="bibr" target="#b22">(20)</ref>. The second traversal is bottom-up and computes the updates for the decision nodes' hyperparameters. We run again over the nodes n ∈ N , but in bottom-up, breadth-first order. Once a node n is visited, we compute u n = u n R + (u n L − u n R )β i n (x i+1 ) and κ n as per <ref type="bibr" target="#b20">(18)</ref>. Finally, we calculate Σ i+1 n and µ i+1 n using <ref type="bibr" target="#b18">(16)</ref> and <ref type="formula" target="#formula_0">(17)</ref>, since all the required quantities are available. A summary is provided in Alg. 1.</p><p>Fast, single path inference During inference, exact computation of the posterior predictive distribution requires traversing the tree entirely.</p><p>The complexity is thus O <ref type="figure">(|N |d 2 )</ref>, where d is the maximum decision node feature dimensionality. However, since the routing function of each tree gets peaked on a single path after a reasonable number of samples (i ≫ 0) have been observed, we can obtain a good approximation of the posterior predictive distribution by taking at each decision node n the direction where the sample x has the highest probability to be routed to, i.e. left if β i n (x) &gt; 0.5, and right otherwise. This decision can be taken efficiently by evaluating the sign of µ i⊤ n ξ n (x), because β i n (x) &gt; 0.5 ⇐⇒ µ i⊤ n ξ n (x) &gt; 0. With this trick, we reduce the per-tree complexity during inference to O(d log 2 |N |), which is the same as for offline, oblique decision trees <ref type="bibr" target="#b14">[12,</ref><ref type="bibr" target="#b17">15]</ref>. Please note, that log 2 |N | is much smaller for our compact trees than for typically deeper oblique decision trees.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>We assess several variants of our algorithm on different datasets, including standard machine learning (ML) classification benchmarks (Sec. 5.1) and pixel-wise semantic labelling of Kinect <ref type="bibr" target="#b23">[21]</ref> data (Sec. 5.2). For all experiments, we provide baseline results of state-ofthe-art online random forest approaches, using their publicly available reference implementations. We validate our learner against Mondrian Forests (MF) <ref type="bibr" target="#b15">[13]</ref>, Online Random Forests (ORF) <ref type="bibr" target="#b22">[20]</ref>, and Consistent Online Forests (COF) <ref type="bibr" target="#b12">[10]</ref> (the latter code includes a re-implementation of ORF). As additional baselines, we provide results for offline random forests (RF) <ref type="bibr" target="#b6">[4]</ref> and offline oblique random forests (obRF) <ref type="bibr" target="#b17">[15]</ref>. Please note however, that offline forest results are not directly comparable to online results, as their training expects the entire dataset to be given in advance. We fix ǫ = 0.01 (see last section), which may serve as guideline for other datasets (we did not experience large sensitivity when varying it). For each dataset we train at most 8 trees for our method (no reasonable improvement was found with more) and we allow only a single epoch over the data for our trees to properly simulate an online scenario unless explicitly stated otherwise. Instead, all forest competitors (offline and online) comprised 100 trees with up to 15 epochs over the data (specifically recommended for <ref type="bibr" target="#b22">[20,</ref><ref type="bibr" target="#b12">10]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Classification performance on ML datasets</head><p>We tested on G50c, dna, satimages and USPS since they were also (partially) selected in <ref type="bibr" target="#b12">[10,</ref><ref type="bibr" target="#b15">13]</ref>, and cover different granularity of difficulty with respect to dataset characteristics (#feature dimensions, #classes, #train/#test samples). A summary is provided on top of Tab. 1, followed by blocks for offline ( <ref type="bibr" target="#b6">[4,</ref><ref type="bibr" target="#b17">15]</ref>, grayed block) and online forest results, respectively. All reported scores are average classification errors with standard deviations in [%] (from 10 repetitions or cross-validation folds for standard partitioning of datasets), i.e., lower is better. Offline forest results should mainly demonstrate the effects due to different complexities of decision node functions: For instance, <ref type="bibr" target="#b6">[4]</ref> uses randomly selected, single feature channels (i.e. axisaligned splits) while <ref type="bibr" target="#b17">[15]</ref> applies more complex, oriented hyperplanes, thus incorporating a larger feature space. Online forest results for ORF, MF and COF could be approximately reproduced with default parameter settings in their code (or suggested in their papers), which we also used for training/testing on datasets not evaluated in their papers.</p><p>We dub our method as Bayesian Online Forest 2 (BOF) in case no projection is performed, i.e. P n = I, BOF-P when we perform a randomly selected projection, BOF-B when using bagging (i.e. a tree discards a sample with probability τ ), and BOF-BP when bagging and random projection are applied. All methods were trained on the same splits into training/testing. As a rule of thumb, we define a datasetspecific set of possible tree depths from where the actual tree depth is randomly selected. Specifically, we sample the tree depth from {⌈log 2 (|Y|)⌉, . . . , ⌈log 2 (|Y|)⌉+2} such that there are at least as many leaves as number of classes. E.g., the satimages dataset has 6 classes which means that we randomly select a tree depth between 3 and 5. This max tree depth is also applied for some oblique forest configurations, indicated by the super-and subscripts. For instance, obRF 8 BOF means that 8 trees with the same max depth as our Bayesian trees were grown, while obRF 100 25 means that 100 trees with max depth 25 were trained.</p><p>We obtain scores that are similar or better than all online methods we compare to, considering their 15-epoch results ORF ×15 , MF ×15 , COF ×15 . For the dna dataset we evaluate with two different feature space sizes like <ref type="bibr" target="#b15">[13]</ref>. MF seems to struggle with higher-dimensional inputs (see error values of ≈33% vs. ≈9%), whereas pre-selection of informative dimensions yields ≈1% in accuracy gain for our approach. On the satimages dataset we perform similar (or slightly worse) than our competitors. Finally, we also list single-epoch results for ORF ×1 , MF ×1 and COF ×1 , which, except for MF, show drastic performance reductions, inhibiting online learning without additional samples.</p><p>To illustrate the ensemble effect, we obtain the following classification errors (in [%]) when using <ref type="bibr" target="#b3">(1,</ref><ref type="bibr" target="#b6">4,</ref><ref type="bibr" target="#b10">8)</ref> BOF trees. G50c: <ref type="bibr">(8.1, 7.9, 7.7)</ref>. dna(180): (6.1, 5.9, 5.8). dna(60): <ref type="bibr">(5.5, 5.0, 4.9)</ref>. satimages: (13.0, 11.3, 11.1). USPS: (8.3, 6.5, 6.2). Since our trees are Bayesian, they exhibit less variance than standard trees. We thus require Sequential data arrival performance on USPS In <ref type="figure">Fig. 3</ref> we show the results when training and testing our tree ensemble from sequentially arriving data of the USPS dataset, akin to the experiment in <ref type="bibr" target="#b12">[10]</ref>. The curves show the performance on the test dataset as a function of the number of training samples presented to the algorithms. As can be seen, our algorithm initially performs comparably with the other methods but begins to even surpass re-trained offline forests <ref type="bibr" target="#b6">[4]</ref> after seeing more than 500 training samples. The numbers for our method are averaged over 10 runs and again each sample was presented only once while <ref type="bibr" target="#b12">[10,</ref><ref type="bibr" target="#b22">20]</ref> allowed 15 epochs in their training protocol.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Kinect dataset</head><p>In this experiment we perform the task of pixel-wise semantic labelling (body part recognition, see <ref type="figure" target="#fig_3">Fig. 5</ref>), using synthetically generated depth input images. We used the publicly available 3 dataset of <ref type="bibr" target="#b12">[10]</ref>, which provides predefined splits into training and test images, as well as the specific order and training sample center locations presented to the learners. The training set contains 2000 images (i.e. poses with 19 body part classes + 1 background class label) from where ≈50 samples per body part and image were collected, resulting in roughly 2 million training samples. Testing was conducted on all foreground pixels of the 500 images in the test set. Since the exact order and locations of training samples are given, we can provide a direct comparison to the baseline scores reported for <ref type="bibr" target="#b22">[20]</ref> and <ref type="bibr" target="#b12">[10]</ref>. Both trained forests of 25 trees, <ref type="bibr" target="#b22">[20]</ref> was limited to depth 8 for memory reasons (to keep memory consumption &lt;10GB) while <ref type="bibr" target="#b12">[10]</ref> reports no restriction on their maximum depth. Moreover, <ref type="bibr" target="#b12">[10]</ref> reports that their trees were allowed to evaluate 2000 candidate offsets with 10 candidate split node tests at a memory consumption of 1.6GB, when limiting their approach to 1000 active leaf nodes.</p><p>We trained again ensembles comprising 8 balanced Bayesian trees, using maximum depths of 7 or 8 (yielding 127/255 split nodes and 128/256 leaf nodes per tree, respectively). Instead of granting access to 2000 candidate offsets, we used d = 100 randomly chosen offsets (+1 bias dimension) per split node (dubbed BOF-S), which we sampled from a log-polar space with maximum distance of ≈35 pixels, akin to <ref type="bibr" target="#b12">[10]</ref>. In such a way, the total number of parameters per tree for our approach is |N | · ((d + 1) 2 + (d + 1) + d) + 2|N | · |Y| (requiring 101 2 per Σ n , 101 per µ n , 100 for subspace selection via P n and 20 per α ℓ ), resulting in ≈5/10MB per tree (depth 7/8). When using only axis-aligned, diagonal covariance matrices Σ n (denoted as BOF-SD), the model memory requirement reduces to ≈160/320kB per tree. Once only inference has to be performed, memory consumption can be reduced to ≈110/220kB (for both, BOF-S and BOF-SD) when using the fast, single-path routing described in Sec. 4.. <ref type="figure" target="#fig_2">Fig. 4</ref> shows the pixel labelling accuracy (percentage of correctly labelled foreground pixels of test set as in <ref type="bibr" target="#b12">[10]</ref>) as a function of presented training data. We outperform both baselines by a significant margin over the entire sweep of training samples when using 8 BOF-S trees. For instance, after 500k training samples we improve by ≈6/8% over <ref type="bibr" target="#b12">[10]</ref> and ≈11/13% over [20] (depth 7/8). Conversely, we approximately match the final performance of <ref type="bibr" target="#b12">[10]</ref> at 2M samples with our depth 8 ensemble after seeing only 20k (i.e. 1/100th) training samples. Also, we only need 3 of our trees in order to reach comparable final performance to <ref type="bibr" target="#b12">[10]</ref>, which we illustrate with dashed lines in the plot. With our faster 8 tree training variant BOF-SD (ie. diagonal Σ n , shown in cyan), we approximately match the performance of the full covariance version at the maximum number of training samples. Finally, note that all results for our approaches were obtained by performing the fast, single-path inference described at the end of Sec. 4. More experimental insights in the supplementary material include i) details on timings in Sec. C.1, ii) plots and discussions of an increasing ensemble size for both, depth 7 and depth 8 BOF-S ensembles in Sec. C.2, iii) a guide on how to perform model selection based on the online development of the ensemble training loss in Sec. C.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The plot in</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Summary and Future Work</head><p>In this paper we have proposed a novel approach for online learning of classification trees, driven by ideas from Bayesian online learning theory. Our solution departs from state-of-the-art approaches by trying to build tree ensembles that consist of only few and compact trees with good generalization capability. We achieved this goal by adopting a Bayesian learning procedure that iteratively refines a posterior distribution within a pre-defined parametric family in a way to best incorporate information carried by new data samples. The experimental evaluation has shown that our approach is able to perform on par or better than state-ofthe-art online forest algorithms on a variety of classification tasks, while using smaller models.</p><p>We plan to extend our approach to regression, by replacing the prediction model in the leaves and derive proper update formulae for the related parameters. We also plan to investigate a semi-parametric, Bayesian setting in order to let the tree structure be driven by the data.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Algorithm 1</head><label>1</label><figDesc>Online learning of Bayesian classification tree Require: (x i+1 , y i+1 ): next training sample Require: h i : latest surrogate posterior parameter (if i &gt; 0) Require:Ŝ 0 : a tree structure (if i =</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Sequential data arrival experiments for Kinect dataset (see Sec. 5.2), showing test data classification accuracies as function of seen training samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Color-coded, qualitative examples of Kinect semantic labelling experiment (ground truth vs. obtained result), see Sec. 5.2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>Figure 3. Sequential data arrival experiments for USPS dataset (see last paragraph in Sec. 5.1), showing test data classification accuracies as function of seen training samples. smaller ensembles to achieve similar/better accuracy. For different variants of our method we experience only minor performance drop when applying bagging (τ = 0.3), which however linearly reduces training times. We obtain both, improvement of classification error and reduction in training time for G50c and USPS when applying randomly chosen projections to lower-dimensional feature spaces (by applying projection matrices P n ). As target feature projection dimensionality we choose values approximately around d/2, i.e. P n ∈ R d/2×d . We provide a table with detailed information on Matlab timings in the supplementary material (Sec. C.1), corresponding to the experiments in Tab. 1.</figDesc><table>Training data size 

10 2 
10 3 
10 4 

Accuracy [%] 

75 

80 

85 

90 

95 

USPS Forest Accuracy 

Offline RF 
COF [Denil et al.] 
ORF [Saffari et al.] 
MF [Lakshm. et al.] 
Ours 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">1 ℓւn and 1 nցℓ are both zero for all n not being ancestors of ℓ. Hence, the corresponding factors in (3) yield 1 when assuming 0 0 = 1.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">While knowing that we introduced an ensemble of Bayesian trees rather than a Bayesian forest, we use the term forest as we average akin to conventional forests</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">http://mdenil.com/projects/#random-forests</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments. This research has received funding from the European Unions Horizon 2020 research and innovation programme under grant agreement No 687757.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Kinect Forest Accuracy ORF</title>
		<imprint/>
	</monogr>
	<note>Saffari et al.</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cof [denil</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">trees, depth 8) BOF-S (3 trees, depth 8) BOF-SD (8 trees, depth 8) BOF-SD (3 trees, depth 8) BOF-S (8 trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bof-S</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>depth 7) BOF-S (3 trees. depth 7</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Dynamic trees for streaming and massive data contexts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Anagnostopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Gramacy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1201.5568</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">New ensemble methods for evolving data streams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bifet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Pfahringer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kirkby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gavaldà</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;09</title>
		<meeting>the 15th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;09<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="139" to="148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Bayesian hierarchical mixtures of experts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Svensén</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Conference on Uncertainty in Artificial Intelligence</title>
		<meeting>of Conference on Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page">5764</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Random forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="5" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning classification trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Buntine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Stat. Comput</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="63" to="73" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Bayesian cart model search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chipman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">I</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Mcculloch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Am. Stat. Assoc</title>
		<imprint>
			<biblScope unit="page" from="935" to="948" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">BART: Bayesian additive regression trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chipman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">I</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Mcculloch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Applied Statistics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="266" to="298" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Hierarchical priors for Bayesian cart shrinkage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chipman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Mcculloch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Stat. Comput</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="17" to="24" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Decision Forests in Computer Vision and Medical Image Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Denil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Matheson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>De Freitas</surname></persName>
		</author>
		<title level="m">Consistency of online random forests. In (ICML)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Mining high-speed data streams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Domingos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hulten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Knowl. Discov. and Data Mining</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="71" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Induction of oblique decision trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Heath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kasif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Salzberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="32" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Mondrian forests: Efficient online random forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Inform. Process. Syst</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Stochastic models, estimation and control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Maybeck</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1982" />
			<publisher>Academic Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">On oblique random forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">H</forename><surname>Menze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">M</forename><surname>Kelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">N</forename><surname>Splitthoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Koethe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Hamprecht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning and Knowledge Discovery in Databases</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">6912</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Estimating a dirichlet distribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">P</forename><surname>Minka</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Expectation propagation for approximate bayesian inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">P</forename><surname>Minka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th Conference in Uncertainty in Artificial Intelligence, UAI &apos;01</title>
		<meeting>the 17th Conference in Uncertainty in Artificial Intelligence, UAI &apos;01</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="362" to="369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A bayesian approach to on-line learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Opper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">On-line Learning in Neural Networks</title>
		<editor>D. Saad</editor>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="363" to="378" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Online bagging and boosting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Oza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Russel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Artificial Intell. and Statistics</title>
		<meeting>of Artificial Intell. and Statistics</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="105" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">On-line random forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saffari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Leistner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Santner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Godec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE -ICCV Workshop on Online Learning for Computer Vision</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Efficient human pose estimation from single depth images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fitzgibbon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sharp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Finocchio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kipman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Dynamic trees for learning and design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Taddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Gramacy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">G</forename><surname>Polson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Am. Stat. Assoc</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="issue">493</biblScope>
			<biblScope unit="page" from="109" to="123" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Semanticpaint: Interactive 3d labeling and learning at your finger tips</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Valentin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nießner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Izadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Verikas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gelzinis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bacauskiene</surname></persName>
		</author>
		<title level="m">Mining data with random forests: A survey and results of new tests. Pattern Recognition (PR)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="330" to="349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Bayesian CART: Prior specification and posterior simulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tjelmeland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>West</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Comput. Graph. Stat</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="44" to="66" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A review of ensemble methods in bioinformatics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Zomaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Current Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="296" to="308" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
