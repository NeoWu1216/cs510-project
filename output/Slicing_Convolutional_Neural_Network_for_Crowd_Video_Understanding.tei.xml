<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Slicing Convolutional Neural Network for Crowd Video Understanding</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Shao</surname></persName>
							<email>jshao@ee.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electronic Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong</orgName>
								<address>
									<settlement>Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
							<email>ccloy@ie.cuhk.edu.hk</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Information Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Kang</surname></persName>
							<email>kkang@ee.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electronic Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong</orgName>
								<address>
									<settlement>Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
							<email>xgwang@ee.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electronic Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong</orgName>
								<address>
									<settlement>Kong</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Slicing Convolutional Neural Network for Crowd Video Understanding</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:46+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Learning and capturing both appearance and dynamic representations are pivotal for crowd video understanding. Convolutional Neural Networks (CNNs) have shown its remarkable potential in learning appearance representations from images. However, the learning of dynamic representation, and how it can be effectively combined with appearance features for video analysis, remains an open problem. In this study, we propose a novel spatio-temporal CNN, named Slicing CNN (S-CNN), based on the decomposition of 3D feature maps into 2D spatio-and 2D temporal-slices representations. The decomposition brings unique advantages: (1) the model is capable of capturing dynamics of different semantic units such as groups and objects, (2) it learns separated appearance and dynamic representations while keeping proper interactions between them, and (3) it exploits the selectiveness of spatial filters to discard irrelevant background clutter for crowd understanding. We demonstrate the effectiveness of the proposed S-CNN model on the WWW crowd video dataset for attribute recognition and observe significant performance improvements to the state-of-the-art methods (62.55% from 51.84% <ref type="bibr" target="#b20">[21]</ref>).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Understanding crowd behaviours and dynamic properties is a crucial task that has drawn remarkable attentions in video surveillance research <ref type="bibr">[2, 4, 8, 10, 11, 13, 17-20, 28, 29, 32]</ref>. Despite the many efforts, capturing appearance and dynamic information from a crowd remains non-trivial. Ideally, as in most activity analysis studies, objects (i.e. groups or individuals) of interests should be segmented from the background, they should be further detected into different categories, and tracking should be performed to capture the movements of objects separately. One can then jointly consider the extracted dynamics for global understanding. Unfortunately, this typical pipeline is deemed too challenging for crowd videos.</p><p>Can deep learning offers us a tool to address the aforementioned challenges? Contemporary CNNs are capable of learning strong generic appearance representations from static image sets such as ImageNet. Nevertheless, they lack of the critical capability for learning dynamic representation. In existing approaches, a video is treated as a 3D volume and 2D CNN is simply extended to 3D CNN <ref type="bibr" target="#b4">[5]</ref>, mixing the appearance and dynamic feature representations in the learned 3D filters. Instead, appearance and dynamic features should be extracted separately, since they are encoded in different ways in videos and convey different information. Alternative solutions include sampling frames along the temporal direction and fusing their 2D CNN feature maps at different levels <ref type="bibr" target="#b6">[7]</ref>, or feeding motion maps obtained by existing tracking or optical flow methods <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b26">27]</ref>. While computationally more feasible than 3D CNN, these methods lose critical dynamic information at the input layer.</p><p>In this study, we wish to show that with innovative model design, appearance and dynamic information can be effectively extracted at a deeper layer of CNN that conveys richer semantical notion (i.e. groups and individuals). In our new model design, appearance and dynamics have separate representations yet they interact seamlessly at semantic level. We name our model as Slicing CNN (S-CNN). It consists of three CNN branches each of which adopts different 2D spatio-or temporal-filters. Specifically, the first S-CNN branch applies 2D spatio-slice filters on video volume (xy-plane) to extract 3D feature cuboids. The other two CNN branches take the 3D feature cuboids as input and apply 2D temporal-slice filters at xt-plane and yt-plane of the 3D feature cuboids, respectively. An illustration of the model is shown in <ref type="figure" target="#fig_1">Fig. 3</ref>.</p><p>This design brings a few unique advantages to the task of crowd understanding.</p><p>(1) Object-aware -A 3D feature cuboid generated by a 2D spatial filter records the movement of a particular semantic unit (e.g. groups or individual objects). An example is shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, the feature map from a selected filter of a CNN hidden layer only shows high responses on the ice ballet dancers, while that from another filter shows high responses on the audience. Segregating such semantic classes in a complex scene is conventionally deemed challenging if not impossible for crowd video understanding.</p><p>(2) Selectiveness -The semantic selectiveness exhibited by the 2D spatial filters additionally guides us to discriminatively prune irrelevant filters such as those corresponding to the background clutter.</p><p>(3) Temporal dynamics at semantic-level -By applying temporal-slice filters to 3D feature cuboids generated by spatial filters at semantic-level, we can extract motion features of different semantic units, e.g. speed and acceleration in xand y-directions.</p><p>We conduct empirical evaluations on the proposed deep structure and thoroughly examine and analyze the learned spatio-and temporal-representations. We apply the proposed model to the task of crowd attribute recognition on the WWW Crowd dataset <ref type="bibr" target="#b17">[18]</ref> and achieve significant improvements against state-of-the-art methods that either apply a 3D-CNN <ref type="bibr" target="#b4">[5]</ref> or Two-stream CNN <ref type="bibr" target="#b20">[21]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Compared to applying CNN to the static image analysis, there are relatively few works on the video analysis <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b29">30]</ref>. A 3D-CNN extends appearance feature learning in a 2D CNN to its 3D counterpart to simultaneously learn appearance and motion features on the input 3D video volume <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5]</ref>.</p><p>It has been reported effective on the task of human action recognition. However, to capture long-term dependency, larger filter sizes and more layers need to be employed and the model complexity increases dramatically. To reduce model complexity, Karpathy et al. <ref type="bibr" target="#b6">[ 7]</ref> studied different schemes of sampling frames and fused their features at multiple stages. These approaches did not separate appearance and dynamic representations. Nevertheless, traditional activity studies always segment objects of interests first and perform tracking on multiple targets that capture movements of different objects separately <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24]</ref>. It shows that space and time are not equivalent components and thus should be learned in different ways. Ignoring <ref type="figure">Figure 2</ref>. The slices over a raw video volume may inevitably mix the dynamics of different objects. For the raw video volume on the left, the xt-slice in the middle represents the dynamics of both the dancers and background scene (i.e. ice rink), while the yt-slice capture the dynamics of audience, dancers, as well as ice rink.</p><p>such prior knowledge and learning feature representation blindly would not be effective. Alternatively, two-branch CNN models <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b26">27]</ref> have been proposed to extract appearance and dynamic cues separately with independent 2D CNNs and combine them in the top layers. The input of the motion branch CNN is either 2D motion maps (such as optical flow fields <ref type="bibr" target="#b20">[21]</ref> and dynamic group motion channels <ref type="bibr" target="#b17">[18]</ref>). Different from 3D convolutions, a two-branch CNN is at the other extreme, where the extractions of appearance and dynamic representations have no interactions. These variants are of low cost in memory and calculation, but they inevitably sacrifice the descriptive ability for the inherent temporal patterns.</p><p>Albeit video-oriented CNNs have achieved impressive performances on video related tasks, alternative video representations other than spatial-oriented inputs are still under-explored. Besides representing a video volume as a stack of spatial xy-slices cut along the dimension t, previous works have shown that another two representations of xt-slices in dimension y and yt-slices in dimension x can boost feature learning of both appearance and dynamics on a variety of video-tasks <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b30">31]</ref>. However, they extract the motion feature slices directly from video volumes, but ignore the possibility that multiple objects or instances presented in one slice may occupy distinct motion patterns. Therefore, their dynamic feature representation may mix the motion patterns from different objects and thus fail to describe a particular type of motion patterns. An example is shown in <ref type="figure">Fig. 2</ref>. Moreover, the internal properties and connections among different slices were not well learned but just handled independently.</p><p>The proposed Slicing CNN model overcomes the limitations listed above. With innovative model design, appearance and dynamic informations can be effectively learned from semantic levels, separately and interactively. In addition, the proposed model is capable of extracting appearance and dynamic informations from long-range videos (i.e. 100 frames) without sampling or compression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Slicing CNN Model</head><p>In this paper, we propose a new end-to-end model named as Slicing CNN (S-CNN) consisting of three branches. We first learn appearance features by a 2D CNN model on each  frame of the input video volume, and obtain a collection of semantic feature cuboids. Each feature cuboid captures a distinct visual pattern, or an object instance/category. Based on the extracted feature cuboids, we introduce three different 2D spatio-and temporal-filters (i.e. xy-, xt-, and yt-) to learn the appearance and dynamic features from different dimensions, each of which is followed by a 1D temporal pooling layer. Recognition of crowd attribute is achieved by applying a classifier on the concatenated feature vector extracted from the feature maps of xy-, xt-, and yt-branch.</p><p>The complete S-CNN model is shown in <ref type="figure" target="#fig_1">Fig. 3</ref>, and the detailed architecture of the single branch (i.e. S-CNN-xy, S-CNN-xt, and S-CNN-yt) is shown in <ref type="figure" target="#fig_3">Fig. 6</ref>. Their implementation details can be found in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Semantic Selectiveness of Feature Maps</head><p>Recent studies have shown that the spatial filters in 2D CNNs on image-related tasks posses strong selectiveness on patterns corresponding to object categories and object identities <ref type="bibr" target="#b24">[25]</ref>. Specifically, the feature map obtained by a spatial filter at one intermediate layer of a deep model records the spatial distribution of visual pattern of a specific object. From the example shown in <ref type="figure">Fig. 4</ref>, convolutional layers of the VGG model <ref type="bibr" target="#b21">[22]</ref> pre-trained on ImageNet depict visual patterns in different scales and levels, in which the conv4 3 layer extracts the semantic patterns in object level. For instance, the filter #26 in this layer precisely cap-  <ref type="figure">Figure 4</ref>. Feature responses of selective filters from different convolutional layers of the VGG model, in which conv4 3 layer owns the best description power for semantic visual patterns in object level. This semantic feature maps precisely capture the dancers in ice ballet at all frames presented.</p><p>tures ice ballet dancers in all frames. Further examining the selectiveness of the feature maps, <ref type="figure" target="#fig_2">Fig. 5</ref>(a-c) demonstrates that different filters at conv4 3 layer are possibly linked to different visual patterns. For example, filter #5 indicates the pedestrians on the crosswalk and filter #211 means extremely dense crowd; both of them extract patterns related to crowd. While filter #297 and #212 correspond to background contents like trees and windows of building. Motivated by the aforementioned observations, we could actually exploit such feature cuboids to separately monitor the movements of different object categories, both spatially and temporally, while reducing the interference caused by the background clutter and irrelevant objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Feature Map Pruning</head><p>The selectiveness of feature cuboids allows us to design models on a particular set of feature cuboids so as to capture crowd-related dynamic patterns and reject motions from irrelevant background contents. As shown in <ref type="figure" target="#fig_2">Fig. 5</ref>, some feature maps rarely respond to the subjects in crowd but mainly to background regions. How to efficiently learn dynamic feature representations from temporal slices obtained from these feature cuboids? Are all the feature cuboids meaningful to learn dynamic patterns? We answer these questions by pruning spatial filters that generate "irrelevant" feature maps and investigate its impact to the attribute recognition performance.</p><p>The "relevance" of a feature map is estimated by investigating their spatial distributions over a fixed validation set of images whose foreground crowds are annotated. The annotation is a binary mask estimated by a crowd segmentation method <ref type="bibr" target="#b5">[6]</ref>, denoted as S i for a query image i ∈I , which is then resized to match the resolution of the extracted feature maps. We adopt two scores (i.e. affinity score and conspicuous score) to measure the "relevance". Affinity score. The affinity score α n i measures the overlap ratio of the crowd foreground instances between the mask S i and the n th binarized feature map F n i ∈F i ,</p><formula xml:id="formula_0">α n i = 1 [F n i &gt;0] • S i 1 / S i 1 ,<label>(1)</label></formula><p>where 1 <ref type="bibr">[·]</ref> is an indicator function that returns 1 when its  input argument is true. • denotes the element-wise multiplication. Conspicuous score. The conspicuous score κ n i calculates the feature's energy inside the crowd foreground annotated in the mask S i against its overall energy,</p><formula xml:id="formula_1">κ n i = F n i • S i 1 / F n i 1 .<label>(2)</label></formula><p>We then construct a histogram H with respect to the filters in a certain layer. For filter #n, if the feature map F n i satisfies either α n i &gt;τ α or κ n i &gt;τ κ , given two thresholds τ α and τ κ , we have the value of its histogram bin as</p><formula xml:id="formula_2">H(n)=H(n)+1 [α n i &gt;τα∪κ n i &gt;τκ] , ∀i ∈I.<label>(3)</label></formula><p>By sorting H(n) in a descending order, we retain the first r spatial filters but prune the left filters. The reserved filters are denoted as N r .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Semantic Temporal Slices</head><p>Existing studies typically learn dynamic features from raw video volumes <ref type="bibr" target="#b6">[7]</ref> or hand-crafted motion maps <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b26">27]</ref>. However, much information is lost at the input layer since they compress the entire temporal range by subsampling frames or averaging spatial feature maps along the time dimension. Indeed, dynamic feature representations can also be described from 2D temporal slices that cut across 3D volume from another two orthogonal planes, as xt-oryt-slices shown in <ref type="figure">Fig. 2</ref>. They explicitly depict the temporal evolutions of objects, for example, the dancers in the xt-slice and audience in the yt-slice.</p><p>It is a general case that a xt-oryt-slice captured from a raw video volume contains motion patterns of multiple objects of different categories, which cannot be well separated since the features that identify these categories always refer to appearance but not motion. For instance, the yt-slice in <ref type="figure">Fig. 2</ref> contains motion patterns from audience, dancers and ice rink. It is not a trivial task to divide their motion patterns apart without identifying these objects at first.</p><p>Motivated by this observation, we propose Semantic Temporal Slice (STS) extracted from semantic feature cuboids, which are obtained from the xy convolutional layers, as shown in <ref type="figure" target="#fig_1">Fig. 3</ref>. As discussed in the previous subsections, such kind of slices can distinguish and purify the dynamic representation for a certain semantic pattern without the interference from other objects, instances or visual patterns inside one temporal slice. Furthermore, given multiple STSs extracted from different horizontal and vertical probe lines and fed into S-CNN, their information can be combined to learn long-range dynamic features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">S-CNN Deep Architecture</head><p>In this section, we provide the architecture details of each branch (i.e. S-CNN-xy, S-CNN-xt, S-CNN-yt) and their combination (i.e. S-CNN).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Single Branch of S-CNN Model</head><p>Our S-CNN starts with designing a CNN for extracting convolutional feature cuboids from the input video volume. In principle, any kind of CNN architecture can be used for feature extraction. In our implementation, we choose the VGG architecture <ref type="bibr" target="#b21">[22]</ref> because of its excellent performance in image-related tasks. As shown in <ref type="figure" target="#fig_3">Fig. 6</ref>, for an input raw video volume, we first follow the original setting of the lower layers of VGG-16 from conv1 1 to conv4 3 1 to extract spatial semantic feature maps. The size of the feature cuboid F s i of time i is c × h s × w s , where c is the number of feature maps determined by the number of neurons, h s and w s denote the size of each feature map in the xy-plane. The number of feature cuboid is determined by the input video length τ .</p><p>S-CNN-xy branch. The S-CNN-xy branch learns spatiotemporal features from the xy-plane by xy-convolutional filters. Based on the spatial feature cuboids {F s i } τ i=1 , we continue convolving feature maps with xy-filters from conv5 1 to conv5 3, following VGG-16's structure to get the xy-temporal feature cuboids with a size of τ × c × h t × w t . In other words, there are cxyspatio-temporal feature cuboids F xy , each of which is τ × h t × w t .A1 × 1 filter is then adopted on each F xy i to fuse the temporal information from different frames. The spatio-temporal feature maps F xy(t) are fed into three fully-connected layers to classify the crowd-related attributes.</p><p>S-CNN-xt /-yt branch. For the purpose of learning features from {F s i } τ i=1 by xt-o ryt-branch, we first swap dimensions of the original xy-plane to the corresponding xt-oryt-plane. Take xt-branch as an example, as shown in <ref type="figure" target="#fig_3">Fig. 6</ref>, the semantic feature cuboids turn to be h s ×c×τ ×w s after swapping dimensions. We then substitute the xyconvolutional filters used in xy-branch with xt-filters for conv5 1 to conv5 3 layers. Before temporal pooling at the last stage, again we need to swap dimensions from xtplane to xy-plane. The following structures are the same as those in xy-branch. The yt-branch is similar to xt-branch but with a different types of convolutional filters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Combined S-CNN Model</head><p>After training each branch separately, we fuse the features learned from different spatial and temporal dimensions together by concatenating the spatio-temporal feature maps (i.e. F xy(t) , F xt(y) , and F (x)ty ) from three branches with ℓ 1 normalization. Linear SVM is adopted as the classifier for the sake of its efficiency and effectiveness on highdimensional feature representations. We train a SVM independently for each attribute, thus there are 94 models in total. To train each SVM, we consider videos containing the target attribute as the positive samples and leave all the rest as the negative samples. The complete S-CNN model is visualized in <ref type="figure" target="#fig_1">Fig. 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Experimental Setting</head><p>Dataset. To demonstrate the effectiveness of the proposed S-CNN deep model, we investigate it on the task of crowd attribute recognition with the WWW Crowd Dataset <ref type="bibr" target="#b17">[18]</ref>, which is a comprehensive crowd dataset collecting videos from movies, surveillance and web. It covers 10, 000 videos with 94 crowd attributes including places (Where), subjects (Who), and activities (Why). Following the original setup in <ref type="bibr" target="#b17">[18]</ref>, we train the models on 7220 videos and use a set of 936 videos as validation, while test the results over the rest 1844 videos. These sets have no overlap on scenes to guarantee the attributes are learned scene-independently. Evaluation Metrics. We adopt both Area Under ROC Curve (AUC) and Average Precision (AP) as the evaluation metrics 2 . AUC is a popular metric for classification and its lower-bound is fixed to 0.5. It fails to carefully measure the performance if the ratio between the positive and negative samples is extremely unbalanced, which is just the case we confront. AP is effective to evaluate the multi-attribute detection performance, which is lower bounded by the ratio of positive samples over all the samples. Its lower bound can be written as mAP lb = 1 Nattr Nattr k=1 |T k |/|T |, where N attr is the number of attributes, T is the test set, T k is the set of samples with the attribute indexed by k. In our experiments, the theoretical lower bound is 0.067. Model Pre-training. As a common practice in most deep learning frameworks for visual tasks, we initialize the proposed S-CNN models with the parameters pre-trained on ImageNet. This is necessary since VGG requires diverse data to comprehensively tune its parameters. Although WWW crowd dataset has million of images, the diversity of scenes is low (i.e. around 8000). Specifically, we employ the VGG-16 model with 13 convolutional (conv) layers and 3 fully-connected (fc) layers. All conv layers in S-CNN models are initialized with the pre-trained model while three fc layers are randomly initialized by Gaussian distributions. We keep the first two fc layers with 4096 neurons followed by Rectified Linear Units (ReLUs) and Dropout while the last fc layer with 94 dimensions (attributes) followed by a cross-entropy loss function. If no specific clarifications are stated, we apply this strategy to initialize all experimental models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Ablation Study of S-CNN</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Level of Semantics and Temporal Range</head><p>The unique advantage of S-CNN is that it is capable of learning temporal patterns from semantic layer (higher layer of deep network). In addition, S-CNN can naturally accommodate larger number of input frames due to its effective network design, thus capable of capturing long-range dynamic features.</p><p>To understand the benefits of learning long-range dynamic features from semantic level, we compare the recognition performance of the proposed S-CNN models based on semantic temporal slices (STS) extracted from layer conv4 3 and raw temporal slices (RTS) extracted directly from the video volume. The video length τ has three ranges: 20, 50, and 100 frames, denoted as S(/R)TS <ref type="bibr">[τ ]</ref> . Due to hardware limitation of current implementation, we cannot afford RTS <ref type="bibr">[100]</ref> with full spatial information. Low-level v.s. Semantic-level Temporal Slices. In comparison with the results by RTS <ref type="bibr">[τ ]</ref> , STS <ref type="bibr">[τ ]</ref> (τ =2 0 , 50)i s superior especially in mAP scores, as shown in <ref type="table">Table 1</ref>. The results of xt/yt− semantic slices in <ref type="table">Table 2</ref> also reveal that the feature learning stage discovers motion patterns for semantic visual patterns, and they act well as the proxies to convey the motion patterns. Short-range v.s. Long-range Feature Learning. As shown in <ref type="table">Table 2</ref> other variants under both evaluation metrics. It demonstrats that the learned long-range features can actually increase the recognition power to find the crowd attributes that distinctively respond to long-range dynamics but are less likely to be identified by appearance alone, such as "performance" and "skate". See examples in <ref type="figure" target="#fig_4">Fig. 7(c)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Pruning of Features</head><p>Feature pruning is discussed in Section 3.2. Here we show that by pruning features that are less relevant to the characteristics of crowd, it is promising to observe that the pruned irrelevant features cuboids do not make an significant drop on the performance of crowd attribute recognition. In particular, we prune 412 and 256 feature cuboids respectively out of the total set (i.e. 512) at the layer conv4 3 with respect to the score defined in Section 3.2, and re-train the proposed deep models under the same setting as that of STS [100] 3 . Their mAUC and mAP are reported in comparison with the results by the default STS <ref type="bibr">[100]</ref> in <ref type="table">Table 3</ref>.</p><p>Compared with the default model STS <ref type="bibr">[100]</ref> with |N r | = 512, the models with |N r | = 256 (1) approach to the recognition results by STS [100] , (2) outperform STS [100] on 13 attributes, 7 of which belong to "why" (e.g. "board", "kneel", and "disaster"), and (3) save about 3% on memory and 34% on time. With 100 feature cuboids remained, the proposed S-CNN can still perform well, and superior to the state-ofthe-art methods (i.e. DLSF+DLMF <ref type="bibr" target="#b17">[18]</ref> and 3D-CNN <ref type="bibr" target="#b4">[5]</ref>), even with a single branch. For example, the xt-branch has 50.32% mAP which improves 9.1% and 11.2% from DLSF+DLMF and 3D-CNN respectively, and approaches to 51.84% by the Two-stream <ref type="bibr" target="#b20">[21]</ref>. To further demonstrate the proposed pruning strategy, we randomly pruned half of the filters (|N r | = 256 rnd) for the comparison. As observed from the <ref type="table">Table 3</ref>, the proposed pruning method performs much better than random pruning, suggesting the effectiveness of the proposed pruning strategy.</p><p>The results demonstrate: 1) the relevant spatial features are always companied with top ranks in H(n), proving the effectiveness of the proposed criteria. 2) spatial and dynamic representations can be represented by sparse yet effective feature cuboids. A small fraction of semantic feature cuboids are enough to fulfil crowd attribute recognition.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">Single Branch Model v.s. Combined Model</head><p>The combination of appearance and dynamic features indeed composes representative descriptions that identify crowd dynamics. Not surprisingly, the combined model integrating xy-, xtand yt-branches outperforms all singlebranch models under both evaluation metrics. Under the setting of semantic temporal slices with a temporal range of 100 frames and keeping all feature cuboids, the combined model S-CNN reports remarkable mAUC score 94.04% and mAP score 62.55%, which improve the optimal results of single-branch models by 3.3% (reported by xt-branch) in mAP. The improvement over mAUC is only 0.71%,b u t it might attribute to the deficiency of evaluation power. As shown in <ref type="table">Table 3</ref>, the S-CNN with |N r | = 100 and |N r | = 256 are also superior to the optimal single branch with improvements of 2.82% and 5.45% respectively. Qualitative comparisons between the spatial branch S-CNN-xy and the combined model S-CNN are in <ref type="figure" target="#fig_4">Fig. 7(b)</ref>, which further demonstrate the significance of the temporal branches as they help to improve the performance for most attributes. In particular, for attributes of motion like "mob" and "fight", "sit", "stand", "walk" and etc, S-CNN presents a remarkable discriminative power for identification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Comparison with State-of-the-Art Methods</head><p>We evaluate the combined Slicing CNN model (S-CNN) with recent state-of-the-art spatio-temporal deep feature learning models: 1) DLSF+DLMF <ref type="bibr" target="#b17">[18]</ref>. The DLSF+DLMF model is originally proposed for crowd attribute recognition. It is a twobranch model with a late fusion scheme. We employ their published model with the default setting.</p><p>2) Two-stream <ref type="bibr" target="#b20">[21]</ref>. The Two-stream contains two branches as a spatial net and a temporal net. We follow the setting by inputting 10-frame stacking optical flow maps for temporal net as adopted by both <ref type="bibr" target="#b20">[21]</ref> and <ref type="bibr" target="#b17">[18]</ref>. Besides, the parame- The upper one is evaluated by mean AUC and the lower one is by mean AP. The histograms are formed based on the mean scores for attributes of "Where", "Who" and "Why", respectively. "WWW" represents the evaluations on all attributes.</p><p>ters for temporal nets are also initialized with the VGG-16 model, as that in <ref type="bibr" target="#b26">[27]</ref> for action recognition.</p><p>3) 3D-CNN <ref type="bibr" target="#b4">[5]</ref>. A 3D-CNN model requires very large memory to capture long-range dynamics. As <ref type="bibr" target="#b4">[5]</ref> applied 3D kernels on hand-crafted feature maps, for fair comparison, we mimic it by extracting features in lower layers of STS <ref type="bibr">[100]</ref> , and substitute 3 × 3 × 33D kernels for all 2D kernels after conv4 3 layer and cut off half kernel numbers 4 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">Quantitative Evaluation</head><p>As shown in <ref type="figure" target="#fig_5">Fig. 8</ref>, histograms with respect to mAUC and mAP scores are generated to measure the performance on each type of crowd attributes, e.g. "Where", "Who" and "Why", as well as on the complete set "WWW". Clearly the proposed model outperforms the state-of-the-art meth-  <ref type="figure">Figure 9</ref>. Average precision scores for all attributes by S-CNN and Two-stream. The set of bars marked in red, green, and blue refer to "where", "who", and "why" respectively. The bars are sorted according to the larger APs between these two methods.</p><formula xml:id="formula_3">S D U N E H D F K LQ G R R U R X WG R R U U X Q Z D \ V WU H H W U LQ N F R Q F H UW E D WW OH I LH OG H V F D OD WR U U H V WD X U D Q W WH P S OH V WD G LX P F R Q I H U H Q F H F H Q WH U V K R S S LQ J B P D OO S OD WI R UP E D ] D D U OD Q G P D U N V T X D UH V F K R R O D LU S R UW F OD V V U R R P F K X U F K V WD J H V X E Z D \ S D V V D J H Z D \ V WR F N B P D UN H W WL F N H WB F R X Q WH U V N D WH U P R G H O V Z LP P H U P R E U X Q Q H U V R OG LH U D X G LH Q F H F K R LU S H U I R UP H U S LO J UL P S H G H V WU LD Q F R Q G X F WR U V WD U S D U D G H U S D V V H Q J H U Q H Z O\ Z H G B F R X S OH V WX G H Q W F X V WR P H U S U R WH V WH U S K R WR J U D S K H U WH D F K H U G D Q F H U V S H D N H U S R OL F H G LV D V WH U V Z LP P D U D WK R Q V N D WH I D V K LR Q B V K R Z E D Q G B S H U I R UP D Q F H Z D ON U X Q U H G F D U S H WB V K R Z S D U D G H I LJ K W Z D WF K B S H UI R U P D Q F H S H U I R UP D Q F H F K R U X V V LW S LO J UL P D J H V WD Q G J U D G X D WL R Q V K R S S LQ J G LQ LQ J D WW H Q G B F OD V V H V Z H G G LQ J Z D U S U R WH V W U</formula><p>ods under both metrics, and shows a large margin (particularly on mAP) over the second best approach in each sub-category. Among the reference methods, the Twostream presents the best performance in all sub-categories. DLSF+DLMF wins 3D-CNN by the mAUC score on all three attribute types but loses at "Why" by mAP score. The reference methods tend to perform worst on motion-related attributes like "why", because they can neither capture longterm dynamics as Two-stream or 3D-CNN, nor extract dynamic features from specific and hand-craft motion feature maps as DLSF+DLMF. Since the proposed method is able to capture the dynamic feature representations from longrange crowd video and semantically push the features to be crowd-related, its result is thus superior over all the rest methods. Notice that S-CNN also incorporates the appearance features, which increases the performance of attributes at "Where" and "Who" even further. Even with a pruning of 412 feature cuboids from S-CNN model, it can still reach 53.14% mAP which also outperforms 51.84% by Two-stream <ref type="bibr" target="#b20">[21]</ref>. We are also interested in the performance of each attribute. <ref type="figure">Fig. 9</ref> shows the overlapped histograms of average precisions for all attributes by Two-stream and S-CNN. The bars are grouped by their sub-categories and sorted in descending order according to the larger AP between these methods at one attribute. It is easy to find that the envelope superimposing this histogram is always supported by the bars of S-CNN with prominent performance gain against Two-stream, while just in 15 attributes the latter wins. Among the failure attributes, most of them contain ambiguities with each other and have low APs for both methods. It means the recognition power is defective to these attributes by the existing deep learning methods. For example, "cheer" and "wave" may be confused with each other, "queue" and "stand" may happen in similar scenes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">Qualitative Evaluation</head><p>We also conduct quantitative evaluations for a list of exemplar crowd videos as shown in <ref type="figure" target="#fig_4">Fig. 7(a)</ref>. The bars are shown as prediction probabilities. Although the probabilities of one attribute do not directly imply its actual recognition results, they uncover the discriminative power of different method as lower probability corresponds to ambiguity or difficulty in correctly predicting one attribute. The proposed S-CNN reliably predicts these attributes with complex or long-range dynamic features, like "graduation" and "ceremony", "parade" and "ride", "check-in/out"and etc. Moreover, some attributes that cannot be well defined by motion can also be revealed by S-CNN, for example "restaurant", "soldier" and "student". The appearance branch of S-CNN indeed captures the inherent appearance patterns belonging to these attributes. Some ambiguous cases do occur, e.g., "outdoor" in top-left and "sit" in bottom-left examples. The top-left instance takes place in a scene of airport/railway station -it is unclear whether the scene is an outdoor or indoor area.The bottom-left instance is a graduation ceremony, in which both "walk" and "sit" co-exist.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we present a novel Slicing CNN (S-CNN) for crowd video understanding, with only 2D filters. We show that the spatial (xy-) filters capture appearance information, while temporal-slice (xt-and yt-) filters capture dynamic cues like speed and acceleration in xand y-directions respectively. Their combination shows strong capacity in capturing spatio-temporal patterns, as evidence its results present superior performance in crowd attribute recognition on a large-scale crowd video dataset, against state-of-the-art deep models. We further show that spatial feature cuboids pruning could reduce redundancy leading to a sparser network. It is interesting to explore more strategies on feature cuboids selection in the future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>From a crowd video volume ice ballet performance in (b), several representative semantic feature cuboids and their temporal slices (xt and yt) are shown in (a) and (c). The temporal slices in the first row of (a) and (c) represent the dynamic patterns of dancers. Slices for the background visual patterns are visualized in the second row, where the pattern in (a) corresponds to background scene and that in (c) is audience.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>The architecture of the three-branch S-CNN model (i.e. S-CNN). The three branches share the same feature extraction procedure in the lower layers while adopt different 2D spatioand temporal-filters (i.e. xy-, xt-, yt-) in feature learning. A classifier (e.g. SVM) is applied to the concatenated features obtained from the three branches for crowd attribute recognition.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 .</head><label>5</label><figDesc>Semantic selectiveness of visual patterns by the spatial filters in conv4 3 layer of the VGG model. The orange patches in (a) and (c) mark the receptive fields of the strongest responses with a certain filter on the given crowd images in (b). The top five receptive fields from images in WWW crowd dataset that have the strongest responses of the corresponding filters are listed aside. (d) and (e) present patches that have strongest responses for the reserved spatial filters and pruned spatial filters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 .</head><label>6</label><figDesc>Single branch structure (i.e. S-CNN-xt). The whole structure is the same as the VGG-16 except the swap layers and the temporal pooling layer. Arrows in different colors denote different dimensions. (i) The first four bunches of convolutional layers in red are 2D convolutions on xy-slices, while the last bunch in orange are 2D convolutions on xt-slices. Following each bunch of the convolutional layers is a pooling layer. (ii) After the last convolutional layer (i.e. conv5 3), a temporal pooling layer in violet is adopted to fuse cues learned from different xt-slices by a 1 × 1 filter. (iii) The first two fully-connected layers both have 4096 neurons while the last one is 94 as the number of crowd attributes. All three branches (i.e. S-CNN-xt/-yt/-xy) use the same structure except with different types of filters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 .</head><label>7</label><figDesc>Qualitative recognition results on ground truth attributes annotated for the given examples. (a) Comparison between S-CNN and state-of-the-art methods. (b) Results by feeding temporal branch (S-CNN) and without feeding (S-CNN-xy). (c) S-CNN-STS learns on 100 and 20 frames. Different colors represent different methods. Bars are plot by the predict probabilities. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 .</head><label>8</label><figDesc>Performance comparisons with the referenced methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head></head><label></label><figDesc>Table 1. Results of S-CNN-xys learned from raw-and semanticlevel with different temporal ranges (bolds are the best).</figDesc><table>mean AUC 

mean AP 
τ 
20 
50 
100 
20 
50 
100 
RTS-xy 
91.06 92.28 
-
49.56 52.05 
-
STS-xy 
91.76 92.39 92.52 54.97 55.31 55.67 

methods 
τ 
mean AUC 
mean AP 
xy 
xt 
yt 
xy 
xt 
yt 
STS 
20 
91.76 91.11 90.52 54.97 52.38 50.08 
STS 
100 92.52 93.33 92.62 55.67 59.25 57.57 

Table 2. Results of S-CNNs learned from semantic-level with 
short-and long-range temporal slices. Results in bold are the best. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head></head><label></label><figDesc>,STS[100]  performs the best and beats the</figDesc><table>|Nr| 
mean AUC 
mean AP 
xy 
xt 
yt 
xyt 
xy 
xt 
yt 
xyt 
100 
91.02 91.70 91.16 92.31 46.83 50.32 48.18 53.14 
256 
92.61 92.49 92.22 93.49 54.68 54.13 53.26 60.13 
256 rnd 91.40 92,32 90.21 92.69 51.87 53.12 46.38 57.03 
512 
92.52 93.33 92.62 94.04 55.67 59.25 57.57 62.55 

Table 3. Results of STS [100] learned from different number of se-
mantic neurons. Results by single-branch models (xy, xt and yt) 
and the complete model (xyt) are presented. 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">This structure is used for all experiments except S-CNN-RTS (raw temporal slices from video volume), whose lower layers are not for feature extraction, but also fine-tuned for feature learning.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2"><ref type="bibr" target="#b17">[18]</ref> only uses AUC for evaluation.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Without other notations, STS [100] denotes the 100 frames-based S-CNN without feature cuboids pruning.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">It needs 90G to handle 100 frames by the original number of kernels.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Spatiotemporal energy models for the perception of motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Adelson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Bergen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JOSA A</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="284" to="299" />
			<date type="published" when="1985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Human action recognition in videos using kinematic features and multiple instance learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Sequential Deep Learning for Human Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Baccouche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mamalet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Baskurt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Human Behavior Understanding</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Violent flows: Real-time detection of violent crowd behavior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Itcher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Kliper-Gross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">3d convolutional neural networks for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>TPAMI</publisher>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Fully convolutional neural networks for crowd segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Large-scale Video Classification with Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Anomaly detection in extremely crowded scenes using spatio-temporal motion pattern models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kratz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nishino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">On space-time interest points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="107" to="123" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Crowd counting and profiling: Methodology and evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Modeling, Simulation and Visual Analysis of Crowds</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="347" to="382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">From semi-supervised to transfer counting of crowds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Crossing the line: Crowd counting by integer programming with local features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Abnormal crowd behavior detection using social force model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mehran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Motion analysis and segmentation through spatio-temporal slices processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-W</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-C</forename><surname>Pong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="341" to="355" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Analyzing gait with spatiotemporal surfaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Niyogi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Adelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Motion of Non-Rigid and Articulated Objects, Proceedings of the 1994 IEEE Workshop on</title>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="page" from="64" to="69" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Real-time tracking of moving persons by exploiting spatio-temporal image slices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ricquebourg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bouthemy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="797" to="808" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Datadriven crowd analysis in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Audibert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deeply Learned Attributes for Crowded Scene Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Scene-independent group profiling in crowd</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Learning sceneindependent group descriptors for crowd understanding. preprint, TCSVT</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Very Deep Convolutional Networks for Large-Scale Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Action recognition by dense trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Klaser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Action recognition with improved trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Visual tracking with fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Action recognition with trajectory-pooled deep-convolutional descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Towards good practices for very deep two-stream convnets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Unsupervised activity perception in crowded and complicated scenes using hierarchical bayesian models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">E L</forename><surname>Grimson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="539" to="555" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Chaotic invariants of lagrangian particle trajectories for anomaly detection in crowded scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">E</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Recognize complex events from static images by fusing deep channels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Dynamic texture recognition using local binary patterns with an application to facial expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pietikainen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>TPAMI</publisher>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="915" to="928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Measuring crowd collectiveness. TPAMI</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1586" to="1599" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
