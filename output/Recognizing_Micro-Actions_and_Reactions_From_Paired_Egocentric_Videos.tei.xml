<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Recognizing Micro-Actions and Reactions from Paired Egocentric Videos</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryo</forename><surname>Yonetani</surname></persName>
							<email>yonetani@iis.u-tokyo.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="institution">The University of Tokyo Tokyo</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kris</forename><forename type="middle">M</forename><surname>Kitani</surname></persName>
							<email>kkitani@cs.cmu.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Carnegie Mellon University Pittsburgh</orgName>
								<address>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoichi</forename><surname>Sato</surname></persName>
							<email>ysato@iis.u-tokyo.ac.jp</email>
							<affiliation key="aff2">
								<orgName type="institution">The University of Tokyo Tokyo</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Recognizing Micro-Actions and Reactions from Paired Egocentric Videos</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:44+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We aim to understand the dynamics of social interactions between two people by recognizing their actions and reactions using a head-mounted camera. Our work will impact several first-person vision tasks that need the detailed understanding of social interactions, such as automatic video summarization of group events and assistive systems. To recognize micro-level actions and reactions, such as slight shifts in attention, subtle nodding, or small hand actions, where only subtle body motion is apparent, we propose to use paired egocentric videos recorded by two interacting people. We show that the first-person and second-person points-of-view features of two people, enabled by paired egocentric videos, are complementary and essential for reliably recognizing micro-actions and reactions. We also build a new dataset of dyadic (two-persons) interactions that comprises more than 1000 pairs of egocentric videos to enable systematic evaluations on the task of micro-action and reaction recognition.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The dynamics of social interactions between two people can be decomposed into a sequence of action and reaction pairs (such as pointing and sharing a point of attention, gesturing and nodding in agreement, or laughing and gesturing disagreement) to convey to each other a sense of their internal states. Our everyday interactions even include microactions and micro-reactions in which only subtle body motion is apparent, such as slight changes in focus of attention (small movement of the head in response to pointing), subtle nodding, or small hand actions. The ability to understand interaction dynamics with such micro-behaviors is important for human-to-human communications, as this mode of non-verbal communication is perhaps our primary means of understanding and expressing our internal state. Towards understanding the deeper complexities of social interaction dynamics, this work attempts to take the first step by developing a method to recognize micro-actions and reactions.  <ref type="formula" target="#formula_0">(1)</ref> and <ref type="bibr" target="#b1">(2)</ref>. Hand motion by person A is difficult to observe from the A's points-of-view in <ref type="bibr" target="#b1">(2)</ref> and <ref type="formula" target="#formula_3">(3)</ref>.</p><p>To enable such recognition ability, we show in this work that it is critical to have access to a pair of egocentric videos taken by two interacting parties. Particularly, we focus exclusively on dyadic (i.e., two-person) interactions and assume that both people are equipped with a headmounted camera. In this setting, each person always has a first-person point-of-view (POV) observation of one's self in one's own video and a second-person POV observation of the self in another video. For example, <ref type="figure" target="#fig_0">Figure 1</ref>(1) shows person A pointing from both his own POV (left) and the POV of person B (right). In this way, egocentric videos are advantageous from a sensing perspective since the head motion and hand motion of camera wearers are often observed clearly in such videos, making it possible to perform various forms of first-person action recognition <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b29">30]</ref>. They can also be used to see the behavior of other people up-close from the second-person POV <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b44">45]</ref>.</p><p>One key observation to use a pair of egocentric videos is that a head-worn camera naturally amplifies subtle head motion and hand motion needed to recognize micro-actions  <ref type="figure">Figure 2</ref>. Our approach. Paired egocentric videos recorded by persons A and B are used to provide first-person and second-person POV features of both A and B, which are complementary and essential for recognizing micro-actions and reactions. Cumulative displacement patterns <ref type="bibr" target="#b26">[27]</ref> and improved dense trajectories <ref type="bibr" target="#b38">[39]</ref> are respectively visualized as examples of the first-person and second-person features. and micro-reactions. For example, slight changes in focus of attention or subtle nodding cannot be adequately recognized from a second-person POV because they only induce slight variations in local motion (e.g., person B seen in A's POV videos in the left of <ref type="figure" target="#fig_0">Figure 1</ref>(1)(2)). However, if we can gain access to the first-person POV of B, a small change in head pose translates to a large change in optical flows (the right of (1)(2)), making it possible to detect such microreactions. By contrast in <ref type="figure" target="#fig_0">Figure 1</ref>(2)(3), while hand motion of person A is not always large enough to be observed in the first-person POV (the left of (2)(3)), it is often more visible in the second-person POV including that person up-close (the right of (2)(3)).</p><p>Another key observation that motivates our work is that micro-actions and reactions are often correlated and best recognized when one has access to egocentric videos of both interacting parties. For example, in <ref type="figure" target="#fig_0">Figure 1</ref>(1), person A performs the action of pointing, which induces a micro-reaction of a shift in attention by person B. In other words, the context of pointing allows us to expect a responsive change in attention. <ref type="figure" target="#fig_0">Figure 1</ref>(2)(3) show other action-reaction pairs: hand gesture and positive response, and passing and receiving of an item. In fact our results show that such micro-actions and reactions cannot be reliably recognized without both sources of information.</p><p>Based on these two observations, we address the task using paired egocentric videos recorded by persons A and B to recognize micro-actions or reactions done by person A. Our proposed method works as follows. For each video, we first extract features of first-person POV observations of the self and second-person observations of his/her partner person (each row in <ref type="figure">Figure 2</ref>). Features for each person are then collected across videos to provide multiple POV features of the behavior (each column in the figure). These features are finally trained individually for A and B and fused to recognize A's micro-actions and reactions.</p><p>The main contributions of this work are as follows: (1) we propose the concept of micro-actions and micro-reactions, which are crucial for understanding the dynamics of social interactions; (2) we show that first-person and second-person POV features of two interacting parties are complementary and essential for recognizing micro-actions and reactions; and (3) we construct a new dataset of dyadic interactions comprising more than 1000 pairs of egocentric videos to enable systematic evaluations on micro-action and reaction recognition.</p><p>Related Work. First-person vision is one of the emerging topics in computer vision, which greatly affects several applications such as automatic activity summarization <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b42">43]</ref> and assistive systems <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b35">36]</ref>. Many studies have used egocentric videos to recognize behaviors of camera wearers, such as action/activity recognition <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b29">30]</ref>, object recognition <ref type="bibr" target="#b9">[10]</ref>, and gaze estimation <ref type="bibr" target="#b16">[17]</ref>, where all the visual events in input videos are implicitly assumed to be relevant to the wearer's behaviors. More recently, there has been an interest in understanding group activities recorded in the egocentric videos: e.g., social relationships <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>, eye contacts <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b44">45]</ref>, or joint attention <ref type="bibr" target="#b7">[8]</ref>. Particularly, Ryoo and Matthies have addressed the problem of recognizing interactions from egocentric videos <ref type="bibr" target="#b28">[29]</ref>. They, however, relied on a single video and recognized what a person in the video was doing to a stationary observer. In short, how we can use egocentric videos of two interacting people for recognizing micro-actions and reactions is still unexplored.</p><p>Similar to this work, there have been some attempts to use multiple videos but for other purposes. Temporallyaligned egocentric videos can be used for identifying wearers <ref type="bibr" target="#b45">[46]</ref> and estimating joint focus of attention <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23]</ref>. Other work has associated egocentric videos with thirdperson POV videos (e.g., surveillance videos) for wearer identification <ref type="bibr" target="#b25">[26]</ref> and localization <ref type="bibr" target="#b4">[5]</ref>. Another relevant task in which multiple videos are used is cross-view action recognition <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b40">41]</ref>, where they focus on the variations in appearances of actions in accordance with the changes in the pose and position of third-person cameras.</p><p>To the best of our knowledge, this work is the first to focus on micro-actions and reactions in human-to-human interactions. It is also unlike previous studies that recognize interactions from a third-person POV <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b36">37]</ref>. We will show that the combination of first-person and secondperson POV information enabled by the egocentric videos allows us to recognize various micro-actions and reactions that cannot be well observed in the third-person videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Our Approach</head><p>Suppose that we are given a pair of egocentric videos captured synchronously by person A and his/her partner person B during a dyadic interaction. In each video pair, we assume that person A performs one of several microactions and reactions. The goal of this work is to classify these micro-actions/reactions of A from the paired videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Recognition from Paired Egocentric Videos</head><p>Our recognition method relies on the multiple POV observations of both persons A and B presented in <ref type="figure">Figure 2</ref>.</p><p>To this end, we first consider first-person POV features denoted by f A , f B ∈ R N first (N first is the number of feature dimensions). These features are extracted from videos recorded by the self to describe a holistic change in the videos such as global motion patterns induced by head motion. We also introduce second-person POV features of each person obtained from the video taken by the other person (i.e., A observed from B's POV and vice versa), f A←B , f B←A ∈ R N second (N second is the number of feature dimensions). These features should be useful for capturing whole body appearance and motion of the person.</p><p>The first-person POV feature f A and second-person POV feature f A←B are then combined to provide multiple POV features of person A. To recognize A's actions and reactions, we define a standard linear decision function to describe the relative importance of the first-person and second-person features:</p><formula xml:id="formula_0">c A = (w (A) first ) T f A + (w (A) second ) T f A←B + u (A) ,<label>(1)</label></formula><p>where c A ∈ R is a decision score indicating how likely A is to perform a certain action or reaction. w</p><formula xml:id="formula_1">(A) first ∈ R N first , w (A)</formula><p>second ∈ R N second , and u A ∈ R are model parameters describing the importance of each feature; they can be optimized by training any classifiers such as a linear SVM.</p><p>Likewise, the multiple POV features for person B are obtained by combining the first-person feature f B and second-person one f B←A . We observe that actions or reactions taken by B are often affected by those of A, and thus the features extracted from B's behaviors can be a salient cue to recognize A's actions/reactions. For example, actions of passing an item by A can come with reactions of B receiving the item. Horizontal head rotations of A can stand for a negative response when B is talking to A, while they mean a shift in attention if B is pointing somewhere.</p><p>Our proposed method takes advantage of this relationship between A and B by refining A's decision score c A with B's multiple POV features. Specifically, we introduce another decision function that classifies A's actions and reactions but is learned from f B and f B←A :</p><formula xml:id="formula_2">c B = (w (B) first ) T f B + (w (B) second ) T f B←A + u (B) .<label>(2)</label></formula><p>Finally, c A is biased by the score c B :</p><formula xml:id="formula_3">c ′ A = c A + c B .<label>(3)</label></formula><p>This bias can work as follows. To enable classification, we learn functions in Eqs. <ref type="formula" target="#formula_0">(1)</ref> and <ref type="formula" target="#formula_2">(2)</ref> for each of several actions and reactions. Even if two micro-reactions (e.g., a negative response and an attention orientation with slight head motion) have similar scores in c A , the difference in actions by B appears in the score c B so that we can correct classification results in c ′ A .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">First-Person POV Features</head><p>In this section, we discuss how various features for firstperson action recognition can be used as a first-person POV feature to enable micro-action and reaction recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Egocentric and Object Features</head><p>Li et al. have focused on actions during hand-manipulation activities (e.g., cooking) <ref type="bibr" target="#b17">[18]</ref>. They revealed that effective features included head motion (homography between consecutive frames), hand manipulation points, and object features aligned with dense trajectories <ref type="bibr" target="#b38">[39]</ref> around points of gaze and manipulation points, where these features are individually encoded by the Fisher vector (FV) <ref type="bibr" target="#b23">[24]</ref>.</p><p>We expect head motion and object features to work robustly in our dyadic interaction scenarios. Although there may be fewer hand manipulations, the object features could be helpful when large hand motion is apparent from the first-person POV. We therefore adopt the FVs from head motion (E) and the combination of the FVs from head motion and object features (E+O).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Cumulative Displacement Patterns</head><p>Poleg et al. have proposed egocentric motion descriptors to enable temporal segmentation of egocentric videos based on activity classes <ref type="bibr" target="#b26">[27]</ref>. They rely on cumulative displacement (CD) patterns of motion vectors uniformly sampled in video frames, in which we can see long-term trends of egocentric motion in videos. In this work, we aim to use various perframe features extracted from the CD patterns (such as their slope, motion magnitudes, and radial projection responses that Poleg et al. presented <ref type="bibr" target="#b26">[27]</ref>) to indicate gradual changes of attentional directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.3">Pooled Time-Series Encoding</head><p>Since the CD features are designed to deal with long-term activities by smoothing motion patterns over time, it may not be optimal to describe short-term cyclic patterns such as head nodding and shaking. We therefore propose to encode the features with the pooled time series (PoT) recently presented by Ryoo et al. <ref type="bibr" target="#b29">[30]</ref>, which we refer to as PoTCD. In the encoder, per-frame CD features are first segmented temporally and hierarchically into several shorter patterns. Features in each segment are then encoded by a set of temporal pooling operators such as max/sum pooling and histograms of the positive/negative gradients. This way, our PoTCD features can deal with head motion patterns in detail as well as the gradual changes of attentional directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Second-Person POV Features</head><p>We introduce several generic action descriptors for second-person POV features that do not particularly require human detection. These features allow us to capture detailed appearances and motion of people observed from other people and work robustly against significant global motion induced by the head motion of camera wearers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1">Improved Dense Trajectory</head><p>The improved dense trajectory (IDT) <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b38">39]</ref> is a standard feature descriptor for action recognition used in thirdperson POV videos <ref type="bibr" target="#b41">[42]</ref> as well as egocentric videos <ref type="bibr" target="#b17">[18]</ref>. In the IDT, feature points are densely sampled based on the good-feature-to-track <ref type="bibr" target="#b30">[31]</ref> and tracked over a short time period (e.g., 15 frames) in accordance with dense optical flow fields. Features such as the histogram of oriented gradients (HOG), the histogram of oriented flows (HOF), and motion boundary histograms (MBH) are then extracted along trajectories and encoded by the FV.</p><p>We believe that the IDT is well suited to describe people from a second-person POV since it can extract relevant motion of the people without tracking them explicitly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2">Two-Stream Convolutional Networks</head><p>Instead of hand-crafted features like the dense trajectory, Simonyan and Zisserman <ref type="bibr" target="#b31">[32]</ref> learned feature representations and action classifiers in a convolutional neural network (CNN). Particularly, they introduced two-stream CNN (TCNN) where two CNNs individually learned the appearances and motions over a short period (e.g., 20 frames).</p><p>A CNN trained on a relevant dataset (e.g., action recognition datasets such as UCF101 <ref type="bibr" target="#b32">[33]</ref> and HMDB51 <ref type="bibr" target="#b13">[14]</ref>) can also be used as a feature descriptor. In this study, we use some mid-level convolution outputs drawn from the two CNNs and encoded them by the FV to serve as secondperson POV features. For input motion sequences, we compute local motion vectors by subtracting global motion displacements from original optical flow fields.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.3">Trajectory-Pooled Convolutional Descriptors</head><p>While the TCNN can provide rich information on both of the appearances and motion of people in videos, it encodes all the events occurring in the videos regardless of whether they belong to foregrounds (people) or backgrounds. To resolve this problem, we further pool TCNN features along dense trajectories as proposed by Wang et al. <ref type="bibr" target="#b39">[40]</ref> (which they refer to as TDD). Features extracted in this way can be limited to relevant events where trajectories appear.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experiments</head><p>We first systematically evaluate how the features introduced in the previous section can work on the task of detecting specific micro-actions and reactions observed during dyadic interactions in Sections 3.3 and 3.4. We also investigate how our method can classify micro-actions and reactions in Sections 3.5 and 3.6. Implementation details are described in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Paired Egocentric Video Dataset</head><p>Among the datasets of egocentric videos released to date, only a few include interaction scenes. JPL interaction dataset <ref type="bibr" target="#b28">[29]</ref> and EGO-GROUP dataset <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref> comprise only one POV video for each interaction scene. While the firstperson social interactions dataset <ref type="bibr" target="#b7">[8]</ref>, CMU first-person video dataset <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref>, and ego-surfing dataset <ref type="bibr" target="#b45">[46]</ref> provide egocentric videos of multiple people, none has enough interaction sequences to enable supervised learning of microactions and reactions from multiple POV observations.</p><p>In this work, we present a new video dataset named Paired Egocentric Video (PEV) dataset, a large collection of paired egocentric videos recorded during dyadic humanto-human interactions. The dataset contains 1226 pairs of videos in total, each of which includes a single micro-action or reaction pattern of a person regarded as target person A (see <ref type="figure">Figure 4</ref> for examples). All video pairs were selected from several continuous recordings of face-to-face conversations. There were six subjects wearing different clothes in eight different everyday environments such as a cafeteria and an office. Actions and reactions in the data have variability in motion and appearance since we did not particularly instruct subjects on how and when to perform actions or reactions during the recordings. We did however inform each subject of the following seven action/reaction types that we aimed to collect. Note that the remaining 315 pairs in the dataset contain noninteraction patterns where person A is just moving that are irrelevant to the current context of interactions: e.g., placing an item on a table or looking at a certain location to which person B did not particularly point. Each video has 90 frames (1.5 seconds at 60 fps) and the spatial resolution of 320x180, where the 30th frame of each video was adjusted to the onset of actions and reactions of A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Evaluation Scheme</head><p>Since the six subjects formed three pairs in the dataset, we conducted a three-fold cross validation by splitting the data into subsets on based on the pairs. We trained the decision functions in Eq. (1) and Eq. (2) by using two training subsets and evaluated performance with one testing subset.</p><p>In Sections 3.3 and 3.4, we evaluate detection performance by the area under the receiver-operating characteristic curve (AUC score) computed from decision scores (e.g., c A , c ′ A ) and binary ground-truth labels (1 for the correct actions/reactions and 0 otherwise) collected from all three tests. In the classification task in Sections 3.5 and 3.6, we further normalize the decision scores to have zero-mean and unit-variance for each action/reaction and compare them for each sample to find the most probable one. Average accuracies over all the actions and reactions are calculated for the classification performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Comparison among First-Person and Second-Person POV Features</head><p>We first focused on the use of single egocentric videos and compared detection performance for first-person POV features (E <ref type="bibr" target="#b17">[18]</ref>, E+O <ref type="bibr" target="#b17">[18]</ref>, CD <ref type="bibr" target="#b26">[27]</ref>, PoTCD <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b29">30]</ref>) and second-person ones (IDT <ref type="bibr" target="#b38">[39]</ref>, TCNN <ref type="bibr" target="#b31">[32]</ref> and TDD <ref type="bibr" target="#b39">[40]</ref>) of target person A. To this end, we performed detection based on A's decision score c A where the function in Eq. <ref type="bibr" target="#b0">(1)</ref> was learned from either f A or f A←B . <ref type="table" target="#tab_1">Table 1</ref>(1) shows AUC scores using first-person features. Overall, these features worked well for detecting reactions with head motion such as attention, positive, and negative. CD had a limited performance as it was not well suited to cyclic motion such as nodding. E+O performed better on receiving when large hand motion was made at the center of first-person POV clearly and captured by object features.</p><p>Among the second-person POV features described in Table 1(2), IDT performed better when the actions and reactions involved hand motion such as pointing, passing, receiving, and gesture. It worked particularly well on receiving since people often received items in front of their body that were clearly visible from the second-person POV. On the other hand, TCNN and TDD provided inferior scores. We found that the location where people appeared in egocentric videos often changed drastically over time due to significant head motion of camera wearers. As CNNs used in these two methods encoded appearances and motion at every fixed location at predefined intervals (20 frames), resultant features often became irrelevant when the location of people changed in a time shorter than the interval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Combining Multiple POV Features</head><p>We then investigated how the performance was improved by combining multiple POV features. In what follows, we pick out the four features producing AUC scores over 0.7 in the previous section: E, E+O, PoTCD, and IDT. <ref type="table" target="#tab_1">Table 1</ref>(3) shows results from the combination of firstperson POV and second-person POV features of target person A. Specifically, we evaluated A's decision score c A learned from both f A and f A←B . All the combined methods performed well regardless of whether actions and reactions came with head and/or hand motion, meaning that first-person and second-person features worked complementarily in the methods. Furthermore, <ref type="table" target="#tab_1">Table 1</ref>(4) confirms that the combination of multiple POV features of person A and those of B performed the best. In the Proposed method, we evaluated decision score c ′ A in Eq. (2) where the feature PoTCD was used for f A , f B and IDT for f A←B , f B←A . These results indicate that first-person and second-person POV observations of two people are essential for recognizing micro-actions and reactions.</p><p>To analyze the effect of using paired egocentric videos in more detail, we implemented some degraded versions of Proposed given only one of the two videos. In Degraded-A, we used the video recorded by person A to learn c A from only f A and c B from f B←A . On the other hand, Degraded-B accepted the video recorded by B and adopted f A←B in c A and f B in c B . Note that Degraded-B has the same conditions as the work of Ryoo and Matthies <ref type="bibr" target="#b28">[29]</ref>: only videos including a target person from the secondperson POV were available. The decreased performance of these methods in <ref type="table" target="#tab_1">Table 1</ref>(4) indicates the necessity of observing both videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Classifying Micro-Actions and Reactions</head><p>We finally investigated how our method could classify different micro-actions and reactions. We picked out PoTCD in <ref type="table" target="#tab_1">Table 1</ref>(1), IDT in (2), PoTCD in <ref type="formula" target="#formula_3">(3)</ref>, and Proposed in (4) as they provided good detection performance. <ref type="figure" target="#fig_1">Figure 3</ref> describes confusion matrices. As passing, receiving, and gesture often appeared subtly in front of one's body, they were difficult to classify where only the firstperson feature PoTCD was given. On the other hand, IDT could classify them while it was less discriminative on micro-reactions with subtle head motion such as attention, positive, and negative. We confirmed that PoTCD+IDT inherited the advantages of first-person and second-person features. Proposed further improved the performance on attention, positive, and receiving even when they came with small motions because these actions/reactions of A often induced different behavior of B. <ref type="figure">Figure 4</ref> presents some visual examples of classification results together with dense trajectories <ref type="bibr" target="#b38">[39]</ref> and cumulative displacement patterns <ref type="bibr" target="#b26">[27]</ref>. When hand motion was distinct in both of the first-person and second-person POVs (e.g., the pointing action annotated by the arrows in example (1)), all the methods were able to predict a correct action. Some micro-actions and reactions were observed with the combination of head and hand motions. These motions were not always large enough, as the pointing by person A could not be seen well in his/her first-person POV in example (2), <ref type="table">Table 2</ref>. Classification accuracies on the JPL dataset <ref type="bibr" target="#b28">[29]</ref>.</p><formula xml:id="formula_4">f B f A←B Degraded-B 0.61 0.70 0.75</formula><p>or the nodding was very slight in the second-person POV in <ref type="formula" target="#formula_3">(3)</ref>. Even for such cases, these two POV sources complementarily worked in PoTCD+IDT and Proposed. People seen in the second-person POV were often partially occluded especially when they were focusing on objects of interest as annotated in example <ref type="bibr" target="#b3">(4)</ref>. As the second-person features in Section 2.3 did not rely on human detection, our approach was robust against such cases. We also found some temporal structures between the head motion of two people. For instance, the cumulative displacement patterns in example <ref type="bibr" target="#b4">(5)</ref> illustrate that the head motion of A induced by the shift in attention was followed by the head motion of B to share attention. Similarly, mutual head motion was found when responding negatively as annotated in example <ref type="bibr" target="#b5">(6)</ref>. Proposed could classify such micro-reactions successfully by exploiting both actions and reactions of the two persons.</p><p>Gesture was the most difficult class to recognize even for our method (examples <ref type="formula">(7)</ref> and <ref type="formula">(8)</ref>). As annotated in the examples, gesture required both first-person and secondperson features since it often came with head and hand motion. This motion was however sometimes similar to other actions or reactions such as pointing, positive, and negative.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Evaluation on the JPL Dataset</head><p>We also evaluated the classification performance of our method on the JPL dataset <ref type="bibr" target="#b28">[29]</ref> 1 . It includes seven activities of a target person such as handshakes, hugs, and punches, some of which lasted longer (several seconds) than ours. As this dataset is composed of only the egocentric videos of a stationary observer (standing for person B), we compared Degraded-B against its degraded versions using either firstperson feature f B or second-person one f A←B to see the effectiveness of combining multiple POV features. We followed the same protocol as Ryoo and Matthies <ref type="bibr" target="#b28">[29]</ref> and repeated two-fold cross validations 100 times.</p><p>As shown in <ref type="table">Table 2</ref>, we found that the combination of first-person and second-person features in Degraded-B performed the best. Note that the method of Ryoo and Matthies <ref type="bibr" target="#b28">[29]</ref> performs better (0.896 as a classification accuracy) by incorporating structured prediction tailored to long-term activities with multiple sub-events. Future work will be to extend our method to cope with multiple action and reaction sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7.">Limitations</head><p>One current limitation of our method is that it only considers behaviors of two people taking place in the same time period. Recognizing actions and reactions with a large amount of delay will require a structured prediction <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b36">37]</ref>. In addition, we currently focus on only two-person scenarios. To generalize our work to deal with group interactions where more than two people are present, wearer identification <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b45">46]</ref> will be necessary to obtain a second-person POV observation of specific persons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusions</head><p>We have introduced the task of recognizing microactions and reactions in dyadic human-to-human interactions. The key finding of our work is that the microactions and reactions can be best recognized by utilizing first-person and second-person POV features of two interacting people. Understanding social interaction dynamics by recognizing micro-actions and reactions will impact several first-person vision tasks such as video summarization of social events and assistive systems, and also raise new problems such as wearer identification in crowded scenes and modeling of group interaction dynamics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Implementation Details</head><p>We adopted a linear SVM for the decision functions in Eq. (1) and Eq. (2) and trained them via stochastic gradient descent as it performed the best. As CD <ref type="bibr" target="#b26">[27]</ref> features were obtained per frame, we computed decision scores for each frame and averaged them over time. In PoTCD <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b29">30]</ref>, we used only three pyramids (split into one, two, and four segments) as we worked on short video clips.</p><p>On E, E+O <ref type="bibr" target="#b17">[18]</ref>, IDT <ref type="bibr" target="#b38">[39]</ref>, TCNN <ref type="bibr" target="#b31">[32]</ref> and TDD <ref type="bibr" target="#b39">[40]</ref>, we learned some additional models for the FV in the training datasets: the principal component analysis (PCA) to  <ref type="figure">Figure 4</ref>. Our proposed method working on the PEV dataset. The first row of each example shows 40th and 70th frames of the video recorded by target person A as well as its cumulative displacement patterns <ref type="bibr" target="#b26">[27]</ref> (motion vectors uniformly sampled in video frames and accumulated over time) that are encoded by pooled time-series <ref type="bibr" target="#b29">[30]</ref> in our proposed method. Dense trajectories <ref type="bibr" target="#b38">[39]</ref> are visualized by the yellow arrows in each video frame. The second row of each example provides the same visualization but for the video recorded by person B. Titles describe classification results (correct classifications are highlighted in green) as well as the ground-truth label. Micro-actions and reactions annotated by the pink arrows are discussed in Section 3.5.</p><p>perform a dimensionality reduction on features and the Gaussian mixture model (GMM) to generate the FV. We followed the original papers <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b39">40]</ref> to determine the number of components for the PCA and GMM. The PCA with half the number of original feature dimensions and the GMM with 64 components were used for E and E+O, and the PCA with 64 components and the GMM with 256 components were used for IDT, TCNN, and TDD. All the FVs were further applied the power and L2 normalizations <ref type="bibr" target="#b23">[24]</ref>. We used the code available on the web 2 for dense trajectories in E+O, IDT and TDD. As hand manipulations were barely found in the PEV dataset, in E+O we extracted the object features along all the trajectories. We adopted the CNNs trained by Wang et al. <ref type="bibr" target="#b39">[40]</ref> for TCNN and TDD. Based on the results from each convolution layer in the work of Wang et al. <ref type="bibr" target="#b39">[40]</ref>, we concatenated the outputs of the conv4 layer of spatial CNN and the conv3 layer of temporal CNN as second-person features.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Challenges of recognizing micro-actions. Slight head motion of person B induces only slight local motion in the person A's points-of-view in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>FirstFigure 3 .</head><label>3</label><figDesc>-person POV feature of person A (PoTCD; acc: 0.41) Second-person POV feature of person A (IDT ; acc: 0.41) Multiple POV features of A (PoTCD+IDT ; acc: 0.59) Multiple POV features of A and B (Proposed ; acc: 0.66) Confusion matrices and average accuracies of the classification task on the PEV dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>( 1 )</head><label>1</label><figDesc>GT:Pointing | PoTCD:Pointing | IDT:Pointing | PoTCD+IDT:Pointing | Proposed:Pointing (2) GT:Pointing | PoTCD:Attention | IDT:Pointing | PoTCD+IDT:Pointing | Proposed:Pointing (3) GT:Positive | PoTCD:Positive | IDT:Attention | PoTCD+IDT:Positive | Proposed:Positive (6) GT:Negative | PoTCD:Gesture | IDT:Receiving | PoTCD+IDT:Receiving | Proposed:Negative (4) GT:Passing | PoTCD:Attention | IDT:Passing | PoTCD+IDT:Passing | Proposed:Passing (5) GT:Attention | PoTCD:Pointing | IDT:Gesture | PoTCD+IDT:Gesture | Proposed:Attention (7) GT:Gesture | PoTCD:Passing | IDT:Receiving | PoTCD+IDT:Receiving | Proposed:Gesture (8) GT:Gesture | PoTCD:Positive | IDT:Passing | PoTCD+IDT:Passing | Proposed</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>1 .</head><label>1</label><figDesc>Pointing (154 samples): Pointing to a certain location, an item, or person B to initiate interaction, which is followed by B's reactions such as orienting of attention and positive or negative responses. Gesture (168 samples): Doing head and/or hand gestures to converse with B, which can be followed by B's gesture and positive or negative responses.</figDesc><table>2. Attention (97 samples): Orienting attention with 
slight head motion to what is pointed to by B. 

3. Positive (159 samples): Responding positively by 
widely or subtly nodding and/or by laughing with body 
motion to B's pointing or gesture. 

4. Negative (40 samples): Responding negatively by 
shaking or slightly cocking one's head and/or crossing 
arms to B's pointing or gesture. 

5. Passing (150 samples): Initiating or finishing passing 
an item to B in order to exchange it. 

6. Receiving (143 samples): Initiating or finishing re-
ceiving what B is trying to pass. 

7. </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 1 .</head><label>1</label><figDesc>AUC scores on the detection task. (1) First-person POV features of target person A. (2) Second-person POV features of A. (3) Combinations of first-person and second-person POV features of A. (4) Combinations of multiple POV features of persons A and B.</figDesc><table>Pointing Attention Positive Negative Passing Receiving Gesture Average 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">http://michaelryoo.com/jpl-interaction.html</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://lear.inrialpes.fr/people/wang/ improved_trajectories</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research was supported by CREST, JST and Kayamori Foundation of Informational Science Advancement.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Head pose estimation in first-person camera views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Alletto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Serra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Calderara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cucchiara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Pattern Recognition (ICPR)</title>
		<meeting>the International Conference on Pattern Recognition (ICPR)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Understanding social relationships in egocentric vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Alletto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Serra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Calderara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cucchiara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">From ego to nos-vision: Detecting social relationships in first-person views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Alletto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Serra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Calderara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Solera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cucchiara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="594" to="599" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Automatic editing of footage from multiple social cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Arev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hodgins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shamir</surname></persName>
		</author>
		<idno>81:1-81:11</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Egocentric fieldof-view localization using first-person point-of-view devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Bettadapura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Essa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pantofaru</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<meeting>IEEE Winter Conference on Applications of Computer Vision (WACV)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Discovering social interactions in real work environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ugarte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Aghajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Automatic Face Gesture Recognition and Workshops (FG)</title>
		<meeting>the IEEE International Conference on Automatic Face Gesture Recognition and Workshops (FG)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="933" to="938" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Understanding egocentric activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="407" to="414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Social interactions: A first-person perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Hodgins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning to recognize daily actions using gaze</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="314" to="327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning to recognize objects in egocentric activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="3281" to="3288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Action-reaction: Forecasting the dynamics of human interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Kitani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="489" to="504" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Viewindependent action recognition from temporal selfsimilarities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Junejo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Dexter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="172" to="185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Fast unsupervised ego-action learning for first-person sports videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kitani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Okabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sugimoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3241" to="3248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">HMDB: a large video database for human motion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Garrote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision (ICCV)</title>
		<meeting>the International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<biblScope unit="page" from="2556" to="2563" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Discovering important people and objects for egocentric video summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1346" to="1353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Visual navigation aid for the blind in dynamic environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-S</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Medioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="153" to="158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning to predict gaze in egocentric video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3216" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Delving into egocentric actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="287" to="295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Cross-view action recognition via view knowledge transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kuipers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="3209" to="3216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Story-driven summarization for egocentric video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2714" to="2721" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">3d social saliency from head-mounted cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Advances in Neural Information Processing Systems (NIPS)</title>
		<meeting>the Advances in Neural Information Processing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Predicting primary gaze behavior using social saliency fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Social saliency prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4777" to="4785" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Improving the fisher kernel for large-scale image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="143" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Detecting activities of daily living in first-person camera views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2847" to="2854" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Head motion signatures from egocentric videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Poleg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Peleg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Asian Conference on Computer Vision (ACCV)</title>
		<meeting>the Asian Conference on Computer Vision (ACCV)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Temporal segmentation of egocentric videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Poleg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Peleg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="2537" to="2544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Spatio-temporal relationship match: Video structure comparison for recognition of complex human activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ryoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Aggarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">First-person activity recognition: What are they doing to me?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Ryoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Matthies</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Pooled motion features for first-person videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Ryoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rothrock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Matthies</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="896" to="904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Good features to track</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tomasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="page" from="593" to="600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Advances in Neural Information Processing Systems (NIPS)</title>
		<meeting>the Advances in Neural Information Processing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">UCF101: A Dataset of 101 Human Action Classes From Videos in The Wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">An assistive eyewear prototype that interactively converts 3d object locations into spatial audio</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">J J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Symposium on Wearable Computers (ISWC)</title>
		<meeting>the ACM International Symposium on Wearable Computers (ISWC)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="119" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Placeavoider: Steering first-person cameras away from sensitive spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Templeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Korayem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Crandall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kapadia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Network and Distributed System Security Symposium (NDSS)</title>
		<meeting>the Annual Network and Distributed System Security Symposium (NDSS)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Wearable navigation system for the blind people in dynamic environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Cyber Technology in Automation, Control and Intelligent Systems (CYBER)</title>
		<meeting>the Cyber Technology in Automation, Control and Intelligent Systems (CYBER)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="153" to="158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A discriminative key pose sequence model for recognizing human interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranjbar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops (ICCV Workshops)</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops (ICCV Workshops)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1729" to="1736" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Action recognition by dense trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Klaser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="3169" to="3176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Action recognition with improved trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="3551" to="3558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Action recognition with trajectory-pooled deep-convolutional descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="4305" to="4314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">View invariant human action recognition using histograms of 3d joints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Aggarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="20" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A survey on aggregating methods for action recognition with dense trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Multimedia Tools and Applications</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Gaze-enabled egocentric video summarization via constrained submodular maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Warner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2235" to="2244" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Detecting eye contact using wearable eyetracking glasses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rozga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D</forename><surname>Abowd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Conference on Ubiquitous Computing (UbiComp)</title>
		<meeting>the ACM Conference on Ubiquitous Computing (UbiComp)</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="699" to="704" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Detecting bids for eye contact using a wearable camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bridges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rozga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Automatic Face and Gesture Recognition (FG)</title>
		<meeting>the IEEE International Conference on Automatic Face and Gesture Recognition (FG)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Ego-surfing firstperson videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yonetani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Kitani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
