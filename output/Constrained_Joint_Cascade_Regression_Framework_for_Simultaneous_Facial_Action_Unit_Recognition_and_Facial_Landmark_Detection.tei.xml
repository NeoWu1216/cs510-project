<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Constrained Joint Cascade Regression Framework for Simultaneous Facial Action Unit Recognition and Facial Landmark Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">ECSE Department</orgName>
								<orgName type="institution">Rensselaer Polytechnic Institute</orgName>
								<address>
									<addrLine>110 8th street</addrLine>
									<settlement>Troy</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Ji</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">ECSE Department</orgName>
								<orgName type="institution">Rensselaer Polytechnic Institute</orgName>
								<address>
									<addrLine>110 8th street</addrLine>
									<settlement>Troy</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Constrained Joint Cascade Regression Framework for Simultaneous Facial Action Unit Recognition and Facial Landmark Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:38+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Cascade regression framework has been shown to be effective for facial landmark detection. It starts from an initial face shape and gradually predicts the face shape update from the local appearance features to generate the facial landmark locations in the next iteration until convergence. In this paper, we improve upon the cascade regression framework and propose the Constrained Joint Cascade Regression Framework (CJCRF) for simultaneous facial action unit recognition and facial landmark detection, which are two related face analysis tasks, but are seldomly exploited together. In particular, we first learn the relationships among facial action units and face shapes as a constraint. Then, in the proposed constrained joint cascade regression framework, with the help from the constraint, we iteratively update the facial landmark locations and the action unit activation probabilities until convergence. Experimental results demonstrate that the intertwined relationships of facial action units and face shapes boost the performances of both facial action unit recognition and facial landmark detection. The experimental results also demonstrate the effectiveness of the proposed method comparing to the state-of-the-art works.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Facial action unit recognition and facial landmark detection are two important tasks for face analysis. Facial action unit recognition refers to the automatic estimation of the Action Units (AU) defined in the Facial Action Coding System (FACS) <ref type="bibr" target="#b2">[3]</ref>. FACS system and the facial action units provide objective measurements of the facial muscle movements and the facial motions. Facial landmark detection refers to the localization of the facial key points, such as the eye corners and the nose tip on facial images. The locations of the detected landmarks characterize the face shape. Both facial action unit recognition and facial landmark detection  would enable the machine understanding of human facial behavior, intent, emotion etc.</p><p>Facial action unit recognition and facial landmark detection are related tasks, but they are seldomly exploited jointly in the literatures. For example, the face shape defined by the landmark locations are considered as effective features for AU recognition. But, the landmark location information is usually extracted beforehand with facial landmark detection algorithms. On the other hand, the Action Unit information is rarely utilized in the literature to help facial landmark detection, even though the facial muscle movements and the activation of specific facial action unit can cause the appearance and shape changes of the face which significantly affect facial landmark detection. The mutual information and intertwined relationship among facial action unit recognition and facial landmark detection should be utilized to boost the performances of both tasks.</p><p>Cascade regression framework has been shown to be an effective method for face alignment recently <ref type="bibr" target="#b18">[19]</ref> <ref type="bibr" target="#b12">[13]</ref>. It starts from an initial face shape (e.g. mean face) and it iteratively updates the facial landmark locations based on the local appearance features until convergence. Several regression models have been applied to learn the mapping from the local appearance features to the face shape update.</p><p>To leverage the success of the cascade regression framework and to achieve the goal of joint facial action unit recognition and landmark detection, we proposed the Constrained Joint Cascade Regression Framework (CJCRF). The general framework of the proposed method is shown in <ref type="figure" target="#fig_1">Figure 1</ref>. First, we learn the relationship among facial shapes and facial action units as a constraint before the cascade training. Second, in the constrained joint cascade regression framework, with the help from the constraint, we iteratively update the landmark locations and the action unit activation probabilities. When updating the landmark locations, we use the local appearance information, the currently estimated AU activation probabilities, and the pre-trained constraint that captures the joint relationship among AUs and face shapes. When updating the AU activation probabilities, we use the local appearance information, currently estimated landmark locations, and the pre-trained constraint that captures the relationship among AUs and face shapes. The landmark detection and AU estimation interact to reach convergence.</p><p>The major contributions of the proposed method are as follows:</p><p>• Joint cascade regression framework: We improve upon the effective cascade regression framework and propose the Constrained Joint Cascade Regression Framework (CJCRF). CJCRF jointly performs facial landmark detection and facial action unit recognition, which is novel comparing to most of the existing methods that estimate them separately. • Constraint: CJCRF explicitly utilizes the relationship among facial action units and face shapes as a constraint to improve both the facial action unit recognition and facial landmark detection. In addition, the AU relationship and face shape patterns are embedded in the constraint. • Experiments: The experiments show that the joint cascade regression framework boosts the performances of both facial action unit recognition and facial landmark detection, comparing to the state-of-the-art works.</p><p>The remaining part of the paper is organized as follows. In section 2, we review the related work. In section 3, we discuss the proposed method. In section 4, we evaluate the proposed method and compare it to state-of-the-art works. We conclude the paper in section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Facial landmark detection</head><p>The facial landmark detection algorithms can be classified into the holistic methods <ref type="bibr" target="#b0">[1]</ref> <ref type="bibr" target="#b10">[11]</ref>, the Constrained Local Method (CLM) <ref type="bibr" target="#b1">[2]</ref>, and the regression based methods <ref type="bibr" target="#b14">[15]</ref> <ref type="bibr" target="#b18">[19]</ref>. The holistic methods learn holistic appearance and face shape models, while the CLM learns the holistic shape model with local appearance model. The regression based methods directly learn the mapping from image appearance to the face shape without explicit appearance and shape models.</p><p>The regression based methods can be classified into the direct mapping methods and the cascade regression framework. The direct mapping methods directly map the image appearance to the absolute landmark locations <ref type="bibr" target="#b14">[15]</ref>. The cascade regression framework starts from an initial face shape (e.g. mean face shape), and it gradually updates the landmark locations from the local appearance information collected around the currently predicted landmark locations to generate the facial landmark locations in the next iteration until convergence. The cascade regression based methods usually differ in the features and the regression models. For example, in <ref type="bibr" target="#b18">[19]</ref>, SIFT features and linear regression model is used. In <ref type="bibr" target="#b12">[13]</ref>, local binary features learned from the regression tree models are combined with the linear regression models to predict the landmark locations. The proposed method follows the cascade regression framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Facial action unit recognition</head><p>The facial action unit recognition algorithms usually focus on the improvements of the features or the classifier design. The features can be classified into appearance features and shape features. The popular appearance features include the LBP features <ref type="bibr" target="#b6">[7]</ref>, the Local Gabor Binary Pattern (LGBP) features <ref type="bibr" target="#b13">[14]</ref>, and the Discrete Cosine Transform (DCT) features <ref type="bibr" target="#b4">[5]</ref>. The shape features are extracted from the facial landmark locations, and typical shape features are the absolute landmark locations, the distance between pairs of points, and the angles defined by a set of three points. In terms of classifier design, the facial action units can be recognized independently or jointly with machine learning techniques. Comparing to the independent AU recognition methods <ref type="bibr" target="#b9">[10]</ref> that ignore the AU relationship, the joint methods usually can achieve better performance by adding the AU relationship or dynamic dependencies. For example, in <ref type="bibr" target="#b3">[4]</ref>, the AU relationship is embedded in the Multiconditional Latent Variable Model (MC-LVM). In <ref type="bibr" target="#b17">[18]</ref>, the global AU relationship is modeled by the Hierarchical Restricted Boltzmann Machine (HRBM) model. In <ref type="bibr" target="#b19">[20]</ref>, joint AU recognition task is formulated as a Multi-Task Multiple Kernel Learning (MTMKL) problem. In <ref type="bibr" target="#b21">[22]</ref>, pairwise AU relationships are learned. In <ref type="bibr" target="#b7">[8]</ref>, the AU relationship is learned from prior knowledge. In <ref type="bibr" target="#b16">[17]</ref>, the temporal information is incorporated for AU recognition.</p><p>Although facial action unit recognition and facial landmark detection are related tasks, their interaction is usually one way in the aforementioned methods, i.e. facial landmark locations are extracted as features for facial action unit recognition. <ref type="bibr" target="#b8">[9]</ref> is the most similar work that jointly performs facial landmark tracking and facial action unit recog-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1: General Cascade Regression Framework</head><p>Initialize the landmark locations x 0 using the mean face. for t=1, 2, ..., T or convergence do Update the landmark locations, given the image and the current landmark location.</p><formula xml:id="formula_0">ft : I, x t−1 → ∆x t x t = x t−1 + ∆x t end</formula><p>Output the estimated landmark locations x T . nition by building a hierarchical Dynamic Bayesian Network to capture their joint relationship. Our model differs from <ref type="bibr" target="#b8">[9]</ref>. While they capture local dependencies, our model captures global AU relationship, global face shape patterns, and global dependencies among AU and facial landmarks. While they perform one-time prediction, we iteratively update the AU activation probabilities and landmark locations to achieve robustness. In <ref type="bibr" target="#b20">[21]</ref>, facial expression recognition is considered as an auxiliary task to help the learning of better feature representations for facial landmark detection. The auxiliary tasks do not include action unit recognition, and their performances are not reported. In contrast, we aim to exploit the interdependencies between AU and landmarks to improve both tasks. In addition, we require less training data, comparing to this deep method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Constrained Joint Cascade Regression Framework</head><p>In this section, we first review the general cascade regression framework and then introduce the proposed Constrained Joint Cascade Regression Framework (CJCRF).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">General cascade regression framework</head><p>Before we discuss the proposed method, we first review the general cascade regression framework, which has been successfully applied to facial landmark detection <ref type="bibr" target="#b18">[19]</ref>[13]. The overall algorithm is shown in Algorithm 1. Assume that the facial landmark locations are denoted as x ∈ ℜ 2D , where D is the number of facial landmark points. The test image is denoted as I. Starting from the mean face x 0 , the cascade regression method iteratively predicts the face shape update ∆x t based on the local appearance features with regression model, and adds the face shape update to the current shape x t−1 to generate the shape x t for the next iteration until convergence.</p><p>The existing methods use different image features and regression functions in the cascade regression framework. For example, in the Supervised Descent Method (SDM) <ref type="bibr" target="#b18">[19]</ref>, linear regression model is used to learn the</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 2: Constrained Joint Cascade Regression Framework (CJCRF)</head><p>Learn the constraint: Learn the joint relationship among facial landmark locations and action unit labels, the AU relationship, face shape patterns as a constraint, denoted as C(.).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Constrained Joint Cascade Regression Framework:</head><p>Initialize the landmark locations x 0 using the mean face; Initialize the AU activation probability as 0.5, p 0 i = 0.5, ∀i for t=1, 2, ..., T or convergence do Update the landmark locations, given the image, the current landmark location, the current AU activation probabilities, and the constraint.</p><formula xml:id="formula_1">ft : I, x t−1 , p t−1 , C(.) → ∆x t x t = x t−1 + ∆x t</formula><p>Update the AU activation probabilities, given the image, the current landmark location, and the constraint.</p><formula xml:id="formula_2">gt : I, x t , C(.) → ∆p t p t = p t−1 + ∆p t end</formula><p>Output the estimated landmark locations x T and the AU activation probabilities p T .</p><p>prediction.</p><formula xml:id="formula_3">f t : ∆x t = R t Φ(x t−1 , I),<label>(1)</label></formula><p>where Φ(x t−1 , I) ∈ ℜ 128D denotes the local appearance features (e.g. SIFT) centered at the currently predicted landmark locations, and R t is the parameter of the linear regression model. For training, given the training data {x * m , I m }, where x * m represents the ground truth facial landmark locations, the ground truth face shape update can be calculated as ∆x t, * m = x * m − x t−1 m . Then, the model parameter R t can be estimated in a least square formulation and solved with the closed form solution:</p><formula xml:id="formula_4">R t = arg min R m ∆x t, * m − RΦ(x t−1 m , I m ) 2 2 . (2)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Constrained joint cascade regression framework</head><p>The proposed Constrained Joint Cascade Regression Framework (CJCRF) improves upon the general cascade regression framework and it jointly performs facial landmark detection and facial action unit recognition. Assume the binary facial action unit labels are denoted as a ∈ {0, 1} N , where N is the number of estimated facial action units. p i = P (a i = 1; I) denotes the AU activation probability of action unit a i for the testing image I. p = {p 1 , p 2 , ..., p N } refers to the AU activation probability vector for all the AUs. The goal of CJCRF is to jointly estimate the landmark locations x and the AU activation probabilities p given the testing image, from which we estimate the AU labels a.</p><p>The general framework of CJCRF is shown in <ref type="figure" target="#fig_1">Figure 1</ref> and Algorithm 2. First, we learn the joint relationship among facial landmark locations x and facial action unit labels a. Note that AU relationship and face shape patterns are also embedded. Second, in the constrained joint cascade regression framework, we iteratively update the landmark locations and the AU activation probabilities, with the help from the pre-learned joint relationship as a constraint (denoted as C(.)). In particular, we initialize the landmark locations x 0 using the mean face, and initialize the initial AU activation probabilities p 0 i = 0.5, ∀i. When updating the landmark locations, we predict the face shape update ∆x t based on the currently estimated landmark locations x t−1 , the AU activation probabilities p t−1 , and the constraint C(.). The estimated shape update is added to the current face shape to generate the landmark locations for the next iteration. When updating the AU activation probabilities, we predict the probability update ∆p t based on the currently estimated landmark locations x t and the constraint C(.). The probability update is then added to the currently estimated AU activation probabilities.</p><p>One important property of the proposed method is that we iteratively update both the landmark locations and AU activation probabilities. The iterative procedure would accumulate information from all the iterations to achieve robustness comparing to one-time estimation. In the following, we discuss the three major components of the proposed method, including the joint relationship constraint, landmark location prediction, and AU activation probability prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">AU and facial landmark relationship constraint</head><p>We learn the joint relationship among face shapes and the binary action unit labels using the Restricted Boltzmann Machine model (RBM). As shown in <ref type="figure" target="#fig_2">Figure 2</ref>, RBM model captures the joint probability of visible variables through several binary hidden nodes h. In our application, the visible nodes are the continues facial landmark locations x and the binary facial action unit labels a: Here, E(a, x, h; θ) is the energy function, and Z(θ) is the partition function. The parameter θ = {W x , W a , b x , b a , c} includes the pairwise parameters W x ∈ ℜ 2D×K , W a ∈ ℜ N ×K , and the biases b x ∈ ℜ 2D , b a ∈ ℜ N , c ∈ ℜ K . K is the number of hidden variables. Given the ground truth pairwise training data with AU labels and landmark locations {a m , x m } M m=1 , model training can be performed with Contrastive Divergence algorithm (CD) <ref type="bibr" target="#b5">[6]</ref>.</p><formula xml:id="formula_5">P (a, x; θ) = h e −E(a,x,h;θ) Z(θ) (3) E(a, x, h; θ) = j (x j − b x,j ) 2 2 − b T a a − c T h − x T W x h − a T W a h<label>(4)</label></formula><formula xml:id="formula_6">Z(θ) = a,x,h e −E(a,x,h;θ)<label>(5)</label></formula><p>The model captures three levels of relationships. First, the joint probability P (a, x; θ) captures the global and joint relationship among facial landmark locations and facial action unit labels. Second, the AU relationship is embedded in the marginal distribution P (a; θ). Third, the face shape patterns are also embedded in the marginal distribution P (x; θ). Those three levels of information can be used in two ways in the constrained joint cascade regression framework. On one hand, once we know the activation of particular action unit (e.g. a i = 1), we could find out the face shape that is consistent with the AU, and this information should be included to help the facial landmark detection. In particular, we use the AU dependent expected face shape, where the the expectation is taken over the marginal conditional probabilities P (x|a i = 1; θ) embedded in the RBM model.</p><formula xml:id="formula_7">E P (x|ai=1;θ) [x] = x xP (x|a i = 1; θ)dx<label>(6)</label></formula><p>The AU dependent shapes characterize the common properties of the face shapes that have the activation of particular AU, and they change with different AUs. For example, as shown in <ref type="figure" target="#fig_3">Figure 3</ref>, given the activation of different AUs, we have different prior knowledge of the face shapes. On the other hand, once we have some knowledge of the current face shape x, we could also estimate the AU activation probabilities from the model P (a i = 1|x; θ), and this information should be incorporated for AU recognition. For example, <ref type="figure" target="#fig_4">Figure 4</ref> shows the estimated AU activation probabilities given one particular face shape from the model. It can be seen that, the face shape would provide distinct knowledge about the activations of AUs. In the following, we further explain how to use the learned joint relationship as a constraint to help the predictions of action unit activation probability and landmark locations.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Update the landmark locations with constraint</head><p>Given the learned relationships among facial landmark locations and facial action unit labels as a constraint, the constrained cascade regression method iteratively predicts the facial landmark locations and the facial action unit activation probabilities. When updating the landmark locations, the method predicts the facial shape update based on the currently estimated landmark locations x t−1 , the AU activation probabilities p t−1 , and the RBM model with parameter θ that embeds the relationships. It is formulated as a general optimization problem:</p><formula xml:id="formula_8">minimize ∆x t ∆x t − R t Φ(x t−1 , I) 2 2 ,</formula><p>subject to x t = x(p t−1 , θ)</p><p>where x t = x t−1 + ∆x t . The formulation leverages two information resources. In the objective function, similar to the conventional cascade regression method in Equation 1, we predict the face shape update ∆x t from the local appearance features Φ(x t − 1, I) with linear regression model. In addition, we explicitly add the constraint to ensure that the predicted face shape is consistent with the currently predicted AU activation probabilities p t−1 . In particular, x(p t−1 , θ) denotes the estimation of the facial landmark locations from the the joint relationship embedded in RBM based on the currently estimated AU activation probabilities.</p><formula xml:id="formula_10">x(p t−1 , θ) = i E P (x|ai=1;θ) [x] p t−1 i l p t−1 l<label>(8)</label></formula><p>Here, E P (x|ai=1;θ) [x] is the AU dependent expected face shape, calculated from the RBM model that captures the relationship among AUs and facial shape in Equation <ref type="bibr" target="#b5">6</ref>.</p><p>To take into account of the currently estimated AU activation probabilities, those AU dependent expected face shape are combined with weights that are based on the currently estimated AU activation probabilities. The intuition is to have higher weight for specific AU dependent expected face shape if the activation probability of the particular AU is high in the current testing image. In inference, with lagrangian relaxation, Equation 7 becomes a standard least square problem, and it can be solved in the closed form solution. Model training refers to the learning of the linear regression parameter R t . Given the training data, the learning of R t is similar to that in the general cascade regression framework as shown in Equation 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Update the action unit activation probabilities with constraint</head><p>When updating the AU activation probabilities, we predict the AU activation probability update from the current landmark locations x t , the current AU activation probabilities p t−1 , and the RBM model with parameter θ. We formulate this as a general optimization problem:</p><formula xml:id="formula_11">minimize ∆p t ∆p t − T t Φ(x t , I) 2 2 , subject to p t i = P (a i = 1|x t ; θ), ∀i 0 ≤ p t ≤ 1 (9)</formula><p>where p t = p t−1 + ∆p t . The prediction of AU activation probabilities utilizes two information resources. In the objective function, we use linear regression model with parameter T t to predict the AU activation probability update from the local appearance features Φ(x t , I). In the constraint, we ensure that the predicted AU activation probability p t i is consistent with the current landmark locations x t . In particular, given the current shape x t , we could estimate the AU activation probabilities from the RBM model that captures the relationship among AUs and facial shape using P (a i = 1|x t ; θ) as illustrated in section 3.2.1. The estimated AU activation probability of the same AU a i for the current testing image, denoted as p t i , should be close to the prediction from the prior RBM model.</p><p>In inference, with the lagrangian relaxation, the optimization problem in Equation 9 becomes a least square problem and it can be solved with the closed form solution. Model training refers to the learning of the linear regression function T t , and the estimation is similar to that shown in Equation 2. In particular, given the training data, the ground truth probabilities p * m can be generated based on the ground truth AU labels a * m for image I m (p * i = P (a i = 1) = 1 if a * i = 1). The ground truth probability update can be calculated as ∆p t, * m = p * m −p t−1 m . Then the parameter estimation is formulated as a least square problem with the closed form solution:</p><formula xml:id="formula_12">T t = arg min T m ∆p t, * m − TΦ(x t m , I m ) 2 2<label>(10)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation details</head><p>Database: We evaluate the proposed method on both the posed and spontaneous data. We use the Extended Cohn-Kanade database(CK+) <ref type="bibr" target="#b9">[10]</ref> as the posed data. It contains 593 facial activity videos from 210 subjects with various ethnicities. The peak frames are FACS coded with 30 facial action units. We manually labeled 28 facial landmarks on the peak frames from each sequence and performed the evaluations on the peak frames. To evaluate the experiments on spontaneous data, we use the SEMAINE database <ref type="bibr" target="#b11">[12]</ref> and the FERA database <ref type="bibr" target="#b15">[16]</ref>, in which the facial sequences undergo significant variations of speech related movements, head poses, etc. In the SEMAINE database, the emotionally colored conversation between the human subjects and machine agents is recorded, where the emotion related facial activity of subjects are naturally induced by the agent. FACS are coded for 180 images from 8 sessions. We also manually labeled 28 facial landmarks on those frames. The FERA database contains 87 AU related sequences of 7 actors, and conversion is also involved. 50 AU annotations for each frame are provided and we manually annotated 28 facial landmarks on 260 frames for the experiments.</p><p>Model: The RBM model that learns the joint relationship among AUs and landmarks, contains 150 hidden node, and we train it with 800 epochs. To calculate the local appearance features, we use SIFT and set the local region with a radius about 0.166 of the face size. There are four iterations in the cascade framework. Following the previous cascade regression method <ref type="bibr" target="#b18">[19]</ref>, we augment the training images by adding random scale, rotation, and translation perturbations to the initial face shape. When calculating the AU dependent face shapes in Equation 6, we average the training data with activated corresponding AU. The lagrangian relaxation parameters are set to be 0.5.</p><p>Evaluation criteria: To calculate the facial landmark detection error, we use the distance between the detected point and the ground truth point normalized by the interocular distance. We calculate the mean error by averaging over all the available points and testing images. To evaluate the facial action unit recognition accuracy, we use the F 1 score and the area under the ROC curve (AUC). Similar to the previous work <ref type="bibr" target="#b9">[10]</ref>, the score is weighted by the frequency of the presents of AUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Performance for posed facial actions 4.2.1 Evaluation of the proposed method</head><p>Depending on whether we use the joint relationships among facial shape and AUs as equality constraints in Equation 7 and 9, there are four versions of the proposed method. In (1) CJCRF-NoConstraint, there is no equal- ity constraint. Facial landmark detection is performed independently without AU information. AU recognition is only based on the local appearance information. In <ref type="formula">(2)</ref> CJCRF-ConstraintLandmark, AU information is incorporated through the constraint to help landmark detection. In (3) CJCRF-ConstraintAU, facial shape information is incorporated through the constraint to help AU recognition, comparing to (1). In (4) CJCRF, the joint relationships among landmark locations and AUs are incorporated to help both tasks. Here, we evaluate the four versions of the proposed method on the posed CK+ database with five-folder cross validation and mutually excluded subjects.</p><p>The experimental results are shown in <ref type="table" target="#tab_0">Table 1</ref>. There are a few observations. First, without the constraint on AU and Landmark detection, the baseline method (CJCRF-NoConstraint) performs the worst. As shown in <ref type="figure" target="#fig_5">Figure 5</ref>, without the AU information, the standard cascade regression landmark detection algorithm <ref type="bibr" target="#b18">[19]</ref> would fail with limited training data.</p><p>Second, comparing CJCRF-ConstraintLandmark to CJCRF-NoConstraint, by adding the constraint to landmark prediction, the landmark detection error is significantly reduced with the slight improvement of AU recognition accuracy. Third, comparing CJCRF-ConstraintAU to CJCRF-NoConstraint, by adding the constraint to AU prediction, the AU recognition performance improves obviously. Lastly, with the constraint on both the landmark and AU prediction, the full model (CJCRF) achieves the best performances.  <ref type="figure" target="#fig_7">Figure 6</ref> shows that by using the proposed full CJCRF method, both the facial landmark detection and AU recognition performances increase over cascade iterations and they converge quickly. With Matlab implementation, the full model achieves 5 fps on a single core machine.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Comparison with state-of-the-arts</head><p>To compare the proposed method to the state-of-the-art works, we retrain the full model (CJCRF) with leave-one- <ref type="table">Table 2</ref>. Comparison of the proposed method to state-of-the-art works on CK+ database <ref type="bibr" target="#b9">[10]</ref>. "*" denotes the reported results from the original paper. "(.)" denotes the results with different experimental settings.   subject-out cross validation setting, following the previous works <ref type="bibr" target="#b17">[18]</ref>. Since different algorithms may be evaluated on different AUs, we show the performances of the proposed method on three AU sets (trained also with those AUs  <ref type="table">Table 2</ref>. Although the performances on CK+ become saturate, especially for AU recognition, our algorithm still can improve the accuracies. For facial landmark detection, the proposed method significantly outperforms the Active Ap-pearance Model as used in <ref type="bibr" target="#b9">[10]</ref>, and the Supervised Descent Method (SDM) <ref type="bibr" target="#b18">[19]</ref>. The results from the DBN <ref type="bibr" target="#b8">[9]</ref> are not directly comparable, since their model was evaluated on a particular data subset, which is not accessible. <ref type="figure" target="#fig_8">Figure 8</ref> (a) shows more sample images with landmark detection results on the CK+ database. For AU recognition, there are a few observations. First, on 17 AU set, the performance of the proposed method is better than <ref type="bibr" target="#b9">[10]</ref> that uses SVM, and the HRBM <ref type="bibr" target="#b17">[18]</ref>. We also implemented another baseline AU recognition method that uses SVM and concatenate SIFT features extracted around detected landmark locations with SDM algorithm for each AU independently. Its F 1 score and AU C are 71.33% and 91.03% respectively. Second, on the 13 AU set and 8 AU set, the proposed method achieves the best performance. The recognition performance on each individual AU using the proposed method and the baseline method <ref type="bibr" target="#b9">[10]</ref> is shown in <ref type="figure" target="#fig_6">Figure 7</ref> (a).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Performance for non-posed facial actions</head><p>In this section, we evaluate the proposed constrained joint cascade regression method (full model CJCRF) on the spontaneous SEMAINE database <ref type="bibr" target="#b11">[12]</ref> and FERA database <ref type="bibr" target="#b15">[16]</ref>. For the SEMAINE database, similar as the previous work <ref type="bibr" target="#b17">[18]</ref>, we perform facial action unit recognition of the 10 most frequent AUs: 1, 2, 4, 5, 6, 7, 12, 17, 25, and 26. For the FERA database, we evaluate the 8 most frequent AUs: 1, 2, 4, 6, 7, 12, 15, and 17.</p><p>The experimental results on SEMAINE database are shown in <ref type="table" target="#tab_3">Table 3</ref>. We evaluate the proposed method in  two settings. In the first cross database setting, we train the model on CK+ database, and test it on the SEMAINE database. In the second setting, we perform leave-onesubject-out cross validation on the SEMAINE database. There are a few observations. First, with the cross database setting, for facial landmark detection, CJCRF is significantly better than SDM <ref type="bibr" target="#b18">[19]</ref> as an conventional cascade regression method. For AU recognition, the proposed CJCRF method outperforms the baseline SVM algorithm and is comparable to the HRBM method <ref type="bibr" target="#b17">[18]</ref>. One possible reason is that HRBM uses manually labeled eye locations to extract the features, while we rely on automatically detected facial landmarks. Second, with the leave-one-subject-out cross validation setting, the proposed method is better than SVM as the baseline. Third, the performance of all methods drop significantly on the difficult SEMAINE database.</p><p>The experimental results on FERA database are shown in <ref type="table">Table 4</ref>. With the same leave-one-subject-out cross validation setting as the previous work <ref type="bibr" target="#b7">[8]</ref>, the proposed method outperforms the LGBP <ref type="bibr" target="#b13">[14]</ref>, the Data-free Model <ref type="bibr" target="#b7">[8]</ref>, the HRBM <ref type="bibr" target="#b17">[18]</ref> method, and DCT <ref type="bibr" target="#b4">[5]</ref> for facial action unit recognition. For facial landmark detection, the proposed method outperforms the SDM <ref type="bibr" target="#b18">[19]</ref>.</p><p>In <ref type="figure" target="#fig_6">Figure 7</ref> (b)(c), we plot the F 1 scores of AU recognition performance on each individual AU for the spontaneous <ref type="table">Table 4</ref>. Comparison of the proposed method to state-of-the-art works on FERA database <ref type="bibr" target="#b15">[16]</ref>. "*" denotes the reported results from the original paper. detection AU recognition error F1(%) SDM <ref type="bibr" target="#b18">[19]</ref> 6.17 -LGBP <ref type="bibr" target="#b13">[14]</ref> -46.24* Data-free <ref type="bibr" target="#b7">[8]</ref> -52.62* HRBM <ref type="bibr" target="#b17">[18]</ref> -54.60* DCT <ref type="bibr" target="#b4">[5]</ref> -56.58* CJCRF (proposed) 5.90 59.66 databases. <ref type="figure" target="#fig_8">Figure 8</ref> (b)(c) show some sample images with landmark detection results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we proposed the Constrained Joint Cascade Regression Framework (CJCRF) that improves over the existing effective cascade regression method. The CJCRF method jointly performs facial action unit recognition and facial landmark detection. The model first learns the joint relationship among facial action units and face shapes as a constraint. Then, in the constrained joint cascade regression framework, with the help from the constraint, the method iteratively updates the facial landmark locations and the AU activation probabilities until convergence. The experiments demonstrate that the intertwined relationship of facial action units and face shapes boost the performances of both facial landmark detection and facial action unit recognition. They also demonstrate the effectiveness of the proposed method comparing to state-of-the-art works on both poses and spontaneous databases.</p><p>In the future, we would extend the proposed method to perform AU recognition on images with large head movements. One possible direction is to decouple the rigid head movement and non-rigid deformation before estimation. Furthermore, the proposed method can be improved using more advanced features and dynamic information.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>Constrained joint cascade regression framework for simultaneous facial action unit recognition and landmark detection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Restricted Boltzmann Machine (RBM) model that embeds the joint relationships among AUs and landmark, the AU relationship, and the face shape patterns.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Blue solid shape: AU dependent expected face shape. Red dotted shape: mean face. (Better see in color)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Estimated AU activation probabilities (right) given the facial shape (left). Blue: current face shape. Red: mean face.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Left two images: landmark detections without AU info. Right two images: proposed method with AU info.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .</head><label>7</label><figDesc>F1 scores for individual action unit on different databases. The baseline methods are: (a) SVM<ref type="bibr" target="#b9">[10]</ref>, (b) SVM, (c) Data-free<ref type="bibr" target="#b7">[8]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 .</head><label>6</label><figDesc>Performances of the proposed method over cascade iterations. (a) Landmark detection errors. (b) AU recognition, F1 scores.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 .</head><label>8</label><figDesc>Facial landmark detection results using the proposed method on sample images from different databases.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 .</head><label>1</label><figDesc>Comparison of different versions of the proposed methods. See text for the descriptions of each version.</figDesc><table>detection AU recognition 
error 
F1(%) AUC 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head></head><label></label><figDesc>12, 15 and 17. For each AU set, the baseline methods may either predict only the AUs in the set, or predict more AUs but share the common AUs as the proposed method.The experimental comparison results are shown in</figDesc><table>). The 
17 AU set contains AU 1, 2, 4, 5, 6, 7, 9, 11, 12, 15, 17, 
20, 23, 24, 25, 26 and 27. The 13 AU set contains AU1, 
2, 4, 5, 6, 7, 9, 12, 15, 17, 24, 25, and 27. The 8 AU set 
contains AU1, 2, 4, 6, 7, </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="true"><head>Table 3 .</head><label>3</label><figDesc>Comparison of the proposed method to state-of-the-arts on SEMAINE database<ref type="bibr" target="#b11">[12]</ref>. Cross database: train on CK+, test on SEMAINE. Within database: cross subject validation on SE-MAINE. "*" denotes the reported results from the original paper.</figDesc><table>Cross database 
Within database 
detection 
AU (F1) detection 
AU (F1) 
SDM [19] 
11.06 
-
12.46 
-
SVM 
-
47.70 
-
51.74 
HRBM [18] 
-
54.76* 
-
-
CJCRF 
8.28 
52.22 
10.38 
56.68 
(proposed) 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements: The work described in this paper was supported in part by the Federal Highway Administration via grant DTFH6114C00005 and in part by the IBM Ph.D. Fellowship.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Active appearance models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">F</forename><surname>Cootes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="681" to="685" />
			<date type="published" when="2001-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Automatic feature localisation with constrained local models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cristinacce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cootes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="3054" to="3067" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Facial Action Coding System: A Technique for the Measurement of Facial Movement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ekman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Friesen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1978" />
			<publisher>Consulting Psychologists Press</publisher>
			<pubPlace>Palo Alto</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multiconditional latent variable model for joint facial action unit detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Eleftheriadis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Rudovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV), 2015 IEEE International Conference on</title>
		<imprint>
			<date type="published" when="2015-12" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A common framework for real-time emotion recognition and facial action unit detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gehrig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ekenel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<publisher>CVPRW</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Training products of experts by minimizing contrastive divergence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1771" to="1800" />
			<date type="published" when="2002-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Action unit detection using sparse appearance descriptors in space-time video volumes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Valstar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automatic Face Gesture Recognition and Workshops</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="314" to="321" />
		</imprint>
	</monogr>
	<note>2011 IEEE International Conference on</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Data-free prior model for facial action unit recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on affective computing</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="127" to="141" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Simultaneous facial feature tracking and facial expression recognition. Image Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The extended cohn-kanade dataset (ck+): A complete dataset for action unit and emotion-specified expression. In Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Saragih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ambadar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Society Conference on</title>
		<imprint>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Active appearance models revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="135" to="164" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The semaine database: Annotated multimodal records of emotionally colored conversations between a person and a limited agent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mckeown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Valstar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cowie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schroder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5" to="17" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Face alignment at 3000 fps via regressing local binary features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Combining aam coefficients with lgbp histograms in the multi-kernel svm framework to detect facial action units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Senechal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Rapp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Salam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Seguier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bailly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Prevost</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automatic Face Gesture Recognition and Workshops</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="860" to="865" />
		</imprint>
	</monogr>
	<note>2011 IEEE International Conference on</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep convolutional network cascade for facial point detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3476" to="3483" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The first facial expression recognition and analysis challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Valstar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mehu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automatic Face Gesture Recognition and Workshops</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="921" to="926" />
		</imprint>
	</monogr>
	<note>2011 IEEE International Conference on</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Fully automatic recognition of the temporal phases of facial actions. Systems, Man, and Cybernetics, Part B: Cybernetics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Valstar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2012-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Capturing global semantic relationships for facial action unit recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV), 2013 IEEE International Conference on</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="3304" to="3311" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Supervised descent method and its applications to face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>De La Torre Frade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A lpnorm mtmkl framework for simultaneous detection of multiple facial action units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mahoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mavadati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Applications of Computer Vision (WACV), 2014 IEEE Winter Conference on</title>
		<imprint>
			<date type="published" when="2014-03" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Facial landmark detection by deep multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 13th European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Joint patch and multi-label learning for facial action unit detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>De La Torre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
