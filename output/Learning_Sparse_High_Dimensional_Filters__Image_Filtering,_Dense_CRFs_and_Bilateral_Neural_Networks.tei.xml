<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Sparse High Dimensional Filters: Image Filtering, Dense CRFs and Bilateral Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Jampani</surname></persName>
							<email>varun.jampani@tuebingen.mpg.de</email>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Intelligent Systems</orgName>
								<address>
									<postCode>72076</postCode>
									<settlement>Tübingen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Kiefel</surname></persName>
							<email>martin.kiefel@tuebingen.mpg.de</email>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Intelligent Systems</orgName>
								<address>
									<postCode>72076</postCode>
									<settlement>Tübingen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Bernstein Center for Computational Neuroscience</orgName>
								<address>
									<postCode>72076</postCode>
									<settlement>Tübingen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
							<email>peter.gehler@tuebingen.mpg.de</email>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Intelligent Systems</orgName>
								<address>
									<postCode>72076</postCode>
									<settlement>Tübingen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Bernstein Center for Computational Neuroscience</orgName>
								<address>
									<postCode>72076</postCode>
									<settlement>Tübingen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Sparse High Dimensional Filters: Image Filtering, Dense CRFs and Bilateral Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:42+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Bilateral filters have wide spread use due to their edgepreserving properties. The common use case is to manually choose a parametric filter type, usually a Gaussian filter. In this paper, we will generalize the parametrization and in particular derive a gradient descent algorithm so the filter parameters can be learned from data. This derivation allows to learn high dimensional linear filters that operate in sparsely populated feature spaces. We build on the permutohedral lattice construction for efficient filtering. The ability to learn more general forms of high-dimensional filters can be used in several diverse applications. First, we demonstrate the use in applications where single filter applications are desired for runtime reasons. Further, we show how this algorithm can be used to learn the pairwise potentials in densely connected conditional random fields and apply these to different image segmentation tasks. Finally, we introduce layers of bilateral filters in CNNs and propose bilateral neural networks for the use of highdimensional sparse data. This view provides new ways to encode model structure into network architectures. A diverse set of experiments empirically validates the usage of general forms of filters.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Image convolutions are basic operations for many image processing and computer vision applications. In this paper we will study the class of bilateral filter convolutions and propose a general image adaptive convolution that can be learned from data. The bilateral filter <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b42">42,</ref><ref type="bibr" target="#b45">45]</ref> was originally introduced for the task of image denoising as an edge preserving filter. Since the bilateral filter contains the spatial convolution as a special case, we will in the following directly state the general case. Given an image v = (v 1 , . . . , v n ), v i ∈ R c with n pixels and c channels, and for every pixel i, a d dimensional feature vector f i ∈ R d (e.g., the (x, y) position in the image f i = (x i , y i ) ⊤ ). The bilateral filter then computes</p><formula xml:id="formula_0">v ′ i = n j=1 w fi,fj v j .<label>(1)</label></formula><p>for all i. Almost the entire literature refers to the bilateral filter as a synonym of the Gaussian parametric form</p><formula xml:id="formula_1">w fi,fj = exp (− 1 2 (f i − f j ) ⊤ Σ −1 (f i − f j ))</formula><p>. The features f i are most commonly chosen to be position (x i , y i ) and color (r i , g i , b i ) or pixel intensity. To appreciate the edgepreserving effect of the bilateral filter, consider the fivedimensional feature f = (x, y, r, g, b)</p><p>⊤ . Two pixels i, j have a strong influence w fi,fj on each other only if they are close in position and color. At edges the color changes, therefore pixels lying on opposite sides have low influence and thus this filter does not blur across edges. This behaviour is sometimes referred to as "image adaptive", since the filter has a different shape when evaluated at different locations in the image. More precisely, it is the projection of the filter to the two-dimensional image plane that changes, the filter values w f ,f ′ do not change. The filter itself can be of c dimensions w fi,fj ∈ R c , in which case the multiplication in Eq. (1) becomes an inner product. For the Gaussian case the filter can be applied independently per channel. For an excellent review of image filtering we refer to <ref type="bibr" target="#b34">[35]</ref>. The filter operation of Eq. (1) is a sparse highdimensional convolution, a view advocated in <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b35">36]</ref>. An image v is not sparse in the spatial domain, we observe pixels values for all locations (x, y). However, when pixels are understood in a higher dimensional feature space, e.g., (x, y, r, g, b), the image becomes a sparse signal, since the r, g, b values lie scattered in this five-dimensional space. This view on filtering is the key difference of the bilateral filter compared to the common spatial convolution. An image edge is not "visible" for a filter in the spatial domain alone, whereas in the 5D space it is. The edge-preserving behaviour is possible due to the higher dimensional operation. Other data can naturally be understood as sparse signals, e.g., 3D surface points.</p><p>The contribution of this paper is to propose a general and learnable sparse high dimensional convolution. Our technique builds on efficient algorithms that have been developed to approximate the Gaussian bilateral filter and re-uses them for more general high-dimensional filter operations. Due to its practical importance (see related work in Sec. 2) several efficient algorithms for computing Eq. (1) have been developed, including the bilateral grid <ref type="bibr" target="#b35">[36]</ref>, Gaussian KDtrees <ref type="bibr" target="#b2">[3]</ref>, and the permutohedral lattice <ref type="bibr" target="#b1">[2]</ref>. The design goal for these algorithms was to provide a) fast runtimes and b) small approximation errors for the Gaussian filter case. The key insight of this paper is to use the permutohedral lattice and use it not as an approximation of a predefined kernel but to freely parametrize its values. We relax the separable Gaussian filter case from <ref type="bibr" target="#b1">[2]</ref> and show how to compute gradients of the convolution (Sec. 3) in lattice space. This enables learning the filter from data. This insight has several useful consequences. We discuss applications where the bilateral filter has been used before: image filtering (Sec. 4) and CRF inference (Sec. 5). Further we will demonstrate how the free parametrization of the filters enables us to use them in deep convolutional neural networks (CNN) and allow convolutions that go beyond the regular spatially connected receptive fields (Sec. 6). For all domains, we present various empirical evaluations with a wide range of applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>We categorize the related work according to the three different generalizations of this work.</p><p>Image Adaptive Filtering: The literature in this area is rich and we can only provide a brief overview. Important classes of image adaptive filters include the bilateral filters <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b45">45,</ref><ref type="bibr" target="#b42">42]</ref>, non-local means <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b4">5]</ref>, locally adaptive regressive kernels <ref type="bibr" target="#b44">[44]</ref>, guided image filters <ref type="bibr" target="#b23">[24]</ref> and propagation filters <ref type="bibr" target="#b38">[38]</ref>. The kernel least-squares regression problem can serve as a unified view of many of them <ref type="bibr" target="#b34">[35]</ref>. In contrast to the present work that learns the filter kernel using supervised learning, all these filtering schemes use a predefined kernel. Because of the importance of the bilateral filtering to many applications in image processing, much effort has been devoted to derive fast algorithms; most notably <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b20">21]</ref>. Surprisingly, the only attempt to learn the bilateral filter we found is <ref type="bibr" target="#b24">[25]</ref> that casts the learning problem in the spatial domain by rearranging pixels. However, the learned filter does not necessarily obey the full region of influence of a pixel as in the case of a bilateral filter. The bilateral filter also has been proposed to regularize a large set of applications in <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b7">8]</ref> and the respective optimization problems are parametrized in a bilateral space. In these works the filters are part of a learning system but unlike this work restricted to be Gaussian.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dense CRF:</head><p>The key observation of <ref type="bibr" target="#b30">[31]</ref> is that meanfield inference update steps in densely connected CRFs with Gaussian edge potentials require Gaussian bilateral filtering operations. This enables tractable inference through the application of a fast filter implementation from <ref type="bibr" target="#b1">[2]</ref>. This quickly found wide-spread use, e.g., the combination of CNNs with a dense CRF is among the best performing segmentation models <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b49">49,</ref><ref type="bibr" target="#b10">11]</ref>. These works combine structured prediction frameworks on top of CNNs, to model the relationship between the desired output variables thereby significantly improving upon the CNN result. Bilateral neural networks, that are presented in this work, provide a principled framework for encoding the output relationship, using the feature transformation inside the network itself thereby alleviating some of the need for later processing. Several works <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b49">49,</ref><ref type="bibr" target="#b39">39]</ref> demonstrate how to learn free parameters of the dense CRF model. However, the parametric form of the pairwise term always remains a Gaussian. Campbell et al. <ref type="bibr" target="#b13">[14]</ref> embed complex pixel dependencies into an Euclidean space and use a Gaussian filter for pairwise connections. This embedding is a pre-processing step and can not directly be learned. In Sec. 5 we will discuss how to learn the pairwise potentials, while retaining the efficient inference strategy of <ref type="bibr" target="#b30">[31]</ref>.</p><p>Neural Networks: In recent years, the use of CNNs enabled tremendous progress in a wide range of computer vision applications. Most CNN architectures use spatial convolution layers, which have fixed local receptive fields. This work suggests to replace these layers with bilateral filters, which have a varying spatial receptive field depending on the image content. The equivalent representation of the filter in a higher dimensional space leads to sparse samples that are handled by a permutohedral lattice data structure. Similarly, Bruna et al. <ref type="bibr" target="#b11">[12]</ref> propose convolutions on irregularly sampled data. Their graph construction is closely related to the high-dimensional convolution that we propose and defines weights on local neighborhoods of nodes. However, the structure of the graph is bound to be fixed and it is not straightforward to add new samples. Furthermore, re-using the same filter among neighborhoods is only possible with their costly spectral construction. Both cases are handled naturally by our sparse convolution. Jaderberg et al. <ref type="bibr" target="#b26">[27]</ref> propose a spatial transformation of signals within the neural network to learn invariances for a given task. The work of <ref type="bibr" target="#b25">[26]</ref> propose matrix backpropagation techniques which can be used to build specialized structural layers such as normalized-cuts. Graham et al. <ref type="bibr" target="#b22">[23]</ref> propose extensions from 2D CNNs to 3D sparse signals. Our work enables sparse 3D filtering as a special case, since we use an algorithm that allows for even higher dimensional data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Splat Convolve</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gauss Filter</head><p>Slice <ref type="figure">Figure 1</ref>. Schematic of the permutohedral convolution. Left: splatting the input points (orange) onto the lattice corners (black); Middle: The extent of a filter on the lattice with a s = 2 neighborhood (white circles), for reference we show a Gaussian filter, with its values color coded. The general case has a free scalar/vector parameter per circle. Right: The result of the convolution at the lattice corners (black) is projected back to the output points (blue). Note that in general the output and input points may be different.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Learning Sparse High Dimensional Filters</head><p>In this section, we describe the main technical contribution of this work, we generalize the permutohedral convolution <ref type="bibr" target="#b1">[2]</ref> and show how the filter can be learned from data.</p><p>Recall the form of the bilateral convolution from Eq. (1). A naive implementation would compute for every pixel i all associated filter values w fi,fj and perform the summation independently. The view of w as a linear filter in a higher dimensional space, as proposed by <ref type="bibr" target="#b35">[36]</ref>, opened the way for new algorithms. Here, we will build on the permutohedral lattice convolution developed in Adams et al. <ref type="bibr" target="#b1">[2]</ref> for approximate Gaussian filtering. The most common application of bilateral filters use photometric features (XYRGB). We chose the permutohedral lattice as it is particularly designed for this dimensionality, see <ref type="figure">Fig. 7</ref> in <ref type="bibr" target="#b1">[2]</ref> for a speed comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Permutohedral Lattice Convolutions</head><p>We first review the permutohedral lattice convolution for Gaussian bilateral filters from Adams et al. <ref type="bibr" target="#b1">[2]</ref> and describe its most general case. As before, we assume that every image pixel i is associated with a d-dimensional feature vector f i . Gaussian bilateral filtering using a permutohedral lattice approximation involves 3 steps. We begin with an overview of the algorithm, then discuss each step in more detail in the next paragraphs. <ref type="figure">Fig. 1</ref> schematically shows the three operations for 2D features. First, interpolate the image signal on the d-dimensional grid plane of the permutohedral lattice, which is called splatting. A permutohedral lattice is the tessellation of space into permutohedral simplices. We refer to <ref type="bibr" target="#b1">[2]</ref> for details of the lattice construction and its properties. In <ref type="figure" target="#fig_0">Fig. 2</ref>, we visualize the permutohedral lattice in the image plane, where every simplex cell receives a different color. All pixels of the same lattice cell have the same color. Second, convolve the signal on the lattice. And third, retrieve the result by interpolating the signal at the original d-dimensional feature locations, called slicing. For example, if the features used are a combination of position and into the 5D cross product space of position and color and then convolved with a 5D tensor. The result is then mapped back to the original space. In practice we use a feature scaling Df with a diagonal matrix D and use separate scales for position and color features. The scale determines the distance of points and thus the size of the lattice cells. More formally, the computation is written by v ′ = S slice BS splat v and all involved matrices are defined below. For notational convenience we will assume scalar input signals v i , the vector valued case is analogous, the lattice convolution changes from scalar multiplications to inner products.</p><formula xml:id="formula_2">color f i = (x i , y i , r i , g i , b i ) ⊤ , the input signal is mapped (a) Sample Image (b) Position (c) Color (d) Position, Color</formula><p>Splat: The splat operation (cf. left-most image in <ref type="figure">Fig. 1</ref>) finds the enclosing simplex in O(d 2 ) on the lattice of a given pixel feature f i and distributes its value v i onto the corners of the simplex. How strong a pixel contributes to a corner j is defined by its barycentric coordinate t i,j ∈ R inside the simplex. Thus, the value l j ∈ R at a lattice point j is computed by summing over all enclosed input points; more precisely, we define an index set J i for a pixel i, which contains all the lattice points j of the enclosing simplex</p><formula xml:id="formula_3">ℓ = S splat v; (S splat ) j,i = t i,j , if j ∈ J i , otherwise 0. (2)</formula><p>Convolve: The permutohedral convolution is defined on the lattice neighborhood N s (j) of lattice point j, e.g., only s grid hops away. More formally</p><formula xml:id="formula_4">ℓ ′ = Bℓ; (B) j ′ ,j = w j,j ′ , if j ′ ∈ N s (j), otherwise 0. (3)</formula><p>An illustration of a two-dimensional permutohedral filter is shown in <ref type="figure">Fig. 1</ref> (middle). Note that we already presented the convolution in the general form that we will make use of. The work of <ref type="bibr" target="#b1">[2]</ref> chooses the filter weights such that the resulting operation approximates a Gaussian blur, which is illustrated in <ref type="figure">Fig. 1</ref>. Further, the algorithm of <ref type="bibr" target="#b1">[2]</ref> takes advantage of the separability of the Gaussian kernel. Since we are interested in the most general case, we extended the convolution to include non-separable filters B.</p><p>Slice: The slice operation (cf. right-most image in <ref type="figure">Fig. 1</ref>) computes an output value v ′ i ′ for an output pixel i ′ again based on its barycentric coordinates t i,j and sums over the corner points j of its lattice simplex</p><formula xml:id="formula_5">v ′ = S slice ℓ ′ ; (S slice ) i,j = t i,j , if j ∈ J i , otherwise 0 (4)</formula><p>The splat and slice operations take a role of an interpolation between the different signal representations: the irregular and sparse distribution of pixels with their associated feature vectors and the regular structure of the permutohedral lattice points. Since high-dimensional spaces are usually sparse, performing the convolution densely on all lattice points is inefficient. So, for speed reasons, we keep track of the populated lattice points using a hash table and only convolve at those locations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Learning Permutohedral Filters</head><p>The fixed set of filter weights w from <ref type="bibr" target="#b1">[2]</ref> in Eq. 3 are designed to approximate a Gaussian filter. However, the convolution kernel w can naturally be understood as a general filtering operation in the permutohedral lattice space with free parameters. In the exposition above we already presented this general case. As we will show in more detail later, this modification has non-trivial consequences for bilateral filters, CNNs and probabilistic graphical models.</p><p>The size of the neighborhood N s (k) for the blur in Eq. 3 compares to the filter size of a spatial convolution. The filtering kernel of a common spatial convolution that considers s points to either side in all dimensions has</p><formula xml:id="formula_6">(2s + 1) d ∈ O(s d ) parameters. A comparable filter on the permutohedral lattice with an s neighborhood is specified by (s + 1) d+1 − s d+1 ∈ O(s d ) elements (cf. Supplemen- tary material)</formula><p>. Thus, both share the same asymptotic size. By computing the gradients of the filter elements we enable the use of gradient based optimizers, e.g., backpropagation for CNN in the same way that spatial filters in a CNN are learned. The gradients with respect to v and the filter weights in B of a scalar loss L are:</p><formula xml:id="formula_7">∂L ∂v = S ′ splat B ′ S ′ slice ∂L ∂v ′ ,<label>(5)</label></formula><formula xml:id="formula_8">∂L ∂(B) i,j = S ′ slice ∂L ∂v i (S splat v) j .<label>(6)</label></formula><p>Both gradients are needed during backpropagation and in experiments, we use stochastic backpropagation for learning the filter kernel. The permutohedral lattice convolution is parallelizable, and scales linearly with the filter size. Specialized implementations run at interactive speeds in image processing applications <ref type="bibr" target="#b1">[2]</ref>. Our implementation in caffe deep learning framework <ref type="bibr" target="#b27">[28]</ref> allows arbitrary filter parameters and the computation of the gradients on both CPU and GPU. The code is available online 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Single Bilateral Filter Applications</head><p>In this section we will consider the problem of joint bilateral upsampling <ref type="bibr" target="#b29">[30]</ref> as a prominent instance of a single bilateral filter application. See <ref type="bibr" target="#b37">[37]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Joint Bilateral Upsampling</head><p>A typical technique to speed up computer vision algorithms is to compute results on a lower scale and upsample the result to the full resolution. This upsampling step may use the original resolution image as a guidance image. A joint bilateral upsampling approach for this problem setting was developed in <ref type="bibr" target="#b29">[30]</ref>. We describe the procedure for the example of upsampling a color image. Given a high resolution gray scale image (the guidance image) and the same image on a lower resolution but in colors, the task is to upsample the color image to the same resolution as the guidance image. Using the permutohedral lattice, joint bilateral upsampling proceeds by splatting the color image into the lattice, using 2D position and 1D intensity as features and the 3D RGB values as the signal. A convolution is applied in the lattice and the result is read out at the pixels of the high resolution image, that is using the 2D position and intensity of the guidance image. The possibility of reading out (slicing) points that are not necessarily the input points is an appealing feature of the permutohedral lattice convolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Color Upsampling</head><p>For the task of color upsampling, we compare the Gaussian bilateral filter <ref type="bibr" target="#b29">[30]</ref> against a learned generalized filter. We experimented with two different datasets: Pascal VOC2012 segmentation <ref type="bibr" target="#b18">[19]</ref> using train, val and test splits, and 200 higher resolution (2MP) images from Google image search <ref type="bibr" target="#b0">[1]</ref> with 100 train, 50 validation and 50 test images. For training we use the mean squared error (MSE) criterion and perform stochastic gradient descent with a momentum term of 0.9, and weight decay of 0.0005, found using the validation set. In <ref type="table">Table 1</ref> we report result in terms of PSNR for the upsampling factors 2×, 4×, 8× and 16×. We compare a standard bicubic interpolation, that does not use a guidance image, the Gaussian bilateral filter case (with feature scales optimized on the validation set), and the learned filter. All filters have the same support. For all upsampling factors, joint bilateral Gaussian upsampling outperforms bicubic interpolation and is in turn improved using a learned filter. A result of the upsampling is shown in <ref type="figure" target="#fig_1">Fig. 3</ref> and more results are included in the supplementary material. The learned filter recovers finer details in the images</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Depth Upsampling</head><p>The work <ref type="bibr" target="#b17">[18]</ref> predicts depth estimates from single RGB images, we use their results as another joint upsampling task. We use The dataset of <ref type="bibr" target="#b40">[40]</ref> that comes with predefined train, validation and test splits. The approach of <ref type="bibr" target="#b17">[18]</ref> is a CNN model that produces a result at 1/4th of the input resolution due to down-sampling operations in maxpooling layers. Furthermore, the authors downsample the 640 × 480 images to 320 × 240 as a pre-processing step before CNN convolutions. The final depth result is bicubic interpolated to the original resolution. It is this interpolation that we replace with a Gaussian and learned joint bilateral upsampling. The features are five-dimensional position and color information from the high resolution input image. The filter is learned using the same protocol as for color upsampling minimizing MSE prediction error. The quantitative results are shown in <ref type="table">Table 1</ref>, the Gaussian filter performs equal to the bicubic interpolation (p-value 0.311), the learned filter is better (p-value 0.015). Qualitative results are shown in <ref type="figure" target="#fig_1">Fig 3,</ref> both joint bilateral upsampling respect image edges in the result. For this <ref type="bibr" target="#b19">[20]</ref> and other tasks specialized interpolation algorithms exist, e.g., deconvolution networks <ref type="bibr" target="#b48">[48]</ref>. Part of future work is to equip these approaches with bilateral filters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Learning Pairwise Potentials in Dense CRFs</head><p>The bilateral convolution from Sec. 3 generalizes the class of dense CRF models for which the mean-field inference from <ref type="bibr" target="#b30">[31]</ref> applies. The dense CRF models have found wide-spread use in various computer vision applications <ref type="bibr" target="#b43">[43,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b49">49,</ref><ref type="bibr" target="#b47">47,</ref><ref type="bibr" target="#b46">46,</ref><ref type="bibr" target="#b10">11]</ref>. Let us briefly review the dense CRF and then discuss its generalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Dense CRF Review</head><p>Consider a fully pairwise connected CRF with discrete variables over each pixel in the image. For an image with n pixels, we have random variables X = {X 1 , X 2 , . . . , X n } with discrete values {x 1 , . . . , x n } in the label space x i ∈ {1, . . . , L}. The dense CRF model has unary potentials ψ u (x) ∈ R L , e.g., these can be the output of CNNs. The pairwise potentials in <ref type="bibr" target="#b30">[31]</ref> are of the form ψ ij</p><formula xml:id="formula_9">p (x i , x j ) = µ(x i , x j )k(f i , f j ) where µ is a label compatibility ma- trix, k is a Gaussian kernel k(f i , f j ) = exp(−(f i − f j ) ⊤ Σ −1 (f i − f j )</formula><p>) and the vectors f i are feature vectors, just alike the ones used for the bilateral filtering, e.g., (x, y, r, g, b). The Gibbs distribution for an image v thus reads p(x|v) ∝ exp(− i ψ u (x i ) − i&gt;j ψ ij p (x i , x j )). Because of dense connectivity, exact MAP or marginal inference is intractable. The main result of <ref type="bibr" target="#b30">[31]</ref> is to derive the mean-field approximation of this model and to relate it to bilateral filtering which enables tractable approximate inference. Mean-field approximates the model p with a fully factorized distribution q = i q i (x i ) and solves for q by minimizing their KL divergence KL(q||p). This results in a fixed point equation which can be solved iteratively t = 0, 1, . . . to update the marginal distributions q i ,</p><formula xml:id="formula_10">q t+1 i (x i ) = 1 Z i exp{−ψ u (x i ) − l∈L j =i ψ ij p (x i , l)q t j (l)</formula><p>bilateral filtering }.</p><p>(7) for all i. Here, Z i ensures normalizations of q i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Learning Pairwise Potentials</head><p>The proposed bilateral convolution generalizes the class of potential functions ψ ij p , since they allow a richer class of kernels k(f i , f j ) that furthermore can be learned from data. So far, all dense CRF models have used Gaussian potential functions k, we replace it with the general bilateral convolution and learn the parameters of kernel k, thus in effect learn the pairwise potentials of the dense CRF. This retains the desirable properties of this model class -efficient inference through mean-field and the feature dependency of the pairwise potential. In order to learn the form of the pairwise potentials k we make use of the gradients for filter parameters in k and use back-propagation through the mean-field iterations <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b33">34]</ref> to learn them. The work of <ref type="bibr" target="#b31">[32]</ref> derived gradients to learn the feature scaling D but not the form of the kernel k, which still was Gaussian. In <ref type="bibr" target="#b13">[14]</ref>, the features f i were derived using a nonparametric embedding into a Euclidean space and again a Gaussian kernel was used. The computation of the embedding was a pre-processing step, not integrated in a end-toend learning framework. Both aforementioned works are generalizations that are orthogonal to our development and can be used in conjunction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Experimental Evaluation</head><p>We evaluate the effect of learning more general forms of potential functions on two pixel labeling tasks, semantic segmentation of VOC data <ref type="bibr" target="#b18">[19]</ref> and material classification <ref type="bibr" target="#b10">[11]</ref>. We use pre-trained models from the literature and compare the relative change when learning the pairwise potentials, as in the last Section. For both the experiments, we use multinomial logistic classification loss and learn the filters via back-propagation <ref type="bibr" target="#b16">[17]</ref>. This has also been understood as recurrent neural network variants, the following experiments demonstrate the learnability of bilateral filters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">Semantic Segmentation</head><p>Semantic segmentation is the task of assigning a semantic label to every pixel. We choose the DeepLab network <ref type="bibr" target="#b14">[15]</ref>, a variant of the VGGnet <ref type="bibr" target="#b41">[41]</ref> for obtaining unaries. The DeepLab architecture runs a CNN model on the input image to obtain a result that is down-sampled by a factor of 8. The result is then bilinear interpolated to the desired resolution and serves as unaries ψ u (x i ) in a dense CRF. We use the same Pott's label compatibility function µ, and also use two kernels k 1 (f i , f j ) + k 2 (p i , p j ) with the same features f i = (x i , y i , r i , g i , b i ) ⊤ and p i = (x i , y i ) ⊤ as in <ref type="bibr" target="#b14">[15]</ref>.  <ref type="table">Table 2</ref>. Improved mean-field inference with learned potentials. (top) Average IoU score on Pascal VOC12 validation/test data <ref type="bibr" target="#b18">[19]</ref> for semantic segmentation; (bottom) Accuracy for all pixels / averaged over classes on the MINC test data <ref type="bibr" target="#b10">[11]</ref> for material segmentation.</p><formula xml:id="formula_11">+ MF-1step + MF-2 step + loose MF-2 step</formula><p>Thus, the two filters operate in parallel on color &amp; position, and spatial domain respectively. We also initialize the mean-field update equations with the CNN unaries. The only change in the model is the type of the pairwise potential function from Gauss to a generalized form. We evaluate the result after 1 step and 2 steps of meanfield inference and compare the Gaussian filter versus the learned version (cf. Tab. 2). First, as in <ref type="bibr" target="#b14">[15]</ref> we observe that one step of mean field improves the performance by 2.48% in Intersection over Union (IoU) score. However, a learned potential increases the score by 2.93%. The same behaviour is observed for 2 steps: the learned result again adds on top of the raised Gaussian mean field performance. Further, we tested a variant of the mean-field model that learns a separate kernel for the first and second step <ref type="bibr" target="#b33">[34]</ref>. This "loose" mean-field model leads to further improvement of the performance. It is not obvious how to take advantage of a loose model in the case of Gaussian potentials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">Material Segmentation</head><p>We adopt the method and dataset from <ref type="bibr" target="#b10">[11]</ref> for material segmentation task. Their approach proposes the same architecture as in the previous section; a CNN to predict the material labels (e.g., wool, glass, sky, etc.) followed by a densely connected CRF using Gaussian potentials and mean-field inference. We re-use the pre-trained CNN and choose the CRF parameters and Lab color/position features as in <ref type="bibr" target="#b10">[11]</ref>. Results for pixel accuracy and class-averaged pixel accuracy are shown in <ref type="table">Table 2</ref>. Following the CRF validation in <ref type="bibr" target="#b10">[11]</ref>, we ignored the label 'other' for both the training and evaluation. For this dataset, the availability of training data is small, 928 images with only sparsely segment annotations. While this is enough to cross-validate few hyperparameters, we would expect the general bilateral convolution to benefit from more training data. Visuals are shown in <ref type="figure" target="#fig_2">Fig. 4</ref> and more are included in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Bilateral Neural Networks</head><p>Probably the most promising opportunity for the generalized bilateral filter is its use in Convolutional Neural Net- works. Since we are not restricted to the Gaussian case anymore, we can run several filters sequentially in the same way as filters are ordered in layers in typical spatial CNN architectures. Having the gradients available allows for endto-end training with backpropagation, without the need for any change in CNN training protocols. We refer to the layers of bilateral filters as "bilateral convolution layers" (BCL). As discussed in the introduction, these can be understood as either linear filters in a high dimensional space or a filter with an image adaptive receptive field. In the remainder we will refer to CNNs that include at least one bilateral convolutional layer as a bilateral neural network (BNN).</p><p>What are the possibilities of a BCL compared to a standard spatial layer? First, we can define a feature space f i ∈ R d to define proximity between elements to perform the convolution. This can include color or intensity as in the previous example. We performed a runtime comparison between our current implementation of a BCL and the caffe <ref type="bibr" target="#b27">[28]</ref> implementation of a d-dimensional convolution. For 2D positional features (first row), the standard layer is faster since the permutohedral algorithm comes with an overhead. For higher dimensions d &gt; 2, the runtime depends on the sparsity; but ignoring the sparsity is quickly leading to intractable runtimes. The permutohedral lattice convolution is in effect a sparse matrix-vector product and thus performs favorably in this case. In the original work <ref type="bibr" target="#b1">[2]</ref> it was presented as an approximation to the Gaussian case, here we take the viewpoint of it being the definition of the convolution itself. 30741.8 ± 9170.9 / 1446.2 ± 304.7 6.2 ± 0.7 / 3.8 ± 0.5 5D-(x, y, r, g, b)</p><p>out of memory 7.6 ± 0.4 / 4.5 ± 0.4 <ref type="table">Table 3</ref>. Runtime comparison: BCL vs. spatial convolution.</p><p>Average CPU/GPU runtime (in ms) of 50 1-neighborhood filters averaged over 1000 images from Pascal VOC. All scaled features (x, y, r, g, b) ∈ [0, 50). BCL includes splatting and splicing operations which in layered networks can be re-used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">An Illustrative Example: Segmenting Tiles</head><p>In order to highlight the model possibilities of using higher dimensional sparse feature spaces for convolutions through BCLs, we constructed the following illustrative problem. A randomly colored foreground tile with size 20 × 20 is placed on a random colored background of size 64 × 64. Gaussian noise with std. dev. 0.02 is added and color values normalized to [0, 1], example images are shown in <ref type="figure" target="#fig_3">Fig. 5</ref>(a). The task is to segment out the smaller tile. Some example images. A pixel classifier can not distinguish foreground from background since the color is random. We train CNNs with three conv/relu layers and varying filters of size n × n, n ∈ {9, 13, 17, 21} but with The schematic of the architecture is shown in <ref type="figure" target="#fig_0">Fig 5(b) (32, 16, 2 filters)</ref>.</p><p>We create 10k training, 1k validation and 1k test images and, use the validation set to choose learning rates. In <ref type="figure" target="#fig_3">Fig. 5</ref>(c) we plot the validation IoU against training epochs. Now, we replace all spatial convolutions with bilateral convolutions for a full BNN. The features are f i = (x i , y i , r i , g i , b i ) ⊤ and the filter has a neighborhood of 1. The total number of parameters in this network is around 40k compared to 52k for 9 × 9 up to 282k for a 21 × 21 CNN. With the same training protocol and optimizer, the convergence rate of BNN is much faster. In this example as in semantic segmentation discussed in the last section, color is a discriminative information for the label. The bilateral convolutions "see" the color difference, the points are already pre-grouped in the permutohedral lattice and the task remains to assign a label to the two groups.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Character Recognition</head><p>The results for tile, semantic, and material segmentation when using general bilateral filters mainly improved because the feature space was used to encode useful prior information about the problem (similar RGB of close-by pixels have the same label). Such prior knowledge is often available when structured predictions are to be made but the input signal may also be in a sparse format to begin with. Let us consider handwritten character recognition, one of the prime cases for CNN use.</p><p>The Assamese character dataset <ref type="bibr" target="#b5">[6]</ref> contains 183 different Indo-Aryan symbols with 45 writing samples per class. Some sample character images are shown in <ref type="figure" target="#fig_4">Fig. 6(a)</ref>. This dataset has been collected on a tablet PC using a pen input device and has been pre-processed to binary images of size 96 × 96. Only about 3% of the pixels contain a pen stroke, which we will denote by v i = 1.</p><p>A CNN is a natural choice to approach this classification task. We experiment with two CNN architectures for this experiment that have been used for this task, LeNet-7 from <ref type="bibr" target="#b32">[33]</ref> and DeepCNet <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b21">22]</ref>. The LeNet is a shallower network with bigger filter sizes whereas DeepCNet is deeper with smaller convolutions. Both networks are fully specified in the supplementary. In order to simplify the task for the networks we cropped the characters by placing a tight bounding box around them and providing the bounding boxes as input to the networks. We will call these networks "Crop-"LeNet, resp. DeepCNet. For training, we randomly divided the data into 30 writers for training, 6 for validation and the remaining 9 for test. <ref type="figure" target="#fig_4">Fig.6</ref>(b) and <ref type="figure" target="#fig_4">Fig. 6(c)</ref> show the training progress for various LeNet and DeepCNet models respectively. DeepCNet is a better choice for this problem and for both cases pre-processing the data by cropping improves convergence.</p><p>The input is spatially sparse and the BCL provides a natural way to take advantage of this. For both networks we create a BNN variant (BNN-LeNet and BNN-DeepCNet) by replacing the first layer with a bilateral convolutions using the features f i = (x i , y i ) ⊤ and we only consider the foreground points v i = 1. The values (x i , y i ) denote the position of the pixel with respect to the top-left corner of the bounding box around the character. In effect the lattice is very sparse which reduces runtime because the convolutions are only performed on 3% of the points that are actually observed. A bilateral filter has 7 parameters compared to a receptive field of 3 × 3 for the first DeepCNet layer and 5 × 5 for the first LeNet layer. Thus, a BCL with the same number of filters has fewer parameters. The result of  <ref type="table">Table 4</ref>. Results on Assamese character images. Total recognition accuracy for the different models.</p><p>the BCL convolution is then splatted at all points (x i , y i ) and passed on to the remaining spatial layers. The convergence behaviour is shown in <ref type="figure" target="#fig_4">Fig.6</ref> and again we find faster convergence and also better validation accuracy. The empirical results of this experiment for all tested architectures are summarized in <ref type="table">Table 4</ref>, with BNN variants clearly outperforming their spatial counterparts. The absolute results can be vastly improved by making use of virtual examples, e.g., by affine transformations <ref type="bibr" target="#b21">[22]</ref>. The purpose of these experiments is to compare the networks on equal grounds while we believe that additional data will be beneficial for both networks. We have no reason to believe that a particular network benefits more.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>We proposed to learn bilateral filters from data. In hindsight, it may appear obvious that this leads to performance improvements compared to a fixed parametric form, e.g., the Gaussian. To understand algorithms that facilitate fast approximate computation of Eq. (1) as a parameterized implementation of a bilateral filter with free parameters is the key insight and enables gradient descent based learning. We relaxed the non-separability in the algorithm from <ref type="bibr" target="#b1">[2]</ref> to allow for more general filter functions. There is a wide range of possible applications for learned bilateral filters <ref type="bibr" target="#b37">[37]</ref> and we discussed some generalizations of previous work. These include joint bilateral upsampling and inference in dense CRFs. We further demonstrated a use case of bilateral convolutions in neural networks.</p><p>The bilateral convolutional layer allows for filters whose receptive field change given the input image. The feature space view provides a canonical way to encode similarity between any kind of objects, not only pixels, but e.g., bounding boxes, segmentation, surfaces. The proposed filtering operation is then a natural candidate to define a filter convolutions on these objects, it takes advantage of sparsity and scales to higher dimensions. Therefore, we believe that this view will be useful for several problems where CNNs can be applied. An open research problem is whether the sparse higher dimensional structure also allows for efficient or compact representations for intermediate layers inside CNN architectures.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Visualization of the Permutohedral Lattice. (a) Input image; Lattice visualizations for different feature spaces: (b) 2D position features: 0.01(x, y), (c) color features: 0.01(r, g, b) and (d) position and color features: 0.01(x, y, r, g, b). All pixels falling in the same simplex cell are shown with the same color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Guided Upsampling. Color (top) and depth (bottom) 8× upsampling results using different methods (best viewed on screen).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Segmentation results. An example result for semantic (top) and material (bottom) segmentation. (c) depicts the unary results before application of MF, (d) after two steps of loose-MF with a learned CRF. More examples with comparisons to Gaussian pairwise potentials can be found in the supplementary material.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Segmenting Tiles. (a) Example tile input images; (b) the 3-layer NN architecture used in experiments. "Conv" stands for spatial convolutions, resp. bilateral convolutions; (c) Training progress in terms of validation IoU versus training epochs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>Character recognition. (a) Sample Assamese character images [6]; and training progression of various models with (b) LeNet and (c) DeepCNet base networks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>for a recent overview 1 http://bilateralnn.is.tuebingen.mpg.de</figDesc><table>Upsampling factor 
Bicubic 
Gaussian 
Learned 

Color Upsampling (PSNR) 
2x 
24.19 / 30.59 
33.46 / 37.93 
34.05 / 38.74 
4x 
20.34 / 25.28 
31.87 / 35.66 
32.28 / 36.38 
8x 
17.99 / 22.12 
30.51 / 33.92 
30.81 / 34.41 
16x 
16.10 / 19.80 
29.19 / 32.24 
29.52 / 32.75 

Depth Upsampling (RMSE) 
8x 
0.753 
0.753 
0.748 

Table 1. Joint bilateral upsampling. (top) PSNR values corre-
sponding to various upsampling factors and upsampling strate-
gies on the test images of the Pascal VOC12 segmentation / high-
resolution 2MP dataset; (bottom) RMSE error values correspond-
ing to upsampling depth images estimated using [18] computed on 
the test images from the NYU depth dataset [40]. 

of other bilateral filter applications. Further experiments on 
image denoising and 3D body mesh denoising are included 
in the supplementary material, together with details about 
exact experimental protocols and more visualizations. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head></head><label></label><figDesc>Next we illustrate two use cases of BNNs and compare against spatial CNNs. The supplementary material contains further explanatory experiments with examples on MNIST digit recognition.</figDesc><table>Dim.-Features 
d-dim caffe 
BCL 

2D-(x, y) 
3.3 ± 0.3 / 0.5± 0.1 
4.8 ± 0.5 / 2.8 ± 0.4 
3D-(r, g, b) 
364.5 ± 43.2 / 12.1 ± 0.4 
5.1 ± 0.7 / 3.2 ± 0.4 
4D-(x, r, g, b) 
</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
				<ptr target="https://images.google.com/" />
	</analytic>
	<monogr>
		<title level="m">Google Images</title>
		<imprint>
			<date type="published" when="2015-03" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Fast high-dimensional filtering using the permutohedral lattice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
		<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2010" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="753" to="762" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Gaussian kd-trees for fast high-dimensional filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gelfand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dolson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Levoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Non-linear Gaussian filters performing edge preserving diffusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Aurich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weule</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">DAGM</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1995" />
			<biblScope unit="page" from="538" to="545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Higher-order image statistics for unsupervised, information-theoretic, adaptive, image filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Awate</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Whitaker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="44" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">UCI machine learning repository</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bache</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lichman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Fundamental relationship between bilateral filtering, adaptive smoothing, and the nonlinear diffusion equation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Barash</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Analysis and Machine Intelligence (T-PAMI)</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="844" to="847" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Fast bilateral-space stereo for synthetic defocus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hernández</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">The Fast Bilateral Solver</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015-11" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Intrinsic images in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">159</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Material recognition in the wild with the materials in context database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Upchurch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6203</idno>
		<title level="m">Spectral networks and locally connected networks on graphs</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A non-local algorithm for image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Buades</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Coll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-M</forename><surname>Morel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="60" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Fully-connected CRFs with non-parametric pairwise potential</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Subr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1658" to="1665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Semantic image segmentation with deep convolutional nets and fully connected CRFs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multi-column deep neural networks for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ciresan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3642" to="3649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning graphical model parameters with approximate marginal inference. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Domke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2454" to="2467" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Depth map prediction from a single image using a multi-scale deep network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2366" to="2374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">The PASCAL VOC2012 challenge results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Variational depth superresolution using example-based edge representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ferstl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ruether</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Adaptive manifolds for real-time high-dimensional filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">S</forename><surname>Gastal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Oliveira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">33</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Graham</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.6070</idno>
		<title level="m">Spatially-sparse convolutional neural networks</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Sparse 3D convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Graham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Guided image filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Analysis and Machine Intelligence (T-PAMI)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1397" to="1409" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Trained bilateral filters and applications to coding artifacts reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>De Haan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">325</biblScope>
		</imprint>
	</monogr>
	<note>Image Processing</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Matrix backpropagation for deep networks with structured layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vantzos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Conference on Multimedia</title>
		<meeting>the ACM International Conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="675" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Human pose estimation with fields of parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kiefel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="331" to="346" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Joint bilateral upsampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Uyttendaele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">96</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Efficient inference in fully connected CRFs with Gaussian edge potentials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information and Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Parameter learning and convergent inference for dense random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="513" to="521" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Gradientbased learning applied to document recognition. Proceedings of the IEEE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Mean-field networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Workshop on Learning Tractable Probabilistic Models</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A tour of modern image filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Milanfar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A fast approximation of the bilateral filter using a signal processing approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Paris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<biblScope unit="page" from="568" to="580" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Bilateral filtering: Theory and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Paris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kornprobst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tumblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Now Publishers Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Propagated image filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H. Rick</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C. Frank</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="10" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02351</idno>
		<title level="m">Fully connected deep structured networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Indoor segmentation and support inference from rgbd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="746" to="760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>abs/1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">SUSAN -a new approach to low level image processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Brady</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="45" to="78" />
			<date type="published" when="1997-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A fully-connected layered model of foreground and background flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">B</forename><surname>Sudderth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2451" to="2458" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Kernel regression for image processing and reconstruction. Image Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Takeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Farsiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Milanfar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="349" to="366" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Bilateral filtering for gray and color images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tomasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Manduchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sixth International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="839" to="846" />
		</imprint>
	</monogr>
	<note>Computer Vision</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Posefield: An efficient mean-field based method for joint estimation of human pose, segmentation and depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sheasby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Warrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMMCVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Filter-based mean-field inference for random fields with higher order terms and product label-spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Warrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Deconvolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="2528" to="2535" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jayasumana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<title level="m">Conditional random fields as recurrent neural networks. International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
