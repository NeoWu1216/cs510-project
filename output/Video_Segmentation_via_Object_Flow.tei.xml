<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Video Segmentation via Object Flow</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsuan</forename><surname>Tsai</surname></persName>
							<email>ytsai2@ucmerced.edu</email>
							<affiliation key="aff0">
								<orgName type="department">MPI for Intelligent Systems</orgName>
								<orgName type="institution">UC Merced</orgName>
								<address>
									<settlement>Merced</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
							<email>mhyang@ucmerced.edu</email>
							<affiliation key="aff0">
								<orgName type="department">MPI for Intelligent Systems</orgName>
								<orgName type="institution">UC Merced</orgName>
								<address>
									<settlement>Merced</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
							<email>black@tuebingen.mpg.de</email>
							<affiliation key="aff0">
								<orgName type="department">MPI for Intelligent Systems</orgName>
								<orgName type="institution">UC Merced</orgName>
								<address>
									<settlement>Merced</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Video Segmentation via Object Flow</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:48+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Video object segmentation is challenging due to fast moving objects, deforming shapes, and cluttered backgrounds. Optical flow can be used to propagate an object segmentation over time but, unfortunately, flow is often inaccurate, particularly around object boundaries. Such boundaries are precisely where we want our segmentation to be accurate. To obtain accurate segmentation across time, we propose an efficient algorithm that considers video segmentation and optical flow estimation simultaneously. For video segmentation, we formulate a principled, multiscale, spatio-temporal objective function that uses optical flow to propagate information between frames. For optical flow estimation, particularly at object boundaries, we compute the flow independently in the segmented regions and recompose the results. We call the process object flow and demonstrate the effectiveness of jointly optimizing optical flow and video segmentation using an iterative scheme. Experiments on the SegTrack v2 and Youtube-Objects datasets show that the proposed algorithm performs favorably against the other state-of-the-art methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Our goal is to segment video sequences, classifying each pixel as corresponding to a foreground object or the background in every frame. Critical to solving this task is the integration of information over time to maintain a consistent segmentation across the entire video. Numerous methods have been proposed to enforce temporal consistency in videos by tracking pixels, superpixels or object proposals <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b39">40]</ref>. Another line of research formulates this problem with a graphical model and propagates the foreground regions throughout an image sequence <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b35">36]</ref>. In addition, several algorithms <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b44">45]</ref> focus on object-level segmentations that favor temporal consistency. Such object-level methods, however, may not be accurate on the pixel level, generating inaccurate object boundaries.</p><p>Optical flow estimation has been extensively studied <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b29">30]</ref> and it is widely used for video segmentation and   <ref type="bibr" target="#b29">[30]</ref> from frame t − 1 to t. (d) optical flow that is updated using the segmentation marked by the red contour. The motions within the object are more consistent and the motion boundaries are more precise compared with the initial flow.</p><p>related problems <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b41">42]</ref>. For instance, graph-based video segmentation methods <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b34">35]</ref> use optical flow in the formulation of pairwise potentials that ensure frame-toframe segmentation consistency. However, estimated optical flow may contain significant errors, particularly due to large displacements or occlusions <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b38">39]</ref>. To compute accurate optical flow fields, it is common to segment images or extract edges to preserve motion details around object boundaries <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b46">47]</ref>. However, most methods do not consider both flow estimation and video segmentation together.</p><p>In contrast, we estimate object segmentation and optical flow synergistically such that the combination improves both. <ref type="figure" target="#fig_1">Figure 1</ref> summarizes the main ideas of this work. If the segmentation of the object is known, optical flow within the same object should be smooth but flow across the boundary need not be smooth. Similarly if an object moves differently from the background, then the motion boundary will correspond to the object boundary. Hence, accurate optical flow facilitates detecting precise object boundaries and vice versa.</p><p>This notion, of course, is not entirely new, but few methods have tried to integrate video segmentation and flow estimation. Specifically, in this paper, we address the above problems by considering object segmentation and optical flow simultaneously, and propose an efficient algorithm, which we refer as object flow. For the segmentation model, we construct a multi-level graphical model that consists of pixels and superpixels, where each of these play different roles for segmentation. On the superpixel level, each superpixel is likely to contain pixels from the foreground and background as the object boundary may not be clear. On the pixel level, each pixel is less informative although it can be used for more accurate estimation of motion and segmentation. With the combination of these two levels, the details around the object boundary can be better identified by exploiting both statistics contained in superpixels and details on the pixel level. Furthermore, we generate superpixels by utilizing supervoxels <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b41">42]</ref> between two frames to exploit temporal information in addition to the use of optical flow. After obtaining the segmentation results, we apply the foreground and background information to re-estimate optical flow <ref type="figure" target="#fig_1">(Figure 1</ref>), and then iteratively use the updated optical flow to re-segment the object region.</p><p>We evaluate the proposed object flow algorithm on the SegTrack v2 <ref type="bibr" target="#b18">[19]</ref> and Youtube-Objects <ref type="bibr" target="#b24">[25]</ref> datasets. We work in the standard paradigm of tracking that assumes an initialization of the object segmentation in the first frame, which could come from simple user input <ref type="bibr" target="#b26">[27]</ref> . We quantitatively compare our segmentation accuracy to other stateof-the-art results and show improvements to the estimated optical flow. With the updated optical flow using the segmentation, we show that the iterative procedure improves both segmentation and optical flow results in terms of visual quality and accuracy.</p><p>The contributions of this work are as follows. First, we propose a multi-level spatial-temporal graphical model for video object segmentation and demonstrate that it performs better than single-level models. Second, we show that the segmentation results can be used to refine the optical flow, and vice versa, in the proposed object flow algorithm. Third, we demonstrate that our joint model of segmentation and optical flow can be efficiently computed by iterative optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work and Problem Context</head><p>Segment-based Tracking. Several segment-based tracking methods <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b39">40]</ref> have been developed. A Hough voting based algorithm <ref type="bibr" target="#b11">[12]</ref> performs online tracking and generates segmentation using GrabCut <ref type="bibr" target="#b26">[27]</ref>. Wang et al. <ref type="bibr" target="#b37">[38]</ref> propose a discriminative appearance model based on superpixels to distinguish the target object from the background. A recent part-based method tracks object seg-ments <ref type="bibr" target="#b39">[40]</ref> with the assumption that the number of parts and superpixels are known in advance. As these approaches only consider superpixels that are generated independently in each frame, they do not explicitly exploit temporal information among regions.</p><p>Li et al. <ref type="bibr" target="#b18">[19]</ref> track object-level region proposals in consecutive frames. However, it is computationally expensive to compute region proposals and the generated object boundaries are not accurate. Lalos et al. <ref type="bibr" target="#b16">[17]</ref> also propose an algorithm called object flow but the goals are significantly different from ours in that they focus on estimating displacements of objects. In addition, this method neither addresses generic flow estimation nor integrates segmentation and flow.</p><p>Graph-based Models. One approach to segment objects in videos is to propagate foreground labels <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b35">36]</ref> between frames based on graphical models. Graphical models for video segmentation typically use unary terms that are determined by the foreground appearance, motions or locations, and pairwise terms that encode spatial and temporal smoothnesses. These methods typically use optical flow to maintain temporal links, but they are likely to fail when the flow is inaccurate. In addition, graphical models can be used for refining segments <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b23">24]</ref>. Lee et al. <ref type="bibr" target="#b17">[18]</ref> use ranked object proposals and select key segments for shape matching. Similar to the issues mentioned in <ref type="bibr" target="#b18">[19]</ref>, it is computationally expensive to generate proposals and they are likely to contain foreground and background pixels.</p><p>Layered Models. Video segmentation and motion estimation are closely related. Layered models <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b45">46]</ref> jointly optimize for segmentation and optical flow. These models can be extended to the temporal domain with more than two frames <ref type="bibr" target="#b30">[31]</ref>. Early methods focus only on motion information but more recent formulations combine image and motion information in segmenting the scene into layers <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b40">41]</ref>. Most layered methods use complicated and computationally expensive inference, thereby limiting their applications.</p><p>In this paper, we propose an efficient algorithm that jointly updates object segmentation and optical flow models using an iterative scheme. The optical flow helps identify temporal connections throughout the video, and the segmentation improves the optical flow estimation at motion boundaries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Video Object Segmentation</head><p>In this section, we first explain how we construct the object segmentation model. Given the initialization in the first frame, we aim to propagate the foreground label throughout the entire video. Note that, in contrast to unsupervised methods <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref>, that rely on motion and object proposals and process the entire video offline in batch mode, <ref type="figure">Figure 2</ref>. Overview of the proposed model. For segmentation, we consider a multi-level spatial-temporal model. Red circles denote pixels, which belong to the superpixel marked by the turquoise circles. The black and the red lines denote the spatial and temporal relationships, respectively. The relationships between the pixels and the superpixel are denoted by the turquoise lines. After obtaining the object mask, Mt, we use this mask to re-estimate the optical flow, and update both models iteratively. the proposed algorithm is able to track and segment objects for online applications. Before assigning each pixel a label, we search the possible locations for the object in each frame to reduce the background noise. A multi-level spatial-temporal graphical model is then applied to the estimated object regions. In this stage, unary and pairwise terms for superpixels are introduced by the supervoxel to better ensure temporal consistency. <ref type="figure">Figure 2</ref> shows the proposed multi-level segmentation model.</p><p>Object Location. Instead of using the segmentation model on the entire image, we first estimate the object location to reduce the computational load and the effect of background noise. We design a scoring function for each pixel based on color and location:</p><formula xml:id="formula_0">S t (x t i ) = A t (x t i ) + L t (x t i , M t−1 ),<label>(1)</label></formula><p>where A t is the color score on the pixel x t i computed by a Gaussian Mixture Model (GMM), and L t is the location score measured by the Euclidean distance transform of the binary object mask M t−1 in the previous frame.</p><p>Since we do not know the exact object location or shape in the current frame, we assume that the object does not move abruptly. Therefore, we generate the rough object mask in the current frame t using the segmentation mask M t−1 in the previous frame translated by the average optical flow vector within the mask. We then use and expand this coarse mask on a local search region that is s times the size of the object mask M t−1 (s is from 2 to 3 depending on the object size in this work). This mask ensures that most of the pixels within the object are covered. After obtaining the local search region, we use the distance transform on <ref type="bibr">Figure 3</ref>. Estimated object location Rt. Given the object mask Mt−1 marked as the red contour, we search for a local region in the current frame t and compute the foreground scores based on color and location. The estimated foreground region is used for label assignment.</p><p>this expanded mask to compute location scores. Similarly, we also consider color scores based on this local region. <ref type="figure">Figure 3</ref> illustrates one example of the combination of two scores. A threshold is then applied to select the object location R t for further determining label assignment.</p><p>Graphical Model. After the object region for label assignment is selected, we utilize a spatial-temporal graphical model to assign each pixel with a foreground or background label. We define an energy function in a Conditional Random Field (CRF) form for the pixel x t i ∈ X with label ∈ {0, 1}:</p><formula xml:id="formula_1">E pix (X) =Ū t (X, M t−1 ) + γ s 1 (i,j,t)∈EtV t (x t i , x t j ) + γ t 1 (i,j,t)∈EtW t (x t−1 i , x t j ),<label>(2)</label></formula><p>whereŪ t is the unary potential for the cost to be foreground or background, andV t andW t are pairwise potentials for spatial and temporal smoothnesses with weights γ s 1 and γ t 1 , respectively. Both of the pairwise terms are defined as in <ref type="bibr" target="#b23">[24]</ref>. Note that we only consider E t within the region R t generated in the object location estimation step.</p><p>For the unary term in <ref type="formula" target="#formula_1">(2)</ref>, we consider appearance and location energies defined byŪ t (X,</p><formula xml:id="formula_2">M t−1 )= α 1 (i,t)∈RtΦ a (x t i ) + β 1 (i,t)∈RtΦ l (x t i , M t−1 ), (3)</formula><p>whereΦ a is the appearance term, andΦ l is the location term defined similar to the one in (1). The difference is that for the nodes in the previous frame, we can simply compute the distance transform of the object mask M t−1 . For the appearance term, we construct the color GMM in the first frame, and an online SVM model with CNN features <ref type="bibr" target="#b19">[20]</ref> updated every frame. The weight α 1 consists of α col 1 and α cnn 1 for the color and CNN features, respectively. By minimizing (2), we obtain labels within R t and thus the object mask M t , and then continue to propagate to the next frame.</p><p>Multi-level Model. Although the model based on pixels can achieve fine-grained segmentation, pixels are usually sensitive to noise when optical flow is not accurately estimated. On the other hand, an alternative way is to use larger segments such as superpixels that contain more information by considering every pixel in the neighborhood (i.e., spatial support). However, superpixels may not contain the entire object or may have imprecise object boundaries due to occlusion or motion blur (See <ref type="figure" target="#fig_2">Figure 4)</ref>. Therefore, we construct a multi-level graphical model including pixels and superpixels to ensure boundary as well as temporal consistency.</p><p>In this model, the energy terms for both pixels and superpixels have unary and pairwise potentials, and a pairwise term is used for the connection where pixels belong to a superpixel (See <ref type="figure">Figure 2</ref>). In addition, since optical flow may not be estimated correctly due to large displacement, we use supervoxels <ref type="bibr" target="#b12">[13]</ref> between two frames to generate coherent superpixels and enhance temporal consistency.</p><p>The multi-level model is formulated by</p><formula xml:id="formula_3">E seg = λ 1 E pix (X) + λ 2 E sup (Y ) + λ 3 E pair (X, Y ),<label>(4)</label></formula><p>where E pix (X) is the model based on pixels in <ref type="formula" target="#formula_1">(2)</ref>; E sup (Y ) is the energy function based on superpixels y t m ∈ Y ; E pair (X, Y ) is the pairwise term between pixels and superpixels; and λ i is the weight. We define E sup (Y ) as:</p><formula xml:id="formula_4">E sup (Y ) =Û t (Y ) + γ 2 (m,n,t)∈EtV t (y t m , y t n ),<label>(5)</label></formula><p>whereÛ t is the unary potential for labeling a superpixel as foreground or background, andV t is the spatial smoothness within the region R t . Note that it is not necessary to model the temporal smoothness in (5) since we design a term for the supervoxel and optical flow in the unary term (explained in detail below). The unary termÛ t is defined in a way similar to <ref type="formula">(3)</ref>:</p><formula xml:id="formula_5">U t (Y ) = α 2 (m,t)∈RtΦ a (y t m ) + β 2 (m,t)∈RtΦ l (y t m ),<label>(6)</label></formula><p>whereΦ a is the color term defined as the mean color likelihood over all pixels within the superpixel, and a location termΦ l measures the consistency between the optical flow and the supervoxels. The location term is defined as:</p><formula xml:id="formula_6">Φ l (y) = f low(y) × obj(y),<label>(7)</label></formula><p>where f low(y) is defined by the percentage of pixels in y that are successfully transferred to the next time instant. A successful transfer means that a pixel x t−1 i transfers from a superpixel y t−1 m to a superpixel y t n via optical flow, and y t−1 m and y t n belong to the same supervoxel. In addition, obj(y) computes the percentage of pixels within the superpixel belonging to the segmentation mask M.</p><p>The value of the first term in <ref type="formula" target="#formula_6">(7)</ref>  second term basically measures the likelihood that a superpixel is part of the object. Note that, to compute obj(y) for superpixel nodes in the current frame t, since the object location is unknown, we use the approximate object mask as described in the step for estimating the object location.</p><p>E pair (X, Y ) models the relationship between superpixels and pixels, encouraging pixels inside the superpixel to have the same label. This pairwise term is defined by</p><formula xml:id="formula_7">E pair (x t i , y t m ) = 1 − |(p(x t i ) − p(y t m ))| if l x = l y 0 else,</formula><p>where p is the foreground color probability computed by a color GMM, and l x and l y are the labels for the pixel and superpixel. This energy computes the penalty of assigning different labels to pixel x and superpixel y. The subtraction of probabilities indicates how similar x and y are, and the absolute value is from 0 to 1. That is, if the color of the pixel is similar to the mean color of the superpixel, it is likely to have the same label and should have a higher penalty if assigning to different labels.</p><p>Overall, to propagate foreground labels, we estimate the object location guided by the optical flow, and utilize a multi-level model to assign the label to each pixel. On the pixel level, optical flow is used to maintain temporal smoothness, whereas for the superpixel, the model measures the consistency between supervoxels and optical flow, and propagates the location information to the next frame.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Object Flow</head><p>In the previous section, we address video segmentation by utilizing a multi-level spatial-temporal graphical model with the use of optical flow and supervoxels. The ensuing question is how to use the segmentation results to help the estimation of optical flow and vice versa? Since flow vectors within the same object are likely to be similar, we propose to estimate them on the object level. The updated optical flow can then be used again to improve object segmentation. The problem is formulated jointly as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Object Flow</head><p>Initialize: u, v by minimizing <ref type="bibr" target="#b7">(8)</ref> while not converged do u p ← u, v p ← v Segmentation Compute energy terms for (4) using u p , v p minimize (4) by graph cuts and obtain M M p ← M Optical Flow if large displacement then minimize (9) using M p and obtain u, v</p><formula xml:id="formula_8">u p ← u, v p ← v end if M ← M p , u ← u p , v ← v p end while</formula><p>Optical Flow with Segmentation. We minimize the classical robust optical flow objective function <ref type="bibr" target="#b29">[30]</ref>,</p><formula xml:id="formula_9">E(u, v; R) = i,j∈R {ρ D (I t−1 (i, j) − I t (i + u i,j , j + v i,j )) +λ[ρ S (u i,j − u i+1,j ) + ρ S (u i,j − u i,j+1 ) +ρ S (v i,j − v i+1,j ) + ρ S (v i,j − v i+1,j )]},<label>(8)</label></formula><p>where u and v are the horizontal and vertical components of the optical flow from image I t−1 to I t , and ρ D and ρ S are robust data and spatial penalty functions. We further consider the object mask M obtained from the segmentation step. One can consider this as a binary assignment of pixels to layers in a layered flow model. Here we use it to decompose the flow problem into two separate estimations,</p><formula xml:id="formula_10">E f low (u f g , v f g , u bg , v bg ) = E(u f g , v f g ; f g)+E(u bg , v bg ; bg),<label>(9)</label></formula><p>where f g and bg are local regions that are slightly larger than the foreground and background regions. This step ensures that the optical flow estimation is less affected by the background noise but still takes partial background regions into account. The final optical flow can be merged by applying the segmentation mask. That is, u = M · u f g + (1 − M) · u bg , which are obtained from E(u f g , v f g ; f g) and E(u bg , v bg ; bg), respectively.</p><p>Joint Formulation. To formulate the joint problem for segmentation and optical flow, we combine (4) and (9) as:</p><formula xml:id="formula_11">min M,u,v E total = E seg + E f low .<label>(10)</label></formula><p>Note that in E seg , we use optical flow in <ref type="formula" target="#formula_1">(2)</ref> and <ref type="formula" target="#formula_4">(5)</ref>, and the estimated object location. In E f low , we use the segmenta-tion mask obtained by E seg to decide the foreground and background local regions. We optimize (10) by iteratively updating the two models once the segmentation or optical flow energy converges. First, we initialize and fix the optical flow to minimize E seg using graph cuts <ref type="bibr" target="#b3">[4]</ref>. We then optimize the optical flow by fixing the segmentation mask M, and minimizing E f low using the Classic+NL method <ref type="bibr" target="#b29">[30]</ref>.</p><p>Optimization Details. The main steps of the optimization procedure for (10) are summarized in Algorithm 1. We make a few assumptions to expedite the process. First, we find that for many frames, optical flow can be obtained accurately without the need to re-estimate. For instance, this is true when the object is stationary or moves slightly. In addition, we observe that if the consistency between supervoxels and optical flow is low, it is a good indication that the object moves by a large displacement. Thus, we design a strategy that only re-estimates the optical flow if the value of f low(y) in <ref type="formula" target="#formula_6">(7)</ref> is less than a threshold. This speeds up the process significantly while maintaining good accuracy.</p><p>Second, since our goal is to find a stable object mask M, instead of using the energy E seg to decide the convergence status during the update of the segmentation model, we measure the difference of object mask solutions M. If the overlap ratio of M is larger than a value (e.g. 95%), it should be a stable solution. In our experiments, the entire optimization process for (10) converges within five iterations, and converges in two iterations for most frames from our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experimental Results</head><p>Implementation Details. To evaluate the proposed algorithm, we first construct the foreground and background color GMMs in the RGB space from the first frame, and set K to 5 for each GMM. For learning the online SVM model, we extract hierarchical CNN features <ref type="bibr" target="#b19">[20]</ref> combining the first 5 convolutional layers from a pre-trained VGG net <ref type="bibr" target="#b27">[28]</ref> into 1472 dimensional vectors. We use the method <ref type="bibr" target="#b12">[13]</ref> to generate supervoxels and convert them to superpixels in each frame. For parameters in the graphical model, we use α col 1 = 1, α cnn 1 = 3, β 1 = 2, γ s 1 = 3 and γ t 1 = 0.2 on the pixel level. On the superpixel level, parameters are set as α 2 = 1, β 2 = 1 and γ 2 = 2. For (4), we set λ 1 = 1, λ 2 = 15 and λ 3 = 5. Since one superpixel contains numerous pixels, we use larger weight for λ 2 on the superpixel level as otherwise the superpixel energy is easily absorbed to have the same label as the pixels (a similar issue holds for λ 3 ). All these parameters are fixed in the experiments. The MATLAB code will be made available at https: //sites.google.com/site/yihsuantsai/.</p><p>SegTrack v2 Dataset. We first evaluate the proposed algorithm on the SegTrack v2 dataset <ref type="bibr" target="#b18">[19]</ref> which consists of 14 videos with 24 objects and 947 annotated frames. The  dataset includes different challenging sequences with large appearance change, occlusion, motion blur, complex deformation and interaction between objects. For videos containing multiple objects, since instance-level annotations are provided, each object can be segmented in turn, treating each as a problem of segmenting that object from the background. We first present segmentation results and demonstrate the effectiveness of the multi-level model. <ref type="table" target="#tab_0">Table 1</ref> shows segmentation accuracy of the proposed algorithm and the state-of-the-art methods including tracking and graph based approaches <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b39">40]</ref>. Note that our model can generate labeled results on both pixel and superpixel levels, and we present the pixel level finegrained segmentation results. We use the intersection-overunion (overlap) ratio for evaluation as it has been shown that the pixel error metric used in the SegTrack dataset is sensitive to object size <ref type="bibr" target="#b18">[19]</ref> and is less informative.</p><p>Overall, the proposed algorithm achieves favorable results in most sequences especially for non-rigid objects (Hummingbird, Worm, Soldier). These sequences usually contain large deformation due to fast motions or complex cluttered backgrounds. The superpixel-based tracking methods <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b39">40]</ref> do not perform well on these sequences since superpixels may not preserve object boundaries well. The Hough-based tracking method <ref type="bibr" target="#b11">[12]</ref> only uses pixels, which may result in noisy temporal links. In the proposed spatial-temporal multi-level model, we consider both pixels and superpixels in the tracking and segmentation to maintain object boundaries as well as temporal consistency.</p><p>For the Penguin and Frog sequences, the object appearance is similar to the background with slow motions. The off-line methods <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref> that generate object proposals from all frames, are likely to have large segmentation errors due to wrong association or incomplete regions that contain foreground and background pixels. In contrast, the proposed algorithm performs well in these sequences with objects surrounded by other objects or background with similar appearance. In the location term (7), our model considers consistency between supervoxels and optical flow, and this helps maintain temporal consistency. We present qualitative segmentation results in <ref type="figure" target="#fig_3">Figure 5</ref> and show more results in the supplementary material. To evaluate the proposed multi-level model that integrates both levels, we compare to our model using only pixels or superpixels in <ref type="table" target="#tab_1">Table 2</ref>. The information contained on these two levels complement each other as the one based on the superpixel level maintains local region consistency, while the one based on the pixel level refines incomplete object contours (See <ref type="figure" target="#fig_2">Figure 4)</ref>. Specifically, the location term in (7) enhances temporal information such that the model can handle cases including fast movements and background noise in sequences. In addition, the proposed multi-level model with superpixels performs better than that using only superpixels, which demonstrates that the refinement with the pixel level information is critical for obtaining good performance especially in sequences that contain unclear object boundaries. We also note that our single-level models already perform comparably to the state-of-the-art methods.</p><p>Youtube-Objects Dataset. The Youtube-Objects dataset <ref type="bibr" target="#b24">[25]</ref> contains 1407 videos with 10 object categories, and the length of the sequences is up to 400 frames. We evaluate the proposed algorithm in a subset of 126 videos with more than 20000 frames, where the pixel-wise annotations in every 10 frames are provided by Jain and Grauman <ref type="bibr" target="#b13">[14]</ref>. <ref type="table">Table 3</ref> shows the segmentation results of the proposed algorithm and other state-of-the-art methods 1 . For tracking-based or foreground propagation algorithms <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b35">36]</ref>, ground truth annotations in the first frame are used as initializations to propagate segmentation masks. For multiple objects in videos, the proposed algorithm is able to propagate multiple object segmentations at the same time. Note that there are no instance-level annotations provided.</p><p>Overall, the proposed algorithm performs well in terms of overlap ratio, especially in 8 out of 10 categories. Compared to optical flow based methods <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b35">36]</ref>, the proposed algorithm performs well on fast moving objects such as car and motorbike as the optical flow errors are reduced. Although the recent method <ref type="bibr" target="#b13">[14]</ref> utilizes long-term supervoxels to enhance the temporal connection, the segmentation results contain noisy object boundaries as only superpixels are used. In contrast, the proposed algorithm considers visual information at multiple levels and delineates boundaries well especially on non-rigid objects (dog, horse, cow). We show qualitative results in <ref type="figure" target="#fig_3">Figure 5</ref>. More results are presented in the supplementary material.</p><p>Optical Flow. We demonstrate the effectiveness of iteratively optimizing two models for updated optical flow and segmentation results (See Algorithm 1) on the SegTrack v2 dataset. Here, we only consider sequences with large displacements in which the flow is re-estimated. First, we measure the quality of updated optical flow. As the optical flow ground truth is not available, we warp the object segmentation ground truth from frame t to frame t − 1 using optical flow with the bicubic interpolation. We then compute the overlap ratio between the warped and ground truth masks. Since we focus on optical flow of the object, this metric measures consistency for flow directions and whether they connect to the same objects between two frames. <ref type="table" target="#tab_3">Table 4</ref> shows results compared to two other optical flow methods <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b29">30]</ref> and the layered model <ref type="bibr" target="#b31">[32]</ref>. The updated results improve the optical flow estimation <ref type="bibr" target="#b29">[30]</ref> consistently in all the sequences, especially for fast moving objects (Girl, Drifting, BMX). This validates our approach since the method <ref type="bibr" target="#b29">[30]</ref> is used to compute the initial flow. <ref type="figure">Figure 6</ref> illustrates the optical flow results. Compared to the other three methods, the updated optical flow exhibits clearer object boundaries. It shows the importance of computing optical flow on local object regions. In contrast, the results from <ref type="bibr" target="#b5">[6]</ref> are usually oversmoothed around the object boundaries, and the layered model <ref type="bibr" target="#b31">[32]</ref> generates incomplete flows inside objects.</p><p>In addition, we use the normalized interpolation error (NE) as described in <ref type="bibr" target="#b1">[2]</ref> for evaluation. Similarly, the ground truth of the colored object is warped by the optical flow from frame t to t − 1. The average NE of the updated optical flow is better than <ref type="bibr" target="#b29">[30]</ref>, but slightly worse than <ref type="bibr" target="#b5">[6]</ref>. It can be attributed to the fact that the oversmoothed optical flow in <ref type="bibr" target="#b5">[6]</ref> usually generates more complete warped images after interpolation; this is favored by the NE metric.</p><p>Second, by using the updated optical flow, we reestimate object segmentations and measure overlap ratios.  <ref type="figure">Figure 6</ref>. Results for updated optical flow on the SegTrack v2 dataset. We present our updated optical flow compared to the initial flow <ref type="bibr" target="#b29">[30]</ref>, and the other two methods, Brox <ref type="bibr" target="#b5">[6]</ref> on the left and Sun <ref type="bibr" target="#b31">[32]</ref> on the right. Our results contain object boundaries guided by the segmented object marked with the red contour. Note that in the same sequence with multiple objects, the updated optical flow varies depending on the segmentation. Best viewed in color with enlarged images. sequences, and the average overlap ratio is increased from 72.9% to 75.1% in sequences that rely on the optical flow. The improvement varies in different sequences since the segmentation model also takes other cues such as appearance and location into account. For instance, the improvement of overlap ratio is limited in the Bird of Paradise and Parachute sequences since the objects move steadily. On the other hand, for objects with noisy cluttered backgrounds (Drifting-#2) or with similar appearance to the background regions (Worm), the overlap ratios are improved by 2.4% and 2.9% respectively. More results and comparisons are provided in the supplementary material.</p><p>Runtime Performance. Our MATLAB implementation of object flow takes 3 to 20 seconds per frame on the SegTrack v2 dataset depending on the object size, and on average it takes 12.2 seconds per frame. In contrast, the state-of-theart method <ref type="bibr" target="#b39">[40]</ref> takes 59.6 seconds per frame on average. Note that all the timings are measured on the same computer with 3.60 GHz Intel i7 CPU and 32 GB memory, and exclude the time to compute optical flow as each method uses different algorithms. We use the MATLAB implementation of <ref type="bibr" target="#b29">[30]</ref> to generate optical flow fields (around 30 seconds per frame) and it could be replaced by faster algorithms <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b40">41]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Concluding Remarks</head><p>In this paper, we present a novel algorithm for joint optimization of segmentation and optical flow in videos, and show that the problem can be efficiently solved. For segmentation, a multi-level model containing pixels and superpixels is utilized to track objects. We show that both levels complement each other and maintain object boundaries and temporal consistency throughout the video. Using the segmentation, we modify the optical flow estimation to be performed within local foreground and background regions, resulting in more accurate optical flow, particularly around object boundaries. We show that our method performs favorably against state-of-the-art methods on two datasets, and both the segmentation and optical flow results are improved by iteratively updating both models.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>Object flow. (a)-(b) two consecutive frames. (c) optical flow computed by</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>is high if the supervoxel and optical flow mostly agree with each other. Segmentation results at different levels with overlap ratios with respect to the ground truth mask. On the pixel or superpixel level, both results are not complete. The results on the pixel level miss part of the leg, while the results on the superpixel level include part of the bike. The multi-level segmentation approach exploits results from both levels for higher accuracy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Example results for segmentation on the SegTrack v2 (first row) and Youtube-Objects (second and third rows) datasets. The output on the pixel level of our multi-level model is indicated as the red contour. The results show that our method is able to track and segment (multiple) objects under challenges such as occlusions, fast movements, deformed shapes and cluttered backgrounds. Best viewed in color with enlarged images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><head>Table 1 .</head><label>1</label><figDesc>Segmentation results on the SegTrack v2 dataset with the overlap ratio. Note that "-" indicates that the method fails to track an object and is excluded in measuring accuracy.</figDesc><table>Sequence/Object 
[40] [19] [18] [13] [12] [38] Ours 

Online? 
√ 
√ 
√ 
√ 

Unsupervised? 
√ 
√ 
√ 

Girl 
83.7 89.2 87.7 31.9 53.6 52.4 87.9 
Birdfall 
77.5 62.5 49.0 57.4 56.0 32.5 57.4 
Parachute 
94.4 93.4 96.3 69.1 85.6 69.9 94.5 
Cheetah-Deer 
63.1 37.3 44.5 18.8 46.1 33.1 33.8 
Cheetah-Cheetah 35.3 40.9 11.7 24.4 47.4 14.0 70.4 
Monkeydog-Monkey 82.2 71.3 74.3 68.3 61.0 22.1 54.4 
Monkeydog-Dog 21.1 18.9 4.9 18.8 18.9 10.2 53.3 
Penguin-#1 
92.7 51.5 12.6 72.0 54.5 20.8 93.9 
Penguin-#2 
91.8 76.5 11.3 80.7 67.0 20.8 87.1 
Penguin-#3 
91.9 75.2 11.3 75.2 7.6 10.3 89.3 
Penguin-#4 
90.3 57.8 7.7 80.6 54.3 13.0 88.6 
Penguin-#5 
76.3 66.7 4.2 62.7 29.6 18.9 80.9 
Penguin-#6 
88.7 50.2 8.5 75.5 2.1 32.3 85.6 
Drifting-#1 
67.3 74.8 63.7 55.2 62.6 43.5 84.3 
Drifting-#2 
63.7 60.6 30.1 27.2 21.8 11.6 39.0 
Hummingbird-#1 58.3 54.4 46.3 13.7 11.8 28.8 69.0 
Hummingbird-#2 50.7 72.3 74.0 25.2 -45.9 72.9 
BMX-Person 
88.9 85.4 87.4 39.2 2.0 27.9 88.0 
BMX-Bike 
5.7 24.9 38.6 32.5 -
6.0 7.0 
Frog 
61.9 72.3 0.0 67.1 14.5 45.2 81.4 
Worm 
76.5 82.8 84.4 34.7 36.8 27.4 89.6 
Soldier 
81.1 83.8 66.6 66.5 70.7 43.0 86.4 
Monkey 
86.0 84.8 79.0 61.9 73.1 61.7 88.6 
Bird of Paradise 
93.0 94.0 92.2 86.8 5.1 44.3 95.2 

Mean per Object 
71.8 65.9 45.3 51.8 40.1 30.7 74.1 
Mean per Sequence 72.2 71.2 57.3 50.8 41.0 37.0 75.3 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 2 .</head><label>2</label><figDesc>Segmentation results using multi-level and single-level models on the SegTrack v2 dataset with the overlap ratio. Note that there are results on both pixel and superpixel levels using the multi-level model.</figDesc><table>Methods 
Pixel 
Pixel Superpixel Superpixel 
multi-level only multi-level 
only 

Mean per Object 
74.1 
69.6 
65.6 
50.3 

Table 3. Segmentation results on the Youtube-Objects dataset with 
the overlap ratio. 

Category [22] [14] [36] [12] [24] [23] Ours 

aeroplane 89.0 86.3 79.9 73.6 70.9 13.7 89.9 
bird 
81.6 81.0 78.4 56.1 70.6 12.2 84.2 
boat 
74.2 68.6 60.1 57.8 42.5 10.8 74.0 
car 
70.9 69.4 64.4 33.9 65.2 23.7 80.9 
cat 
67.7 58.9 50.4 30.5 52.1 18.6 68.3 
cow 
79.1 68.6 65.7 41.8 44.5 16.3 79.8 
dog 
70.3 61.8 54.2 36.8 65.3 18.0 76.6 
horse 
67.8 54.0 50.8 44.3 53.5 11.5 72.6 
motorbike 61.5 60.9 58.3 48.9 44.2 10.6 73.7 
train 
78.2 66.3 62.4 39.2 29.6 19.6 76.3 

Mean 
74.0 67.6 62.5 46.3 53.8 15.5 77.6 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head></head><label></label><figDesc>The updated segmentation results are improved for all the</figDesc><table>Segmentation 

Ours 
Sun [30] 
Brox [6] 
Segmentation 
Ours 
Sun [30] 
Sun [32] 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="true"><head>Table 4 .</head><label>4</label><figDesc>Intersection-over-union ratio for warped images by interpolation and updated optical flow on the SegTrack v2 dataset. The last row shows the average of normalized interpolation error. The performance is evaluated on frames with sigificant motion.Sequence/ObjectOurs Sun<ref type="bibr" target="#b29">[30]</ref> Brox<ref type="bibr" target="#b5">[6]</ref> Sun<ref type="bibr" target="#b31">[32]</ref> </figDesc><table>Girl 
64.6 
56.1 
63.2 
64.6 
Parachute 
86.8 
84.9 
83.2 
83.3 
MonkeyDog-Monkey 70.8 
70.0 
67.4 
69.0 
MonkeyDog-Dog 
69.0 
69.0 
68.9 
75.0 
Drifting-#1 
89.5 
86.6 
91.3 
82.5 
Drifting-#2 
87.3 
82.5 
87.7 
79.8 
Hummingbird-#1 
55.2 
52.2 
52.6 
51.3 
Hummingbird-#2 
71.1 
70.6 
68.7 
65.5 
Worm 
73.3 
71.1 
69.8 
91.1 
Monkey 
77.4 
76.3 
80.9 
70.5 
Soldier 
84.5 
83.5 
80.8 
82.7 
Bird of Paradise 
94.4 
87.9 
88.3 
89.7 
BMX-Person 
80.0 
77.4 
75.0 
72.2 
BMX-Bike 
38.4 
33.9 
37.5 
38.3 

Mean 
74.5 
71.6 
72.5 
72.5 

Average NE 
0.36 
0.38 
0.32 
0.37 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We evaluate the code of<ref type="bibr" target="#b39">[40]</ref> released by the authors. However, the algorithm requires different parameter settings for challenging sequences. We discuss and report results in the supplementary material.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments. This work is supported in part by the NSF CAREER Grant #1149783 and NSF IIS Grant #1152576, and portions of this work were performed while Y.-H. Tsai was an intern at MPI.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Label propagation in video sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Galasso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A database and evaluation methodology for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Scharstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Estimating optical flow in segmented images using variable-order parametric models with local deformations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jepson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996-10" />
			<publisher>PAMI</publisher>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="972" to="986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">An experimental comparison of min-cut/max-flow algorithms for energy minimization in vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Boykov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kolmogorov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>PAMI</publisher>
			<biblScope unit="page" from="1124" to="1137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Video object segmentation by tracking regions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Brendel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Todorovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Large displacement optical flow: descriptor matching in variational motion estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>PAMI</publisher>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="500" to="513" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Topology-constrained layered tracking with latent flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Fisher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A video representation using temporal superpixels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Fisher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Large displacement optical flow from nearest neighbor fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Adaptive fragments-based tracking of non-rigid objects using level sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chockalingam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Pradeep</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Birchfield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A unified video segmentation benchmark: Annotation, metrics and analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Galasso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Nagaraja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cardenas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Hough-based tracking of non-rigid objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Godec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Efficient hierarchical graph-based video segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Grundmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kwatra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Essa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Supervoxel-consistent foreground propagation in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">D</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Mixture models for optical flow computation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jepson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning flexible sprites in video layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jojic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Frey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Object flow: Learning object displacement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lalos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Grabner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Varvarigou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV workshop</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Key-segments for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Video segmentation by tracking many figure-ground segments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Humayun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Hierarchical convolutional features for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Maximum weight cliques with mutex constraints for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Latecki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Video segmentation with just a few strokes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">S</forename><surname>Nagaraja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Segmentation of moving objects by long term video analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ochs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>PAMI</publisher>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Fast object segmentation in unconstrained video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Papazoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning object class detectors from weakly annotated video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Prest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Leistner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Civera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Tracking as repeated figure/ground segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Grabcut: Interactive foreground extraction using iterated graph cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kolmogorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition. CoRR, abs/1409</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1556</biblScope>
			<biblScope unit="page" from="1187" to="1200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Local layering for joint motion estimation and occlusion detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pfister</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A quantitative analysis of current practices in optical flow estimation and the principles behind them</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="115" to="137" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Layered segmentation and optical flow estimation over time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">B</forename><surname>Sudderth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A fully-connected layered model of foreground and background flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">B</forename><surname>Sudderth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Dense point trajectories by gpu-accelerated large displacement optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sundaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">An integrated Bayesian approach to layer extraction from image sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Anandan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001-03" />
			<publisher>PAMI</publisher>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="297" to="303" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Motion coherent tracking with multi-label mrf optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Flagg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Active frame selection for label propagation in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Representing moving images with layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y A</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Adelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="625" to="638" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Superpixel tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deepflow: Large displacement optical flow with deep matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Jots: Joint online tracking and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Efficient sparse-to-dense optical flow estimation using a learned basis and layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Streaming hierarchical video segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A segmentation based variational model for accurate optical flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Motion detail preserving optical flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Matsushita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Video object segmentation through spatially accurate and temporally dense extraction of primary object regions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Javed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A background layer model for object tracking through occlusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Consistent segmentation for optical flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jojic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">B</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
