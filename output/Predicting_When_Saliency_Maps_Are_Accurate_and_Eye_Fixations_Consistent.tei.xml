<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Predicting When Saliency Maps are Accurate and Eye Fixations Consistent</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Volokitin</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Computer Vision Laboratory</orgName>
								<orgName type="institution">ETH Zurich</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Gygli</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Computer Vision Laboratory</orgName>
								<orgName type="institution">ETH Zurich</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Boix</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">CBMM</orgName>
								<orgName type="institution" key="instit2">Massachusetts Institute of Technology</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Predicting When Saliency Maps are Accurate and Eye Fixations Consistent</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:44+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Many computational models of visual attention use image features and machine learning techniques to predict eye fixation locations as saliency maps. Recently, the success of Deep Convolutional Neural Networks (DCNNs) for object recognition has opened a new avenue for computational models of visual attention due to the tight link between visual attention and object recognition. In this paper, we show that using features from DCNNs for object recognition we can make predictions that enrich the information provided by saliency models. Namely, we can estimate the reliability of a saliency model from the raw image, which serves as a meta-saliency measure that may be used to select the best saliency algorithm for an image. Analogously, the consistency of the eye fixations among subjects, i.e. the agreement between the eye fixation locations of different subjects, can also be predicted and used by a designer to assess whether subjects reach a consensus about salient image locations.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Gaze shifting allocates computational resources by selecting a subset of the visual input to be processed, c.f . <ref type="bibr" target="#b32">[33]</ref>. Computational models of visual attention provide a reductionist view on the principles guiding attention. These models are used both to articulate new hypotheses and to challenge the existing ones. Machine learning techniques that can make predictions directly from the image have facilitated the study of visual attention in natural images. Also, these models have found numerous applications in visual design, image compression, and some computer vision tasks such as object tracking.</p><p>Many computational models of attention predict the image location of eye fixations, which is represented with the so called saliency map. The seminal paper by Koch and Ullman introduced the first computational model for saliency prediction <ref type="bibr" target="#b20">[21]</ref>. This model is rooted in the feature integration theory, that pioneered the characterisation of many of the behavioural and physiological observed phenomena of visual attention <ref type="bibr" target="#b31">[32]</ref>. Since then, a rich variety of models have been introduced to extract the saliency map, e.g. <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b34">35]</ref>.</p><p>Some authors stressed the need to predict properties of the eye fixations beyond the saliency map to study different phenomena of visual attention and to allow for new applications, e.g. <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b26">27]</ref>. Since visual attention is strongly linked to object recognition, the advent of near-human performing object recognition techniques based on DCNNs opens a new set of possibilities for models of visual attention. In this paper, we analyze two ways to augment the eye fixation location information delivered by saliency models by using features extracted from DCNNs trained for object recognition.</p><p>Firstly, inspired by machine learning techniques that provide an estimate of their own accuracy, we show that the accuracy of a saliency model for a given image can be predicted directly from image features. Our results show that whether predicting the location of the human eye fixations is possible depends on the object categories present in the image.</p><p>Secondly, we show that the consistency of eye fixation locations among subjects can also be predicted from features based on object recognition. In <ref type="figure" target="#fig_0">Fig. 1</ref> we show images with different degrees of consistency among subjects, that illustrate that eye fixation consistency varies depending on the image. There is a plethora of results in the literature showing that consistency varies depending on the group the subjects belong to. There are marked differences between subjects with autism spectrum disorders and those without <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b19">20]</ref>, between subjects from different cultures <ref type="bibr" target="#b4">[5]</ref>, and between fast and slow readers <ref type="bibr" target="#b18">[19]</ref>. Yet, the causes of eye fixation inconsistencies among individual subjects rather than for groups may be difficult to explain in natural images, especially because natural images are not designed to isolate a specific effect.</p><p>The model we introduce to predict the eye fixation consistency substantially improves the performance of a previous attempt <ref type="bibr" target="#b23">[24]</ref>, and it shows that the eye fixation consis- tency depends on the object categories present in the image. Also, we show that current saliency models and our eye fixation consistency model describe complementary aspects of viewing behaviour, and should be used in conjunction for a more complete characterisation of viewing patterns. Finally, our results reveal that, like memorability <ref type="bibr" target="#b12">[13]</ref> and interestingness <ref type="bibr" target="#b9">[10]</ref>, eye fixation consistency is an attribute of natural images that can be predicted.</p><formula xml:id="formula_0">(a) (b) (c) (d) (e)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Eye Fixation and Saliency Maps</head><p>In this section, we introduce the datasets we use and review how to build the eye fixation and saliency maps. This will serve as the basis for the rest of the paper.</p><p>Datasets We use the MIT <ref type="bibr" target="#b16">[17]</ref> dataset, which includes 1003 images with everyday indoor and outdoor scenes. All images are presented to 15 subjects for 3 seconds. This dataset is one of the standard benchmarks to evaluate the prediction of eye fixation locations in natural images. To show the generality of our conclusions, we also report results on the PASCAL saliency dataset <ref type="bibr" target="#b25">[26]</ref>. This dataset uses the 850 natural images of the validation set of the PAS-CAL VOC 2010 segmentation challenge <ref type="bibr" target="#b7">[8]</ref>, with the eyefixations during 2 seconds of 8 different subjects.</p><p>Eye Fixation Maps An eye fixation map is constructed for each subject by taking the set of locations where the eyes are fixated for a certain period of time (conventionally taken to be 50ms). The fixation map is a probability distribution over salient locations in an image, and ideally would be computed by taking an average over infinite subjects. In practice, the eye fixation map is computed by summing eye fixation maps of the individual subjects (which are binary images, with ones at fixation locations and zeroes elsewhere). The result is smoothed with a Gaussian of width dependent on the eye tracking set up (1 degree of visual angle in the MIT dataset, and σ = 0.03 × image width in the PASCAL dataset). Finally, the map is normalised to sum to one.</p><p>Saliency Maps A saliency map is the prediction of the eye fixation map by an algorithm. We use seven stateof-the-art models to predict the saliency maps: Boolean Map based Saliency (BMS) <ref type="bibr" target="#b36">[37]</ref>, Adaptive Whitening Saliency Model (AWS) <ref type="bibr" target="#b8">[9]</ref>, Graph-based Visual Saliency (GBVS) <ref type="bibr" target="#b10">[11]</ref>, Attention based on Information Maximization (AIM) <ref type="bibr" target="#b2">[3]</ref>, Saliency using Natural Statistics (SUN) <ref type="bibr" target="#b37">[38]</ref>, the original saliency model developed by <ref type="bibr">Itti et al. (IttiKoch)</ref>  <ref type="bibr" target="#b13">[14]</ref>, and a new DCNN-based model called SALICON <ref type="bibr" target="#b11">[12]</ref>. We use the standard procedure (with code from <ref type="bibr" target="#b15">[16]</ref>) to optimise these saliency maps for the MIT dataset. For a complete review of these algorithms, we refer the reader to the thorough analysis of Borji et al. <ref type="bibr" target="#b1">[2]</ref> and Judd et al. <ref type="bibr" target="#b15">[16]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Enriching Saliency Maps</head><p>In this section we introduce the two estimates we use to add to a saliency map: the estimate of the saliency map accuracy, and the consistency of the eye fixations among subjects. We introduce the computational model to predict them in section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Predicting the Saliency Map Accuracy</head><p>We explore whether the saliency model accuracy can be predicted by features extracted directly from the image, i.e. before computing the saliency map. This prediction depends on the algorithm for saliency prediction, and also, it depends on the metric used to evaluate the accuracy of the saliency map.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Metric of the Saliency Map Accuracy</head><p>Since there is no consensus among researchers about which metric best captures the accuracy of the saliency map (c.f . <ref type="bibr" target="#b29">[30]</ref>), we follow the lead of <ref type="bibr" target="#b15">[16]</ref> and report 3 metrics. Now we briefly define the metrics used in this paper, and refer the reader to <ref type="bibr" target="#b29">[30]</ref> for a more complete treatment. Under all of these metrics a higher score indicates better performance. Below, M F is the map of eye fixation map (ground truth) and M S is the (predicted) saliency map: -Similarity (Sim). The similarity metric is also known as the histogram intersection metric, and it is defined as S = The saliency map is treated as a binary classifier to separate positive from negative samples at various intensity thresholds. It is called shuffled because the points of the saliency map are sampled from fixations on other images to discount the effect of center bias. This metric can take values between 0.5 and 1. Although the previous two metrics are symmetric, meaning the two maps are interchangeable, this one is not.</p><p>Applications Our goal is to predict the evaluation metrics of a saliency model we just introduced, for a given image. Providing such estimate of the saliency model accuracy may be used to select the best saliency algorithm for a specific image. Additionally, the accuracy estimate could be used as a meta-saliency measure that indicates the quality of the saliency map to the user.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Predicting the Eye Fixations Consistency</head><p>The second estimate we provide to enrich the saliency map is the eye fixation consistency among subjects, i.e. the amount of inter-subject variability in viewing the image. To do this, we first measure the true eye fixation consistency given the eye fixations of individual subjects, adapting a procedure used in <ref type="bibr" target="#b30">[31]</ref>, which we introduce next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Metric of the Eye Fixation Consistency</head><p>The eye fixation consistency metric tests whether the fixation map computed from a subset of subjects can predict the fixation map computed from the rest of the subjects. Let O be the set of all subjects (e.g. 15 in MIT dataset), and H be the subset of K subjects held out for testing. We compute two eye fixation maps: M H from H, and M O\H from O \ H (the remaining 15 − K subjects). We define the consistency score to be the score of M H in predicting M O\H using any of the standard metrics for evaluating saliency prediction algorithms (introduced previously in section 3.1). To be consistent in our evaluation of consistency, M H is treated as the saliency map, and M O\H as the eye fixation map, as it is computed from more subjects than M H .</p><p>In the experiments we analyse several properties of this metric of the eye fixation consistency, such as the dependency on the number of subjects, K, and that the metric is independent on the subjects chosen to build the eye fixation map M O\H and M H . The results show that our metric can generalise to different subjects, but that we need to evaluate different values of K and ways to compare the saliency maps (e.g. Sim, sAUC and CC) because the results highly depend on these parameters.</p><p>A possible alternative to the metric we use is the Shannon entropy of the eye fixation map. In fact, the Shannon entropy has been used as an alternative consistency measure in <ref type="bibr" target="#b16">[17]</ref>. This measure makes the assumptions that inconsistent viewing patterns will yield a flat fixation map, while consistent ones will yield a map with sharp peaks. The entropy is high in the first case, and low in the second. There are a few cases where these assumptions do not hold. The eye fixation map might have several sharp peaks, and thus low entropy, but the subjects can be inconsistent by each only looking at a subset of the peaks. This is the kind of viewing behaviour sometimes exhibited on natural images with several salient regions (e.g. in <ref type="figure">Fig. 6</ref> in the image in the bottom right, there is a person standing near the edge of the image that not all subjects notice). Thus, the entropy is not equivalent to consistency. In the experiments, we corroborate this point by showing that the entropy of the eye fixation maps and the consistency measure we use are correlated but up to a certain extent.</p><p>Applications We aim at estimating the eye fixation consistency among subjects from the raw image, by predicting the value of the aforementioned eye fixation consistency metric. The prediction of the eye fixation consistency can be used to enrich the information provided by the saliency map, because current saliency models have no measure of the consistency of the eye fixations of the subjects, and in this sense are incomplete. The reader may object that since the entropy of the eye fixation map is related to the consistency, it could be that the entropy of the saliency map is also related to the eye fixation consistency. Then saliency models would already provide an estimate of the consistency through the entropy of the saliency map. Our results discard this hypothesis by showing that the entropy of the saliency maps are uncorrelated with the eye fixation consistency.</p><p>Applications that make use of saliency maps, such as visual design, could incorporate eye fixation consistency information to create designs with a greater consensus of fixations in the location of the designer's choice. Since advertisement needs to have maximal effect in minimal time, it is desirable to have viewers consistently attending to specific locations.</p><p>Also, as mentioned in the introduction, some groups in the population have distinct viewing patterns. To better study these phenomena in the laboratory, it might be advantageous to use images in which the individual viewing pattern variability is controlled for. Thus, our computational model could be used to determine a priori which images naturally produce very variable viewing patterns in all subjects, to deliberately include or exclude such images in the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Computational Model</head><p>To predict both accuracy and consistency we train a regressor between the features extracted from the image and the response variable. The features and learner are the same same for both applications. We use a Support Vector Regressor <ref type="bibr" target="#b33">[34]</ref> with the χ 2 kernel. We introduce several image features to test the hypothesis that the saliency accuracy and consistency can be predicted from the spatial distribution and the categories of the objects in the image. The splits are done taking randomly 60% of images for training and the rest for testing. The learning parameters are set with a 10 fold cross-validation using LIBSVM to determine the cost C (range 2 −4 to 2 6 ) and ǫ (2 −8 to 2 −1 ) of the ǫ-SVR.</p><p>Deep Convolutional Neural Networks To capture the spatial distribution and category of the objects in the image, we use features taken from the layers of a DCNN. A DCNN is a feedforward neural network with constrained connections between layers, that take the form of convolutions or spatial pooling, besides other possible nonlinearities, e.g. <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b28">29]</ref>. We use the DCNN called AlexNet <ref type="bibr" target="#b21">[22]</ref> with trained parameters in ImageNet, which achieved striking results in the task of object recognition. It consists of eight layers, the last three of which are fully connected.</p><p>Let y l be a two-dimensional matrix that contains the responses of the neurons of the DCNN at the layer l. y l has size s l × d l , that varies depending on the layer. The first dimension of the table indexes the spatial location of the center of the neuron's receptive field, and the second dimension indexes the patterns to which the neuron is tuned. The response of a neuron, y l [i][j], has a high response when pattern j is present at location i. Neural responses at higher layers in the network encode more meaningful semantic representations than at lower layers <ref type="bibr" target="#b35">[36]</ref>, but the spatial resolution at the last layers is lower than at the first layers.</p><p>The neural responses from the top of each layer y l are used as features.</p><p>Spatial Distribution of Objects We introduce two different features to capture the spatial distribution of the objects without describing their object categories. The first feature is based on the DCNNs previously introduced. We take the neural responses in a layer, y l , and convert them into a feature that has one response for each location that corresponds to the presence of a pattern or object detected by the CNN (it has dimensions s l × 1). To do so, we discard information about which pattern is present at a certain location and simply take the highest response among the patterns. Thus, the image feature is f l [i] = max j y l [i] <ref type="bibr">[j]</ref>. This corresponds to max pooling over the pattern responses.</p><p>A second feature we introduce is based on the objectness, or the likelihood that a region of an image contains an object of any class <ref type="bibr" target="#b0">[1]</ref>. Objectness is based on detecting properties that are general for any object, such as the closedness of boundaries. We use the code provided by <ref type="bibr" target="#b3">[4]</ref> to generate bounding boxes ranked by the probability that they contain an object. We take the top 500 boxes to create a heatmap. The intensity of each pixel in this heatmap is proportional to the number of times it has been included in an objectness proposal 1 . We divide the heatmap into sub-regions at four different levels of resolution and evaluate the L 2 energy in each sub-region, creating a spatial pyramid <ref type="bibr" target="#b22">[23]</ref>. This feature gives an indication of how objects are located in the image. We call this feature PyrObj.</p><p>Object Categories For each not fully connected layer of the DCNN, we construct a feature with only semantic information analogously to the feature with only spatial information. This image feature is f l [j] = max i y l [i][j], and is of dimension 1 × d l . This corresponds to max pooling over space. The last layers of the DCNN already capture object categories, as they transform the neural responses to object classification scores that contain little to no information about the location of the objects in the image.</p><p>Gist of the scene This descriptor of length 512, introduced by <ref type="bibr" target="#b27">[28]</ref>, gives a representation of the structure of real world scenes where local object information is discarded. Scenes belonging to the same semantic categories (such as streets, highways and coasts) have similar GIST descriptors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>We now report results on the MIT benchmark <ref type="bibr" target="#b16">[17]</ref> and PASCAL saliency dataset <ref type="bibr" target="#b25">[26]</ref> (introduced in section 2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Predicting the Saliency Map Accuracy</head><p>Performance of the Predictor of the Saliency Map Accuracy <ref type="figure" target="#fig_2">Fig. 2</ref> shows the results for predicting saliency model accuracy for the different features we have introduced. We report the Spearman correlation between the true and predicted values. The results show that the PyrObj objectness feature can partially describe the object distribution and performs similarly to the spatial features of the DCNN. In general, Gist performs better than PyrObj, on par with the best spatial feature. Interestingly, we see that the semantic feature is much more informative for predicting consistency than the spatial feature, which suggests that semantic information has a greater contribution to predicting saliency map accuracy than information about the distribution of the objects. Also, we can observe that some of the differences between the performance of the features are due to overfitting. Features with fewer dimensions (highest layers, or features with only semantic or spatial information) achieve better generalisation than features with more dimensions, until saturation (and under-fitting), and this point depends on the metric (predicting CC suffers less over-fitting than sAUC). In fact, the performance decreases significantly (&lt; 0.10) when training using a concatenation of neural activations from all layers.</p><p>Finally, note that the best performing feature is the whole layer of the DCNN, achieving a ρ of above 0.4 for all metrics. To show that this performance is also obtained in other datasets, we evaluate our model on the PASCAL dataset and summarise our results in <ref type="table" target="#tab_1">Table 1</ref>. The accuracy prediction of the saliency models performs similarly or better on PAS-CAL dataset, with a maximum correlation of 0.80 vs 0.52 on MIT. This results show that our method provides a useful prediction to automatically assess the quality of saliency. Also, when this prediction is used to select the best algorithm for saliency prediction per image, we find a (modest) absolute improvement of about 1%. <ref type="figure">Fig. 3</ref>     <ref type="figure">Figure 3</ref>: Qualitative results. We show images that have accurate and inaccurate AWS saliency maps (under the crosscorrelation metric). gt is the ground truth which corresponds to the cross-correlation score of the saliency map, ps is the predicted score. Scores are predicted by the whole fifth layer feature. Both scores and predictions scaled between 0 and 1. The images are place in a row: original, fixation map, saliency map. ferent model to predict the eye fixation consistency. If the tasks were similar enough there would be no need to introduce two different models. In <ref type="bibr" target="#b15">[16]</ref>, Judd et al. qualitatively analyse the consistency of the eye fixations and the saliency map accuracy, and suggest that there maybe be a relationship, but it depends on the evaluation metric. We extend this result by directly evaluating the Spearman correlation between the accuracy of the saliency map and eye fixation consistency. This is shown in <ref type="table" target="#tab_2">Table 2</ref> for all methods. All the correlations are low for sAUC and CC metrics (around 0.2 − 0.3), and high for Sim (around 0.82) 2 . Yet, SALICON shows the opposite trend, as the rest of models have added blur for optimal performance, while SALICON remains peaky. If we use the entropy of the fixation map as a consistency metric ( <ref type="table" target="#tab_2">Table 2</ref>, right) we see the same situation. Thus, in general, a different model is needed to predict the eye fixation consistency independently on the metric used for the accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Predicting the Eye Fixations Consistency</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Analysis of the Metric of the Eye Fixation Consistency</head><p>Recall that the metric we use to evaluate the eye fixation consistency, tests whether the fixation map computed from a subset of subjects (M H ) is similar the fixation map computed from the rest of the subjects (M O\H ). Eye fixation consistency may vary depending on the number of subjects <ref type="bibr" target="#b1">2</ref> The Sim metric tends to assign higher scores when the eye fixation maps are relatively flat, independently of the saliency map. Recall that the Sim metric calculates intersection distance, i.e. the sum for all pixels of the minimum value between the saliency and eye fixation maps. If the eye fixation map is flat (inconsistent eye fixations), most pixel values ≈ 1 num pixels , while if the map has peaks (consistent eye fixations), the most pixel values (and the minimum) are 0. As a result, a flat (inconsistent) eye fixation map is likelier to have higher Sim than other eye fixation maps.</p><p>in H, i.e. K. For low values of K, consistency is lower than for high values of K because the individual characteristics of each subject have not been averaged with other subjects. In the limiting case of having infinite subjects, increasing K will eventually lead to the consistency score saturating. In fact, in many works on saliency prediction, the value of the evaluation metric at K → ∞ is used as an upper bound of the achievable prediction score. Thus, to characterise consistency, we need to report results using different numbers of subjects. We test K up to a value equal to half of the number of subjects in the dataset, as when |O \ H| becomes small, M O\H does not represent the totality of the users well anymore. Besides K, consistency also depends on the subjects used to compute M H , which may introduce some bias specific to the group of subjects H. To remove this variability, we evaluate the consistency multiple times with different H, and average the consistency scores. Let S be the number of different H sets used to compute the average. In <ref type="figure" target="#fig_3">Fig. 4a</ref>, we show the mean consistency score as a function of K, and we see that it increases because H becomes more representative as more subjects are added to it. Then, <ref type="figure" target="#fig_3">Fig. 4b</ref> shows that our metric is not dependent on the particular subject in each group. To show this, we check that the consistency scores do not vary when we choose different subjects in H. We compute the average consistency two times for different groups H, and then, we compute the Spearman correlation between these two consistency scores. This procedure is repeated ten times and the results averaged. Thus, <ref type="figure" target="#fig_3">Fig. 4b</ref> shows the average correlation between two measures of the eye fixation consistency for different subjects in H (for different K, and different number of groups averaged to obtain the consistency score, S). We can see that for any K, when S is sufficiently This shows that the consistency score does not depend on the subjects in H when S is sufficiently large. Finally, we check the agreement between the consistency metric we use, and the metric based on the entropy of the eye fixation map used in previous works <ref type="bibr" target="#b16">[17]</ref>. Recall from section 3.2, that the entropy may not capture some cases of inconsistency of eye fixations, because the assumptions that inconsistent viewing patterns will yield a flat fixation map, while consistent ones will yield a map with sharp peaks may not always hold (e.g. an eye fixation map with few peaks but each subject only looks at a subset of these peaks). To show this point, we plot our consistency metric against the entropy of the eye fixation map in <ref type="figure" target="#fig_3">Fig. 4c</ref> (using K = 7, S = 15, and the Sim metric for consistency). We can see that the correlation is negative because the entropy measures inconsistency rather than consistency. Although the correlation is quite high, about 0.85, we see that the entropy does not fully capture the consistency of eye fixations.</p><p>Performance of the Predictor of the Eye Fixation Consistency We now evaluate the performance of the prediction of the eye fixation consistency. We report the Spearman correlation between the true and predicted values in <ref type="figure">Fig. 5</ref>. The same features perform well as in the accuracy prediction task, although the correlation values are higher in this task, achieving a ρ of around 0.5. Subsequent layers outperform the preceding ones, except of the last prob layer, which performs slightly worse. This could happen because the prob layer has lost all spatial information.</p><p>The previous work that also used machine learning to predict the eye fixation consistency <ref type="bibr" target="#b23">[24]</ref>, reports a Pearson correlation of 0.27 on a set of 27 images they have selected at hand, which shows the challenge of this task. Our results substantially improve over previous work, mainly because we use features based on object recognition. Our results  reveal that the eye fixation consistency among subjects is an attribute of natural images that can be predicted. In <ref type="figure">Fig. 6</ref>, we present images that are consistent and inconsistent vs predictable and unpredictable. We see several examples of the consistency being at odds with the entropy.</p><p>Is the Eye Fixation Consistency Predicted by the Entropy of the Saliency Map? Finally, to make sure that the prediction of the eye fixation consistency enriches the information of the saliency map, we check whether eye fixation consistency information is already encoded in saliency maps. Recall that we showed that the entropy of the eye fixation map is correlated with eye fixation consistency. Thus, if the saliency map predicts eye fixation consistency, this would be encoded in the entropy of the saliency map. In <ref type="table" target="#tab_4">Table 3</ref>, we report the correlation between the entropy of the saliency map and the consistency of the fixations based on the three metrics. All methods had a weak correlation (≤ 0.25), except SALICON. We suggest that the leading performance of SALICON on benchmarks is due to it encoding consistency much better than other methods. Note that our computational model can also enrich the saliency map of SALICON, as our computational model predicts the : Qualitative results. We show images that have consistent and inconsistent viewing patterns. gt is ground truth which corresponds to the eye fixation consistency measure, ps is the predicted score.</p><p>consistency more accurately than SALICON.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>We used machine learning techniques and automatic feature extraction to predict the accuracy of saliency maps and the eye fixation consistency among subjects in natural images. This was possible due to the good performance of DC-NNs for object recognition, since eye fixations locations are strongly related to the object categories. Our results showed that saliency models can be enriched with the two predictions made from our model, because saliency models themselves do not capture eye fixation consistency among subjects, and their accuracy has not been estimated for a given image. Also, we observed that the eye fixation consistency among subjects is an attribute of natural images that can be predicted from object categories. We expect that all these results allow for numerous applications in computer vision and visual design.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Fixations from individual subjects. (a) the raw image, (b) averaged fixation map, (c) -(e) individual fixations from subjects. The top row shows an image where fixations are highly consistent, and the bottom shows one where the fixations are inconsistent.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>x min(M F (x), M S (x)). -Cross Correlation (CC). This metric quantifies to what extent there is a linear relationship between the two maps. It is defined: CC = cov(M F , M S )/(σ M F σ M S ), where σ M is the standard deviation of the map M . -Shuffled Area under the Curve (sAUC).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Evaluation of the Prediction of the Saliency Accuracy. The correlation between the predicted accuracy and the true accuracy of the saliency map is evaluated using different input features (including each of the 7 layers of the DCNN). The metric used to evaluate the accuracy of the saliency map is (a) sAUC, (b) CC, and (c) Sim. The trend has the same shape for all methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Analysis of the Metric of the Eye Fixation Consistency. (a) Consistency metric as the average of the sAUC score between the eye fixation maps of two groups of subjects of K and 15 − K subjects, respectively. (b) Correlation between two consistency metrics evaluated with different subjects. (c) Correlation between entropy of the eye fixation map and consistency of the eye fixations. The trends are similar for Sim and CC consistency. large to average the possible individual characteristics of the groups of subjects (S &gt; 10), the correlation becomes about 0.95, i.e. almost the maximum possible correlation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :Figure 6</head><label>56</label><figDesc>Evaluation of the Prediction of the Eye Fixations Consistency. The correlation between the predicted consistency of the eye fixation and the true consistency is evaluated using different input features (including each of the 7 layers of the DCNN). The metric used to evaluate the consistency uses K = 7 subjects and S = 15 groups and is based on: (a) sAUC, (b) CC, and (c) Sim. The results show a similar trend with different values of K</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 1 :</head><label>1</label><figDesc>Evaluation of the Prediction of the Saliency Accuracy. Spearman correlation between the predicted accuracy of the saliency map using layer 5 of the DCNN and the ground truth accuracy.</figDesc><table>Consistency 
Entropy Fixation Map 
sAUC 
CC 
Sim 
sAUC 
CC 
Sim 
SALICON 
0.63 
0.44 
−0.10 
−0.55 
−0.52 
−0.04 
BMS 
0.34 
−0.19 
0.81 
−0.50 
0.26 
0.94 
GBVS 
0.27 
−0.24 
0.81 
−0.46 
0.31 
0.94 
AIM 
0.32 
−0.31 
0.82 
−0.49 
0.40 
0.95 
AWS 
0.32 
−0.25 
0.82 
−0.49 
0.34 
0.95 
SUN 
0.18 
−0.33 
0.83 
−0.41 
0.45 
0.95 
IttiKoch 
0.22 
−0.29 
0.82 
−0.41 
0.37 
0.94 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc>Does</figDesc><table>the Accuracy of the Saliency Map Predict the 
Eye Fixation Consistency? Spearman correlation between 
the accuracy of the saliency map and (left) the consistency 
(K = 7 and S = 15, with the same metric consistency 
and accuracy evaluation), and (right) with the entropy of 
the fixation map. 

saliency model accuracy is high and low vs predictable and 
unpredictable. 

Does the Accuracy of the Saliency Map Predict the Eye 
Fixation Consistency? Now that we have shown that the 
accuracy of the saliency map can be predicted from our 
model, the reader may ask whether we really need a dif-
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc>Is the Eye Fixation Consistency Predicted by the Entropy of the Saliency Map? Correlation between the entropy of the saliency map and (left) the eye fixation consistency (K = 7 and S = 15, with the same metric consistency and accuracy evaluation), and (right) entropy of the fixation map.</figDesc><table></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">This heatmap, when normalised, was also evaluated as a saliency map. Interestingly, it achieved results close to the Judd et al. model<ref type="bibr" target="#b15">[16]</ref> on AUC and NSS metrics (objectness heatmap achieves AUC = 0.83, NSS = 1.23; Judd model achieves AUC = 0.81, NSS = 1.18). This could be explained by the fact that objects predict fixations better than low level features<ref type="bibr" target="#b6">[7]</ref>.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements We thank Luc Van Gool and Qi Zhao for their useful comments and advice. This work was supported by the Singapore Ministry of Education Academic Research Fund Tier 2 under Grant R-263-000-B32-112, the Singapore Defence Innovative Research Programme under Grant 9014100596, and the ERC Advanced Grant VarCity.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Measuring the objectness of image windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Alexe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Analysis of scores, datasets, and models in visual saliency prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Tavakoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">N</forename><surname>Sihite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Attention based on information maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bruce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tsotsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Vision</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="950" to="950" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Bing: Binarized normed gradients for objectness estimation at 300fps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Cultural variation in eye movements during scene perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">F</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Boland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Nisbett</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<publisher>PNAS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Gaze fixation and the neural circuitry of face processing in autism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Dalton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">M</forename><surname>Nacewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Johnstone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Schaefer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Gernsbacher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Goldsmith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Davidson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature neuroscience</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Objects predict fixations better than early saliency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Einhäuser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Spain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Vision</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">On the relationship between optical variability, visual saliency, and eye fixations: A computational approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Garcia-Diaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Leboran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">R</forename><surname>Fdez-Vidal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">M</forename><surname>Pardo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Vision</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The interestingness of images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gygli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Grabner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Riemenschneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Nater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Graph-based visual saliency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Harel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Salicon: Reducing the semantic gap in saliency prediction by adapting deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Boix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">What makes an image memorable</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A model of saliency-based visual attention for rapid scene analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Niebur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis &amp; Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1254" to="1259" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning to predict sequences human visual fixations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Boix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Roig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TNNLS</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A benchmark of computational models of saliency to predict human fixations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Judd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MIT Technical Report</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning to predict where humans look</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Judd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ehinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A nonparametric approach to bottom-up visual saliency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kienzle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wichmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Franz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Tracking the mind during reading: the influence of past, present, and future words on fixation durations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kliegl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nuthmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Engbert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of experimental psychology: General</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Visual fixation patterns during viewing of naturalistic social situations as predictors of social competence in individuals with autism. Archives of general psychiatry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Klin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schultz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Volkmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Shifts in selective visual attention: towards the underlying neural circuitry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ullman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Human Neurobiology</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="1985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Prediction of the inter-observer visual congruency (iovc) and application to image ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">Le</forename><surname>Meur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Baccino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roumy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM international conference on Multimedia</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Handwritten digit recognition with a back-propagation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">E</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">E</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>D. S. Touretzky</editor>
		<imprint>
			<publisher>Morgan-Kaufmann</publisher>
			<date type="published" when="1990" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="396" to="404" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The secrets of salient object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rehg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Action from Still Images Datasets and Models to Learn Task Specific Human Visual Scanpaths</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mathe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Modeling the shape of the scene: A holistic representation of the spatial envelope</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A highthroughput screening approach to discovering good forms of biologically inspired visual representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Doukhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Dicarlo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">D</forename><surname>Cox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS computational biology</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Saliency and human fixations: State-of-the-art and study of comparison metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Riche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Duvinage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mancas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gosselin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dutoit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Henderson. Contextual guidance of eye movements and attention in real-world scenes: the role of global features in object search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Castelhano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological review</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A feature-integration theory of attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Treisman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gelade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Psychology</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Mechanisms of visual attention in the human cortex. Annual review of neuroscience</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ungerleider</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">The nature of statistical learning theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Modeling attention to salient protoobjects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Walther</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Saliency detection: A boolean map approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Cottrell. Sun: A bayesian framework for saliency using natural statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">K</forename><surname>Marks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of vision</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="32" to="32" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
