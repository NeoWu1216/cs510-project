<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improving Human Action Recognition by Non-action Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Wang</surname></persName>
							<email>wang33@cs.stonybrook.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Stony Brook University</orgName>
								<address>
									<postCode>11794</postCode>
									<settlement>Stony Brook</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh</forename><surname>Hoai</surname></persName>
							<email>minhhoai@cs.stonybrook.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Stony Brook University</orgName>
								<address>
									<postCode>11794</postCode>
									<settlement>Stony Brook</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Improving Human Action Recognition by Non-action Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:41+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we consider the task of recognizing human actions in realistic video where human actions are dominated by irrelevant factors. We first study the benefits of removing non-action video segments, which are the ones that do not portray any human action. We then learn a nonaction classifier and use it to down-weight irrelevant video segments. The non-action classifier is trained using Action-Thread, a dataset with shot-level annotation for the occurrence or absence of a human action. The non-action classifier can be used to identify non-action shots with high precision and subsequently used to improve the performance of action recognition systems.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The ability to recognize human actions in video has many potential applications in a wide range of fields, ranging from entertainment and robotics to security and health-care. However, human action recognition <ref type="bibr">[1, 8, 13-15, 29, 37]</ref> is tremendously challenging for computers due to the complexity of video data and the subtlety of human actions. Most current recognition systems flounder on the inability to separate human actions from the irrelevant factors that usually dominate subtle human actions. This is particularly problematic for human action recognition in TV material, where a single human action may be dispersedly portrayed in a video clip that also contains video shots for setting the scene and advancing dialog. For example, consider the video clips from the Hollywood2 dataset <ref type="bibr" target="#b21">[22]</ref> depicted in <ref type="figure" target="#fig_0">Figure 1</ref>. These video clips are considered as 'clean' examples for their portrayed actions, but 3 out of 6 shots do not depict the actions of interest at all. Although recognizing human actions in TV material is an important and active area of research, existing approaches often assume the contiguity of human action in video clip and ignore the existence of irrelevant video shots.</p><p>In this paper, we first present out findings on the benefits of having purified action clips where irrelevant video shots are removed. We will then propose a dataset and a method to learn a non-action classifier, one that can be used to remove or down-weight the contribution of video segments that are unlikely to depict a human action.</p><p>Of course, identifying all non-action video segments is an ill-posed problem. First, there is no definition for what a general human action is. Second, even for the classes of actions that are commonly considered such as hug and handshake, the temporal extent of an action is highly ambiguous. For example, when is the precise moment of a hug? When two people start opening their arms or when the two bodies are in contact? Because of the ambiguities in human actions, our aim in this paper is to identify video segments that are unlikely related to the actions of our interest. Many of those segments can be unarguably identified, e.g., video shots that contain no people, show the close-up face of a character, exhibit little motion, or are a part of a dialog (some examples are shown in <ref type="figure" target="#fig_0">Figure 1</ref>). However, instead of manually defining what a non-action segment should be, in this paper we will use supervised learning and train a non-action classifier using video data that has shotlevel annotation. The classifier is based on Support Vector Machines <ref type="bibr" target="#b33">[34]</ref> and appearance and motion features. More specifically, we will combine Fisher Vector encoding <ref type="bibr" target="#b26">[27]</ref> of Dense Trajectory Descriptors <ref type="bibr" target="#b36">[37]</ref> and the deep learning features of a Two-stream ConvNet <ref type="bibr" target="#b28">[29]</ref>.</p><p>It should be noted that we propose to learn a non-action classifier for generic human actions. This has several benefits over an action-specific classifier that only aims to identify video segments that are irrelevant to a specific action.</p><p>First, a generic non-action classifier is universal; it can be used to improve the recognition performance of action classes that do not have detailed training annotation. Second, even when detailed annotation exists, it would still be difficult to obtain a good action-specific segment classifier. To some extent, having a good classifier that can remove segments that are not related to a specific class is equivalent to having a good action recognizer already. Thus, an actionspecific classifier brings no complementary benefits, while a generic action classifier does and can be used to increase the signal-to-noise ratio of actions in video clips.</p><p>The rest of this paper is structured as follows. We first review some related topics in Section 2. Section 3 presents the empirical evidence that pruning irrelevant shots leads to significant improvement on human action recognition. This is followed by the experiment on learning and evaluating a non-action classifier in Section 4. In Section 5, we propose an approach for using the non-action classifier for human action recognition and describe the performance gains in several experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>In this work, we propose to learn a non-action classifier to predict whether a video subsequence is an action instance <ref type="bibr" target="#b18">[19]</ref>. This is related to defining and measuring objectness in image windows <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b32">33]</ref>, which can assist some common visual tasks like object proposal and detection. Learning such high-level concept often relies on well-defined visual features such as saliency <ref type="bibr" target="#b16">[17]</ref>, color, edges <ref type="bibr" target="#b39">[40]</ref> and super-pixels. There have been some recent attempts <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">7]</ref> to extend objectness to actionness. They often measure the actionness by fusing different feature channels such as space-time saliency <ref type="bibr" target="#b23">[24]</ref>, optical flow <ref type="bibr" target="#b6">[7]</ref>, body configuration <ref type="bibr" target="#b15">[16]</ref> and deep learning features <ref type="bibr" target="#b8">[9]</ref>, sometimes with human input like eye fixation <ref type="bibr" target="#b22">[23]</ref>. However, compared to objectness, actionness in videos is still not sufficiently explored due to the computational intensity in video space and the subtlety of human actions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Benefits of pruning irrelevant shots</head><p>We now present our findings on the statistics of nonaction shots in a typical human action dataset and the benefits of removing them for human action recognition. We will defer the description of a method for classifying nonaction shots to the next section. In this section, we assume there is an oracle for identifying non-action shots.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">ActionThread dataset</head><p>For the studies in this section, we consider the Action-Thread dataset <ref type="bibr" target="#b11">[12]</ref>. This is a typical human action dataset of which the method to collect human action samples is similar to that of many human action datasets, including Holly- wood2 <ref type="bibr" target="#b21">[22]</ref>, TVHID <ref type="bibr" target="#b24">[25]</ref>, and Hollywood3D <ref type="bibr" target="#b9">[10]</ref>. The Ac-tionThread dataset consists of video samples for 13 actions, a superset of the actions considered in Hollywood2 <ref type="bibr" target="#b21">[22]</ref> and TVHID <ref type="bibr" target="#b24">[25]</ref>. The video samples were automatically located and extracted around human action occurrences using script mining in 15 different TV series. They are split into training and test sets such that the two subsets do not share samples from the same TV series.</p><p>Amazon Mechanical Turk (AMT) workers were asked to annotate the occurrence of human actions shot-by-shot for each video. Each video shot of the dataset was labeled by three AMT workers. Most (86.3%) of the shots received the same annotation by three AMT workers. We then manually reviewed and carefully relabeled those shots with conflicting annotations. Of those shots we relabeled, around 60% were consistent with the majority vote. Videos without action occurrences were eliminated. Finally, we have a dataset of 3,035 videos for 13 actions. <ref type="table">Table 1</ref> shows detailed statistics for the refined ActionThread dataset. On average, one video contains roughly 6.5 shots, 60% of which are nonaction shots.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Action recognition system</head><p>We hypothesize that removing non-action shots will improve the recognition performance. We study this hypothesis with a popular action recognition method that holds the state-of-the-art performance on many datasets. This method is based on Dense Trajectory Descriptors <ref type="bibr" target="#b36">[37]</ref>, Fisher Vector encoding <ref type="bibr" target="#b26">[27]</ref>, and Least-Squares Support Vector Machines <ref type="bibr" target="#b30">[31]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Trajectory features</head><p>The feature representation is based on improved Dense-Trajectory Descriptors (DTDs) <ref type="bibr" target="#b36">[37]</ref>. DTD extracts dense trajectories and encodes gradient and motion cues along trajectories. Each trajectory leads to four feature vectors: Trajectory, HOG, HOF, and MBH, which have dimensions of 30, 96, 108, and 192 respectively. The procedure for extracting DTDs is the same as <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref>, and we refer the reader to <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b36">37]</ref> for more details. Note that each trajectory has a temporal span of 15 frames, and the temporal location of each trajectory is taken as the index of the middle frame (the 8 th frame). For efficiency, we only extract trajectory descriptors for each video clip once. If an experiment requires pruning some segments of the video clip, we simply remove the trajectories that are associated with the frames inside the segments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Fisher Vector encoding</head><p>To encode features, we use Fisher Vector <ref type="bibr" target="#b26">[27]</ref>. A Fisher Vector encodes both first and second order statistics between the feature descriptors and a Gaussian Mixture Model (GMM). In <ref type="bibr" target="#b36">[37]</ref>, Fisher Vector shows an improved performance over bag of features for action classification. Following <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b36">37]</ref>, we first reduce the dimension of DTDs by a factor of two using Principal Component Analysis (PCA). We set the number of Gaussians to k = 256 and randomly sample a subset of 1,000,000 features from the training sets to learn the GMM. There is one GMM for each feature type. A video sequence is represented by a 2dk dimensional Fisher Vector for each descriptor type, where d is the descriptor dimension after performing PCA. As in <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b36">37]</ref>, we apply power (α = 0.5) and L 2 normalization to the Fisher Vectors. We combine all descriptor types by concatenating their normalized Fisher Vectors, leading to a single feature vector of 109, 056 dimensions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Least-Squares SVM</head><p>For recognition, we use Least-Squares Support Vector Machines (LSSVM) <ref type="bibr" target="#b30">[31]</ref>. LSSVM, also known as kernel Ridge regression <ref type="bibr" target="#b27">[28]</ref>, has been shown to perform equally well as SVM in many classification benchmarks <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b35">36]</ref>. LSSVM has a closed-form solution, which is a computational advantage over SVM <ref type="bibr" target="#b33">[34]</ref>. We train 13 binary LSSVM classifiers for 13 action classes. For each action class, we train a one-versus-rest classifier where positive training examples are action samples from the class in consideration and negative training examples are action samples from other classes. <ref type="table">Table 2</ref> shows the Average Precision (AP) of the aforementioned action recognition method with and without pruning non-action shots. Here we measure performance using Average Precision, which is an accepted standard for action recognition (e.g., <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b25">26]</ref>). As can be seen, the ability to prune non-action shots leads to significant performance gain, which is shown in the last column of <ref type="table">Table 2</ref>. The average performance gain is 13.7%, and it is as high as 34.1% for DriveCar. <ref type="table">Table 2</ref> shows the benefits of an ideal situation where we can identify all non-action shots. This is perhaps unrealistic in practice, but we can still expect some improvement in the recognition performance even when we cannot remove all irrelevant shots. <ref type="figure" target="#fig_1">Figure 2</ref> shows the mean average precision for recognizing 13 action classes when the percentage for removing non-action shots is varied from 0 to 100%. Given a percentage p (0% ≤ p ≤ 100%), we randomly remove a non-action shot from the video with probability p. We repeat this experiment 20 times and compute the mean and standard deviation for the mean average precision. As can be observed, the performance gain increases as the removal percentage grows. The performance gain is significant even when we can only eliminate 40% of the non-action shots.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Non-action Classification</head><p>Having confirmed that removing non-action shots leads to large performance gain in the action recognition task, we describe in this section our approach for learning a classifier to differentiate between action shots from non-action shots.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Feature computation</head><p>As also mentioned in the introduction, many non-action shots can be identified based on the size of the human characters, the amount of the motion, and the context of the shot in a longer video sequence (e.g., part of a dialog). To capture the discriminative information for classification, we propose to combine DTDs <ref type="bibr" target="#b36">[37]</ref> and deep-learned features from a Two-stream ConvNet <ref type="bibr" target="#b28">[29]</ref>. These features lead to the state-of-the-art performance in many datasets <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b29">30]</ref>. Recent experiments <ref type="bibr" target="#b37">[38]</ref> have also suggested that they are complimentary to each other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Dense Trajectory Features</head><p>Dense Trajectory Features are extracted and used with Fisher Vector encoding as described in Sections 3.2.1 and 3.2.2. Note that each trajectory has a temporal span of 15 frames, and we assign each trajectory to the middle frame (the 8 th frame). Each frame is therefore associated with a set of trajectories. We compute an unnormalized Fisher Vector for each frame, and we have a sequence of frame-wise Fisher Vectors. Since unnormalized Fisher Vector is additive, the unnormalized Fisher Vector for a set of frames is the sum of unnormalized Fisher Vectors for individual frames. Thus, given a sequence of frame-wise unnormalized Fisher Vectors for a video clip, we can efficiently compute the unnormalized Fisher Vector for any subsequence of the video clip. Finally the unnormalized Fisher Vector can be normalized to obtain the DTD feature representation for the subsequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Deep-learning features</head><p>We use deep-learning features from a Two-stream Con-vNet <ref type="bibr" target="#b28">[29]</ref>, which was proposed recently for human action  recognition. In this paper, we use the pre-trained Two-Stream ConvNet provided by Wang et al. <ref type="bibr" target="#b37">[38]</ref> as a generic feature extractor. The model is trained on Split1 of UCF-101 dataset <ref type="bibr" target="#b29">[30]</ref>. This model contains both a spatial and a temporal ConvNet <ref type="bibr" target="#b19">[20]</ref>. The spatial ConvNet is based on VGG-M-2048 model <ref type="bibr" target="#b3">[4]</ref> and fine-tuned with image frames from videos; the temporal ConvNet have a similar structure, but its input is a set of 10 consecutive optical flow maps.</p><p>Video frames are extracted at 25fps and subsequently resized to 256x340 pixels, having the aspect ratio of 4:3. Given the sequence of frames, we calculate the dense optical flow map between pairs of consecutive frames. We use the GPU version of TVL1 algorithm <ref type="bibr" target="#b38">[39]</ref>, for its efficiency and accuracy. Following <ref type="bibr" target="#b28">[29]</ref>, we rescale and discretize the optical flow values into the integer range [0, 255], and store as image using JPEG compression. This greatly reduces the storage size.</p><p>Spatial features. The spatial ConvNet is based on the VGG-M-2048 model <ref type="bibr" target="#b3">[4]</ref>. It requires input as an image region of size 224 × 224 × 3 and outputs a 4096-dim feature vector at the FC6 layer. Note in VGG-M-2048, FC6 is a fully connected layer with 4096 dimensions, as opposed to FC7 which has 2048 dimensions. To compute the feature vector for a video frame, we extract FC6 feature vectors at the center area and four corners of the frame as well as their left-right flipped images. Thus we obtain 10 feature vectors and average them to get a single 4096-dim spatial feature for a frame. Because consecutive frames are often similar, we only compute the spatial feature vector at every five frames. The feature vector for a set of frames (either from a contiguous video sequence or from the union of multiple disjoint video sequences) is the average of the feature vectors computed for the individual frames in the set.</p><p>Temporal features. The computation of temporal feature vectors is similar to that of spatial feature vectors. The only difference is that the input to the temporal ConvNet must be a set of 10 consecutive optical flow maps. So, to compute the temporal feature vector for a frame t, we use the 10-frame volume that is centered at the frame t.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Feature representation for a video shot</head><p>Consider a video shot [s, e] which is a part of a longer video clip <ref type="bibr">[1, n]</ref>. We first compute f in the feature vector for the video shot by concatenating the DTD feature vector, the spatial ConvNet feature vector, and the temporal ConvNet feature vector. Note that the DTD feature vector is power and L 2 -normalized, while the spatial and temporal ConvNet feature vectors are L 2 -normalized. In addition to f in , we also compute two feature vectors f out and f all for the video frames outside [s, e] and all the video frames, respectively. The ultimate feature vector to represent a video shot is taken as [f in , f out , f all ], as illustrated in <ref type="figure" target="#fig_2">Figure 3</ref>. This feature vector encodes the appearance and motion of the video shot as well as its relative comparison with other video shots in its surrounding context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Training a non-action classifier</head><p>We obtain a non-action classifier by training a Least-Squares SVM (Section 3.2.3) using data from the Action-Thread dataset. This dataset is divided into disjoint train and test subsets, which contain 9724 and 9893 shots respectively. Within each subset, around 60% of the shots are non-action. The feature representation for each shot combines both DTD and Two-stream ConvNet, as described in the previous section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Experiments and results</head><p>We measure the performance of non-action classification on the test set of the ActionThread dataset. The test set contains 1,514 videos, with 5877 non-action shots and 4016 action shots. <ref type="table" target="#tab_3">Table 3</ref> shows the Average Precision of the nonaction classifier based on different features. DTD outperforms the Spatial and Temporal features of the Two-Stream ConvNet when they are used individually. When combined, the Spatial and Temporal ConvNets achieve comparable result to DTD. The best performance is achieved when all feature types are combined. From now on, we will use the combined feature vector in all of our experiments.</p><p>The non-action classifier using the combined feature vectors achieve the average precision of 86.1%. This classifier can be used to remove non-action shots and increase the signal-to-noise ratio of the action content in a video clip. In some cases, to minimize the chance of removing action shots, one might want to limit the number of shots removed for each video clip. <ref type="table">Table 4</ref> reports the Average Precision of the non-action classifier when the number of removed shots per video is limited to k, with k = 1, 2, 3, 4. As can be seen, limiting the number of removed shots per video can improve the average precision. <ref type="figure">Figure 4</ref> shows the distribution of non-action confidence scores on video shots of the test set. Each column represents an individual video; the red dots and blue pluses on that column respectively correspond to the non-action and action shots in the video. The points above have higher confidence scores than those below. For visualization, we align each video with the horizontal bar in the middle using the maximum score of the action shots. As can be seen, action and non-action shots are separated fairly well. We also examine the average precision of the non-action classifier for individual videos. <ref type="figure" target="#fig_3">Figure 5</ref> show the distribution of the average precisions computed for individual videos. As can be seen, the big proportion of the videos have the average precision of 1, which means a perfect separation between action and non-action shots. <ref type="figure">Figure 4</ref>. Distribution of non-action scores. Each column represents individual test video; red dots and blue pluses correspond to non-action and action shots, respectively. Shots shown above have higher confidence score than those below. Videos are ordered based on their classification AP. We uniformly sample and display 20% of the test videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Leave-One-Action-Out Generalizability</head><p>So far, we train a non-action classifier based on non-action shots from videos of several human actions. The classifier </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Full-Action</head><p>Leave-One-Out <ref type="figure">Figure 6</ref>. Comparison between the non-action classifier trained with all data (Full-Action) and the leave-one-class-out non-action classifiers (Leave-One-Out). The leave-one-out classifiers are comparable to the classifier trained on the full dataset. This demonstrates the ability to apply the non-action classifier to purify videos of unseen actions.</p><p>achieves the AP of 86.1%, which is encouraging. However, it is still unclear whether this classifier can be used to identify non-action shots in videos of an action that is not among the set of actions in the training data. To test this, we consider the performance of the leave-one-classout classifiers. In particular, we consider 13 action classes of the ActionThread dataset in turn. For each action class, we train a non-action classifier on a reduced training dataset where the videos of the action class in consideration are removed. The obtained non-action classifier is used to identify the non-action shots in the videos of the left-out action class. We compare the average precision of this classifier and the classifier trained with all data. The results are shown in <ref type="figure">Figure 6</ref>. As can be seen, the leave-one-class-out classifiers are comparable to the non-action classifier trained on the full dataset. This demonstrates the ability to apply the non-action classifier to purify videos of unseen actions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Non-action Classifier for Action Recognition</head><p>This section describes the benefits for using the nonaction classifier for human action recognition. We also demonstrate the advantages of the generic non-action classifier over a set of action-specific non-action classifiers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Action recognition with non-action classifier</head><p>Our proposed algorithm is based on the action recognition system described in Section 3.2, using dense trajectory descriptors, Fisher Vector encoding, and LSSVM. The key difference is the incorporation of the non-action classifier to down-weight irrelevant non-action video segments.</p><p>Suppose a video is represented by a set of segments {W i } (discussed below). We first compute the normalized Fisher Vectors {φ i } and non-action confidence scores {s i } of the segments. The feature vector for the video is taken as:</p><formula xml:id="formula_0">φ = i w i φ i , with w i = e −αsi j e −αsj<label>(1)</label></formula><p>Here, we use s i to weight the contribution of φ i using the softmax function. The higher s i the lower the weight is. The parameter α controls the balance between average pooling and max pooling. When α is 0, all w i 's are the same, and this becomes average pooling. If α = ∞, only one segment has the weight of 1, while the weights of others are 0. This is equivalent to max pooling. By tuning α, we can have a good balance between average pooling and max pooling. Here we propose to weight the contribution of video segments instead of removing non-action segments because the nonaction classifier is imperfect (even though the AP is as high as 92.6% if we only remove the most confidence segment in each video). Our approach for weighting the segment contribution is not limited to any segment generation scheme. Any segment proposal method, including random selection, shotbased division, or an action proposal method (if exists), can be incorporated in our framework. In this paper, we propose to use video segments that are generated by using a 25-frame sliding window. Because videos are at 25fps, each segment corresponds to one second of a video. One second is neither too short nor too long, and we can use the nonaction classifier to determine its non-action score. Empirically, the action recognition performance is not sensitive to the segment length around one second. Notably, the segments are not taken as video shots for two reasons. First, the shot classifier itself is imperfect. Second, even for a shot that is not considered as non-action, it might be long and contains many parts that do not portray human action.</p><p>To verify the advantage of our action-weighted feature representation, we train 13 action classifiers using Least-Squares SVM with linear kernel as described in Section <ref type="bibr" target="#b2">3</ref> to normalize scores of different action classes. Note that we apply the segment generation and weighting on both training and test videos. For each video, we compute a single Fisher Vector as described in Equation <ref type="formula" target="#formula_0">(1)</ref>. We report the average precision in the 4 th column of <ref type="table">Table 5</ref>. This method achieves the mean average precision of 52.1%, which is significantly higher than 45.3%, the recognition performance without using the non-action classifier. The improvement is 6.8%, which is comparable to the ability to remove 65% of non-action shots as shown in <ref type="figure" target="#fig_1">Figure 2</ref>.</p><p>Since the Two-stream ConvNet features are already extracted for computing the non-action score, we can also fuse them into the segment representation {φ i }. As shown in the last three columns of <ref type="table">Table 5</ref>, our method as well as the baselines can benefit from the addition of CNN features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Action-specific non-action classifiers</head><p>As an alternative to learning the generic non-action classifier, we can instead train for each action a specific nonaction classifier. The pipeline of this approach is similar to the one described in last section with a few differences. First, the determination of non-action shots is specific to an action. In training, we label a shot as non-action if it does not contain the action in consideration, regardless of the existence of other actions. Second, for representing a video, we use the action-specific non-action classifier and compute a different Fisher Vector feature for each action, as opposed to having the same feature vector for all actions. Thus we can train/test the classifier for each action based on the Fisher Vectors pruned by the corresponding specific non-action classifier. We report the average precision in the 3 rd column of <ref type="table">Table 5</ref>. As can be seen, using the action-specific classifiers improves the recognition performance (compared with No Pruning in <ref type="table">Table 5</ref>), but it is still outperformed by the method that uses the generic non-action classifier. This is probably because the generic classifier can be used to improve the action-to-noise ratio in videos of all action categories, while it is only meaningful to apply an action-specific classifier to videos of a specific category.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">VideoDarwin with non-action classifier</head><p>To understand whether the benefits of using the nonaction classifier is limited to the Fisher Vector encoding, we experiment with a method where we integrate the nonaction classifier with VideoDarwin <ref type="bibr" target="#b7">[8]</ref>, a feature encoding method to capture the temporal evolution in a video sequence. VideoDarwin was proposed recently, and achieved the state-of-the-art performance on a number of datasets. It assumes every frame of a video clip carries some information about the action, and the total information about the action in a video segment correlates with the length. To capture this, VideoDarwin learns a Support Vector Regression (SVR) that maps the feature vector computed for each segment to its own length. More precisely, suppose φ i is the feature vector for the i th frame. VideoDarwin learn the parameters u of an SVR such that:</p><formula xml:id="formula_1">SV R(u) : k i=1 φ i → k i=1 1, k = 1..N<label>(2)</label></formula><p>The learned parameter vector u is then used as the feature representation for the video clip. Notably, this formulation is based on the Matlab implementation 1 of the authors, and it is slightly different from the formulation given in the paper <ref type="bibr" target="#b7">[8]</ref> in which Ranking SVM is used instead. However, the assumption of VideoDarwin that every frame carries some information about the action does not hold due to the existence of non-action shots. To address this problem, we propose VideoDarwin++, a reformulated version that incorporating the outputs of a non-action classifier. VideoDarwin++ learns the parameters u of an SVR such that:</p><formula xml:id="formula_2">SV R(u) : k i=1 w i φ i → k i=1 w i , k = 1..N<label>(3)</label></formula><p>Here the amount of information about an action in a segment does not solely depend on the length, but by the weights that are calculated based on the non-action scores. <ref type="table">Table 6</ref> compares the performance of VideoDarwin features, with and without the using non-action classifier. As can be seen, the non-action classifier provides benefits to this type of feature encoding.  <ref type="table">Table 6</ref>. Action recognition performance using VideoDarwin and VideoDarwin++ for feature encoding. VideoDarwin++ is the reformulated version of VideoDarwin that incorporates the outputs of a non-action classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Cross-dataset generalization</head><p>We have demonstrated the generalizability of the nonaction classifier to videos in the test set and videos of leftout action categories in Section 4. Now we further study the benefits of the non-action classifier to action recognition task in completely different datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.1">Performance on Hollywood2</head><p>We first consider Hollywood2 dataset <ref type="bibr" target="#b21">[22]</ref>, which includes 12 actions and 1,707 videos collected from 69 Hollywood movies. We first divide videos into shots using a shot boundary detection algorithm based on HOG <ref type="bibr" target="#b5">[6]</ref> and SIFT <ref type="bibr" target="#b20">[21]</ref> features, then manually label each shot for action occurrence or absence. Only 31% are non-action shots, and this is 'cleaner' than ActionThread. We apply the nonaction classifier learned from ActionThread onto the Holly-wood2 dataset and report in <ref type="table">Table 7</ref> the action recognition results with and without using the non-action classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.2">Recognizing unseen action categories.</head><p>We also collected human action samples of six actions that are not contained in the ActionThread dataset. Similar to the collection of ActionThread and Hollywood2, we extracted 100 video clips for each action using script mining, and manually examined and accepted those with the human action inside. Finally we have a dataset of 339 videos for 6 actions. We randomly split them into a training set (170 videos) and a test set (169 videos). <ref type="table">Table 8</ref> shows the recognition performance with and without using the non-action  <ref type="table">Table 8</ref>. Action recognition performance on 6 unseen actions.</p><p>classifier (trained on the ActionThread dataset). As can be seen, the benefits of the non-action classifier can be generalized to action categories that do not exist in the training set of the non-action classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>We have studied the benefits of removing non-action shots and proposed a method for detecting them. Our detector is based on Dense Trajectories Descriptors and Twostream ConvNet features. This detector achieves an average precision of 86%, and it can be used to down-weight the contribution of irrelevant segments in the computation of a feature vector to represent a video clip. This approach significantly improves the performance of a recognition system. In our experiments, the improvement is equivalent to the ability to remove 65% of non-action shots without any false positive. Although the non-action classifier is far from perfect, it makes a good step towards the ultimate solution for human action recognition in realistic video.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Examples of non-action shots in typical video clips of human actions. This shows two video clips from the Hollywood2 dataset. Clip 1: the second shot contains no human; Clip 2: the first two shots depict a dialog and exhibit little motion.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Mean average precision as a function of irrelevant-shotpruning percentage. Even when the pruning percentage is far from 100%, there is still significant performance gain.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Features for non-action classification.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Distribution of the average precision measured on individual test video clips for non-action classification. The majority of video clips have the average precision of 1, corresponding to perfect separation between action and non-action shots.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 .</head><label>3</label><figDesc>Evaluation of non-action shot classification.</figDesc><table>Feature 
AP 

Spatial (ConvNet) 
80.8% 
Temporal (ConvNet) 
81.4% 
Spatial+Temporal 
84.1% 
DTD 
84.7% 
DTD+Spatial+Temporal 
86.1% 

AP@k=1 AP@k=2 AP@k=3 AP@k=4 AP@k=∞ 

92.6% 
91.8% 
90.9% 
89.6% 
86.1% 

Table 4. Average Precision of the non-action classifier when we 
remove at most k shots per video. k = ∞ corresponds to no 
constraint on the number of removed shots per video. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head></head><label></label><figDesc>Table 7. Action recognition performance on Hollywood2.</figDesc><table>Feature 
DTD 
DTD + CNN 

Pruning 
None Generic None Generic 

Ans.Phone 
29.3 
33.3 
36.0 
39.9 
DriveCar 
94.2 
95.2 
95.8 
96.4 
Eat 
64.2 
66.4 
63.9 
68.4 
FightPerson 85.9 
89.0 
86.0 
89.2 
GetOutCar 
62.4 
69.5 
70.9 
77.3 
HandShake 45.5 
48.1 
52.8 
57.0 
HugPerson 
50.0 
51.5 
51.7 
55.1 
Kiss 
64.0 
64.0 
67.9 
68.5 
Run 
86.4 
89.5 
89.1 
92.5 
SitDown 
77.4 
82.6 
77.9 
82.3 
SitUp 
32.3 
39.2 
33.4 
41.8 
StandUp 
78.9 
81.4 
80.5 
82.9 

Mean 
64.2 
67.5 
67.2 
71.0 

Feature 
DTD 
DTD + CNN 

Pruning 
None Generic None Generic 

CloseDoor 39.1 
40.6 
38.2 
39.4 
OpenDoor 
63.0 
66.0 
64.3 
66.6 
Downstairs 89.2 
93.0 
89.6 
92.9 
Dance 
67.4 
74.4 
67.4 
75.3 
Drink 
68.9 
68.6 
67.9 
68.7 
Applause 
71.5 
81.5 
73.8 
81.8 

Mean 
66.5 
70.7 
66.9 
70.8 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://bitbucket.org/bfernando/videodarwin</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgment. This project is supported by the National Science Foundation Award IIS-1566248.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Human activity analysis: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ryoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">What is an object?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Alexe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Deselares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Measuring the objectness of image windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Alexe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2189" to="2202" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Return of the devil in the details: Delving deep into convolutional nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chatfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference</title>
		<meeting>the British Machine Vision Conference</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Actionness ranking with lattice conditional ordinal random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Dynamically encoded actions based on spacetime saliency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Wildes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Modeling video evolution for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Oramas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ghodrati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Finding action tubes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.6031</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Hollywood 3D: Recognizing actions in 3D natural scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hadfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bowden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Regularized max pooling for image categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hoai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference</title>
		<meeting>the British Machine Vision Conference</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Thread-safe: Towards recognizing human actions across shot boundaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hoai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Asian Conference on Computer Vision</title>
		<meeting>the Asian Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Improving human action recognition using score distribution and ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hoai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Asian Conference on Computer Vision</title>
		<meeting>the Asian Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Talking heads: Detecting humans and recognizing their interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hoai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Joint segmentation and classification of human actions in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hoai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-Z</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>De La</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torre</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Action recognition from weak alignment of body parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hoai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ladicky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference</title>
		<meeting>the British Machine Vision Conference</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Saliency detection: A spectral residual approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">HMDB: A large video database for human motion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Garrote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision</title>
		<meeting>the International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Video event detection by inferring temporal instance labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-T</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Henderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="541" to="551" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Actions in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marszalek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Stap: Spatial-temporal attention-aware pooling for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">V</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="77" to="86" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Motion part regularization: Improving action recognition via trajectory selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Moulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Patron-Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marszalek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<title level="m">Proceedings of the British Machine Vision Conference</title>
		<meeting>the British Machine Vision Conference</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Structured learning of human interactions in TV shows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Patron-Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marszalek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2441" to="2453" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Improving the fisher kernel for large-scale image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Ridge regression learning algorithm in dual variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Saunders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gammerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vovk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">UCF101: A dataset of 101 human action classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno>CRCV-TR-12-01</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
		<respStmt>
			<orgName>University of Central Florida</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Least squares support vector machine classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A K</forename><surname>Suykens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vandewalle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Processing Letters</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Least Squares Support Vector Machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A K</forename><surname>Suykens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">V</forename><surname>Gestel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Brabanter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Demoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vandewalle</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<publisher>World Scientific</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Selective search for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">E</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="154" to="171" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
		<title level="m">Statistical Learning Theory</title>
		<meeting><address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="1998" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Leave-one-out kernel optimization for shadow detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Vicente</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hoai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Samaras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision</title>
		<meeting>the International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Noisy label recovery for shadow detection in unfamiliar domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Vicente</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hoai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Samaras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Action recognition with improved trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision</title>
		<meeting>the International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.04868</idno>
		<title level="m">Action recognition with trajectory-pooled deep-convolutional descriptors</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A duality based approach for realtime optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="214" to="223" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Edge boxes: Locating object proposals from edges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
