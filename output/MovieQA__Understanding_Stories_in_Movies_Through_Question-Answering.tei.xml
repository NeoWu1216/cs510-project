<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MovieQA: Understanding Stories in Movies through Question-Answering Time Source of Information Plot Video Subtitle Scripts DVS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makarand</forename><surname>Tapaswi</surname></persName>
							<email>tapaswi@kit.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Karlsruhe Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Toronto</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rainer</forename><surname>Stiefelhagen</surname></persName>
							<email>rainer.stiefelhagen@kit.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Karlsruhe Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
							<email>torralba@csail.mit.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Massachusetts Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
							<email>urtasun@cs.toronto.edu</email>
							<affiliation key="aff2">
								<orgName type="institution">University of Toronto</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
							<email>fidler@cs.toronto.edu</email>
							<affiliation key="aff2">
								<orgName type="institution">University of Toronto</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">MovieQA: Understanding Stories in Movies through Question-Answering Time Source of Information Plot Video Subtitle Scripts DVS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>0:00 0:15 1:00 1:15 0:30 0:45 1:30 1:45 2:00 01:04:08 --&gt; 01:04:09 ... you know what I realize? 01:04:17 --&gt; 01:04:18 Ignorance is bliss. 00:40:42 --&gt; 00:40:47</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:43+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Fast progress in Deep Learning as well as a large amount of available labeled data has significantly pushed forward the performance in many visual tasks such as image tagging, object detection and segmentation, action recognition, and image/video captioning. We are steps closer to applications such as assistive solutions for the visually impaired, or cognitive robotics, which require a holistic understanding of the visual world by reasoning about all these tasks in a common framework. However, a truly intelligent machine would ideally also infer high-level semantics underlying human actions such as motivation, intent and emotion, in order to react and, possibly, communicate appropriately. These topics have only begun to be explored in the literature <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b48">49]</ref>.</p><p>A great way of showing one's understanding about the scene is to be able to answer any question about it <ref type="bibr" target="#b22">[23]</ref>. This idea gave rise to several question-answering datasets which provide a set of questions for each image along with multi-choice answers. These datasets are either based on RGB-D images <ref type="bibr" target="#b22">[23]</ref> or a large collection of static photos such as Microsoft COCO <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b46">47]</ref>. The types of questions typically asked are "What" is there and "Where" is it, what attributes an object has, what is its relation to other objects in the scene, and "How many" objects of certain type are present. While these questions verify the holistic nature of Q: How does E.T. show his happiness that he is finally returning home?</p><p>Q: Why do Joy and Jack get married that first night they meet in Las Vegas?</p><p>Q: Why does Forrest undertake a threeyear marathon?</p><p>Q: How does Patrick start winning Kat over?</p><p>A: His heart lights up A: They are both vulnerable and totally drunk A: Because he is upset that Jenny left him A: By getting personal information about her likes and dislikes <ref type="figure">Figure 2</ref>: Examples from the MovieQA dataset. For illustration we show a single frame, however, all these questions/answers are timestamped to a much longer clip in the movie. Notice that while some questions can be answered using vision or dialogs alone, most require both. Vision can be used to locate the scene set by the question, and semantics extracted from dialogs can be used to answer it.</p><p>our vision algorithms, there is an inherent limitation in what can be asked about a static image. High-level semantics about actions and their intent is mostly lost and can typically only be inferred from temporal, possibly life-long visual observations. Movies provide us with snapshots from people's lives that link into stories, allowing an experienced human viewer to get a high-level understanding of the characters, their actions, and the motivations behind them. Our goal is to create a question-answering dataset to evaluate machine comprehension of both, complex videos such as movies and their accompanying text. We believe that this data will help push automatic semantic understanding to the next level, required to truly understand stories of such complexity. This paper introduces MovieQA, a large-scale questionanswering dataset about movies. Our dataset consists of 14,944 multiple-choice questions with five deceiving options, of which only one is correct, sourced from 408 movies with high semantic diversity. For 140 of these movies (6,462 QAs), we have timestamp annotations indicating the location of the question and answer in the video. The questions range from simpler "Who" did "What" to "Whom" that can be solved by vision alone, to "Why" and "How" something happened, that can only be solved by exploiting both the visual information and dialogs (see <ref type="figure">Fig. 2</ref> for a few example "Why" and "How" questions). Our dataset is unique in that it contains multiple sources of information: video clips, subtitles, scripts, plots, and DVS <ref type="bibr" target="#b31">[32]</ref> as illustrated in <ref type="figure">Fig. 1</ref>. We analyze the data through various statistics and intelligent baselines that mimic how different "students" would approach the quiz. We further extend existing QA techniques to work with our data and show that question-answering with such open-ended semantics is hard. We have created an online benchmark (http://movieqa.cs.toronto.edu), encouraging inspiring work in this challenging domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Integration of language and vision is a natural step towards improved understanding and is receiving increasing attention from the research community. This is in large part due to efforts in large-scale data collection such as Microsoft's COCO <ref type="bibr" target="#b21">[22]</ref>, Flickr30K <ref type="bibr" target="#b45">[46]</ref> and Abstract Scenes <ref type="bibr" target="#b49">[50]</ref> providing tens to hundreds of thousand images with natural language captions. Having access to such data enabled the community to shift from hand-crafted language templates typically used for image description <ref type="bibr" target="#b18">[19]</ref> or retrieval-based approaches <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b44">45]</ref> to deep neural models <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b41">42]</ref> that achieve impressive captioning results. Another way of conveying semantic understanding of both vision and text is by retrieving semantically meaningful images given a natural language query <ref type="bibr" target="#b12">[13]</ref>. An interesting direction, particularly for the goals of our paper, is also the task of learning common sense knowledge from captioned images <ref type="bibr" target="#b39">[40]</ref>. This has so far been demonstrated only on synthetic clip-art scenes which enable perfect visual parsing.</p><p>Video understanding via language. In the video domain, there are fewer works on integrating vision and language, likely due to less available labeled data. In <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b40">41]</ref>, the authors caption video clips using LSTMs, <ref type="bibr" target="#b32">[33]</ref> formulates description as a machine translation model, while older work uses templates <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b17">18]</ref>. In <ref type="bibr" target="#b20">[21]</ref>, the authors retrieve relevant video clips for natural language queries, while <ref type="bibr" target="#b28">[29]</ref> exploits captioned clips to learn action and role models. For TV series in particular, the majority of work aims at recognizing and tracking characters in the videos <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b34">35]</ref>. In <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b33">34]</ref>, the authors aligned videos with movie scripts in order to improve scene prediction. <ref type="bibr" target="#b38">[39]</ref> aligns movies with their plot synopses with the aim to allow semantic browsing of large video content via textual queries. Just recently, <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b48">49]</ref> aligned movies to books with the aim to ground temporal visual data with verbose and detailed descriptions available in books.</p><p>Question-answering. QA is a popular task in NLP with significant advances made recently with neural models such as memory networks <ref type="bibr" target="#b35">[36]</ref>, deep LSTMs <ref type="bibr" target="#b11">[12]</ref>, and structured prediction <ref type="bibr" target="#b42">[43]</ref>. In computer vision, <ref type="bibr" target="#b22">[23]</ref> proposed a Bayesian approach on top of a logic-based QA system <ref type="bibr" target="#b19">[20]</ref>, while <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b29">30]</ref> encoded both an image and the question using an LSTM and decoded an answer. We are not aware of QA methods addressing the temporal domain. QA Datasets. Most available datasets focus on image <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b49">50]</ref> or video description <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b8">9]</ref>. Particularly relevant to our work is the MovieDescription dataset <ref type="bibr" target="#b31">[32]</ref> which transcribed text from the Described Video Service (DVS), a narration service for the visually impaired, for a collection of over 100 movies. For QA, <ref type="bibr" target="#b22">[23]</ref> provides questions and answers (mainly lists of objects, colors, etc.) for the NYUv2 RGB-D dataset, while <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b46">47]</ref> do so for MS-COCO with a dataset of a million QAs. While these datasets are unique in testing the vision algorithms in performing various tasks such as recognition, attribute induction and counting, they are inherently limited to static images. In our work, we collect a large QA dataset sourced from over 400 movies with challenging questions that require semantic reasoning over a long temporal domain.</p><p>Our dataset is also related to purely text QA datasets such as MCTest <ref type="bibr" target="#b30">[31]</ref> which contains 660 short stories with 4 multi-choice QAs each, and <ref type="bibr" target="#b11">[12]</ref> which converted 300K news summaries into Cloze-style questions. We go beyond these datasets by having significantly longer text, as well as multiple sources of available information (plots, subtitles, scripts and DVS). This makes our data one of a kind.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">MovieQA dataset</head><p>The goal of our paper is to create a challenging benchmark that evaluates semantic understanding over long temporal data. We collect a dataset with very diverse sources of information that can be exploited in this challenging domain. Our data consists of quizzes about movies that the automatic systems will have to answer. For each movie, a quiz comprises of a set of questions, each with 5 multiplechoice answers, only one of which is correct. The system has access to various sources of textual and visual information, which we describe in detail below.</p><p>We collected 408 subtitled movies, and obtained their extended summaries in the form of plot synopses from Wikipedia. We crawled imsdb for scripts, which were avail- able for 49% (199) of our movies. A fraction of our movies (60) come with DVS transcriptions provided by <ref type="bibr" target="#b31">[32]</ref>.</p><p>Plot synopses are movie summaries that fans write after watching the movie. Synopses widely vary in detail and range from one to 20 paragraphs, but focus on describing content that is directly relevant to the story. They rarely contain detailed visual information (e.g. character appearance), and focus more on describing the movie events and character interactions. We exploit plots to gather our quizzes.</p><p>Videos and subtitles. An average movie is about 2 hours in length and has over 198K frames and almost 2000 shots. Note that video alone contains information about e.g., "Who" did "What" to "Whom", but may be lacking in information to explain why something happened. Dialogs play an important role, and only both modalities together allow us to fully understand the story. Note that subtitles do not contain speaker information. In our dataset, we provide video clips rather than full movies.</p><p>DVS is a service that narrates movie scenes to the visually impaired by inserting relevant descriptions in between dialogs. These descriptions contain sufficient "visual" information about the scene that they allow visually impaired audience to follow the movie. DVS thus acts as a proxy for a perfect vision system, and is another source for answering.</p><p>Scripts. The scripts that we collected are written by screenwriters and serve as a guideline for movie making. They typically contain detailed descriptions of scenes, and, unlike subtitles, contain both dialogs and speaker information. Scripts are thus similar, if not richer in content to DVS+Subtitles, however are not always entirely faithful to the movie as the director may aspire to artistic freedom.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">QA Collection method</head><p>Since videos are difficult and expensive to provide to annotators, we used plot synopses as a proxy for the movie. While creating quizzes, our annotators only referred to the story plot and were thus automatically coerced into asking story-like questions. We split our annotation efforts into two primary parts to ensure high quality of the collected data.  Q and correct A. Our annotators were first asked to select a movie from a large list, and were shown its plot synopsis one paragraph at a time. For each paragraph, the annotator had the freedom of forming any number and type of questions. Each annotator was asked to provide the correct answer, and was additionally required to mark a minimal set of sentences within the plot synopsis paragraph that can be used to both frame the question and answer it. This was treated as ground-truth for localizing the QA in the plot.</p><p>In our instructions, we asked the annotators to provide context to each question, such that a human taking the quiz should be able to answer it by watching the movie alone (without having access to the synopsis). The purpose of this was to ensure questions that are localizable in the video and story as opposed to generic questions such as "What are they talking?". We trained our annotators for about one to two hours and gave them the option to re-visit and correct their data. The annotators were paid by the hour, a strategy that allowed us to collect more thoughtful and complex QAs, rather than short questions and single-word answers.</p><p>Multiple answer choices. In the second step of data collection, we collected multiple-choice answers for each question. Our annotators were shown a paragraph and a question at a time, but not the correct answer. They were then asked to answer the question correctly as well as provide 4 wrong answers. These answers were either deceiving facts from the same paragraph or common-sense answers. The annotator was also allowed to re-formulate or correct the question. We used this to sanity check all the questions received in the first step. All QAs from the "val" and "test" set underwent another round of clean up.</p><p>Time-stamp to video. We further asked in-house annotators to align each sentence in the plot synopsis to the video by marking the beginning and end (in seconds) of the video that the sentence describes. Long and complicated plot sentences were often aligned to multiple, non-consecutive video clips. Annotation took roughly 2 hours per movie. Since we have each QA aligned to a sentence(s) in the plot synopsis, the video to plot alignment links QAs with video clips. We provide these clips as part of our benchmark.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Dataset Statistics</head><p>In the following, we present some statistics of our MovieQA dataset. <ref type="table" target="#tab_2">Table 2</ref> presents an overview of popular and recent Question-Answering datasets in the field. Most datasets (except MCTest) use very short answers and are thus limited to covering simpler visual/textual forms of understanding. To the best of our knowledge, our dataset not only has long sentence-like answers, but is also the first to use videos in the form of movies.</p><p>Multi-choice QA. We collected a total of 14,944 QAs from 408 movies. Each question comes with one correct and four deceiving answers. <ref type="table">Table 1</ref> presents an overview of the dataset along with information about the train/val/test splits, which will be used to evaluate automatically trained QA models. On average, our questions and answers are fairly long with about 9 and 5 words respectively unlike most other QA datasets. The video-based answering split for our dataset, supports 140 movies for which we aligned plot synopses with videos. Note that the QA methods needs to look at a long video clip (∼200s) to answer the question. <ref type="figure" target="#fig_0">Fig. 3</ref> presents the number of questions (bubble area) split based on the first word of the question along with information about number of words in the question and answer. Of particular interest are "Why" questions that require verbose answers, justified by having the largest average number of words in the correct answer, and in contrast, "Who" questions with answers being short people names.  Instead of the first word in the question, a peculiar way to categorize QAs is based on the answer type. We present such an analysis in <ref type="figure" target="#fig_1">Fig. 4</ref>. Note how reasoning based questions (Why, How, Abstract) are a large part of our data. In the bottom left quadrant we see typical question types that can likely be answered using vision alone. Note however, that even the reasoning questions typically require vision, as the question context provides a visual description of a scene (e.g., "Why does John run after Mary?").</p><p>Text sources for answering. In <ref type="table" target="#tab_5">Table 3</ref>, we summarize and present some statistics about different text sources used for answering. Note how plot synopses have a large number of words per sentence, hinting towards the richness and complexity of the source.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Multi-choice Question-Answering</head><p>We now investigate a number of intelligent baselines for QA. We also study inherent biases in the data and try to answer the quizzes based simply on answer characteristics such as word length or within answer diversity.</p><p>Formally, let S denote the story, which can take the form of any of the available sources of informatione.g. plots, subtitles, or video shots. Each story S has a set of questions, and we assume that the (automatic) student reads one question q S at a time. Let {a S j } M j=1 be the set of multiple choice answers (only one of which is correct) corresponding to q S , with M = 5 in our dataset.</p><p>The general problem of multi-choice question answering can be formulated by a three-way scoring function f (S, q S , a S ). This function evaluates the "quality" of the answer given the story and the question. Our goal is thus to pick the best answer a S for question q S that maximizes f :</p><formula xml:id="formula_0">j * = arg max j=1...M f (S, q S , a S j )<label>(1)</label></formula><p>Answering schemes are thus different functions f . We drop the superscript (·) S for simplicity of notation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">The Hasty Student</head><p>We first consider f which ignores the story and attempts to answer the question directly based on latent biases and similarities. We call such a baseline as the "Hasty Student" since he/she is not concerned to read/watch the actual story.</p><p>The extreme case of a hasty student is to try and answer the question by only looking at the answers. Here, f (S, q, a j ) = g H1 (a j |a), where g H1 (·) captures some properties of the answers.</p><p>Answer length. We explore using the number of words in the multiple choices to find the correct answer and explore biases in the dataset. As shown in <ref type="table">Table 1</ref>, correct answers are slightly longer as it is often difficult to frame long deceiving answers. We choose an answer by: (i) selecting the longest answer; (ii) selecting the shortest answer; or (iii) selecting the answer with the most different length.</p><p>Within answer similarity/difference. While still looking only at the answers, we compute a distance between all answers based on their representations (discussed in Sec. 4.4). We then select our answer as either the most similar or most distinct among all answers.</p><p>Q and A similarity. We now consider a hasty student that looks at both the question and answer, f (S, q, a j ) = g H2 (q, a j ). We compute similarity between the question and each answer and pick the highest scoring answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">The Searching Student</head><p>While the hasty student ignores the story, we consider a student that tries to answer the question by trying to locate a subset of the story S which is most similar to both the question and the answer. The scoring function f is f (S, q, a j ) = g I (S, q) + g I (S, a j ) .</p><p>(</p><p>a factorization of the question and answer similarity. We propose two similarity functions: a simple windowed cosine similarity, and another using a neural architecture.</p><p>Cosine similarity with a sliding window. We aim to find the best window of H sentences (or shots) in the story S that maximize similarity between the story and question, and story and answer. We define our similarity function:</p><p>f (S, q, a j ) = max l l+H k=l g ss (s k , q) + g ss (s k , a j ) ,</p><p>where s k denotes a sentence (or shot) from the story S. We use g ss (s, q) = x(s) T x(q) as a dot product between the (normalized) representations of the two sentences (shots). We discuss these representations in detail in Sec. 4.4.</p><p>Searching student with a convolutional brain (SSCB). Instead of factoring f (S, q, a j ) as a fixed (unweighted) sum of two similarity functions g I (S, q) and g I (S, a j ), we build a neural network that learns such a function. Assuming the story S is of length n, e.g. n plot sentences or n video shots, g I (S, q) and g I (S, a j ) can be seen as two vectors of length n whose k-th entry is g ss (s k , q). We further combine all [g I (S, a j )] j for the 5 answers into a n×5 matrix. The vector g I (S, q) is replicated 5-times, and we stack the question and answer matrix together to obtain a tensor of size n × 5 × 2.</p><p>Our neural similarity model is a convnet (CNN), shown in <ref type="figure" target="#fig_2">Fig. 5</ref>, that takes the above tensor, and applies couple layers of h = 10, 1 × 1 convolutions to approximate a family of functions φ(g I (S, q), g I <ref type="figure">(S, a j )</ref>). Additionally, we incorporate a max pooling layer with kernel size 3 to allow for scoring the similarity within a window in the story. The last convolutional output is a tensor with shape ( n 3 , 5), and we apply both mean and max pooling across the storyline, add them, and make predictions using softmax. We train our network using cross-entropy loss and the Adam optimizer <ref type="bibr" target="#b13">[14]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Memory Network for Complex QA</head><p>Memory Networks were originally proposed for text QA and model complex three-way relationships between the story, question and answer. We briefly describe MemN2N proposed by <ref type="bibr" target="#b35">[36]</ref> and suggest simple extensions to make it suitable for our data and task.</p><p>The input of the original MemN2N is a story and question. The answering is restricted to single words and is done by picking the most likely word from the vocabulary V of 20-40 words. Note that this is not directly applicable to MovieQA, as our data set does not have perform vocabulary-based answering.</p><p>A question q is encoded as a vector u ∈ R d using a word embedding B ∈ R d×|V| . Here, d is the embedding dimension, and u is obtained by mean-pooling the representations of words in the question. Simultaneously, the sentences of the story s l are encoded using word embeddings A and C to provide two different sentence representations m l and c l , respectively. m l , the representation of sentence l in the story, is used in conjunction with u to produce an attention-like mechanism which selects sentences in the story most similar to the question via a softmax function:</p><formula xml:id="formula_3">p l = softmax(u T m l ) .<label>(4)</label></formula><p>The probability p l is used to weight the second sentence embedding c l , and the output o = l p l c l is obtained by pooling the weighted sentence representations across the story. Finally, a linear projection W ∈ R |V|×d decodes the question u and the story representation o to provide a soft score for each vocabulary word a = softmax(W (o + u)) .</p><p>The top scoring wordâ is picked from a as the answer. The free parameters to train are the embeddings B, A, C, W for different words which can be shared across different layers.</p><p>Due to its fixed set of output answers, the MemN2N in the current form is not designed for multi-choice answering with open, natural language answers. We propose two key modifications to make the network suitable for our task.</p><p>MemN2N for natural language answers. To allow the MemN2N to rank multiple answers written in natural language, we add an additional embedding layer F which maps each multi-choice answer a j to a vector g j . Note that F is similar to embeddings B, A and C, but operates on answers instead of the question or story. To predict the correct answer, we compute the similarity between the answers g, the question embedding u and the story representation o:</p><formula xml:id="formula_5">a = softmax((o + u) T g)<label>(6)</label></formula><p>and pick the most probable answer as correct. In our general QA formulation, this is equivalent to</p><formula xml:id="formula_6">f (S, q, a j ) = g M 1 (S, q, a j ) + g M 2 (q, a j ),<label>(7)</label></formula><p>where g M 1 attends to parts of the story using the question, and a second function g M 2 directly considers similarities between the question and the answer. Weight sharing and fixed word embeddings. The original MemN2N learns embeddings for each word based directly on the task of question-answering. However, to scale this to large vocabulary data sets like ours, this requires unreasonable amounts of training data. For example, training a model with a vocabulary size 14,000 (obtained just from plot synopses) and d = 100 would entail learning 1.4M parameters for each embedding. To prevent overfitting, we first share all word embeddings B, A, C, F of the memory network. Nevertheless, even one embedding is still a large number of parameters.</p><p>We make the following crucial modification that allows us to use the Memory Network for our dataset. We drop B, A, C, F and replace them by a fixed (pre-trained) word embedding Z ∈ R d1×|V| obtained from the Word2Vec model and learn a shared linear projection layer T ∈ R d2×d1 to map all sentences (stories, questions and answers) into a common space. Here, d 1 is the dimension of the Word2Vec embedding, and d 2 is the projection dimension. Thus, the new encodings are u = T · Zq; m l , c l = T · Zs l ; and g j = T · Za j . <ref type="formula">(8)</ref> Answer prediction is performed as before in Eq. 6.</p><p>We initialize T either using an identity matrix d 1 × d 1 or using PCA to lower the dimension from d 1 = 300 to d 2 = 100. Training is performed using stochastic gradient descent with a batch size of 32.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Representations for Text and Video</head><p>TF-IDF is a popular and successful feature in information retrieval. In our case, we treat plots (or other forms of text) from different movies as documents and compute a weight for each word. We set all words to lower case, use stemming, and compute the vocabulary V which consists of words w that appear more than θ times in the documents. We represent each sentence (or question or answer) in a bagof-words style with an TF-IDF score for each word.</p><p>Word2Vec. A disadvantage of TF-IDF is that it is unable to capture the similarities between words. We use the skip-gram model proposed by <ref type="bibr" target="#b24">[25]</ref> and train it on roughly 1200 movie plots to obtain domain-specific, 300 dimensional word embeddings. A sentence is then represented by mean-pooling its word embeddings. We normalize the resulting vector to have unit norm.</p><p>SkipThoughts.</p><p>While the sentence representation using mean pooled Word2Vec discards word order, SkipThoughts <ref type="bibr" target="#b15">[16]</ref> use a Recurrent Neural Network to capture the underlying sentence semantics. We use the pretrained model by <ref type="bibr" target="#b15">[16]</ref> to compute a 4800 dimensional sentence representation.</p><p>Video. To answer questions from the video, we learn an embedding between a shot and a sentence, which maps the two modalities in a common space. In this joint space, one can score the similarity between the two modalities via a simple dot product. This allows us to apply all of our proposed question-answering techniques in their original form.</p><p>To learn the joint embedding we follow <ref type="bibr" target="#b48">[49]</ref> which extends <ref type="bibr" target="#b14">[15]</ref> to video. Specifically, we use the GoogLeNet architecture <ref type="bibr" target="#b36">[37]</ref> as well as hybrid-CNN <ref type="bibr" target="#b47">[48]</ref> to extract framewise features, and mean-pool the representations over all frames in a shot. The embedding is a linear mapping of the shot representation and an LSTM on word embeddings on the sentence side, trained using the ranking loss on the MovieDescription Dataset <ref type="bibr" target="#b31">[32]</ref> as in <ref type="bibr" target="#b48">[49]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">QA Evaluation</head><p>We present results for question-answering with the proposed methods on our MovieQA dataset. We study how various sources of information influence the performance, and how different levels of complexity encoded in f affects the quality of automatic QA.</p><p>Protocol. Note that we have two primary tasks for evaluation. (i) Text-based: the story takes the form of various texts -plots, subtitles, scripts, DVS; and (ii) Video-based: story is the video, and with/without subtitles.</p><p>Dataset structure. The dataset is divided into three disjoint splits: train, val, and test, based on unique movie titles in each split. The splits are optimized to preserve the ratios between #movies, #QAs, and all the story sources at 10:2:3 (e.g. about 10k, 2k, and 3k QAs). Stats for each split are presented in <ref type="table">Table 1</ref>. The train set is to be used for training automatic models and tuning any hyperparameters. The val set should not be touched during training, and may be used to report results for several models. The test set is a held-  <ref type="table">Table 4</ref>: The question-answering accuracy for the "Hasty Student" who tries to answer questions without looking at the story.</p><p>out set, and is evaluated on our MovieQA server. For this paper, all results are presented on the val set.</p><p>Metrics. Multiple choice QA leads to a simple and objective evaluation. We measure accuracy, the number of correctly answered QAs over the total count.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">The Hasty Student</head><p>The first part of <ref type="table">Table 4</ref> shows the performance of three models when trying to answer questions based on the answer length. Notably, always choosing the longest answer performs better (25.3%) than random (20%). The second part of <ref type="table">Table 4</ref> presents results when using within-answer feature-based similarity. We see that the answer most similar to others is likely to be correct when the representations are generic and try to capture the semantics of the sentence (Word2Vec, SkipThoughts). The most distinct answers performs worse than random on all features. In the last section of <ref type="table">Table 4</ref> we see that computing feature-based similarity between questions and answers is insufficient for answering. Especially, TF-IDF performs worse than random since words in the question rarely appear in the answer.</p><p>Hasty Turker. To analyze the deceiving nature of our multi-choice QAs, we tested humans (via AMT) on a subset of 200 QAs. The turkers were not shown the story in any form and were asked to pick the best possible answer given the question and a set of options. We asked each question to 10 turkers, and rewarded each with a bonus if their answer agreed with the majority. We observe that without access to the story, humans obtain an accuracy of 27.6%. We suspect that the bias is due to the fact that some of the QAs reveal the movie (e.g., "Darth Vader") and the turker may have seen this movie. Removing such questions, and reevaluating on a subset of 135 QAs, lowers the performance to 24.7%. This shows the genuine difficulty of our QAs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Searching Student</head><p>Cosine similarity with window. The first section of <ref type="table" target="#tab_8">Table 5</ref> presents results for the proposed cosine similarity using different representations and text stories. Using the plots to answer questions outperforms other sources (subtitles, scripts, and DVS) as the QAs were collected using plots and annotators often reproduce words from the plot.</p><p>We show the results of using Word2Vec or SkipThought representations in the following rows of <ref type="table" target="#tab_8">Table 5</ref>  SkipThoughts perform much worse than both TF-IDF and Word2Vec which are closer. We suspect that while SkipThoughts are good at capturing the overall semantics of a sentence, proper nouns -names, places -are often hard to distinguish. <ref type="figure" target="#fig_3">Fig. 6</ref> presents a accuracy breakup based on the first word of the questions. TF-IDF and Word2Vec perform considerably well, however, we see a larger difference between the two for "Who" and "Why" questions. "Who" questions require distinguishing between names, and "Why" answers are typically long, and mean pooling destroys semantics. In fact Word2Vec performs best on "Where" questions that may use synonyms to indicate places. SkipThoughts perform best on "Why" questions where sentence semantics help improve answering. SSCB. The middle rows of <ref type="table" target="#tab_8">Table 5</ref> show the result of our neural similarity model. Here, we present additional results combining all text representations (SSCB fusion) via our CNN. We split the train set into 90% train / 10% dev, such that all questions and answers of the same movie are in the same split, train our model on train and monitor performance on dev. Both val and test sets are held out. During training, we also create several model replicas and pick the ones with the best validation performance. <ref type="table" target="#tab_8">Table 5</ref> shows that the neural model outperforms the simple cosine similarity on most tasks, while the fusion method achieves the highest performance when using plot synopses as the story. Ignoring the case of plots, the accuracy is capped at about 30% for most modalities showing the difficulty of our dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Memory Network</head><p>The original MemN2N which trains the word embeddings along with the answering modules overfits heavily on our dataset leading to near random performance on val (∼20%). However, our modifications help in restraining the learning process.   able to sift through thousands of story sentences and performs well on DVS, subtitles and scripts. This shows that complex three-way scoring functions are needed to tackle such QA sources. In terms of story sources, the MemN2N performs best with scripts which contain the most information (descriptions, dialogs and speaker information).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Video baselines</head><p>We evaluate SSCB and MemN2N in a setting where the automatic models answer questions by "watching" all the video clips that are provided for that movie. Here, the story descriptors are shot embeddings.</p><p>The results are presented in <ref type="table" target="#tab_10">Table 6</ref>. We see that learning to answer questions using video is still a hard problem with performance close to random. As visual information alone is insufficient, we also perform and experiment combining video and dialog (subtitles) through late fusion. We train the SSCB model with the visual-text embedding for subtitles and see that it yields poor performance (22.3%) compared to the fusion of all text features (27.7%). For the memory network, we answer subtitles as before using Word2Vec.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We introduced the MovieQA data set which aims to evaluate automatic story comprehension from both video and text. Our dataset is unique in that it contains several sources of information -video clips, subtitles, scripts, plots and DVS. We provided several intelligent baselines and extended existing QA techniques to analyze the difficulty of our task. Our benchmark with an evaluation server is online at http://movieqa.cs.toronto.edu.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Average number of words in MovieQA dataset based on the first word in the question. Area of a bubble indicates #QA.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Stats about MovieQA questions based on answer types. Note how questions beginning with the same word may cover a variety of answer types: Causality: What happens ... ?; Action: What did X do? Person name: What is the killer's name?; etc.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Our neural similarity architecture (see text for details).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>Accuracy for different feature representations of plot sentences with respect to the first word of the question.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc>A comparison of various QA datasets. First three columns depict the modality in which the story is presented. AType: answer type; AW: average # of words in answer(s); MC (N): multiple choice with N answers; FITB: fill in the blanks; *estimated information.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head></head><label></label><figDesc>Text type # Movies # Sent. / Mov. # Words in Sent.</figDesc><table>Plot 
408 
35.2 
20.3 
Subtitle 
408 
1558.3 
6.2 
Script 
199 
2876.8 
8.3 
DVS 
60 
636.3 
9.3 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 3 :</head><label>3</label><figDesc>Statistics for the various text sources used for answering.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head></head><label></label><figDesc>.</figDesc><table>Method 

Plot DVS Subtitle Script 

Cosine TFIDF 
47.6 24.5 
24.5 
24.6 
Cosine SkipThought 
31.0 19.9 
21.3 
21.2 
Cosine Word2Vec 
46.4 26.6 
24.5 
23.4 

SSCB TFIDF 
48.5 24.5 
27.6 
26.1 
SSCB SkipThought 
28.3 24.5 
20.8 
21.0 
SSCB Word2Vec 
45.1 24.8 
24.8 
25.0 

SSCB Fusion 
56.7 24.8 
27.7 
28.7 

MemN2N (w2v, linproj) 40.6 33.0 
38.0 
42.3 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 5 :</head><label>5</label><figDesc>Accuracy for Text-based QA. Top: results for the Searching student with cosine similarity; Middle: Convnet SSCB; and Bottom: the modified Memory Network.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="false"><head>Table 5 (</head><label>5</label><figDesc>bottom) presents results for MemN2N with Word2Vec initialization and a linear projection layer. Using plot synopses, we see a performance closer to SSCB with Word2Vec features. However, in the case of longer stories, the attention mechanism in the network is</figDesc><table>Method 
Video Subtitle Video+Subtitle 

SSCB all clips 
21.6 
22.3 
21.9 
MemN2N all clips 
23.1 
38.0 
34.2 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" validated="true"><head>Table 6 :</head><label>6</label><figDesc>Accuracy for Video-based QA and late fusion of Subtitle and Video scores.</figDesc><table>What 
Who 
Why 
How 
Where 
20 

30 

40 

50 

60 

70 

Accuracy 

TF-IDF 
Word2Vec 
SkipThought 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgment. We thank the Upwork annotators, Lea Jensterle, Marko Boben, and Soča Fidler for data collection, and Relu Patrascu for infrastructure support. MT and RS are supported by DFG contract no. STI-598/2-1, and the work was carried out during MT's visit to U. of T. on a KHYS Research Travel Grant.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">VQA: Visual Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Semisupervised Learning with Constraints for Person Identification in Multimedia Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Baeuml</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tapaswi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Video-In-sentences Out</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Barbu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Burchill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Coroian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dickinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Michaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mussman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Narayanaswamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Salvi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shangguan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Siskind</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Waggoner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UAI</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<title level="m">Finding Actors and Actions in Movies. ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2280" to="2287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Collecting highly parallel data for paraphrase evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">B</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Learning a Recurrent Visual Representation for Image Caption Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.5654</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Movie/Script: Alignment and Parsing of Video and Text Transcription</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Miltsakaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Taskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A Thousand Frames in Just a Few Words: Lingual Description of Videos through Latent Topics and Sparse Object Stitching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">F</forename><surname>Doell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A thousand frames in just a few words: Lingual description of videos through latent topics and sparse object stitching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">F</forename><surname>Doell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Long-term Recurrent Convolutional Networks for Visual Recognition and Description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.4389</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Every Picture Tells a Story: Generating Sentences for Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hejrati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sadeghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rashtchian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hockenmaier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Teaching Machines to Read and Comprehend</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kočisky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Blunsom</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.03340</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep Visual-Semantic Alignments for Generating Image Descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TACL</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<title level="m">Skip-Thought Vectors. NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">What are you talking about? Text-to-Image Coreference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Generating Natural-Language Video Descriptions Using Text-Mined Knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Krishnamoorthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Malkarnenkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2013-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Baby Talk: Understanding and Generating Simple Image Descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Premraj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning dependencybased compositional semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational Linguistics</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<title level="m">Visual Semantic Search: Retrieving Videos via Complex Textual Queries. CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common Objects in Context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A Multi-World Approach to Question Answering about Real-World Scenes based on Uncertain Input</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Ask Your Neurons: A Neural-based Approach to Answering Questions about Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Im2Text: Describing Images Using 1 Million Captioned Photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<title level="m">Inferring the Why in Images. arXiv.org</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Linking People in Videos with &quot;Their&quot; Names Using Coreference Resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno>ECCV. 2014. 2</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Video Event Understanding using Natural Language Descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Exploring Models and Data for Image Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.02074</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Mctest: A challenge dataset for the open-domain machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Burges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Renshaw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A Dataset for Movie Description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tandon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Translating Video Content to Natural Language Descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pinkal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Subtitle-free Movie to Script Alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">V</forename><surname>Jawahar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Who are you?&quot; -Learning person specific classifiers from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="1145" to="1152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">End-To-End Memory Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.08895</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.4842</idno>
		<title level="m">Going deeper with convolutions</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Book2Movie: Aligning Video scenes with Book chapters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tapaswi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bauml</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Aligning Plot Synopses to Videos for Story-based</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tapaswi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bäuml</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Retrieval. IJMIR</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="3" to="16" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning Common Sense Through Visual Abstraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Translating Videos to Natural Language Using Deep Recurrent Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>CoRR abs/1312.6229, cs.CV</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Show and Tell: A Neural Image Caption Generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.4555</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Machine Comprehension with Syntax, Frames, and Semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.05698</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Corpus-guided Sentence Generation of Natural Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Teo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Iii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Aloimonos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="444" to="454" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hodosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hockenmaier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TACL</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Visual Madlibs: Fill in the blank Image Generation and Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Learning Deep Features for Scene Recognition using Places Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Aligning Books and Movies: Towards Story-like Visual Explanations by Watching Movies and Reading Books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Adopting abstract images for semantic scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PP</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2014" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
