<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Recognizing Activities of Daily Living with a Wrist-mounted Camera</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katsunori</forename><surname>Ohnishi</surname></persName>
							<email>ohnishi@mi.t.u-tokyo.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="department">Graduate School of Information Science and Technology</orgName>
								<orgName type="institution">The University of Tokyo</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atsushi</forename><surname>Kanehira</surname></persName>
							<email>kanehira@mi.t.u-tokyo.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="department">Graduate School of Information Science and Technology</orgName>
								<orgName type="institution">The University of Tokyo</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asako</forename><surname>Kanezaki</surname></persName>
							<email>kanezaki@mi.t.u-tokyo.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="department">Graduate School of Information Science and Technology</orgName>
								<orgName type="institution">The University of Tokyo</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsuya</forename><surname>Harada</surname></persName>
							<email>harada@mi.t.u-tokyo.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="department">Graduate School of Information Science and Technology</orgName>
								<orgName type="institution">The University of Tokyo</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Recognizing Activities of Daily Living with a Wrist-mounted Camera</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:44+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a novel dataset and a novel algorithm for recognizing activities of daily living (ADL) from a first-person wearable camera. Handled objects are crucially important for egocentric ADL recognition. For specific examination of objects related to users' actions separately from other objects in an environment, many previous works have addressed the detection of handled objects in images captured from head-mounted and chest-mounted cameras. Nevertheless, detecting handled objects is not always easy because they tend to appear small in images. They can be occluded by a user's body. As described herein, we mount a camera on a user's wrist. A wrist-mounted camera can capture handled objects at a large scale, and thus it enables us to skip the object detection process. To compare a wrist-mounted camera and a head-mounted camera, we also developed a novel and publicly available dataset 1 that includes videos and annotations of daily activities captured simultaneously by both cameras. Additionally, we propose a discriminative video representation that retains spatial and temporal information after encoding the frame descriptors extracted by convolutional neural networks (CNN).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recently, activity recognition from first-person camera views has been attracting increasing interest, motivated by advances in wearable device technology. Recognition of activities of daily living (ADL) from first-person views is an important task related to activity recognition. ADL are basic activities in a typical human life such as "making coffee" or "cutting paper." If the system recognizes ADL properly, then it is applicable to nursing services, rehabilitation, and lifestyle habit improvements.</p><p>To recognize ADL, it is important to examine objects undergoing hand manipulation specifically. For example, a cleaning activity might be recognized only by recognizing that a user is using a vacuum cleaner. One can also recog- nize coffee-making activity if it is observed that a user is handling a mug and coffee beans. Pirsiavash and Ramanan <ref type="bibr" target="#b25">[26]</ref> described the importance of recognizing handled objects for ADL recognition. They developed an ADL dataset collected using a chest-mounted camera. Then, they implemented ADL recognition in different homes by detecting the user's hands and handled objects. The result suggests the crucial importance of detecting the handled objects properly in various environments for ADL recognition.</p><p>In a view from a head-mounted camera or a chestmounted camera, handled objects are captured at a small scale in various positions. Furthermore, many non-handled objects also appear in the captured image. Consequently, many studies have examined hand detection or gaze prediction to develop a means of discerning picked-up and handled objects from other detected objects. However, such approaches entail the following difficulties: (a) Despite the advances in object-detection techniques, object detection is not an easy task in various environments. (b) Discerning a handled object from detected objects with hand detection or gaze prediction is not an easy task. (c) To train object detectors, it is necessary to collect numerous images with bounding boxes. Building a high-quality dataset with bounding boxes requires a considerable amount of labor, which hinders us from expanding a dataset.</p><p>To train an ADL recognition system without pixel-level annotations or bounding boxes of objects, we consider mounting a wearable camera on the wrist of the user's dominant arm because the objects are handled mainly by the user's dominant hand. We designate this camera as a wrist-mounted camera in this paper. For ADL recognition, a wrist-mounted camera has numerous advantages over a head-mounted camera or a chest-mounted camera: (a) Wrist-mounted cameras can capture a large image of the handled objects. (b) Because handled objects are close to a user's dominant hand, the object positions are restricted in the images from the wrist-mounted camera. (c) Because of the above reasons, we can skip object detection and do not need a dataset of ADL with bounding boxes. We only need a dataset of ADL with annotations about the activity time segments in the videos.</p><p>We also propose a recognition system for videos captured by a wrist-mounted camera that has strong spatial bias and weak temporal bias. As shown in <ref type="figure">Figure 2</ref>, an image captured by a wrist-mounted camera has strong spatial bias, meaning that hand-manipulated objects tend to be located at the central area. In addition, the order of manipulated objects is mostly fixed for each action, which we call "weak temporal bias." The state-of-the-art video representation, which extracts local features containing spatial information from pre-trained convolutional neural networks (CNNs), strongly loses spatial information and completely loses temporal information after encoding. Therefore, we also propose a novel video representation that retains spatial and temporal information after encoding to consider the above mentioned biases.</p><p>Our three contributions are the following:</p><p>1. We propose the use of a wrist-mounted camera for ADL recognition instead of a head-mounted camera or a chest-mounted camera. 2. We propose a discriminative video representation that retains spatial and temporal information. This is a method for the dataset captured from a wrist-mounted camera that has a large bias of spatial information and a small bias of temporal information. 3. We developed a novel and publicly available dataset that includes videos and annotations of ADL captured from a head-mounted camera and a wrist-mounted camera simultaneously.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Egocentric vision for ADL recognition</head><p>Various approaches for ADL recognition based on handled objects have been proposed <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b34">35]</ref>. Because wearable devices with cameras such as GoPro and Google Glasses have been developed recently, ADL recognition with viewpoint cameras has received a considerable amount of attention. Some works on egocentric ADL recognition have achieved results in a single environment, such as a kitchen or an office <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19]</ref>.</p><p>For more practical settings, Pirsiavash and Ramanan <ref type="bibr" target="#b25">[26]</ref> estimated the type of a handled object by detecting the object and arm from a wearable camera's viewpoint. They demonstrated that action recognition performs well in diverse environments. However, it is necessary to provide positional information of all objects in all frames of the video at the time of learning. In addition, detecting an entire handled object itself is still difficult. Although their dataset has various annotations, such as type of activity and duration of its completion, as well as type of an object and its location, it took over 1 month to label various annotations by 10 parttime annotators. Consequently, expanding the dataset is not practical. In a more practical setting, an ADL recognition system that uses wearable devices in diverse environments should be trained with labels obtained by simpler annotation methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Video representation for action recognition</head><p>Video representation has been well studied in the action recognition domain. Some deep-learning approaches for action recognition have been proposed <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b30">31]</ref>. However, these approaches require the use of large-scale video datasets (e.g. Sports 1M <ref type="bibr" target="#b13">[14]</ref>), which are difficult to address and which require enormous amounts of time for the whole learning process.</p><p>Motion features: The general pipeline to obtain a video representation for action recognition models the distribution of local features from training videos. Local features representing motion information (e.g., HOG <ref type="bibr" target="#b3">[4]</ref>, HOF <ref type="bibr" target="#b19">[20]</ref>, and MBH <ref type="bibr" target="#b4">[5]</ref>) are usually used. The combination of local features and improved dense trajectory (iDT) <ref type="bibr" target="#b33">[34]</ref>, which compensates for camera motion, is the de facto standard. It has shown great performance for action recognition <ref type="bibr" target="#b32">[33]</ref>.</p><p>CNN descriptors: CNN has achieved superior results to the standard pipeline for object recognition <ref type="bibr" target="#b15">[16]</ref>. Jain et al. <ref type="bibr" target="#b11">[12]</ref> brought CNN to action recognition. They obtain the state of a fully connected layer from each frame in videos and calculate the video representation by averaging all CNN features. Their method therefore exhibits performance that is surprisingly comparable to the combination of iDT, MBH, and Fisher vector (FV) <ref type="bibr" target="#b24">[25]</ref>. To obtain more discriminative features containing spatial information, Xu et al. <ref type="bibr" target="#b35">[36]</ref> proposed the extraction of latent concept descriptors (LCDs) from the pool 5 layer and the application of VLAD <ref type="bibr" target="#b12">[13]</ref> instead of averaging. However, spatial information is ignored when applying VLAD. Since our task is an intermediate task of action recognition and object recognition, we developed the CNN-based video representation above to design the video representation for ADL recognition from wrist-mounted cameras, which have strong spatial information bias. <ref type="figure">Figure 2</ref>. Mean images of a head-mounted camera (left) and a wrist-mounted camera (right). Skin pixels are visible on the right side of the wrist-mounted camera image, although we cannot see anything in the head-mounted camera image. This implies that the user's hand always appears in the right side and handled objects appear near the center of the wrist-mounted camera image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Wrist-mounted cameras</head><p>Some works in the area of interface research have shed light on wrist-mounted cameras <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b14">15]</ref>. In ADL recognition, Maekawa et al. <ref type="bibr" target="#b20">[21]</ref> conducted multimodal ADL recognition using a wrist-mounted device that has a camera, microphone, acceleration sensor, illuminance meter, and digital compass. However, the color histogram alone is used as an image feature. This system is too simple to identify handled objects. Wrist-mounted cameras have never been evaluated carefully in ADL recognition. Therefore, we discuss the superiority of wrist-mounted cameras in this section.</p><p>Wrist-mounted cameras capture handled objects very closely, as shown in <ref type="figure" target="#fig_0">Figure 1</ref>. In addition, as shown in <ref type="figure">Figure 2</ref>, the user's hand invariably appears on the right side of the image captured by a wrist-mounted camera, unlike that by a head-mounted camera. This trend of wristmounted cameras also means that handled objects always appear near the center of the captured image. Because of these strong spatial biases, we can recognize handled objects well even without manually annotating the bounding boxes of objects in the dataset. We need only to annotate the time segments of the activities. Wrist-mounted cameras have limitations: they cannot take pictures of human faces or recognize posture-defined actions such as "jumping" or "skipping." Although there are such limitations, wrist-mounted cameras are more suitable for recognizing ADL, which mostly involves object manipulation.</p><p>As another feature, wrist-mounted cameras can process large motions. Because the handled objects and a wrist-mounted camera move together, other irrelevant parts, which move relative to the wrist-mounted camera, are blurred, whereas the handled objects are captured clearly. In addition, the objects, while moving, appear as static objects in the camera view, which enables robust recognition. This blurring effect is better obtained by setting the focal length to 10-30 cm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Video representation</head><p>We propose a new video representation based on LCDs <ref type="bibr" target="#b35">[36]</ref> to take advantage of the strong spatial bias and weak temporal bias of the video captured by a wrist-mounted camera. Although LCD retains the spatial information in each frame at the descriptor level, spatial information and temporal information are dropped when the descriptors are encoded and aggregated into a video representation. However, the video captured by a wrist-mounted camera has a strong spatial bias, as described in Section 3. Although not as strong as spatial bias, temporal bias also exists because the order of handling objects is fixed roughly in each action class. Therefore, we use the benefits of these spatial and temporal biases specifically for a wrist-mounted camera and ADL.</p><p>Our method encodes LCDs at each location in all frames into single VLAD <ref type="bibr" target="#b12">[13]</ref> vectors and optimizes the weights for the VLAD vectors to aggregate them into a video representation. The weight for a VLAD vector extracted from each location is designated as a spatial weight. Furthermore, we propose a method that divides a video into short sequences and optimizes the temporal weights for aggregating descriptors. Here, we describe the original LCD in Section 4.1, the proposed method to optimize spatial weights in Section 4.2, and the proposed method to optimize spatial and temporal weights in Section 4.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">CNN latent concept descriptors</head><p>Latent concept descriptors <ref type="bibr" target="#b35">[36]</ref> constitute a state-of-theart video representation using CNN, which is obtained as follows. (i) Given a video E including T frames E = {I 1 , I 2 , · · · , I T }, each frame is input to VGG net <ref type="bibr" target="#b26">[27]</ref> pretrained on the ImageNet2012 dataset <ref type="bibr" target="#b5">[6]</ref> to obtain the pool 5 layer's output. The dimension of pool 5 features is a×a×M , where a is the size of the filtered images of the last pooling layer and M is the number of convolutional filters in the last convolutional layer (in the case of VGG net, a = 7 and M = 512). (ii) The responses of M filters are concatenated for the respective locations of the pool 5 layer.</p><p>Then, a set of a 2 descriptors f t (i,j) ∈ R M is obtained from the t-th frame as follows.</p><formula xml:id="formula_0">F t = {f t (1,1) , f t (1,2) , . . . , f t (a,a) }.<label>(1)</label></formula><p>(iii) All descriptors in {F 1 , . . . , F T } are encoded with VLAD into a video representation v. Letting {c 1 , . . . , c K } denote a set of K coarse centers obtained by K-means, we obtain u k (k = 1, . . . , K) as follows:</p><formula xml:id="formula_1">u k = ∑ (t,i,j)∈{(t,i,j)|NN ( f t (i,j) ) =c k } (f t (i,j) − c k ),<label>(2)</label></formula><p>Therein, NN</p><formula xml:id="formula_2">( f t (i,j) )</formula><p>represents the nearest center of f (i,j) . Then, v is obtained as an M K-dimensional VLAD encoding vector by concatenating u k over all K centers. (iv) Finally, v is normalized by power and L2 normalization with intra-normalization <ref type="bibr" target="#b2">[3]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Discriminative spatial aggregated latent concept descriptors</head><p>The original LCD <ref type="bibr" target="#b35">[36]</ref> drops the spatial information in the process of VLAD encoding because the descriptors are equally weighted when they are encoded with VLAD into a video representation v. We introduce spatial weights for the VLAD encoding vectors distinguished by their locations when we aggregate them into a final video representation. Because of the spatial weights, we can address the spatial bias such that the center area is more important than its surroundings, for example, because hand-manipulated objects tend to be located at the center view of a wristmounted camera. Specifically, we obtain a video representation v(i, j) for each cell (i, j) over all the T frames by encoding the descriptors in a set F (i,j) = {f 1 (i,j) , . . . , f T (i,j) }. In the same manner as in <ref type="bibr" target="#b35">[36]</ref>, v(i, j) is normalized by power and L2 normalization with intra-normalization. Letting w (i,j) ∈ R Nsp denote an N sp -dimensional weight vector, we obtain a weighted sum of v(i, j) as</p><formula xml:id="formula_3">V = a ∑ i=1 a ∑ j=1 v(i, j)w ⊤ (i,j) = V sp W sp ,<label>(3)</label></formula><p>where V sp ∈ R M K×a 2 and W sp ∈ R a 2 ×Nsp are defined as shown below.</p><formula xml:id="formula_4">V sp = (v(1, 1), v(1, 2), . . . , v(a, a)),<label>(4)</label></formula><formula xml:id="formula_5">W sp = (w (1,1) , w (1,2) , . . . , w (a,a) ) ⊤ .<label>(5)</label></formula><p>As described in this paper, we obtain W sp by arranging N sp eigenvectors x ∈ R a 2 obtained by partial least squares (PLS) in N sp rows. Note that PLS is a method that can extract common information between sets of observed features. Therefore, N sp represents how many eigenvectors we use were obtained from PLS. Details related to computing the eigenvectors are given in the Supplemental Materials. Finally, we obtain a video representation v DSAR ∈ R M KNsp , which is called discriminative spatial aggregated LCDs (DSAR), by concatenating all elements in V ∈ R M K×Nsp in (3). Here, v DSAR is normalized by power and L2 normalization.</p><p>The idea to use the eigenvectors obtained by PLS as spatial weights was derived from the discriminative spatial pyramid representation (D-SPR) <ref type="bibr" target="#b9">[10]</ref>. Consequently, the proposed method described in this section can be regarded as the combination of LCD and D-SPR. Our method optimizes the weights for a × a cells in the output of pool 5 , whereas D-SPR optimizes the weights for areas in a spatial pyramid of each frame.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Discriminative spatiotemporal aggregated latent concept descriptors</head><p>The wrist-mounted camera dataset not only has a strong spatial information bias, but also has a weak temporal information bias. The original LCD described in Section 4.1 and the proposed DSAR described in Section 4.2 lose temporal information in the process of VLAD encoding. Inspired by the idea of spatiotemporal pyramids <ref type="bibr" target="#b16">[17]</ref>, we introduce temporal weights for the VLAD encoding vectors distinguished by their time stamps when we aggregate them into a final video representation. Because of the temporal weights, we can assign the importance of each frame into a whole video representation. Specifically, we split a video into 2 l sequences consisting of equal numbers of frames T /2 l . The s-th sequence is a set of frames {I (s−1)T /2 l +1 , . . . , I sT /2 l } (s = 1, . . . , 2 l ). Then, we obtain a video representation v l s (i, j) for each cell (i, j) over all the T /2 l frames in the s-th sequence by encoding the descriptors in a set</p><formula xml:id="formula_6">F s (i,j) = {f (s−1)T /2 l +1 (i,j) , . . . , f sT /2 l (i,j) }.</formula><p>Here, we consider multiple levels of the splitting (l = 0, . . . , L) such that we obtain a set of v l s (i, j) as follows: <ref type="figure" target="#fig_0">1, . . . , a, j = 1, . . . , a)</ref>. <ref type="formula">(6)</ref> Again, v l s (i, j) is normalized by power and L2 normalization with intra-normalization. Letting w l s ∈ R Ntmp denote an N tmp -dimensional weight vector, we obtain a weighted sum of v l s (i, j) as follows:</p><formula xml:id="formula_7">V = {v 0 1 (i, j), v 1 1 (i, j), . . . , v L 1 (i, j), . . . , v L 2 L (i, j)} (i =</formula><formula xml:id="formula_8">V = a ∑ i=1 a ∑ j=1 L ∑ l=0 2 l ∑ s=1 v l s (i, j) ⊗ w (i,j) ⊗ w l s .<label>(7)</label></formula><p>Here, we define <ref type="figure" target="#fig_0">(1, 2)</ref>, . . . , v l s (a, a)),</p><formula xml:id="formula_9">V (i, j) ∈ R M K×d , V l s ∈ R M K×a 2 , and W tmp ∈ R d×Ntmp , where d = ∑ L l=0 2 l = 2 L+1 − 1 as shown below. V (i, j) = (v 0 1 (i, j), v 1 1 (i, j), . . . , v L 1 (i, j), . . . , v L 2 L (i, j)),(8) V l s = (v l s (1, 1), v l s</formula><formula xml:id="formula_10">W tmp = (w 0 1 , w 1 1 , . . . , w L 1 , . . . , w L 2 L ) ⊤ .<label>(9)</label></formula><p>As described in this paper, we optimize spatial weights W sp and temporal weights W tmp iteratively and alternately. Specifically, we repeat the following two steps.</p><p>Step 1: optimizing W sp In this step, we fix W tmp and optimize W sp . We obtain M KN tmp -dimensional vectors g (i,j) by concatenating all the elements in V (i, j)W tmp . Letting V ′ ∈ R M KNtmp×a 2 denote (g (1,1) , g (1,2) , . . . , g (a,a) ), we can rewrite <ref type="formula" target="#formula_8">(7)</ref> as</p><formula xml:id="formula_12">V = V ′ W sp .<label>(11)</label></formula><p>This formulation is identical to <ref type="bibr" target="#b2">(3)</ref>. Therefore, we optimize W sp in the manner described in Section 4.2.</p><p>Step 2: optimizing W tmp In this step, we fix W sp and optimize W tmp . We obtain M KN sp -dimensional vectors h l s by concatenating all the elements in V l s W sp . Letting V ′′ ∈ R M KNsp×d denote (h 0 1 , h 1 1 , . . . , h L 1 , . . . , h L 2 L ), we can then rewrite <ref type="formula" target="#formula_8">(7)</ref> as</p><formula xml:id="formula_13">V = V ′′ W tmp .<label>(12)</label></formula><p>This formulation is identical to <ref type="bibr" target="#b2">(3)</ref>. Therefore, we optimize W tmp in the manner presented in Section 4.2. We iterate Step 1 and Step 2 several times. Finally, we obtain a video representation v DSTAR ∈ R M KNspNtmp , which is called discriminative spatiotemporal aggregated LCDs (DSTAR), by concatenating all the elements in V ∈ R M K×Nsp×Ntmp in (7) with power and L2 normalization. An illustration of v DSTAR is shown in <ref type="figure" target="#fig_1">Figure 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Dataset: MILADL</head><p>We created a new ADL dataset that uses both a wristmounted camera and a head-mounted camera because there are as yet no published ADL datasets that use wristmounted cameras. In this section, we present the details of our dataset.</p><p>Note that it is also important to compare a wrist-mounted camera with a chest-mounted camera instead of with a headmounted one since a chest-mounted camera is closer to the user's hands. We encourage to compare wrist-mounting to other mountings <ref type="bibr" target="#b21">[22]</ref> for ADL recognition as future work. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Activity class</head><p>We chose activity classes by referring to previous studies of ADL <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b7">8]</ref>. First, we removed some classes that many users were reluctant to record on video such as "brushing teeth" and "laundry." Next, to introduce more variety into our dataset, we added some actions referring to other ADL recognition studies <ref type="bibr" target="#b29">[30]</ref> and an evaluation of Alzheimer rehabilitation <ref type="bibr" target="#b7">[8]</ref>. As <ref type="table">Table 1</ref> shows, we strove to recognize 23 ADL classes in this study. Detailed information is given in the Supplemental Materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Collection and annotation</head><p>To assemble the dataset, we used a GoPro HERO3+ 2 as the head-mounted camera and an HX-A100 3 as the wristmounted camera. Each user wore these two cameras, as shown in <ref type="figure">Figure 4</ref>. Each user therefore recorded two videos simultaneously. As in a previous ADL egocentric dataset <ref type="bibr" target="#b25">[26]</ref>, we did not instruct the users in detail how to act to obtain realistic data. After taking videos, all users manually annotated the duration and the action class in their own videos. The definition of an action includes some initial and final actions related to the action. For example, the action "cutting paper" is defined as follows: The initial action of "cutting paper" is to take scissors from the table and the final action is to put it on the table. We recruited 20 people to perform these tasks. All users were right handed. Our wrist-Head-mounted camera! Wrist-mounted camera! <ref type="figure">Figure 4</ref>. Wearing a head-mounted camera and a wrist-mounted camera <ref type="figure">Figure 5</ref>. Example images from our head-mounted dataset (tophalf) and wrist-mounted dataset (bottom-half). We present a wide variety of scenes and ADL classes.</p><p>mounted camera and head-mounted camera dataset respectively produced 6.5 h (about 690,000 frames) of images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Characteristics</head><p>Various objects are handled in daily life. Therefore, for ADL recognition, it is important to be able to recognize them in diverse environments. For this study, we asked users to take videos in their own homes. As shown in the examples in <ref type="figure">Figure 5</ref>, the environments caught on camera differ depending on the user. More examples are shown in the Supplemental Materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Experiment protocols</head><p>We used 16-layer VGG net <ref type="bibr" target="#b26">[27]</ref> pre-trained on the Ima-geNet 2012dataset <ref type="bibr" target="#b5">[6]</ref> for the CNN architecture in the same manner as the LCD <ref type="bibr" target="#b35">[36]</ref>. Motion features are also important in action recognition. Therefore, we evaluated our dataset not only with CNN descriptors but also with iDT <ref type="bibr" target="#b33">[34]</ref>. Following <ref type="bibr" target="#b33">[34]</ref>, we reduced the dimensions of the descriptors (HOG, HOF, and MBH) by a factor of 2 with PCA and encode them with FV, where the component number of the Gaussian mixture model was 256. We applied power and L2 normalization to aggregated vectors. As a classifier, we used a one-vs.-all SVM with linear kernel, setting C = 100. We used leave-one-user-out cross-validation for the evaluation so that the same person does not appear across both Video Features WCD HCD LCD+VLAD <ref type="bibr" target="#b35">[36]</ref> 78.6 62.4 LCDspp+VLAD <ref type="bibr" target="#b35">[36]</ref> 73.4</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="51.3">DSAR (ours)</head><p>82.0 61.6 DSTAR (ours) 83.7 62.0 STAR * 77.0 53.5 <ref type="table">Table 2</ref>. Mean classification accuracy of the proposed methods on the wrist-mounted camera dataset (WCD) and the head-mounted camera dataset (HCD). STAR * is the method without weight optimization, which is equivalent to a spatiotemporal pyramid <ref type="bibr" target="#b16">[17]</ref>.</p><p>training and test data for ADL recognition. The iteration number of our methods is fixed at five because it usually converges in a few iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Evaluating DSTAR and our dataset</head><p>We evaluated our approach on our wrist-mounted camera dataset (WCD) and head-mounted camera dataset (HCD). For fair comparison, we reduced the LCD dimensions from 512-D to a various range of dimensions such as 64-D, 128-D, and 256-D with PCA, and encoded them with various numbers of centers K in VLAD such as K = 64, 128, 256, 512, 1024 as in <ref type="bibr" target="#b35">[36]</ref> to find the best ones. We also explored the best choice of dimensions, K, N sp , and N tmp , for our method. We describe the best parameters and how they are determined in the Supplemental Materials because of the limited space here. <ref type="table">Table 2</ref> presents the action classification accuracy of our dataset. Comparing the cameras, we found the accuracy on WCD to be superior to that on HCD for every method. Next, we compared each method on WCD. Actually, DSAR, which retains spatial information after encoding, showed superior performance to LCD; DSTAR, which retains not only spatial information but also temporal information, exhibited superior performance to DSAR on both datasets. Results showed that an LCD with a spatial pooling layer (LCDspp) did not improve performance on our dataset, unlike TRECVID MEDTest 13 and 14 <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. The images captured by a wrist-mounted camera have strong spatial bias, and ADL actions have weak temporal bias, as described in Section 3. From the obtained results, we can confirm that using spatial and temporal bias improves recognition accuracy on WCD. However, DSAR and DSTAR did not improve HCD performance. As shown in <ref type="figure">Figure 2</ref>, we cannot confirm strong spatial bias in the images captured by a head-mounted camera. Cutting features in every cell only made the features sparse. Aggregated video representation does not get more discriminative than without cutting if images have no strong spatial bias. Consequently, DSAR and DSTAR can be shown to improve recognition accuracy more for WCD than for HCD. Through these recognition results, we can confirm that using a wrist-mounted camera and considering spatial and temporal information improved ADL recognition performance. <ref type="figure">Figure 6</ref>. Visualization example of iDT on a head-mounted camera (left) and a wrist-mounted camera (right). These images were captured simultaneously. Green lines are trajectories that were removed from the backgrounds with iDT. It is apparent that many background points in the image of a wrist-mounted camera are regarded as the foreground because of large motion on the camera.  <ref type="table">Table 4</ref>. Mean classification accuracy of combining CNN-based descriptors with motion features, and a wrist-mounted camera with a head-mounted camera. <ref type="table">Table 3</ref> shows how wide our methods can be applied. We first evaluated LCD and our methods on UCIADL <ref type="bibr" target="#b25">[26]</ref>. As shown in the table, our methods did not improve the performance since this dataset has low spatial bias. Moreover, we evaluated them on UCF101 <ref type="bibr" target="#b27">[28]</ref>, which is one of the representative datasets of typical action recognition. Although not as strong as our wrist-mounted dataset, this dataset has substantial bias. Therefore, DSTAR showed better performance on this dataset than LCD did. Details are shown in the Supplemental Materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Applicability in existing datasets</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.">Fusing motion features and cameras</head><p>From Tables 2 and 4, we can confirm that iDT features are less discriminative than CNN-based features on WCD.</p><p>Comparing the iDT on both cameras, we found that iDT on HCD showed better performance than on WCD unlike the CNN descriptors. We can ascertain this reason from <ref type="figure">Figure 6</ref>. As the figure shows, iDT failed to remove the backgrounds from the video captured by a wrist-mounted camera compared with a head-mounted camera because of the large motion of the camera. Therefore, iDT on HCD is superior to that on WCD.</p><p>Jain et al. <ref type="bibr" target="#b10">[11]</ref> showed that combining object features extracted by CNN with motion features such as iDT boosts action classification accuracy. Following their conclusion, we also demonstrate how our method was affected by the combination of motion features. We fused our methods with iDT on each dataset by simply averaging the score obtained using our methods and the mean score obtained by all iDT scores. As <ref type="table">Table 4</ref> shows, the performance of motion features was boosted by our methods more than by LCD. Although iDT features were more discriminative on HCD than on WCD, the combined features showed better performance on WCD than on HCD. Unlike action recognition, object features are more effective than motion features in ADL recognition because the critical key is the handled object. Therefore, we can find that wrist-mounted cameras are more suitable for ADL recognition than head-mounted cameras.</p><p>In case the user wears both a head-mounted camera and a wrist-mounted camera, we can choose superior information from wrist-mounted cameras and head-mounted cameras. Better object information is obtainable from wrist-mounted cameras, but better motion information is obtainable from head-mounted cameras. Therefore, we combined DSTAR on WCD with iDT on HCD to achieve the best accuracy of 89.7% on our dataset. <ref type="figure">Figures 7 and 8</ref> show the absolute values of spatial and temporal weights W sp and W tmp calculated using DSTAR on WCD. This figure presents the optimal discriminative weights for the respective cells.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.">Visualized weights of DSTAR</head><p>Spatial weight: In <ref type="figure">Figure 7</ref>, it is apparent that cells near the center are important for classification, whereas cells on the right side are less important. The user's palm always appears. No object appears on the right side of a wrist-  mounted camera image. Therefore, the right side area in the image has less information for recognition. The features obtained from the upper left cell and the bottom left cell are also less discriminative because backgrounds unrelated to the user's action are often captured in these cells. However, handled objects often appear in the middle area of a wrist-mounted camera image. Discriminative features can be obtained from the cells of these areas.</p><p>Temporal weight: Although not as strong as the spatial bias of the image captured by a wrist-mounted camera, each ADL class has weak temporal bias. As <ref type="figure">Figure  8</ref> shows, although full-length features (level 0) are the most important, temporally cut features (levels 1 and 2) have different weights. Using temporally cut pyramids improved the recognition performance, as presented in <ref type="table">Table 2</ref>. Additionally, slight differences are apparent at the same level. At level 2, the beginning and the end of the action are slightly more important than the middle of the action.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.">Analysis of ADL classification results</head><p>We analyze the classification results here. <ref type="figure" target="#fig_4">Figure 9</ref> presents the results of four different methods: LCD on HCD, LCD on WCD, DSTAR on WCD, and, finally, DSTAR on WCD and iDT on HCD.</p><p>Comparing HCD with WCD: We first compared both cameras using LCD. Results showed that 18 classes showed superior performance on WCD over HCD; these classes improved by 28.1% on average. Especially, "write on paper," "cut paper," and "staple paper" were improved significantly. These classes are actions wherein users use small objects such as a pen, scissors, and a stapler. A head-mounted camera captures these objects at a small scale. However, a wrist-mounted camera can capture large-scale images even of small objects. Four classes, however, showed inferior performance on WCD compared to HCD, and "dry dishes" is the class in which the classification accuracy declined considerably: 18.5%. On WCD, "dry dishes" was more often confused with "wash dishes" than on HCD, although all actions of "wash dishes" were recognized correctly.</p><p>Comparing LCD with DSTAR: Next, we compared DSTAR with LCD on WCD. Using DSTAR instead of LCD improved the accuracy on 14 classes. Its average improvement rate was 10.1%. One significantly improved action class was "vacuuming." When we used a vacuum cleaner with a wrist-mounted camera, the floor and other unrelated backgrounds appeared on the left side of the wrist-mounted camera image. Actually, DSTAR was considered to improve the performance by reducing the importance of the features extracted from these areas. On five classes, the accuracy decreased, but the average rate of decrease was only 5.5%.</p><p>Adding iDT on HCD with DSTAR on WCD: Finally, we noticed how adding iDT affected HCD. Actually, 14 classes improved their performance by adding iDT on HCD; these classes improved by 10.4% on average. Especially, "wipe desk" was highly improved. Because the object information of "wipe desk" on HCD was often confused with other actions done near the table, using a wristmounted camera improved the performance. However, the "wipe desk" motion was distinctive. Consequently, adding iDT on HCD boosted the performance again. Only on five classes did the accuracy decline, the average of which was only 4.7%. In case the user wears both cameras, we can obtain better recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Conclusion</head><p>This study examined the recognition of ADL with a wrist-mounted camera. We developed a publishable dataset of videos taken with a head-mounted camera and a wristmounted camera. Additionally, we proposed a novel video representation that aggregated CNN descriptors spatially and temporally, and optimized their weights both iteratively and alternately. Finally, using the proposed dataset, we quantitatively demonstrated the benefits of a wrist-mounted camera over a head-mounted camera and those of our proposed method over previous methods. We believe that our work will help spread the use of cameras attached to wristmounted devices.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>1 http://www.mi.t.u-tokyo.ac.jp/static/projects/miladl/ Activities of daily living (ADL) captured by a wristmounted camera (left) and a head-mounted camera (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Illustration of the proposed video representation vDSTAR. For this example, we set a = 2 and L = 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 7 .Figure 8 .</head><label>78</label><figDesc>Visualization of DSTAR spatial weights on the wristmounted camera dataset 4 . Visualization of DSTAR temporal weights on the wristmounted camera dataset 4 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 9 .</head><label>9</label><figDesc>This figure represents the recognition accuracy of each ADL class. It also shows the differences between the models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>. Duration of each class and the distribution of the 23 classes in our dataset.</figDesc><table>Action name 
Mean of 
Number of 
length (s) occurrences 
vacuuming 
38.1 
17 
empty trash 
11.5 
22 
wipe desk 
35.7 
23 
turn on air-conditioner 
6.9 
27 
open and close door 
6.4 
34 
make coffee 
88.2 
24 
make tea 
70.0 
22 
wash dishes 
31.3 
29 
dry dishes 
15.7 
27 
use microwave 
33.7 
26 
use refrigerator 
6.8 
42 
wash hands 
11.9 
32 
dry hands 
7.9 
29 
drink water from a bottle 
13.8 
26 
drink water from a cup 
7.8 
31 
read book 
28.5 
28 
write on paper 
16.6 
29 
open and close drawer 
6.0 
30 
cut paper 
14.7 
28 
staple paper 
7.8 
28 
fold origami 
68.7 
23 
use smartphone 
23.1 
29 
watch TV 
21.6 
22 
Table 1</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">http://jp.shop.gopro.com/cameras 3 http://panasonic.jp/wearable/a100</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">These weights were obtained when we dealt with subject nos. 2-20 for training and no. 1 for testing.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Trecvid Med 13</surname></persName>
		</author>
		<ptr target="http://www.nist.gov/itl/iad/mig/med13.cfm.6" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Trecvid Med 14</surname></persName>
		</author>
		<ptr target="http://www.nist.gov/itl/iad/mig/med14.cfm.6" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">All about VLAD</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arandjelovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Human detection using oriented histograms of flow and appearance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning to recognize objects in egocentric activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">An inventory to assess activities of daily living for clinical trials in Alzheimer&apos;s disease</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Galasko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ernesto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Grundman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ferris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Alzheimer Disease &amp; Associated Disorders</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="33" to="39" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Action recognition in awearable assistance system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hanheide</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Hofemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sagerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Discriminative spatial pyramid</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yamashita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kuniyoshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">What do 15,000 object categories tell us about classifying and localizing actions? In CVPR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Van Gemert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Snoek</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">University of amsterdam at THUMOS Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Van Gemert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G M</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV workshop on THUMOS Challenge</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Aggregating local descriptors into a compact image representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pérez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Digits: freehand 3D interactions anywhere using a wrist-worn gloveless sensor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Hilliges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Izadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Butler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Oikonomidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Olivier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UIST</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning realistic human actions from movies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marszałek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rozenfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning to predict gaze in egocentric video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Delving into egocentric actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">An iterative image registration technique with an application to stereo vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">D</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="674" to="679" />
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Object-based activity recognition with heterogeneous sensors on wrist</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Maekawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yanagisawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kishino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ishiguro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kamei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sakurai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Okadome</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPC</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">On the choice and placement of wearable vision sensors. Systems, Man and Cybernetics, Part A: Systems and Humans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">W</forename><surname>Mayol-Cuevas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">J</forename><surname>Tordoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Murray</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="414" to="425" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Beyond short snippets: Deep networks for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Finegrained activity recognition by aggregating abstract object usage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Philipose</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISWC</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Fisher kernels on visual vocabularies for image categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dance</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Detecting activities of daily living in first-person camera views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Ucf101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.0402</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">ADL recognition based on the combination of RFID, and accelerometer sensing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stikic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huynh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Van Laerhoven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pervasive Computing Technologies for Healthcare</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Portable wireless sensors for object usage sensing in the home: Challenges and practicalities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">M</forename><surname>Tapia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Intille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Larson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Ambient Intelligence</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">The wristcam as input device</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vardy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-T</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISWC</title>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Dense trajectories and motion boundary descriptors for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kläser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="60" to="79" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Action recognition with improved trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A scalable approach to activity recognition based on object use</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Osuntogun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Choudhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Philipose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A discriminative CNN video representation for event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
