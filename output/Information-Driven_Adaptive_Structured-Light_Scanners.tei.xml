<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Information-Driven Adaptive Structured-Light Scanners</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guy</forename><surname>Rosman</surname></persName>
							<email>rosman@csail.mit.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">CSAIL</orgName>
								<orgName type="institution" key="instit2">Massachusetts Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniela</forename><surname>Rus</surname></persName>
							<email>rus@csail.mit.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">CSAIL</orgName>
								<orgName type="institution" key="instit2">Massachusetts Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">W</forename><surname>Fisher</surname></persName>
							<email>fisher@csail.mit.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">CSAIL</orgName>
								<orgName type="institution" key="instit2">Massachusetts Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">CSAIL</orgName>
								<orgName type="institution" key="instit2">Massachusetts Institute of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Information-Driven Adaptive Structured-Light Scanners</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:41+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Sensor planning and active sensing, long studied in robotics, adapt sensor parameters to maximize a utility function while constraining resource expenditures. Here we consider information gain as the utility function. While these concepts are often used to reason about 3D sensors, these are usually treated as a predefined, black-box, component. In this paper we show how the same principles can be used as part of the 3D sensor. We describe the relevant generative model for structured-light 3D scanning and show how adaptive pattern selection can maximize information gain in an open-loop-feedback manner. We then demonstrate how different choices of relevant variable sets (corresponding to the subproblems of locatization and mapping) lead to different criteria for pattern selection and can be computed in an online fashion. We show results for both subproblems with several pattern dictionary choices and demonstrate their usefulness for pose estimation and depth acquisition.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Range sensors have revolutionized computer vision in recent years, with commodity RGB-D scanners allowing us to easily tackle challenging problems such as articulated pose estimation <ref type="bibr" target="#b26">[27]</ref>, Simultaneaous Localization and Mapping (SLAM) <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b5">6]</ref>, and object recognition <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b20">21]</ref>. The use of 3D sensors often relies on a simplified model of the resulting depth images that is loosely coupled to the photometric principles behind the design of the scanner. Given this intermediate representation, we deploy computer vision algorithms to understand the world and take actions based on the acquired scene information.</p><p>Significant efforts have been devoted to optimal planning of sensor deployment under resource constraints, e.g., on energy, time, or computation. Sensor planning has been employed in many aspects of vision and robotics, including positioning of 3D sensors and cameras, as well as other active sensing problems, see for example <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b31">32]</ref>. The goal is to focus sensing on the aspects of the environ- ment or scene most relevant to the specific inference task.</p><p>However, the same principles are generally not used to examine the operation of the 3D sensor itself. At a finer scale, each acquisition by a photosensitive sensor is a measurement, and the parameters of the sensors, including any active illumination, are an action parameter (in the decisiontheoretic sense <ref type="bibr" target="#b28">[29]</ref>) to be optimized and planned.</p><p>In this paper we reformulate adaptive selection of patterns in structured-light scanners as the following resource constrained sensor-selection process. We treat the choice of the projected pattern at each time as a planning choice, and the number of projected patterns as a resource. Our goal is to minimize the number of projected patterns while maximizing the task-specific information gain. We compute in-formation gain from the (predicted) observation of the scene given previous observations and a new proposed projected pattern. This allows us to pick the next projected pattern in an online fashion, corresponding to the greedy selection regime in sensor selection.</p><p>The contributions of this paper are: (i) We devise a probabilistic generative graphical model for the 3D scanning process, depicted in <ref type="figure" target="#fig_1">Figure 2</ref>. We estimate mutual information between the observed images and variables in the model in Algs. <ref type="bibr">1,2.</ref> (ii) For the task of range estimation, we demonstrate greedy open-loop pattern selection for the projector in Subsec. 4.1. (iii) For the task of pose estimation, we show which parts of the scene are informative, for several cases of interest, in Subsec. 4.2.</p><p>We note that sensor planning is an instance of experimental design, studied in a variety of domains, including economics <ref type="bibr" target="#b8">[9]</ref>, medical decision making <ref type="bibr" target="#b6">[7]</ref>, robotics <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b10">11]</ref>, and sensor networks <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b13">14]</ref>. While many optimality criteria have been proposed, one commonly used criterion is information gain. It is well-known that selection problems have intractable combinatorial complexity. However, it has been shown that tractable greedy selection heuristics, combined with open-loop feedback control <ref type="bibr" target="#b0">[1]</ref> guarantee near-optimal performance <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b33">34]</ref>, due to the submodular property of conditional mutual information (MI). This assumes one can evaluate the information measure for the set of sensing choices (patterns in our current context). We derive a physics-based model for structured-light sensing that simultaneously lends itself to tractable information evaluation while producing superior empirical results in a real system. We also characterize the informational utility of a given pattern (or class of patterns) in the face of varying relevant versus nuisance parameter choices <ref type="bibr" target="#b17">[18]</ref>. In the process, we demonstrate that the value of a given structured-light pattern changes depending on the specific inference task. We exploit commonly available graphics hardware to efficiently estimate the information gain of a selected pattern and reason about the effect of the dependency structure in the probabilistic model.</p><p>The choice of parameterization for the latent variables in the model is crucial for efficient information gain estimation. This can be seen in the common tasks of range sensing and pose estimation. We consider these two important applications and demonstrate how a careful choice of the scene and scanner representation lends itself to estimation of conditional mutual information.</p><p>In the field of structured-light reconstruction, several studies have suggested adaptive scanners (see for example <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b36">37]</ref>), and energy-efficient designs <ref type="bibr" target="#b23">[24]</ref>. However, unlike previous attempts that observed specific image features and addressed a specific pattern decoding technique, we show how given a generative model for the sensing process we can obtain an adaptive scanner for various tasks, forming a decision-theoretic purposive <ref type="bibr" target="#b21">[22]</ref> 3D scanner.</p><p>We formulate 3D acquisition as a probabilistic inference process within a detailed model for the scene and sensor in Section 2. We discuss methods of representing uncertainty in a manner appropriate for a specific task. In Section 3 we show how MI estimation can be combined with standard approaches for reconstruction in several cases of interest, and demonstrate the integration of MI estimation into a structured-light scanner. Section 4 demonstrates the proposed system in several experiments that exemplify the usefulness of the proposed approach. Section 5 concludes the paper and describes possible new directions. We now describe the generative model used for pattern selection and inferring depth. We adopt a model that describes structured-light and time-of-flight imaging devices and standard cameras or camera-and-projector systems. Estimation of information gain is central to our method and thus impacts the choice of parameterization. We emphasize that approximations we use for estimating information gain and choosing patterns generally do not carry over when we compute the reconstruction. To our knowledge, this is the first analysis of active information-based planning in this setting. The model parameters are roughly partitioned into agent pose, geometry of the scene, and photometry of the scene. We summarize the notation below (see the supplement for further details):</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Modelling Active 3D Computer Vision</head><formula xml:id="formula_0">A G Θ A A l G l η I c I p x ∈ pixels l ∈ viewpoints</formula><p>• A and G denote the photometric and geometric properties of the scene and are modeled as Gaussian per scene element as described in Section 3.</p><p>• Θ denotes the scanner/agent pose. It is distributed as a Gaussian in the Lie-algebra se <ref type="bibr" target="#b2">(3)</ref>. If range estimation is solely of interest, Θ is assumed to be fixed.</p><p>• A l , G l denote the view-dependent representations of the scene. They are not deterministic functions of A, G, Θ due to unmodeled aspects (e.g. occlusions). The geometry and pose determine camera and projector coordinates at each pixel.</p><p>• I c and I p denote the camera and projector intensity values corrupted by additive per-pixel noise η(x). x ∈ R 2 denotes pixels in the camera image plane.</p><p>• A denote the pattern selection.</p><p>The generative graphical model of <ref type="figure" target="#fig_1">Figure 2</ref> depicts the relationships of the variables. Observations are denoted by shaded circles, latent variables by white circles, and parameters by diamonds. As shown in <ref type="figure" target="#fig_1">Figure 2</ref>, the model factorizes as</p><formula xml:id="formula_1">p (A, G, Θ, A l , G l , η, I c , I p ; A) (1) = p (Θ) p (A) p (G) l p (A l |A, Θ) p (G l |G, Θ) l,x p (I c |A l , G l , I p , η) p (I p |G l , Θ; A) p (η) ,</formula><p>where the first line includes prior terms for the scene. The second incorporates projection onto a specific viewpoint of the projector images and world model, and the last line involves sensor image rendering, and noise realization.</p><p>We note that depending on the inference task various latent variables alternate their roles as either relevant or nuisance. We choose patterns in order to maximize focused information gains <ref type="bibr" target="#b17">[18]</ref>, i.e., information regarding the relevant set, rather than information of the non-relevant, or nuisance, variables. We follow the notation of <ref type="bibr" target="#b17">[18]</ref> where R ⊆ U denotes the relevant set and U denotes the set of all nodes. Nuisance parameters have certainly been considered in existing 3D reconstruction methods. Examples include the standard binarize-decode-reconstruct approach for time-multiplexed structured-light scanners or the choice of view-robust descriptors for 3D reconstruction from multiple views <ref type="bibr" target="#b27">[28]</ref>. The utility of the generative model is that nuisances are dealt with in a mathematically-consistent fashion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Inference and Sensor Planning in 3D Vision</head><p>We consider several inference tasks of interest in 3D computer vision and the pattern selection issues which arise. For example, inference of G l given I c , I p , Θ amounts to 3D reconstruction, where G l is assumed to approximate G and A l is treated as a nuisance. Previous methods adopt a probabilistic model for improving structured-light reconstruction <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b25">26]</ref>, but assume a predetermined set of patterns. Alternatively, Simultaneous Localization and Mapping (SLAM) methods incorporate inference steps for the geometry and pose parameters alternating between pose (Θ) updates conditioned on the geometry (G l ) and vice-versa. Updates to the 3D map may be posed as inference of G given G l , Θ. In all cases, limiting assumptions regarding occlusions, the relation of appearance parameters and 3D geometry, and the relation between different range scans of the same scene are typically invoked. For structured-light acquisition, one can associate pixels in I c and I p given the range r at each pixel x (which is a choice for G l ) and the pose Θ. The set of pixels in I p are obtained via Π r,Θ (x) ∈ R 2 by back-projecting x into the 3D world and projecting it into the projector image plane. The relation between the intensity values of these pixels can be given as</p><formula xml:id="formula_2">(a) (b) (c) (d) (e)</formula><formula xml:id="formula_3">I c (x) = a(x)I p (Π r,Θ (x)) + b(x) + η(x),<label>(2)</label></formula><p>where a, b depend on the ambient light, normals, and albedo of the incident surface. For sufficiently large photon count, η is assumed Gaussian accounting for sensor noise and unmodeled phenomena such as occlusions and non-Lambertian lighting components. Utilizing timemultiplexed structured-light, plane-sweeping <ref type="bibr" target="#b25">[26]</ref> enables efficient inference of G l from I c , I p , and incorporation of priors on the scene structure G. For our purposes, one can assume a fixed pose, and limit the inference to estimation of G l . <ref type="figure" target="#fig_2">Figure 3</ref> provides an example of I c , I p , a, b, r for a reconstructed scene with random smoothed patterns (as described in Subsection 4.1). The resulting 3D reconstruction is superior to the classic binarize-decode-triangulate pipeline with respect to robustness to artifacts such as specularities and low SNR conditions. Our goal is to efficiently compute the relevant mutual information quantities I A (x R ; I C ) for different definitions of R, and choices from the set A, alternately considering Θ, G, and A as the relevant variable set x R . Nonlinear correspondence operators (back-projection and projection) linking I c , I p complicate dependency analysis within the model and preclude analytic forms. We exploit common graphics hardware for a straightforward and efficient sampling approach that follows the generative model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Photometric Entropy in Active Illumination 3D Scanning</head><p>When describing 3D scanner, the interplay of photometric models and the reconstruction can lead to improved results <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b22">23]</ref> and warrants examination. In Equation 2, coefficients a and b capture illumination variability. A slightly more detailed description of the photometric model</p><formula xml:id="formula_4">I c = ρ 1 r p (x) 2 n(x), l I p (π r (x)) + ρI amb ,<label>(3)</label></formula><p>aids in our understanding of the contributions of the different factors. Here, ρ is the albedo coefficient, n(x) is the surface normal at a given image location x, l is the projector direction, and I amb is the ambient lighting. r p is the distance from the projector, and I p (π r (x)) is the projector intensity, assumed pixel-wise independent. Observing the pixel intensity entropy associated with different simplifications of this model provides us with intuition on the relative importance of various factors and gives us some bounds on how much information can be gained from modification of the patterns. Specifically, the difference in image entropy between an arbitrary i.i.d. pattern, and a deterministic pattern that deforms according to the geometry gives us a bound on the maximum information gain. In the supplement, we construct a synthetic experiment that evaluates the sensitivity of entropy and information measures to each factor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Estimating Uncertainty in 3D Scanners</head><p>We present two important cases of estimating mutual information gain for pattern selection in structured-light scanners. In each, we consider inference over different subsets of variables, and the mutual information between them and the observed images. Differing assumptions on the fixed/inferred variables and dependency structure in the image formation model lead to different algorithms for MI estimation given as Algorithms 1 and 2.</p><p>An important observation is that given the pose, range measurements and camera image pixel values can be approximated as an independent estimation problem per-pixel (here we model the effect of surface self-occlusions as noise). This provides an efficient and parallelizable estimation procedure for the case of range estimation. This assumption has been exploited in plane-sweeping stereo, and we now utilize it for MI estimation. We note that even where the inter-pixel dependency is not negligible, we can compute an upper bound for the information gain. For example, for the case of pose and range estimation we obtain</p><formula xml:id="formula_5">I(Ic; Θ, r) =H(Ic) − H(Ic|Θ, r) ≤ (4) x H(I x c ) − x H(I (x) c |Θ, r) Î (Ic; Θ),</formula><p>whereÎ is the pixel-wise mutual information between the sensor and the inferred parameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Range Image MI Estimation</head><p>We start with the simple, yet instructive, case of estimating mutual information between the scene geometry and the observed images given a known set of illumination patterns. Here, inference is over G l as represented by the range at each camera pixel r ≡ r(x). We assume a Gaussian prior for a and b.</p><p>We compute the pixel-wise mutual information individually and sum the results. In this subsection, we assume a deterministic choice of pose; the patterns are deterministic throughout the paper, and hence omitted from the notation for I. The mutual information between I c and G l given θ,I p is given bŷ</p><formula xml:id="formula_6">I (Ic; G l |θ) = x I (Ic(x); r(x)|θ) (5) = x E Ic,r|θ log p(Ic|r, θ) p (Ic|θ) .</formula><p>While computing p(I c |r, θ) is straightforward, we are still forced to estimate p(I c |θ), which can be done by marginalizing over r according to our posterior estimates,</p><formula xml:id="formula_7">p(I c |θ) = E r [p (I c |r, θ)].<label>(6)</label></formula><p>For each sample of θ, r, we can then compute the log of the likelihoods ratio, and integrate it. We note the existence of alternatives such as using GMMs or Laplace approximations, for efficient implementation. We perform one sampling loop in order to estimate p(I c |θ). We then use another set of samples in order to estimate I (I c ; G l |θ). Algorithm 1 describes computation of the MI gain for frame T .</p><p>Since a, b, η (0..T ) are all are assumed to be Gaussian conditioned on r, p a, b, I </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Pose MI Estimation with Structured-Light</head><p>A second important case we explore is typical of pose estimation problems, where we try to infer a lowdimensionality latent variable set with global influence, in addition to range uncertainty. In 3D pose estimation, we usually estimate Θ given a model of the world G. In visual SLAM, G, A, A l are commonly used to infer Θ, G l , either as online inference <ref type="bibr" target="#b30">[31]</ref>, or in batch-mode <ref type="bibr" target="#b11">[12]</ref>, where usually a specific function of the input (feature locations from different frames, or correspondence estimates) is taken. In Algorithm 1 MI estimation / pattern selection for range image <ref type="bibr">1:</ref> for pattern p, in each pixel x do 2:</p><p>for samples i = 1, 2, . . . , N hist do 3:</p><p>Sample a range value for x according to p(r).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4:</head><p>Raytrace I p , sample I c . Compute the statistics of a, b, I c conditioned on previous image measurements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:</head><p>Compute probability p(I c |r).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6:</head><p>Update the estimated per-pixel histogram, p(I c ). Draw a new range value for x according to a proposal distribution p(r).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>10:</head><p>Raytrace I p , sample I c . Compute the statistics of a, b, I c conditioned on previous image measurements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>11:</head><p>Compute probability p(I c |r), estimate log p(Ic|r) p(Ic) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>12:</head><p>Update the estimated mutual information. <ref type="bibr">13:</ref> end for 14: end for 15: Pick pattern p with maximum MI sum over the image depth-sensor based SLAM, the range sensors obtain a measurement G l under some active illumination. Θ is then approximated from G, G l .</p><p>We now describe computation of the MI between the pose and the images. As before, we parameterize G l by r(x), and given (Θ, r) we re-establish a correspondence between I p and I c . This is done by computing a backprojected point x 3 j (denoting it is a 3D point), transforming it according to Θ to getx 3 j , and projectingx 3 j onto the camera and projector image. A similar situation would arise where inferring a class variable, where instead of merely inferring Θ we also infer a categorical variable C that determines the class of the observed object. Here too, we can still use the following observations: (i) given the pose parameters, the problem can still be approximated as a perpixel process -this assumption underlies most visual servoing approaches. (ii) the pose parameter space is lowdimensional and can be sampled from, as is often done in particle filters for pose estimation. We can therefore write</p><formula xml:id="formula_8">I I (x) c ; Θ|G l = E Ic,Θ,r   log P (I (x) c |Θ) P I (x) c   ,<label>(7)</label></formula><p>where as before, P (I c |θ) is computed by marginalization over r. This procedure is detailed as Algorithm 2. When computing p(I for each sampled range value r(x) do 5:</p><p>Back-project x 3 , computex 3 = T θi,r (x).  We note that when sampling the pose, different variants of the range images can be used, allowing us to marginalize w.r.t. range uncertainty as well.</p><p>When sampling a conditioned image model per pixel, collisions in the projected pixels can occur. While these can be arbitrated using atomic operations on the GPU, the semantics of write hazards on GPUs are such that invalid pixel states can be avoided. Furthermore, to allow efficient computation on the GPU, we must consider memory access patterns. In our implementation we compute proposal image statistics given θ, and then aggregate the contribution into the accumulators for the mutual information per pixel.</p><p>Extension to classification we could incorporate categorical variables, including object classes as part of Θ. This requires merely changing lines 4,14, in Algorithm 2 to sample a distribution overx 3 j (θ, C, r) instead ofx 3 j (θ, r). This allows us to choose patterns for object classification tasks, which is beyond the scope for this paper.</p><p>While sampling the full space of appearance and range per-pixel is computationally expensive, running the algo-  rithm without any optimizations on a GPU takes approximately one second on an Nvidia Quadro K2000.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Numerical Results</head><p>We conducted several experiments aimed at giving an intuition for the approach proposed in this paper, and demonstrating its utility, with several choices of projector patterns and scenes. In terms of the relevant sets of variables, we have focused on range sensing and pose estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Pattern Choice for Range Sensing</head><p>We first demonstrate the setup used. For pattern libraries we used a set of random patterns generated by smoothing i.i.d. Gaussian noise with Gaussian filters of various scales, and striped patterns of the sort used for gray-code structured-light. They are shown in <ref type="figure" target="#fig_9">Figures 5 and 9</ref>, respectively. We used as test objects both fabricated models with various scales of features, see <ref type="figure" target="#fig_9">Figure 5</ref>, and coated/raw wooden art models. The PointGrey Grasshopper II camera and TI LightCrafter projector used are shown in <ref type="figure" target="#fig_0">Figure 1</ref>. Pixel noise standard deviation was about 2.5/255 for most experiments. We validate the use of the smoothed Gaussian patterns for reconstruction in <ref type="figure" target="#fig_8">Figure 4</ref>, demonstrating the decrease in the average range L2 error measured as we use more patterns for reconstruction. We use the reconstruction from a set of 120 patterns as a ground-truth estimate, making the assumption that the reconstruction is an unbiased estimator, so that reconstruction using all patterns is considered a ground-truth.</p><p>In <ref type="figure" target="#fig_9">Figure 5</ref> we show the MI gain collected over the scene, averaged over 50 random pattern sequences. The amount of information gained from the patterns decreases as we add more patterns, as expected with MI, and surfaces that are well-illuminated and frontal-facing having faster uncer- 20 (strong prior on the appearance). Cyan lineσr = 7mm (low initial uncertainty of the range). Given a good prior on the nuisance parameters of the albedo, range is estimated more quickly in terms of frames. Given a strong range prior, the region does not require as many patterns for estimation, and overall MI gain is smaller. Right: Blue -information gain for a set of different patterns. Green -where only half of the patterns are shown, but they are repeated twice. The information gain is much lower in the second case. tainty reduction. We look at the average MI gain per pattern over various random sequences of patterns, in <ref type="figure" target="#fig_10">Figure 6</ref>. We highlight several interesting cases. The first case (which often occurs in practice) assumes high uncertainty of the range or the appearance coefficients. The second and third cases involve less and more certainty in the appearance coefficients respectively. The fourth case involves having a good initial guess (std. of 7mm) for the range. As expected, the certainty of the appearance coefficients increases the MI between the images and the range. Having a good range prior decreases the amount of information gained per frame and the overall MI.</p><p>We then proceed to perform selection according to MI gain based on the proposed model. Although we perform greedy (pattern at a time) selection, there are bounds guaranteeing the performance of a greedy vs. optimal selection of the whole pattern sequence -see <ref type="bibr" target="#b33">[34]</ref> for such bounds and the relevant terminology. In our test we initialize each attempt from a pair of randomly chosen patterns. At each turn we try ten randomly chosen patterns and compute their image-range MI. We pick the the most informative pattern, and contrast this with a random pattern selection. The MI gains for two scenes are measured in <ref type="table">Table 1</ref>, collected over ten instantiations.</p><p>In one scenario, we modulate the patterns by spatial bands in the projector's image plane: 14 bands in the x and in the y directions with 15 random textures instantiations for each band, see example in <ref type="figure" target="#fig_11">Figure 7</ref>(a). From these we greedily select patterns in ten sequences, and unify them into 69 unique patterns. The patterns are mostly those that illuminate the region of interest, as expected by their high MI gain. The region of interest is defined as the silhoutte of an object (the hand) in the image. A similar test was done with patterns modulated by an exponentially, radially decreasing envelope, illuminating local regions of the projector field of view at each time (see <ref type="figure" target="#fig_11">Figure 7</ref>(d)). 20 random patterns are taken, modulated by 15 random locations. Of these, 65 are selected after removing repetitions. Here the region of interest was the mannequin. We use these pattern sets to reconstruct the range image, and compare to randomly choosing the same number of patterns. Qualitatively, the selected patterns often illuminated parts of the objects which were poorly reconstructed, as expected. As we show in <ref type="figure" target="#fig_11">Figure 7</ref>, we get significantly more accurate reconstruction compared to random selection-18.9mm RMS, compared to 24.1mm RMS for the hand example, and 51.3mm compared to 59.1mm in the mannequin example. This demonstrates the usefulness of our selection criteria when judged by reconstruction accuracy.</p><p>Finally, in order to demonstrate that greedy selection improves reconstruction, on average, per pattern selection, we perform ten greedy selection steps, selecting a single pattern out of ten randomly drawn ones, and demonstrate the resulting reconstruction. We take striped gray-code patterns modulated by radially-decreasing piece-wise smooth masks, centered at various locations, for a total of 240 patterns. The results of adding patterns at random vs. greedy selection show that even when we do not yet have reasonable reconstruction, greedy selection according to MI improved L2 reconstruction error. Despite the fact the L2 reconstruction error does not directly coincide with MI, we show that computing MI gain according to our model results early on in the reconstruction sequence in improved reconstruction results, as shown in <ref type="figure" target="#fig_12">Figure 8</ref>. For example, the depth reconstruction error obtained by 10 random patterns is obtained with less than six patterns in the greedy case, representing a 40% speedup. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Pattern Choice for Pose Estimation</head><p>In <ref type="figure" target="#fig_0">Figures 9-12</ref> we show computed per-pixel MI between a new camera image and the pose, assuming a highly certain range image, as estimated by Algorithm 2. We start in <ref type="figure">Figure 9</ref> with a synthetic case where the results are easy to interpret, with a scene made of a single large corner. The pattern set for this experiement is the standard gray-code striped patterns, shown in the first row. We assume only translational uncertainty; we leave reasoning about the full SE(3) pose space to future work as it is less instructive. We use stripes going from coarse to fine, stopping at a pattern of four pixels stripe width in the projector image plane. At this phase, the appearance coefficients A, G are well estimated. In this example the camera and the projector are facing the z direction, and in front of them there is a large smoothed corner. We compare a case of uncertainty in the xy plane, to that of uncertainty in the z plane in terms of the pixelwise MI gain. The large sloped corner and the edges are the main source of uncertainty reduction in xy since the rest of the scene is planar. In the z uncertain case, the full image is informative to the same extent. The intermediate case is a mix between the two, as expected.</p><p>For pattern selection, in <ref type="figure" target="#fig_0">Figure 10</ref> we demonstrate pattern choice according to the proposed criteria for choosing patterns in a structured-light scanner. This shows that for an unknown pose information can be obtained from edges and corners; given a reasonable model of the scene, we can use mutual information to suggest which pattern to use to project only informative parts of the scene. The patterns chosen consist of a striped pattern projected only along a partial band of the projector screen. <ref type="figure" target="#fig_0">Figure 11</ref>   <ref type="figure" target="#fig_11">Figure 7</ref>. Our MI-greedy approach obtains a larger information gain, and does so faster (in frame counts) than a random ordering of frames. <ref type="figure">Figure 9</ref>. Per-pixel information gain for the case of initial uncertain scanner position. Left-to-right, top row: a set of patterns used for 3D sensing for pose estimation. Middle row: a rendering of the scene with sensor pose samples (black dots) in 3 scenarios, and the fields of view of the projector and camera. Bottom row: pixelwise mutual information estimates: with high uncertainty in the x-y plane of the scanner, uncertainty in x-y-z, and z-only uncertainty in scanner position. Yellow and red marking high and low information gain, respectively. Surfaces at sharp angles to the projector and camera provide greater uncertainty reduction in the x-y directions, whereas for uncertainty in the z axis, all surfaces are informative. different set of patterns, of stripes modulated by a Gaussian mask, allowing to focus a pattern in a small region, which is important in practical applications. As can be seen, the top-ranking patterns are those that illuminate edges in the scene, which should give us high uncertainty reduction. MI for pose estimation can also be seen with real scenes. In <ref type="figure" target="#fig_0">Figure 12</ref> we show pixelwise pose estimate for Gaussian smoothed patterns. The most informative pixels are edges and sloped areas, where the perceived projector intensity changes rapidly as a function of the pose.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this paper we present a novel information-driven approach to planning into 3D sensors at the sensor level. We demonstrate how different uncertainty estimates and sensor models lead to different criteria for pattern selection. Future work includes the completion of a prototype scanner based on the proposed approaches. This decision-theoretic approach where action choice is identified with pattern selection in structured-light easily extends to other reconstruction techniques such as depth-from-focus (see for example <ref type="bibr">Figure 10</ref>. Left: the depth image and the MI scores of vertical and horizonal stripe masks of the patterns with respect to pose estimation in the xy plane. Right: the top-scoring horizontal and vertical patterns, as seen when projected onto the scene. As can be seen, the patterns that were selected are the ones illuminating the edges and corner. <ref type="figure" target="#fig_0">Figure 11</ref>. Left-to-right, top row: the top 3 selected masks from a set of 60 masks, and the range image. Bottom row: a MAP estimated images for the 3 masks, used when estimating the MI for each pattern, followed by the average MI scores for the patterns. Red circles mark the patterns shown. <ref type="figure" target="#fig_0">Figure 12</ref>. Left-to-right, top row: an image of the scene, one of the projected patterns as capture, the range image, the pixelwise mutual information with respect to the pose, which initial uncertainty in the camera's xy plane. The main informative areas are the cones, and regions that face the x, y directions. <ref type="bibr" target="#b35">[36]</ref>) and compressive sensing time-of-flight <ref type="bibr" target="#b9">[10]</ref>. We intend to explore these in future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Illustration of patterns selection. Each row illustrates another turn of pattern selection. For each pattern, the information gain is estimated, shown by different border color around each pattern, and the different stem heights in the plot on the left. Black arrowheads and red circles in the plot mark the selected pattern at each turn. Note the different patterns selected, and diminishing information gain over time. Bottom row: Left: the proposed openloop w/ feedback 3D scanning with pattern selection flowchart. Right: the project/camera system used for 3D scanning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Proposed model for classification with active illumination.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Left-to-right: a) Ic, b) coefficients a and c) b from Equation 2for the MAP-estimated range d) Ip in the camera image plane, e) the range image in mm. Note how parameter b captures scene illumination, whereas parameter a captures the reflectance coefficient of the surface with respect to the projector.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>conditioning on each image t at a time, computing p a, b, I t c |I 0..t−1 c for each t = 0..T iteratively. This allows fast computation on parallel hardware such as graphics processing units (GPUs), without explicit matrix inversion or other costly operations at each kernel.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>c</head><label></label><figDesc>|Θ), p(Θ) can be conditioned on previous observations, and sampled from the current uncertainty es-Algorithm 2 MI estimation / pattern selection for pose estimation1:  for pattern p, in each pixel x do</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>project x 3 , computex 3 = T θi,r (x).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>21: end for 22: Pick pattern p with maximum MI sum over the image. timate for the pose and range.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 4 .</head><label>4</label><figDesc>Left-to-right: a projected Gaussian-smoothed pattern, a captured image, average reconstruction error as a function of the number of patterns used. Dashed lines mark the standard deviation over pattern sequences.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 5 .</head><label>5</label><figDesc>Left-to-right: An indicator image of reflected patterns amplitudes, followed by the mutual information between the image and the range, for random Gaussian-smoothed patterns. The initial patterns are dominated by well-illuminated areas, followed by poorly-illuminated areas (a secondary trend relates to the surface illumination angle).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 6 .</head><label>6</label><figDesc>Left: Mutual information gain under different assumptions on the scene: Blue line -the standard case of large range and albedo uncertainty of σr = 300mm, σa = 3, σ b = 300. Red lineσa = 30, σ b = 3000 (high uncertainty of the appearance). Green line σa = 0.3, σ b =</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 7 .</head><label>7</label><figDesc>Left-to-right: camera image with a projected pattern on the marked object (red overlay marks the mask used for MI integration). The area covered by the mask received significantly more pattern coverage and the reconstruction with these bands is considerably better than random selection. Top: reconstruction with a random set of 69 bands (range RMS=24.1mm) vs. reconstruction with the set of 69 bands selected by a greedy selection (range RMS=18.9mm). Bottom: reconstruction with a random set of 65 blobs (range RMS=59.1mm) -random vs. greedy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 8 .</head><label>8</label><figDesc>Top, Left-to-right: camera image with a projected pattern on the marked object (MI integration mask shown in red), range image of the scene as reconstructed by random selection of 10 patterns, greedy selection of 10 patterns and the full set of 240 patterns, reconstruction squared error as a function of the number of patterns addded, averaged over 20 trials. Bottom: error between partial frame sets reconstruction and the full 240 frames reconstruction, where frames are added at random (green) or using our approach (blue). Greedy selection based on our model improves reconstruction results with significantly fewer frames (50%), as demonstrated by Subfigures (b) and (c).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>demostrates a Table 1. MI gain starting from two random patterns, when using greedy selection, compared to random pattern selection. Resulting MI gains are shown for the hand and mannequin examples from</figDesc><table>Hand 

Mannequin 
Mean MI, 
STDev, Mean MI, 
STDev 
Mean MI, 
STDev, 
Mean MI, 
STDev 
Greedy 
Greedy 
Random 
Random 
Greedy 
Greedy 
Random 
Random 
Step 1 
0.4168 
0.2820 
0.1267 
0.0957 
0.1688 
0.0561 
0.0756 
0.0504 
Step 2 
0.7904 
0.2803 
0.3263 
0.2457 
0.2404 
0.0694 
0.0653 
0.0484 
Step 3 
0.8129 
0.1820 
0.2686 
0.1694 
0.3030 
0.0916 
0.1199 
0.0695 
Step 4 
0.6232 
0.1125 
0.2125 
0.1591 
0.2911 
0.0806 
0.0997 
0.0939 
Step 5 
0.1562 
0.0995 
0.0903 
0.1317 
0.1334 
0.0450 
0.0744 
0.0656 
Step 6 
0.0229 
0.0264 
0.0376 
0.0433 
0.0400 
0.0232 
0.0482 
0.0486 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>The authors thank Christopher Dean for general and helpful discussions. Support for this research has been provided by ONR MURI N00014-09-1-1051, N00014-11-1-0688, and ARO MURI W911NF-11-1-0391. We are grateful for this support.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Dynamic Programming and Optimal Control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Bertsekas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Athena Scientific</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
	<note>2nd edition</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A decision theoretic approach for 3-d vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cooper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="1988-06" />
			<biblScope unit="page" from="964" to="972" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Information theoretic sensor data selection for active object recognition and state estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Denzler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE-TPAMI</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="145" to="157" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Model-driven data acquisition in sensor networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Deshpande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Madden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Hellerstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="588" to="599" />
		</imprint>
		<respStmt>
			<orgName>VLDB Endowment</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Maximum mutual information principle for dynamic sensor query problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ertin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Potter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IPSN</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="558" to="561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Efficient scene simulation for robust Monte Carlo localization using an RGB-D camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Fallon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Johannsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Leonard</surname></persName>
		</author>
		<editor>ICRA</editor>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Value of information analysis in environmental health risk management decisions: Past, present, and future. Risk analysis : an international journal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fumie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Real-time range acquisition by adaptive structured light</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">P</forename><surname>Koninckx</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE-TPAMI</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="432" to="445" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Information value theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Howard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Systems Science and Cybernetics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="22" to="26" />
			<date type="published" when="1966-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Compressive depth map acquisition using a single photon-counting detector: Parametric signal processing meets sparsity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Howell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Howland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirmani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Colaco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">K</forename><surname>Goyal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="96" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Distributed robotic sensor networks: An information-theoretic approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">J</forename><surname>Julian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Angermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schwager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">I. J. Robotic Res</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1134" to="1154" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Parallel tracking and mapping for small AR workspaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Murray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISMAR</title>
		<meeting><address><addrLine>Nara, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Near-optimal sensor placements in Gaussian processes: Theory, efficient algorithms and empirical studies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="235" to="284" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Sensor management using an active sensing approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kreucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kastella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="607" to="624" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A large-scale hierarchical multi-view RGB-D object dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Simultaneous map building and localization for an autonomous mobile robot</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Leonard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">F</forename><surname>Durrant-Whyte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IROS</title>
		<imprint>
			<date type="published" when="1991" />
			<biblScope unit="page" from="1442" to="1447" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Information-rich path planning under general constraints using rapidly-exploring random trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Levine</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
		<respStmt>
			<orgName>MIT, Dept. of Aero.-Astro.</orgName>
		</respStmt>
	</monogr>
	<note>Master&apos;s thesis</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Sensor selection in highdimensional Gaussian trees with nuisances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>How</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Dense depth estimation using adaptive structured light and cooperative algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Biswas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pickering</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Frater</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="21" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Real-time structured light coding for adaptive patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Maurice</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Graebling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doignon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Real-Time Image Processing</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="169" to="178" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Indoor segmentation and support inference from RGBD images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">K</forename><surname>Nathan Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Programmable Imaging: Towards a Flexible Camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Nayar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Branzoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">E</forename><surname>Boult</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. of Computer Vision</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">RGBD-fusion: Real-time high precision depth recovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Or-El</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rosman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wetzler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kimmel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Bruckstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5407" to="5416" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Kutulakos. Homogeneous codes for energy-efficient illumination and imaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>O&amp;apos;toole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Achar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">G</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. on Graphics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="35" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning temporal context in active object recognition using bayesian analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Paletta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Prantl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="695" to="699" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Sparse modeling of shape from structured light</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rosman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dubrovina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kimmel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3DIMPVT</title>
		<meeting><address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="456" to="463" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Real-Time human pose recognition in parts from single depth images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fitzgibbon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sharp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Finocchio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kipman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Steps towards a theory of visual information: Active perception, signal-to-symbol conversion and the interplay between sensing and control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
		<idno>abs/1110.2053</idno>
		<imprint>
			<date type="published" when="2011" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Reinforcement Learning: An Introduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A MRF formulation for coded structured light</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Tardif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3DIM</title>
		<meeting><address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="22" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Robotic mapping: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Exploring Artificial Intelligence in the New Millenium</title>
		<editor>G. Lakemeyer and B. Nebel</editor>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="2002" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Information-seeking control under visibility-based uncertainty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Valente</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><forename type="middle">R</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematical Imaging and Vision</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="339" to="358" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Approximate dynamic programming for communication-constrained sensor network management</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Willsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3995" to="4003" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Performance guarantees for information theoretic active inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Willsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Shading-based shape refinement of RGB-D images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1415" to="1422" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Low-cost compressive sensing for color video and depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Llull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Brady</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Carin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Robust depth sensing with adaptive structured light illumination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Vis. Comm. and Image Representation</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="649" to="658" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Information-driven dynamic sensor collaboration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Reich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="61" to="72" />
			<date type="published" when="2002-03" />
		</imprint>
	</monogr>
	<note>IEEE</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
