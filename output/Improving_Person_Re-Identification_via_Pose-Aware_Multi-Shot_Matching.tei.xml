<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improving Person Re-identification via Pose-aware Multi-shot Matching</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yeong-Jun</forename><surname>Cho</surname></persName>
							<email>yjcho@gist.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Computer Vision Laboratory</orgName>
								<orgName type="institution">GIST</orgName>
								<address>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuk-Jin</forename><surname>Yoon</surname></persName>
							<email>kjyoon@gist.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Computer Vision Laboratory</orgName>
								<orgName type="institution">GIST</orgName>
								<address>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Improving Person Re-identification via Pose-aware Multi-shot Matching</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:41+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Person re-identification is the problem of recognizing people across images or videos from non-overlapping views. Although there has been much progress in person re-identification for the last decade, it still remains a challenging task because of severe appearance changes of a person due to diverse camera viewpoints and person poses. In this paper, we propose a novel framework for person reidentification by analyzing camera viewpoints and person poses, so-called Pose-aware Multi-shot Matching (PaMM), which robustly estimates target poses and efficiently conducts multi-shot matching based on the target pose information. Experimental results using public person reidentification datasets show that the proposed methods are promising for person re-identification under diverse viewpoints and pose variances.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In recent years, a huge number of surveillance cameras have been installed in public places (e.g. offices, stations, airports, and streets) in order to closely monitor the scene and to give early warning of events such as accidents and crimes. However, it requires a lot of human efforts for dealing with large camera networks. In order to reduce the human efforts, an automatic person re-identification, i.e. re-id, task that associates people across images from nonoverlapping cameras has been widely utilized.</p><p>For re-identifying people, most previous works generally rely on people appearances such as color, shape, and texture, since there is no continuity between non-overlapping cameras in terms of time and space. For this reason, many works have been focused on appearance modeling and learning such as feature learning <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b24">25]</ref>, metric learning <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b17">18]</ref>, and saliency learning <ref type="bibr" target="#b23">[24]</ref> for the efficient re-id task. Unfortunately, however, the appearance of a person can change considerably across images depending on camera viewpoints as well as the pose of a person as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>; thus the person re-id task relying only on the appearances is very challenging. Nonetheless, many previous re-id frameworks <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b23">24]</ref> commonly adopt single-shot matching for measuring similarity (or difference) between a pair of person image patches. However, it is still hard to identify people with single-shot appearance matching because of the aforementioned severe appearance changes of people. In order to overcome the limitation of single-shot matching, several multi-shot matching methods <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b18">19]</ref> have been proposed in recent years; however the ambiguity owing to the viewpoint and pose variations is still remained.</p><p>In real world surveillance scenarios, each target (i.e. person) provides multiple observations along with its moving path. Furthermore, surveillance videos contain scene structures as well as scene contexts; a ground plane of the scene, the moving trajectory of a person, etc. In practice, it is possible to estimate the camera viewpoint from the scene information via human height-based auto-calibrations <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b15">16]</ref> and vanishing point-based auto-calibrations <ref type="bibr" target="#b16">[17]</ref>; then the difficulties of person re-id become more tractable.</p><p>In this paper, we propose a novel framework for person re-identification by analyzing camera viewpoints and person poses, so-called Pose-aware Multi-shot Matching (PaMM). We first calibrate camera viewpoints and robustly estimate target poses based on the proposed target pose estimation method. We then generate a multipose model containing four representative features extracted from four image clusters grouped by person poses (i.e. f ront, right, back, left). After generating multi-pose models, we calculate matching scores between multi-pose models in a weighted summation manner based on the pretrained matching weights. With the proposed person reidentification framework, we can exploit additional cues such as person poses and 3D scene information so as to make person re-identification problem more tractable.</p><p>To validate our methods, we extensively evaluate the performance of the proposed methods using public person re-id datasets <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b18">19]</ref>. Experimental results show that the proposed framework is promising for person re-identification under diverse viewpoint and pose variations and outperform other state-of-the-art methods.</p><p>The main ideas of this work are simple but very effective. In addition, our method can flexibly adopt any existing person re-identification methods such as feature learningbased <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b24">25]</ref> and metric learning-based <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b19">20]</ref> methods as the baseline of our re-id framework. To the best of our knowledge, this is the first attempt to exploit viewpoint and pose information for multi-shot person re-identification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Previous Works</head><p>We classify previous person re-identification methods into single-shot matching-based methods and multi-shot matching-based methods and briefly review them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Single-shot matching</head><p>In order to re-identifying people across non-overlapping cameras, most of works generally rely on the appearances of people since there are no spatio-temporal continuity; we cannot fully utilize the motion or spatial information of a target (i.e. person) for person re-identification. For this reason, most of works have focused on appearance-based techniques such as feature and metric learning for the efficient person re-identification.</p><p>For feature learning, M. Farenzena et al. <ref type="bibr" target="#b6">[7]</ref> proposed symmetry-driven accumulation of local features which are extracted based on principles of symmetry and asymmetry of a human body. This method exploits the human body model which is robust to human pose variations. Feature learning methods that select or weight discriminative features have been proposed in <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b24">25]</ref>. These methods enable us to adaptively exploit features depending on the person appearance. Regarding the metric learning, several methods have been proposed such as KISSME <ref type="bibr" target="#b9">[10]</ref>, LMNN-R <ref type="bibr" target="#b5">[6]</ref>, and applied to the re-identification problem. Some works <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b17">18]</ref> extensively evaluated and compared several metric learning methods (e.g. ITML <ref type="bibr" target="#b4">[5]</ref>, KISSME <ref type="bibr" target="#b9">[10]</ref>, LMNN <ref type="bibr" target="#b19">[20]</ref> and Mahalnobis <ref type="bibr" target="#b17">[18]</ref>) and showed the effectiveness of metric learning for re-identification. Similar to the metric learning methods, a saliency learning method was also proposed by R. Zhao et al. <ref type="bibr" target="#b23">[24]</ref> which learns saliency for handling severe appearance changes.</p><p>Recently, deep learning-based person re-identification using a Siamese convolutional network have been proposed <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b21">22]</ref> for simultaneously learning both features and metrics. Also <ref type="bibr" target="#b13">[14]</ref> proposed both feature extraction and metric learning methods for re-identification.</p><p>On the other hand, a few works <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b20">21]</ref> using target pose priors (pose cues) for person re-identification have been proposed very recently. S.Bak et al. <ref type="bibr" target="#b1">[2]</ref> proposed to learn a generic metric pool which consists of metrics, each one learned to match specific pairs of poses. Z.Wu. et al. <ref type="bibr" target="#b20">[21]</ref> proposed person re-identification involving human appearance modeling using pose priors and person-specific feature learning. Although these methods utilized pose priors for person re-identification, they consider single-shot matching that recognizes people by using a single appearance, which has difficulties in handling diverse appearance changes. In this paper, we propose a person re-identification framework using pose cues for efficient multi-shot matching.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Multi-shot matching</head><p>To overcome the limitation of single-shot matchingbased methods, several multi-shot matching-based person re-identification methods have been proposed in recent years. Besides feature learning, Farenzena et al. <ref type="bibr" target="#b6">[7]</ref> also provided multi-shot matching results by comparing each possible pair of histograms between different signatures (a set of appearances) and selecting the obtained lowest distance for the final score of matching. T. Wang et al. <ref type="bibr" target="#b18">[19]</ref> proposed a video ranking method for multi-shot matching which automatically selects discriminative video fragments and learns a video ranking function. Y. Li et al. <ref type="bibr" target="#b12">[13]</ref> also proposed a multi-shot person re-id method based on iterative appearance clustering and subspaces learning for effective multi-shot matching. Even though the multi-shot matching person re-identification methods overcome the limitations of single-shot matching to some extent, the ambiguity owing to the viewpoint and pose changes is still remained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Motivation and Main Ideas</head><p>As shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, person re-identification is quite challenging due to camera viewpoint and target pose variations. However, what if we know the camera viewpoint and the pose priors of targets in every non-overlapping camera in advance? In fact, the progress in auto-calibration techniques <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b15">16]</ref> enable us to extract additional cues such as camera parameters, ground plane, 3D position of the targets without any off-line calibration tasks <ref type="bibr" target="#b22">[23]</ref>. By exploiting those additional cues, we can also estimate target poses as described in Section 4.1. In this paper, we fully exploit those additional cues for multi-shot matching and propose the Pose-aware Multi-shot Matching (PaMM) for person reidentification.</p><p>Suppose that we estimate camera viewpoints and target poses, and there is a simple 2vs2 matching scenario containing one same-pose matching (f ront-f ront) and three different-pose matchings (f ront-right, left-f ront, left-right) as shown in <ref type="figure" target="#fig_2">Fig. 2</ref>. We can expect that the result of the  same-pose matching is generally more reliable than those of different-pose matchings, since targets keep their appearances across cameras when the target poses are the same (In this work, we exclude photometric issues such as illumination changes and camera color response differences). Then, in this multi-shot matching scenario, it is desired that the same-pose matching (f ront-f ront) plays a more important role than different-pose matchings. Hence, in this work, we incorporate this matching idea by aggregating matching scores of all pose matchings in a weighted summation manner as shown in <ref type="figure" target="#fig_2">Fig. 2</ref>, where the thicknesses of lines indicate the matching weights (lines above ③ in <ref type="figure" target="#fig_2">Fig. 2</ref>). We also study how to efficiently match between multi-shot appearances using target pose information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Proposed PaMM Framework</head><p>In the proposed person re-identification framework, we first estimate the camera viewpoint and target poses (Sec. 4.1), and then generate multi-pose models containing four representative features extracted from four image clusters obtained based on the target (i.e. person) poses (i.e. f ront, right, back, left) (Sec. 4.2). After generating multipose models, we calculate matching scores between multipose models in a weighted summation manner based on the pre-trained matching weights (Sec. 4.3). The matching weight training is describe in Sec. 4.4. <ref type="figure" target="#fig_2">Figure 2</ref> illustrates an overall framework for person re-identification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Target pose estimation</head><p>Before estimating target poses, we estimate camera intrinsic and extrinsic parameters (or camera pose) by using auto-calibration algorithms based on the human heights <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b15">16]</ref>. Then, a relation between an image (pixel coordinates) and the real world (world coordinates) is described as</p><formula xml:id="formula_0">  u v 1   = K R t     X Y Z 1     ,<label>(1)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ca</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Target pose Esti atio</head><p>Sec .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Query Gallery</head><p>Multi-pose odel Mat hi g Sec .  where K, R, and t = [X cam , Y cam , Z cam ] ⊤ represent a camera intrinsic matrix, a camera rotation matrix, and a camera position, respectively, In addition, <ref type="bibr">[u, v]</ref> and [X, Y, Z] represent image and world coordinates, respectively. As we know the camera parameters, we can project every object in images onto the ground plane (world XY plane). Object k appearing at frame t in camera C is</p><formula xml:id="formula_1">denoted by O C,k t = (P C,k t , v C,k t , θ C,k t ), where P C,k t = X C,k t , Y C,k t , 1 , v C,k t , θ C,k t</formula><p>are the position, velocity, and target pose angle w.r.t. the camera, respectively.</p><p>Inspired by <ref type="bibr" target="#b20">[21]</ref>, we define the target velocity v C,k t and camera viewpoint vector c C,k t in order to estimate target poses as</p><formula xml:id="formula_2">v C,k t = (X C,k t − X C,k t−1 ), (Y C,k t − Y C,k t−1 ) , (2) c C,k t = (X C cam − X C,k t−1 ), (Y C cam − Y C,k t−1 ) . (3)</formula><p>Assuming that pedestrians mostly walk forward, a target pose angle of the object k can be estimated by (for convenience we omit C from here), <ref type="figure">Figure.</ref> 3 shows the example of estimated target poses. However, initially estimated θ k t is noisy as in <ref type="figure" target="#fig_5">Fig. 4</ref> (a). In order to reduce the noise, we smooth θ k t by using a moving average algorithm in the polar coordinate system aŝ</p><formula xml:id="formula_3">θ k t = arccos c k t T · v k t c k t v k t .<label>(4)</label></formula><formula xml:id="formula_4">θ k t = arctan t+m i=t−m sin θ k i t+m i=t−m cos θ k i ,<label>(5)</label></formula><p>where m is a moving average parameter (we set m = 10).</p><p>Although there are several discontinuities around 0 • and 360 • , the smoothing result is reliable thanks to the smoothing process in polar coordinates, whereas the smoothing result in Cartesian coordinates is not reliable ( <ref type="figure" target="#fig_5">Fig. 4 (b)</ref>,(c)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Multi-pose model generation 4.2.1 Sample selection based on sample confidence</head><p>For generating good multi-pose models, we ought to filter out the unreliable target samples having incorrect angles or polluted appearances along the moving trajectory. To this end, we define sample confidence to measure the reliability of target samples based on following requirements (R1-R3):</p><p>• Variation of angle (R1): We assume that the angle of walking person does not change abruptly between temporally neighboring frames. If there are rapid changes in angle across consecutive frames, we regard them as unreliable samples and filter them out. We observe that, inaccurate localization of a person generally causes large variation in angle. In order to consider it, we measure the angle variation as</p><formula xml:id="formula_5">δ k t = min d θ k t , d θ k t − 360 ,<label>(6)</label></formula><p>where d θk</p><formula xml:id="formula_6">t = θk t−1 −θ k t .</formula><p>Even though there is an angle discontinuity between 0 • and 360 • , δ k t is reliably calculated thanks to the second term of min function.</p><p>• Magnitude of target velocity (R2): When a target is stationary for several frames, the target velocity v k t is close to 0 and the estimated target angle based on Eq. (4) becomes unreliable 1 . To handle the problem, we measure the magnitude of the target velocity as M k t = v k t 2 . A sample with the small velocity magnitude is regarded unreliable.</p><p>• Occlusion rate (R3): A target occluded by others is also not a reliable target sample since the appearance of target <ref type="bibr" target="#b0">1</ref> To estimate target viewpoint angles, we assume that targets mostly move forward in Sec 4.1. However, in the case of stationary targets, the assumption is not satisfied. Note that the stationary targets are likely to have pure rotational motion which cannot be handled by Eq <ref type="bibr" target="#b3">(4)</ref>. is polluted. To deal with the occluded target samples, we measure the occlusion rate of each target as</p><formula xml:id="formula_7">Occ k t = max h∈H k area(B k t ∩ B h t ) area(B k t ) ,<label>(7)</label></formula><p>where B k t is a 2D bounding box of an object k at frame t, B h t is a 2D bounding box occluding B k t , H k is a set of object indexes occluding object k. As we know the 3D position of each target P k t , it is easy to find H k . Based on the above requirements, we define the sample confidence as</p><formula xml:id="formula_8">conf O k t = e −αδ k t ·tanh M k t · 1 − Occ k t ,<label>(8)</label></formula><p>where α is a scale parameter (we set α = 0.01). The sample confidence lies in [0,1]. <ref type="figure" target="#fig_6">Figure 5</ref> shows the sample confidences under various situations. We regard a target sample as a reliable one with high sample confidence when conf O k t &gt; 0.8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Generating multi-pose model</head><p>After the sample selection, we divide target samples into four groups g k p p∈{f,r,b,l} according to their pose angles (i.e. f ront, right, back, left). Each group covers 90 • . It is worthy to note that the proposed sample confidence efficiently filters unreliable samples out as shown in <ref type="figure" target="#fig_7">Fig. 6</ref>. After the clustering, we extract features from four groups and the multi-pose model of object k is defined as where f (·) is a function which extracts features from a set of images. Details of feature extraction is described below.</p><formula xml:id="formula_9">M k = f g k p p∈{f,r,b,l} ,<label>(9)</label></formula><p>Feature extracion: We extract dColorSIFT which is a dense feature descriptor containing dense LAB-color histogram and dense SIFT as in <ref type="bibr" target="#b23">[24]</ref>. The authors pointed out that the densely sampled local features have been widely applied to matching problems due to their robustness in matching. In our feature extraction process, each person image is normalized to 128×48 pixels and we extract dColor-SIFT <ref type="bibr" target="#b23">[24]</ref> descriptors from all images in each group. Then each groups g k p p∈{f,r,b,l} has multiple feature descriptors, respectively. We then select a median dColorSIFT descriptor as the representative descriptor of each group. The selection of the median feature descriptor is reliable since it reflects the characteristics of each group robustly to outliers and furthermore it keeps details. Finally, the multi-pose model M k of each person contains multiple representative dColorSIFT descriptors extracted from multiple groups. Our method can apply any kind of feature descriptors and feature extraction methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Multi-pose model matching</head><p>In this section, we describe the matching process of multi-pose models. Suppose that we have M k , M l which are the multi-pose models of object k and l appeared in different cameras, respectively. In order to measure the similarity between two multi-pose models, we first calculate all pairwise feature distances of multi-pose models as x pq = dist f g k p , f g l q , where p, q ∈ {f, r, b, l} and dist (·) is a distance function. For the distance function, we can use any metrics such as KISSME <ref type="bibr" target="#b9">[10]</ref>, ITML <ref type="bibr" target="#b4">[5]</ref> and LMNN <ref type="bibr" target="#b19">[20]</ref>. Then, the multi-pose model matching cost is computed in a weighted summation manner as S M k , M l = p,q w pq e pq x pq p,q w pq e pq , p, q ∈ {f, r, b, l} , (10)</p><formula xml:id="formula_10">e pq = 1 if (p, q) pair exists 0 otherwise ,<label>(11)</label></formula><p>where w pq is a matching weight that attaches importance to pairwise matching x pq . Training matching weights is described in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Training matching weights</head><p>For training matching weights, we assume that every (p, q) pair exists and omit the normalization term p,q w pq e pq . Then Eq. (10) is rewritten as</p><formula xml:id="formula_11">S (x) = w T x, x = {x f f , x f r , . . . , x ll } T , w = {w f f , w f r , . . . , w ll } T ,<label>(12)</label></formula><p>where x ∈ R 16×1 is a vector of pairwise feature distances and w ∈ R 16×1 is a vector of matching weights. In order to train matching weights w, we collect training sam-</p><formula xml:id="formula_12">ples D = (x i , y i )|x i ∈ R 16×1 , y i ∈ {−1, 1} N i=1 ,</formula><p>where N is the number of training samples and y i is a corresponding class of the sample. Given training set D, we exploit Support Vector Machine (SVM) <ref type="bibr" target="#b3">[4]</ref> to find the weights w by solving following optimization problem: arg min w,ξ</p><formula xml:id="formula_13">1 2 w 2 + λ N i ξ i , s.t. y i w T x i ≥ 1 − ξ i , ξ i ≥ 0, for 1 ≤i ≤ N,<label>(13)</label></formula><p>where λ is a margin trade-off parameter and ξ i is a slack variable. The solution given by SVM assures maximal margin. Details and the results of matching weight training are given in Sec 6.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Datasets and Methodology</head><p>Datasets.</p><p>For training matching weights, we use CUHK02 <ref type="bibr" target="#b11">[12]</ref> and VIPeR <ref type="bibr" target="#b7">[8]</ref>. For test methods, we use iLIDS-Vid <ref type="bibr" target="#b18">[19]</ref>, PRID 2011 <ref type="bibr" target="#b8">[9]</ref> and 3DPeS <ref type="bibr" target="#b2">[3]</ref>.</p><p>• CUHK02 <ref type="bibr" target="#b11">[12]</ref> contains 1,816 persons from five different outdoor camera pairs. Five camera pairs have 971, 306, 107, 193 and 239 persons with the size of 160×60 pixels, respectively. Each person has two images per camera which were taken in different times. Most of people are with burdens (e.g. backpack , handbag, strap bag, or baggage). For our experiments, we manually extract all pose angles of each person in four directions (i.e. f ront, right, back, left) since CUHK02 does not provide the pose angles. This dataset is used for training multi-shot weights w.</p><p>• VIPeR <ref type="bibr" target="#b7">[8]</ref> includes 632 persons and two outdoor cameras under different viewpoints and light conditions. Each person has one image per camera and each image has been scaled to be 128×48 pixels. It provides the pose angle of each person as 0 • (f ront), 45 • , 90 • (right), 135 • , 180 • (back). We use both CUHK02 and VIPeR datasets for training multi-shot matching weight w.</p><p>• iLIDS-Vid <ref type="bibr" target="#b18">[19]</ref> has been created from the pedestrians in 2 non-overlapping cameras, monitoring an airport arrival hall. It provides multiple cropped images for each 300 distinct individual and is very challenging due to clothing sim-(a) camera layout of video sets (b) sample frames of each camera (d-h) <ref type="figure">Figure 7</ref>. Test dataset: 3DPeS <ref type="bibr" target="#b2">[3]</ref> ilarities, lighting and viewpoint variations, cluttered background and severe occlusions.</p><p>• PRID 2011 <ref type="bibr" target="#b8">[9]</ref> provides multiple person trajectories recored from 2 different static surveillance cameras, monitoring crosswalk and sidewalk. In the dataset, 200 persons appear in both views.</p><p>Since the datasets (iLIDS <ref type="bibr" target="#b18">[19]</ref>, PRID [9]) do not provide full surveillance video sequences but provide only cropped images, we could not automatically estimate camera viewpoints and poses of targets. Therefore, in order to evaluate our method with the datasets, we annotated the pose of each target manually. • 3DPeS <ref type="bibr" target="#b2">[3]</ref> has been collected by 8 non-overlapped outdoor cameras, monitoring different sections of the campus. Differently from other re-id datasets (iLIDS, PRID), it provides full surveillance video sequences: providing 6 sets of video pairs, uncompressed images with a resolution of 704x576 pixels at 15 frame rate, containing hundreds of people and calibration parameters. However, this dataset provides ground-truth person identity only for selected snapshots (i.e. no ground-truth for video sequences). For our experiments, we used 3 sets of video pairs (Set3, <ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5)</ref> and manually extracted ground truth labels (identities, center points, widths, heights) of video Set3,4,5. The pose of each target was estimated as described in Sec. 4.1. The camera layout and sample frames are given in <ref type="figure">Fig. 7</ref>. Three video pairs contain 39, 24 and 36 identities, respectively. Evaluation methodology.</p><p>To compare person reidentification methods, we follow the evaluation steps described in <ref type="bibr" target="#b6">[7]</ref>. First, we randomly split person-identities in video pairs into two sets with the equal number of identities, one set for training and the other set for testing. We learn several metrics such as LMNN <ref type="bibr" target="#b19">[20]</ref>, ITML <ref type="bibr" target="#b4">[5]</ref>, KISSME <ref type="bibr" target="#b9">[10]</ref>, and Mahal <ref type="bibr" target="#b17">[18]</ref> for the baseline distance functions of our person re-identification framework. After training distance metrics, we calculate all possible matches between testing video pairs. We repeat the evaluation steps over 10 times. We plot the Cumulative Match Curve (CMC) <ref type="bibr" target="#b7">[8]</ref> representing true match being found within the first n ranks for comparing performances of methods. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Training multi-shot matching weights</head><p>In practice, we need to consider only 10 weights rather than 16 weights due to the weight symmetry: we let w pq = w qp , where p = q. Consequentially, we learn four samepose matching weights (w f f , w rr , w bb , w ll ) and six different -pose matching weights (w f r , w f b , w rb , w rl , w bl , w f l ).</p><p>As mentiond in Sec. 5, for training the weights w ∈ R 10×1 , we use two datasets, CUHK02 <ref type="bibr" target="#b11">[12]</ref> and VIPeR <ref type="bibr" target="#b7">[8]</ref>. By using the datasets, we generate 3,520 positive image pairs and 35,200 negative image pairs that cover diverse pose combinations as shown in <ref type="figure" target="#fig_8">Fig. 8</ref>. Here, a positive image pair is a pair of images of the same person and a negative image pair is a pair of images of the different persons regardless of the poses of persons. We then extract pairwise feature distances {x f f , x rr , . . . , x f l } for all images pairs by following metric learning steps described in Sec. 5. Distributions of feature distances 2 {x pq } are plotted in <ref type="figure">Fig. 9</ref>. For example, <ref type="figure">Fig. 9</ref> (a) shows the feature distance distribution of f ront image pairs of the same person (positive) and difference persons (negative). Note that, a large statistical distance between positive and negative distributions implies the high discriminating power. We observe that the same-pose matchings ( <ref type="figure">Fig. 9 (a,b,f,g)</ref>) are more discriminative than different-pose matchings <ref type="figure">(Fig. 9 (c-e,h-j)</ref>).</p><p>After obtaining distributions of feature distances, we generate training samples (x i , y i ), where x i ∈ R 10×1 , y i ∈ {1, −1} by randomly selecting each x pq from each distribution. <ref type="figure" target="#fig_0">Figure 10</ref> shows the result of weight training using the KISSME [10] distance metric. The result represents that the weights of the same-pose matchings (f f, rr, bb, ll) are generally larger than those of the different-pose matchings (f r, f b, rb, rl, bl, f l). For consecutive experiments, we train each matching weight for each metric learning method, individually. The training results do not depend on the metric learning methods and show similar tendencies.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Performance enhancements via PaMM</head><p>In experiments, we denote our method 'Pose-aware Multi-shot Matching' as PaMM. As a baseline method for PaMM, we can use any conventional single-shot matchingbased methods (e.g. feature learning methods, metric learning methods) -any single-shot matching-based method can be used for computing pairwise feature distance in our framework. In this work, we choose several single-shot person re-identifications 3 based on various metric learnings such as LMNN <ref type="bibr" target="#b19">[20]</ref>, ITML <ref type="bibr" target="#b4">[5]</ref>, KISSME <ref type="bibr" target="#b9">[10]</ref>, and Mahalanobis <ref type="bibr" target="#b17">[18]</ref> for the baselines of PaMM.</p><p>For validating the performance enhancement via PaMM, we compare the person re-identifications with and without PaMM using the dataset 3DPeS-Set3. To evaluate each performance, we follow evaluation steps explained in Sec. 5. As shown in <ref type="table">Table.</ref> 1, all baselines <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b19">20]</ref> are improved considerably for all ranks (r=1,3,5,10) thanks to the proposed PaMM. Especially, the performance enhancement at r=1 is remarkable (achieving 2.7%∼47.4% enhancement). The results imply that proposed PaMM can improve any kind of single-shot matching-based person re-identification. Compared to conventional singleshot matching-based methods, PaMM exploits a plenty of appearances selected based on the sample confidence and matches multiple appearances efficiently using pose cues. In the consecutive experiments, we use KISSME <ref type="bibr" target="#b9">[10]</ref> as the baseline for PaMM. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Performance comparisons with other methods</head><p>For performance comparisons with other methods, we first evaluate methods using 3DPeS dataset providing full surveillance videos. In this section we denote different versions of our person re-identification framework as follows:</p><p>• PaMM-nss: PaMM without sample selection.</p><p>• PaMM-nw: PaMM without weighted multi-pose matching (we use uniform weights w = 1 for PaMM-nw).</p><p>• PaMM: PaMM with all proposed methods.</p><p>We also implemented other multi-shot matching methods for comparison as follows:</p><p>• Full Match-avg: matching all possible pairs between multiple appearances and averaging all matching scores.</p><p>• Full Match-min: matching all possible pairs between multiple appearances and selecting the smallest matching score for the final score as used in <ref type="bibr" target="#b6">[7]</ref>.</p><p>Table. 2 represents that our methods outperform other multi-shot matchings (Full Match-avg, Full Match-min) and all single-shot matching methods. Even though the multi-shot matchings 'Full Match-avg' and 'Full Matchmin' exploit all multiple appearances of targets, the performances of both methods are lower than PaMM. It supports that the proposed PaMM reasonably extract representative features among multiple appearances (Sec. 4.2) and <ref type="table">Table 2</ref>. Performance comparison. † denotes a multi-shot matching method. The best and second best scores in each rank are marked with red and blue. For VaMM-nW, we use uniform weights w = 1. We use the same feature descriptor (dColorSIFT) for all methods. Full Match and PaMM use KISSME <ref type="bibr" target="#b9">[10]</ref> for their metrics. AUC is an area under curve of CMC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3DPeS -Set 3</head><p>3DPeS -Set 4 3DPeS -Set 5 3DPeS -Set All Modality \ Rank r = 1 r = 3 r = 5 AUC r = 1 r = 3 r = 5 AUC r = 1 r = 3 r = 5 AUC r=1 r=5 r=10 r=15 AUC L2 <ref type="table">(</ref>  <ref type="table">Table 3</ref>. Performance comparison with other methods. † denotes a multi-shot matching and aR an average rank. We implemented the multi-shot version of L+XQDA by following <ref type="bibr" target="#b6">[7]</ref>.</p><p>iLIDS-Vid PRID 2011 3DPeS -Set All Modality \ Rank r=1 r=5 r=10 r=20 aR r=1 r=5 r=10 r=20 aR r=1 r=5 r=10 r=15 aR (S1) SDALF-SS <ref type="bibr" target="#b6">[7]</ref> 5 Even though the test datasets 3DPeS-Set3,4,5 contain people having various appearances and poses, they contain a few number of identities (Set3:39, Set4:24, Set5:36). When the number of identities is small, the re-identification task becomes much easier because of the small pool of comparison targets. To show the person re-identification performance under more large scale camera networks, we concatenate all datasets and generate 3DPeS-Set All containing 99 identity pairs. It is reasonable since each dataset (3DPeS-Set3,4,5) does not share identities with each other. <ref type="table">Table.</ref> 2 shows the comparison results with dataset 3DPeS-Set All and represents our methods still show promising performance compared to others.</p><p>We also provide evaluation results and comparisons with other state-of-the art multi-shot matching methods such as SDALF <ref type="bibr" target="#b6">[7]</ref>, Salience+DVR <ref type="bibr" target="#b18">[19]</ref>, and L+XQDA <ref type="bibr" target="#b13">[14]</ref> with more public datasets (iLIDS <ref type="bibr" target="#b18">[19]</ref>, PRID [9]) in <ref type="table">Table.</ref> 3. In <ref type="table">Table 3</ref>, S1, S2, S3, and S4 (single-shot re-id methods) are used as the baselines of M1, M2, M3, and M4 (multishot re-id methods), respectively. Overall, PaMM shows the best performance while significantly enhancing its baseline performance (19%∼32.7% enhancement at r=1).</p><p>Although L+XQDA-MS <ref type="bibr" target="#b13">[14]</ref> shows better performance for PRID, it is mainly because the superior performance of its baseline (S3). Actually, the performance enhancement of M3 from S3 is less (3.7%∼17.5% at r=1) than ours -for iLIDS which is much more challenging than PRID, the improvement of ours and <ref type="bibr" target="#b13">[14]</ref> are 19% and 3.7%, respectively. PaMM shows promising performance regardless of datasets. It should be also noted that PaMM can achieve better performance by adopting better baseline methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusions</head><p>In this paper, we proposed a novel framework for person re-identification, so called Pose-aware Multi-shot Matching (PaMM) that robustly estimates target poses and efficiently conducts multi-shot matching based on the target pose information. We extensively evaluated and compared the performance of the proposed method using public person reidentification datasets. The results showed that proposed methods are promising for person re-identification under diverse target pose variances. The proposed methods can flexibly adopt any existing person re-identification methods for computing pairwise feature distance in our framework.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Challenging in person re-identification due to person appearance changes. Person appearance changes depending on the camera viewpoint and the pose of a person.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Proposed multi-shot matching framework for person re-identification.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 .</head><label>3</label><figDesc>Target pose estimation: (left) estimated 3D structure and target poses along the path , (right) corresponding 2D images and appearances grouped by poses.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>(left) initial pose angle, (middle) smoothing result in Cartesian coordinates, (right) smoothing result in polar coordinates.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 .</head><label>5</label><figDesc>Sample confidence under various conditions (best viewed in color and high resolution image).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 .</head><label>6</label><figDesc>Clustering results according to target angels without and with sample selection. The clusters with sample selection repersent more clear directivity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 .</head><label>8</label><figDesc>Examples of training sample pairs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 9 .Figure 10 .</head><label>910</label><figDesc>Distributions of pairwise feature distances {x pq } extracted from training data. Trained weights (distance metric: KISSME<ref type="bibr" target="#b9">[10]</ref>)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>multi-pose models (Sec. 4.3).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><head>Table 1 .</head><label>1</label><figDesc>Performance enhancement over single-shot matching-based methods via PaMM. † denotes a multi-shot matching method.</figDesc><table>3DPeS -Set 3 
Modality \ Rank 
r = 1 r = 3 r = 5 r = 10 
L2 (Euclidean) 
31.5 
52.6 
63.1 
86.8 
L2-PaMM  † 
34.2 
57.9 
73.7 
89.5 
LMNN [20] 
26.3 
52.6 
68.4 
89.4 
LMNN-PaMM  † 
52.6 
79.0 
86.8 
94.7 
ITML [5] 
31.5 
57.8 
76.3 
89.4 
ITML-PaMM  † 
39.5 
68.4 
81.6 
92.1 
KISSME [10] 
21.0 
50.0 
63.1 
84.2 
KISSME-PaMM  † 
68.4 
89.5 
94.7 
100 
Mahal [18] 
31.5 
63.1 
73.6 
89.4 
Mahal-PaMM  † 
60.5 
78.6 
89.5 
97.4 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>Full Match-avg. † 42.1 63.2 73.7 85.0 50.0 62.5 75.0 79.5 33.3 55.7 69.4 79.6 23.5 49.0 61.2 70.4 78.1 Full Match-min † 47.4 78.9 89.4 92.1 45.8 75.0 91.7 85.4 44.4 66.7 83.3 88.6 35.7 73.5 83.7 89.8 91.0 PaMM-nss † (ours) 57.9 89.5 94.7 95.3 58.3 75.0 83.3 87.2 55.6 77.8 83.3 89.5 52.0 79.6 83.7 91.8 92.7 PaMM-nw † (ours) 68.4 89.5 94.7 95.6 58.3 83.3 83.3 88.9 55.6 77.8 83.3 87.4 56.1 78.6 89.8 91.8 92.4 PaMM † (ours)</figDesc><table>Euclidean) 
31.5 52.6 63.1 79.4 33.3 41.6 54.1 64.6 11.1 27.7 44.4 61.6 19.3 30.6 38.7 50.0 66.8 
LMNN [20] 
26.3 52.6 68.4 80.8 33.3 58.3 66.6 74.3 22.2 47.2 72.2 79.2 24.4 48.7 65.3 74.4 80.8 
ITML [5] 
31.5 57.8 76.3 82.7 41.6 66.7 70.8 76.0 33.3 63.8 72.2 80.9 23.4 52.0 71.4 77.5 82.5 
KISSME [10] 
21.0 50.0 63.1 78.8 25.0 58.3 66.6 71.9 22.2 47.2 63.8 76.5 26.5 52.0 66.3 79.5 82.4 
Mahal [18] 
31.5 63.1 73.6 83.5 33.3 58.3 66.6 73.3 25.0 47.2 69.4 78.4 28.5 50.0 67.3 78.5 82.8 
68.4 89.5 94.7 96.4 58.3 83.3 91.7 88.9 55.6 77.8 83.3 88.6 59.2 82.7 89.8 94.9 94.1 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Unfortunately, we could not make (r, l) pairs using training datasets CUHK02, VIPeR since they do not have such pairs. In order to make the distribution of x rl , we regard x rl follows the similar distribution with x f b .</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">As the appearance of each identity for the single-shot matching-based methods, we randomly select a single appearance for each identity. For unbiased selections, we repeat the appearance selection over 10 times and calculate the average performance for the final result.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">An improved deep learning architecture for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">K</forename><surname>Marks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Differences</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">25</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Person re-identification by pose priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bremond</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IS&amp;T/SPIE Electronic Imaging, pages 93990H-93990H. International Society for Optics and Photonics</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">3dpes: 3d people dataset for surveillance and forensics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Baltieri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vezzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cucchiara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MA3HO</title>
		<meeting><address><addrLine>Scottsdale, Arizona, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Support-vector networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="273" to="297" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Information-theoretic metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">V</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="209" to="216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Pedestrian recognition with a learned metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dikmen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Akbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="501" to="512" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Person re-identification by symmetry-driven accumulation of local features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Farenzena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bazzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Perina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Murino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cristani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="2360" to="2367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Evaluating appearance models for recognition, reacquisition, and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Brennan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PETS</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2007" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Person re-identification by descriptive and discriminative classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hirzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Beleznai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SCIA</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Large scale metric learning from equivalence constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Koestinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hirzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="2288" to="2295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A direct method to self-calibrate a surveillance camera by observing a walking pedestrian</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kusakunniran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">DICTA</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Locally aligned feature transforms across views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Multishot human re-identification using adaptive fisher discriminant analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karanam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Radke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Troy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMVC</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Person re-identification by local maximal occurrence representation and metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Person reidentification: What features are important? In ECCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="391" to="401" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Camera calibration from video of a walking human</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Camera calibration using two or three vanishing points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Orghidan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Salvi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Orza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FedCSIS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Mahalanobis distance learning for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hirzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Köstinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Beleznai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Person Re-Identification</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="247" to="267" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Person reidentification by video ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="688" to="703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Distance metric learning for large margin nearest neighbor classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1473" to="1480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Viewpoint invariant human re-identification in camera networks using pose priors and subject-discriminative features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Radke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep metric learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="34" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Flexible camera calibration by viewing a plane from unknown orientations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="666" to="673" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Unsupervised salience learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="3586" to="3593" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning mid-level filters for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="144" to="151" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
