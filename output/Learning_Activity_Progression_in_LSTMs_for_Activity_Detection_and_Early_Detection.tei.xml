<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Activity Progression in LSTMs for Activity Detection and Early Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shugao</forename><surname>Ma</surname></persName>
							<email>shugaoma@bu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Boston University</orgName>
								<orgName type="institution" key="instit2">Boston University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Sigal</surname></persName>
							<email>lsigal@disneyresearch.com</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Boston University</orgName>
								<orgName type="institution" key="instit2">Boston University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Disney</forename><surname>Research</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Boston University</orgName>
								<orgName type="institution" key="instit2">Boston University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Sclaroff</surname></persName>
							<email>sclaroff@bu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Boston University</orgName>
								<orgName type="institution" key="instit2">Boston University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Activity Progression in LSTMs for Activity Detection and Early Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:42+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this work we improve training of temporal deep models to better learn activity progression for activity detection and early detection tasks. Conventionally, when training a Recurrent Neural Network, specifically a Long Short Term Memory (LSTM) model, the training loss only considers classification error. However, we argue that the detection score of the correct activity category, or the detection score margin between the correct and incorrect categories, should be monotonically non-decreasing as the model observes more of the activity. We design novel ranking losses that directly penalize the model on violation of such monotonicities, which are used together with classification loss in training of LSTM models. Evaluation on ActivityNet shows significant benefits of the proposed ranking losses in both activity detection and early detection tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In this work we study human activity detection and early detection in videos <ref type="figure">(Fig. 1)</ref>. For activity detection, we detect segments of human activities in a video sequence, recognizing the activities' categories and detecting their start and end points. For early detection, we detect the activity segment after observing only a fraction of the activity.</p><p>Automatic detection of human activities, in videos, has many potential applications, such as video understanding and retrieval, automatic video surveillance, and humancomputer interaction. Further, for many applications, such as human-robot interaction it is desirable to detect the activity as early as possible <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b18">19]</ref>, to make the interaction more natural, e.g., deploying a robot to help an elderly patient stand up before he/she is upright and is at risk of a fall.</p><p>Activity detection in realistic settings is quite challenging. There is high variability in the viewpoint from which the activity is observed, the actors and their appearance, as well as the execution and overall duration of the activities (see <ref type="figure" target="#fig_2">Fig. 6</ref>). This is particularly true for relatively long <ref type="figure">Figure 1</ref>: We study two problems: activity detection and early detection. For activity detection, we detect the category of the activity and its start and end point. For early detection, we need to detect the category and the start point of an activity after observing only a fraction of the activity. This example sequence contains the activity using ATM. and complex activities. For example, the activity "making pasta" typically entails cutting vegetables, setting a pot on the fire, boiling water, boiling pasta noodles, cooking pasta sauce, and combining pasta with sauce. To better detect, i.e., recognize and temporally localize such activities, we argue that it is critically important for the learned detector to model the activities' temporal progression.</p><p>Recurrent Neural Network (RNN) models are particularly helpful in this context: the prediction at each time instant is based not only on the observations at that time instant, but also on the previous model hidden states that provide temporal context for the progression of the activity. More specifically, in the Long Short Term Memory (LSTM), a type of RNN, memory is used to capture useful patterns of previous observations, and is used in addition to the previous hidden states to provide longer-range context (e.g., as compared to HMMs) for the current prediction.</p><p>While RNN models are powerful, using only classification loss in training such models typically fails to properly penalize incorrect predictions, i.e., the prediction error is penalized the same no matter how much context the model has already processed. For example, given a video of the activity making pasta, to output the activity class label preparing coffee after the detector sees the activity up to combining pasta with sauce should be penalized more than the same error when the detector only sees activity up to boiling water.</p><p>The above mentioned defect in training RNN models is especially critical for activity detection. Unlike conventional applications of RNNs in machine translation and speech recognition, in which specific output such as words or phonemes continue for a relatively short time, human activities such as making pasta may continue for a relatively long period, e.g., several minutes or thousands of video frames. It is thus very important for the model to learn the progresssion patterns of the activities in training.</p><p>In this work, we introduce novel ranking losses within the RNN learning objective so that the trained model better captures progression of activities. These ranking losses are computed for the prediction at each time point, while also taking into consideration the past predictions starting from the very beginning of the activity.</p><p>The intuition for our formulation is shown in <ref type="figure">Fig. 2</ref>. As the detector sees more of an activity, it should: (1) become more confident of the correct activity category, i.e., output a higher detection score for the correct category as the action progresses, and (2) become more confident of the absence of incorrect categories, i.e., the detection score margin between the correct and incorrect categories should be nondecreasing as the action progresses.</p><p>Thus, we introduce two explicit constraints in RNN training. The first is a ranking loss on the detection score of the correct category, which constrains the detection score of the correct categorty to be monotonically non-decreasing as the activity progress. The second is a ranking loss on the detection score margin between the correct activity category and all other categories, which constrains that this discriminative margin is monotonically non-decreasing.</p><p>In summary, we make the following contributions:</p><p>• We propose formulations for ranking loss on the detection score and on the discriminative margin to better learn models for human activity progression.</p><p>• We implement our proposed ranking losses in training LSTM models, and show significant improvements over LSTM model trained only with classification loss in the tasks of activity detection and early detection.</p><p>• We achieve start-of-the-art performance for activity detection and early detection on a large-scale video dataset: ActivityNet <ref type="bibr" target="#b3">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>A topic closely related to human activity detection is human action recognition. In action recognition, the input video clip is (manually) trimmed so that it only contains <ref type="figure">Figure 2</ref>: As the detector sees more of the activity, it should become more confident of the presence of the correct category and absence of incorrect categories. This example sequence contains a high jump. The blue curve is the detection score of the correct category, which is encouraged to be non-decreasing. The green curve is the detection score of an incorrect category running, whose margin with respect to the correct category (shaded light blue area) is encouraged to be non-decreasing.</p><p>video frames depicting a human action, and the goal is to correctly recognize the action category. Many past works focus on this topic, e.g., encoding video clips using a bagof-words representation over local space-time features and training SVM classifiers <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b22">23]</ref>, or modeling human actions as space-time structures <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25]</ref>. In <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref>, Convolutional Neural Networks (CNNs) with spacetime convolutional filters are trained to capture space-time patterns from training videos. In <ref type="bibr" target="#b19">[20]</ref>, separate CNNs are trained for a spatial stream (i.e., video frames) and motion stream (i.e., optical flow fields) and the features from both CNNs are concatenated to train an action classifier. LSTM models are explored in <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b25">26]</ref> for recognizing human actions. In contrast to these works, we not only recognize activities but also detect their start and end time points.</p><p>Human action detection is also a well studied problem. In <ref type="bibr" target="#b7">[8]</ref>, simple actions are represented as space-time shapes that are matched against over-segmented space-time video volumes. In <ref type="bibr" target="#b27">[28]</ref>, action detection entailed searching for 3D subvolumes of space-time invariant points. In <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b21">22]</ref>, human actions are modeled as space-time structures, using deformable part models <ref type="bibr" target="#b1">[2]</ref>. In <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b17">18]</ref> discriminative handcentric features are explored for fine grained activity detection in cooking, i.e., relatively short sub-activities such as chop and fill. In <ref type="bibr" target="#b2">[3]</ref>, the detector is trained on CNN features extracted from the action tubes in space-time; however, evaluation is on relatively short video clips (i.e., several hundred frames) of relatively short actions. In <ref type="bibr" target="#b26">[27]</ref> an LSTM is trained that takes CNN features of multiple neighboring frames as input to detect actions at every frame; while their model is similar to ours, they focus on detecting simple actions such as stand up that last only for a few video frames, and the training loss accounts only for classification errors. In this work, we focus on accurately localizing activities that are long and complex by learning and enforcing activity progression as part of LSTM learning objective.</p><p>Early recognition of human action or activities, i.e., recognizing human actions or activities given partial observations, has also been studied in previous works e.g., by using dynamic bag-of-words of space-time features <ref type="bibr" target="#b18">[19]</ref>, by modeling actions as a sparse sequence of key-frames <ref type="bibr" target="#b16">[17]</ref>, or by using compositional kernels to hierarchically capture relationships between partial observations <ref type="bibr" target="#b8">[9]</ref>. In <ref type="bibr" target="#b4">[5]</ref> a structured output SVM is used for recognizing and also temporally localizing events given partial observations. Compared to <ref type="bibr" target="#b4">[5]</ref>, which is evaluated on lab collected videos of simple human actions, we use deep learning techniques to solve this problem on large scale realistic video dataset of human activities which are often long and complex. <ref type="figure" target="#fig_0">Fig. 3</ref> illustrates our model for activity detection. It contains two major components: a CNN that computes visual features from each video frame, and an LSTM with a linear layer that computes activity detection scores based on the CNN features of the current frame and the hidden states and memory of the LSTM from the previous time step. We adopt the VGG19 <ref type="bibr" target="#b20">[21]</ref> CNN architecture, whose output of the second fully connected layer (fc7) is fed into the LSTM. We use the LSTM described in <ref type="bibr" target="#b15">[16]</ref> that applies dropout on non-recurrent connections. A similar model has been used in <ref type="bibr" target="#b26">[27]</ref> for detecting relatively short actions. Our key contributions are in exploring the rank losses, during training, that encourage monotinicity in detection score and margin produced by the model as a training activity progresses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Model Overview</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Learning Activity Progression</head><p>To accurately detect the complete duration of human activities, especially for relatively long and complex ones, it is important for the model to capture the progression patterns of activities during training. An RNN only implicitly considers progression via the context that is passed along time in the form of the previous hidden state and, in LSTM, memory as well. We introduce ranking loss into the learning objective, to explicitly capture activity progression globally from the activity start to the current time:</p><formula xml:id="formula_0">L t = L t c + λ r L t r ,<label>(1)</label></formula><p>where L t c and L t r are the classification loss and the ranking loss, respectively, at the current time t, and λ r is a positive scalar constant that controls relative term contribution.</p><p>Usually, for training deep models, the cross entropy loss is used to formulate L t c :</p><formula xml:id="formula_1">L t c = − log p yt t ,<label>(2)</label></formula><p>where y t is the ground truth activity category of the training video sequence at the t-th video frame, and p yt t is the detection score of the ground truth label y t for the t-th frame, i.e., the softmax output of the model.</p><p>We explore two formulations of the ranking loss, L t r . The first constrains the model to output a non-decreasing detection score for the correct category throughout the duration of the activity. Our second ranking loss constrains the output of the model to have non-decreasing discriminative margin: at any point in the activity, the margin between the detection score of the correct category and the maximum detection score among all other categories should be non-decreasing. Detailed formulations of these two ranking losses are given below. For easier reading, we use L t s and L t m to denote ranking loss on the detection score and margin, respectively. While these two ranking losses are different, they are related. Note that the output of softmax layer of the LSTM sums to 1, so L t s considers the margin between p yt t and y ′ =yt p y ′ t , whereas L t m considers the margin between p yt t and max y ′ =yt p y ′ t . We will discuss this more at the end of Section 5.6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Ranking Loss on Detection Score</head><p>Ideally we want the activity detector to produce monotonically non-decreasing detection scores for the correct activity category as the detector sees more of the activity <ref type="figure">(Fig. 2)</ref>. To this end, we introduce the ranking loss L t s into the learning objective at time step t as:</p><formula xml:id="formula_2">L t s = max(0, −δ t · (p yt−1 t − p * t )),<label>(3)</label></formula><p>where δ t is set to 1 if y t−1 = y t , i.e., when there is no activity transition from t − 1 to t according to ground truth <ref type="figure">Figure 4</ref>: Detection score p yt (blue curve) of an activity y t spanning [t s , t]. p yt t b and p yt t are smaller than p yt ta (which is also p * yt t in this example), violating the monotonicity of the detection score, so L t b s and L t s are non-zero.</p><p>labeling (e.g. δ t = 1 for t a , t b and t in <ref type="figure">Fig. 4)</ref>; otherwise, δ t is set to −1.</p><p>In Eq. (3) p * t is computed as:</p><formula xml:id="formula_3">p * t = p * yt t , if δ t = 1, 0, otherwise,<label>(4)</label></formula><p>where</p><formula xml:id="formula_4">p * yt t = max t ′ ∈[ts, t−1] p yt t ′ .<label>(5)</label></formula><formula xml:id="formula_5">t s = min{t ′ | y t ′ = y t , ∀t ′ ∈ [t s , t]},<label>(6)</label></formula><p>where t s is the starting point of the current activity y t , and p * yt t is the highest previous detection score in [t s , t − 1] (illustrated by the dashed horizontal line in <ref type="figure">Fig. 4)</ref>.</p><p>In other words, if there is no activity transition at time t, i.e., y t = y t−1 , then we want the current detection score to be no less than any previous detection score for the same activity, computing the ranking loss as:</p><formula xml:id="formula_6">L t s = max(0, p * yt t − p yt t ).<label>(7)</label></formula><p>On the other hand, if an activity transition happens at time t, i.e., y t = y t−1 , we want the detection score of the previous activity to drop to zero at t and compute the ranking loss as: <ref type="figure">Fig. 4</ref> shows the detection scores p yt (the blue curve) of an activity y t spanning [t s , t]. In [t a + 1, t], the detection scores are smaller than p yt ta , violating the monotonicity of the detection score, so the ranking losses in this period are non-zero, e.g. L t b s and L t s as shown in the figure. One may be tempted to simply require p yt t to be no less than p yt t−1 when there is no activity transition, replacing Eq. 7 with:</p><formula xml:id="formula_7">L t s = p yt−1 t .<label>(8)</label></formula><formula xml:id="formula_8">L t s = max(0, p yt t−1 − p yt t ).<label>(9)</label></formula><p>However, as shown in <ref type="figure">Fig. 4</ref>, in this situation, the ranking loss will be zero in [t b +1, t c ] even though the monotonicity of detection score is also violated. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ranking Loss on Discriminative Margin</head><p>When more of an activity is observed, the detector should become more confident in discriminating between the correct category vs. the incorrect categories. We guide the training of our model to achieve such behavior by implementing the following ranking loss:</p><formula xml:id="formula_9">L t m = max(0, −δ t · (m yt−1 t − m * t )).<label>(10)</label></formula><p>where m y t is the discriminative margin of an activity label y at time step t (the blue point on the red curve at time t in <ref type="figure" target="#fig_1">Fig. 5)</ref>, computed as:</p><formula xml:id="formula_10">m y t = p y t − max{p y ′ t | ∀y ′ ∈ Y, y ′ = y},<label>(11)</label></formula><p>where Y is the set of all activity category labels. The m * t in Eq. 10 is computed as:</p><formula xml:id="formula_11">m * t = m * yt t , if δ t = 1, 0, otherwise.<label>(12)</label></formula><p>where m * yt t is computed as:</p><formula xml:id="formula_12">m * yt t = max t ′ ∈[ts, t−1] m yt t ′ ,<label>(13)</label></formula><p>i.e., the largest previous discriminative margin of the current activity y t that started at t s (illustrated by the dashed horizontal line in <ref type="figure" target="#fig_1">Fig. 5</ref>).</p><p>In other words, when there is no activity transition at t, we want the current discriminative margin to be no less than any previous margin in the same activity, computing the ranking loss as:</p><formula xml:id="formula_13">L t m = max(0, m * yt t − m yt t ).<label>(14)</label></formula><p>If an activity transition happens at time t, we want the discriminative margin of the previous activity to drop and compute the ranking loss as: <ref type="figure" target="#fig_1">Fig. 5</ref> illustrates the discriminative margins (red curve) m yt of the current activity y t spanning [t s , t] . The margin m yt is equal to the difference between the detection scores p yt of the correct category y t (blue curve) and the maximum of the detection scores max y ′ =yt p y ′ for the incorrect categories (dashed blue curve). Note that within the time interval [t a + 1, t], the margins are smaller than m yt ta , violating the monotonicity; consequently, the ranking losses are non-zero within the interval [t a + 1, t]. Also note that simply requiring the current margin to be less then that of the previous timestep is insufficient, which will result in zero ranking loss in interval [t b + 1, t] in <ref type="figure" target="#fig_1">Fig. 5.</ref> </p><formula xml:id="formula_14">L t m = m yt−1 t .<label>(15)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Training</head><p>In training, we compute the gradient of the ranking loss with respect to the softmax output at each time step:</p><formula xml:id="formula_15">∂L t ∂p y t = ∂L t c ∂p y t + λ r ∂L t r ∂p y t<label>(16)</label></formula><p>which is then back propagated through time to compute the gradients with respect to the model parameters. At a nonactivity frame, if the previous frame has activity, i.e., an activity to non-activity transition happens, the ranking loss at this frame is computed according to Eq. 8 or Eq. 15; otherwise, if the previous frame is also a non-activity frame, the ranking loss is fixed to 0. Although L t s and L t m are also functions of p y t ′ for t ′ &lt; t, i.e., the softmax output of previous time steps, to simplify computation, we do not compute and back propagate the gradients of the ranking loss with respect to these variables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>We evaluate our formulation on a large-scale, realistic activity dataset: ActivityNet <ref type="bibr" target="#b3">[4]</ref>. Using our proposed ranking losses in training significantly improves performance in both the activity detection and early activity detection tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Dataset</head><p>The ActivityNet <ref type="bibr" target="#b3">[4]</ref> dataset comprises 28K videos of 203 activity categories collected from YouTube. <ref type="figure" target="#fig_2">Fig. 6</ref> shows sample frames from video sequences of this dataset. The lengths of the videos range from several minutes to half an hour. The total length of the whole dataset is 849 hours. A single video may contain multiple activities and often also contains periods with none of the annotated activities. On average, 1.4 activities are annotated per video. The activity category, along with the start and end point of each activity are annotated by crowd-workers, leading to some annotation noise. Many of the videos are shot by amateurs in uncontrolled environments, and variances within the same activity category are often large. More importantly, many activities are relatively long and complex, and the viewpoint and foreground objects may change significantly within the same activity, e.g., Using ATM and Preparing pasta shown in <ref type="figure" target="#fig_2">Fig. 6</ref>. Given these challenges, it is important that the model learns the progression of activities for accurate activity detection and early detection.</p><p>The authors of ActivityNet use one fourth of the dataset as a validation set, but have not released the test set used in their paper. <ref type="bibr" target="#b0">1</ref> In our experiments, we use the validation set as our test set, and we split the remaining videos into one fifth for validation and four fifths for training. To reduce computational cost, we temporally down-sample the videos to 6 frames per second for all our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Model Training</head><p>For the CNN component (see <ref type="figure" target="#fig_0">Fig. 3</ref>), we first use training video frames of ActivityNet to fine-tune a VGG19 model <ref type="bibr" target="#b20">[21]</ref> that is pre-trained on ImageNet. The output dimension of the softmax layer is 204, which corresponds to the 203 activities plus one additional class corresponding to non-activity. We set the learning batch size to 32. The learning rate starts at 10 −4 and is divided by 10 after every 40K iterations. The fine-tuning stops at 120K iterations.</p><p>For LSTM training, the output of the second fully connected layer (fc7) of the fine-tuned VGG19 model is used as input to the LSTM. We use learning batches of 64 sequences, where each sequence comprises 100 frames. Back propagation through time is performed for 20 time steps. The momentum and weight decay are set to 0.9 and 0.0005 respectively. The learning rate starts at 0.01 and is divided by 10 after every 20K iterations. Training stops after 50K iterations. In this training phase, the CNN fc7 layer is also further trained together with the LSTM but with a lower starting learning rate of 10 −4 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Experimental Setup</head><p>In testing, we run the model across the whole input sequence and output activity detection scores at each input frame. We reset the LSTM memory whenever the model predicts non-activity, which we find slightly improves performance. For both activity detection and early detection, we detect video segments of activities from an input video sequence. To achieve this, we first classify each video frame to the activity category for which the detection score is the highest at this frame. Note that non-activity is treated simply as a special category. We then find continuous video frame segments that are classified to belong to the same activity category; this produces the initial detection spans. Finally, we iteratively merge the detection spans that are temporally close (less than 20 frames apart in our experiments). The score of each detection is then computed as the mean of the detection scores of all its video frames.</p><p>Following <ref type="bibr" target="#b3">[4]</ref>, we use the mAP (mean average precision) in evaluating performance. A detection is a true positive if: 1) its IOU (intersection-over-union) of temporal duration with a ground truth activity is above the IOU threshold, and 2) its activity label is equal to the ground truth activity label. If multiple detections overlap with the same ground truth activity, only the one with the longest duration is treated as a true positive. All the other detections are false positives. For evaluating performance on early detection, we split each input test sequence into multiple sequences so that each new sequence contains the non-activity segment (if there is any) before a test activity, and a portion of the test activity.</p><p>We evaluate the performance of four models: i) the finetuned VGG19 CNN model; ii) the LSTM model shown in <ref type="figure" target="#fig_0">Fig. 3</ref> trained with the classification loss only (Eq. 2); iii) the LSTM-s model trained with both the classification loss and ranking loss on the detection score (Eq. 3); iv) the LSTM-m model trained with classification loss and ranking loss on the discriminative margin (Eq. 10). In LSTM-s and LSTM-m, the weight for ranking loss (λ r in Eq. 1) is empirically set to 6, according to performance on our validation set. We find that using a combination of both ranking losses in training offers no further improvement over using just one so we do not include results for this in the paper. <ref type="table" target="#tab_0">Table 1</ref> shows the activity detection performance of the evaluated models under different IOU thresholds, α. The results of Heilbron et al. <ref type="bibr" target="#b3">[4]</ref> are produced on their test set, which is not publicly available; therefore, their results are not directly comparable to ours. Heilbron et al. sliding window approach to detect activities in the video sequences, where the temporal lengths of the sliding windows are empirically selected and fixed. In our approach the length of each detection is automatically determined as described in Section 5.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Activity Detection</head><p>The LSTM models greatly outperform the CNN model. This demonstrates the benefit of using a recurrent neural network model in activity detection. Both of the proposed ranking losses are beneficial in training a better LSTM model for activity detection: significant improvements are achieved over the LSTM model trained only using classification loss. For LSTM-s the improvements are consistently around 4.1∼5.9% at all IOU thresholds. Note that the relative improvement of LSTM-m and LSTM-s over LSTM increases when requiring the detection to more accurately overlap with ground truth, e.g., growing from 12.3% when α = 0.1 to 16.7% when α = 0.8 with LSTM-s. This shows that the proposed ranking-losses are even more useful in applications where accurate temporal localization is required. <ref type="figure" target="#fig_3">Fig. 7</ref> shows the top 20 activities for which the detection performance are improved the most by using ranking loss (LSTM-s or LSTM-m) in training (IOU threshold α = 0.5). It is interesting to note that using the proposed ranking losses, detection performance improves both for relatively simple activities such as playing saxophone and for relatively complex activities such as high jump. This shows that the proposed ranking losses may improve the detection of various types of activities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Activity Early Detection</head><p>In this experiment, the goal is to recognize and also temporally localize partially observed activities. <ref type="table" target="#tab_1">Table 2</ref> shows the detection performances when we only observe 3/10,   i.e., approximately the first third, of each testing activity. The LSTM models greatly outperform the CNN model on the early detection task. Moreover, the LSTM models trained with the proposed ranking losses (LSTM-s or LSTM-m) clearly outperform the LSTM model trained only with classification loss. For instance, with LSTM-s, the absolute improvements are consistently around 5.6∼3.7% at all IOU thresholds α, with relative improvement increasing from 11.3% at α = 0.1 to 16.5% at α = 0.8. <ref type="figure" target="#fig_4">Fig. 8</ref> shows the performance of early detection when the observed fraction of each test activity increases from 0.1 to 1 with IOU threshold fixed at 0.4 or 0.5. All LSTM models greatly outperform the CNN, no matter how much of each activity is observed. Both ranking losses, LSTMm and LSTM-s, outperform LSTM. Although the increase in detection performance slows down after observing approximately half of each activity, the performance gap between LSTM-s (LSTM-m) and LSTM increases as more of each activity is observed. More interestingly, LSTM-s significantly outperforms LSTM even when we only observe a small faction of each activity, e.g., one tenth. This could be quite useful for applications that require detecting activities as early as possible. <ref type="figure" target="#fig_5">Fig. 9</ref> lists the top 20 activity categories for which the early detection performance improves the most when using either of the proposed ranking losses in training. Interestingly, among these activities, some may have relatively little visual content change across the whole duration of the activity, such as Playing lacrosse, whereas others may undergo significant visual content change, such as Layup drill in basketball. This suggests that the benefits of the proposed ranking losses are applicable to various types of activities in the task of early detection.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.">Effects of the Ranking Losses</head><p>We now analyze what effects the proposed ranking losses introduce over the evolving time scale of model training. We first analyze how the detection score of the correct activity category and the discriminative margin (Eq. 11) change as we train LSTM-m and LSTM-s. We compute the detection scores and the discriminative margins at every frame in each test sequence using snapshots of the LSTM models trained after 10K, 20K, 30K, 40K and 50K iterations. This produces for each activity sequence a curve of the detection score (or discriminative margin) as a function of time. We normalize the curves so that each has a length of 20 points, and finally compute the mean curve over the whole test set. <ref type="figure" target="#fig_6">Fig. 10</ref> shows the mean curves. For both models, the mean curves are approximately non-decreasing, and such monotonicity becomes more apparent as we train for more iterations. The absolute values of the detection scores and discriminative margins increase as we train for more iterations, but converge after roughly 40K training iterations. <ref type="figure" target="#fig_7">Fig. 11</ref> compares the mean curves of the detection score and discriminative margin produced by LSTM-s, LSTMm and LSTM trained after 50K iterations, as well as the CNN model. The mean curves of LSTM-s and LSTMm (solid green curves and solid red curves) for both the detection score and discriminative margin are significantly higher than those of LSTM (blue curves). The LSTM-s and LSTM-m curves also show a more apparent monotone increasing trend compared to LSTM, which tends to be flat after approximately the first half of the activity. We also show the mean detection score curves for the worst negative category, i.e., the negative activity category with the highest detection score for LSTM, LSTM-s and LSTM-m using the dashed curves. The curves of LSTM-m and LSTM-s for the worst negative category are lower than that of LSTM.</p><p>It is interesting to note that each of the proposed ranking losses has useful impacts on both the detection score and the discriminative margin, despite the fact that they are either computed based on detection scores only or discriminative margins only. This conforms to our intuition that encouraging a non-decreasing detection score may help in producing a non-decreasing discriminative margin and vice versa. Also note that LSTM-s produces higher detection scores for the correct category than LSTM-m, while LSTM-m pushes the detection scores of the worst negative category significantly lower, as shown <ref type="figure" target="#fig_7">Fig. 11 (a)</ref>. In practice one can use either of these ranking loss formulations, depending on the application, e.g., selecting the best one via cross-validation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We improve training of the LSTM model to better learn activity progression. We introduce two novel formulations for ranking loss in LSTM training, designed to encourage consistent scoring and margin for detecting the correct activity as more of the activity sequence is observed. We gain significant performance improvements in activity detection and early detection on ActivityNet. In future work, we plan to conduct further in-depth study of the relative advantages of the two ranking losses.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Model overview. At each video frame, the model first computes CNN features (illustrated as fc7) and then the features are fed into the LSTM to compute detection scores of activities and non-activity (BG in the figure).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 5 :</head><label>5</label><figDesc>Discriminative margin m yt (red curve) of an activity y t spanning [t s , t]. The margin m yt is computed as the difference between the ground truth activity detection scores p yt (blue curve) and the maximum detection scores max y ′ =yt p y ′ (dashed blue curve) of all incorrect activity categories at each time point in [t s , t]. m yt t b and m yt t are smaller than m yt ta (which is also m * yt t ), violating the monotonicity of the margin, so L t b m and L t m are non-zero.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 6 :</head><label>6</label><figDesc>Each row contains sample frames of one example video sequence in ActivityNet. Frames with green borders contain the activities labeled on the left. Note the significant viewpoint and foreground object changes within the activities Using ATM, Sailing and Preparing Pasta.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 7 :</head><label>7</label><figDesc>Top 20 activity categories for which the detection performance improved the most by using either LSTM-s or LSTM-m in training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 8 :</head><label>8</label><figDesc>Activity early detection performance plotted as a function of the observed fraction of each test activity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 9 :</head><label>9</label><figDesc>Top 20 activity categories for which the early detection performance improved the most by using either LSTM-s or LSTM-m in training. Only the first 3/10 of each test activity is observed. The IOU threshold α = 0.5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 10 :</head><label>10</label><figDesc>Mean curves of the detection score and the discriminative margin as function of time over all test activity sequences produced by snapshots of the LSTM-m and LSTM-s models trained after 10K, 20K, 30K, 40K and 50K iterations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 11 :</head><label>11</label><figDesc>Mean curves of (a) the detection score and (b) the discriminative margin, as function of time over all test sequences for CNN, LSTM, LSTM-m and LSTM-s. The mean curves of the detection score for the worst negative category, i.e., negative activity category with the highest detection score, are also shown as the dashed curves.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><head>Table 1 :</head><label>1</label><figDesc>Activity detection performance measured in mAP at different IOU thresholds α. Note that the results of Heilbron et al.<ref type="bibr" target="#b3">[4]</ref> are produced on their own test split that is unavailable to us, so their results are not directly comparable to ours.</figDesc><table>Model 
α = 0.1 α = 0.2 α = 0.3 α = 0.4 α = 0.5 α = 0.6 α = 0.7 α = 0.8 
Heilbron et al. [4] 12.5% 
11.9% 
11.1% 
10.4% 
9.7% 
-
-
-
CNN 
30.1% 
26.9% 
23.4% 
21.2% 
18.9% 
17.5% 
16.5% 
15.8% 
LSTM 
48.1% 
44.3% 
40.6% 
35.6% 
31.3% 
28.3% 
26.0% 
24.6% 
LSTM-m 
52.6% 
48.9% 
45.1% 
40.1% 
35.1% 
31.8% 
29.1% 
27.2% 
LSTM-s 
54.0% 50.1% 46.3% 41.2% 36.4% 33.0% 30.4% 28.7% 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Activity early detection performance at different IOU thresholds (α), when only 3/10 of each activity is observed. 

Model 
α = 0.1 α = 0.2 α = 0.3 α = 0.4 α = 0.5 α = 0.6 α = 0.7 α = 0.8 
CNN 
27.0% 
23.4% 
20.4% 
17.2% 
14.6% 
12.3% 
11.0% 
10.3% 
LSTM 
49.5% 
44.7% 
38.8% 
33.9% 
29.6% 
25.6% 
23.5% 
22.4% 
LSTM-m 52.6% 
47.9% 
41.5% 
36.2% 
31.4% 
27.1% 
24.8% 
23.5% 
LSTM-s 55.1% 50.3% 44.0% 38.9% 34.1% 29.8% 27.4% 26.1% 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">According to communication with the authors of<ref type="bibr" target="#b3">[4]</ref>, this test split is kept confidential for use in a future challenge.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments. This work was supported in part by NSF grant 1029430 and a gift from NVIDIA.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal graphs of human activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Brendel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Todorovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained part-based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1627" to="1645" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Finding action tubes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Activitynet: A large-scale video benchmark for human activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">C</forename><surname>Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Max-margin early event detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hoai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>De La</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torre</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">3d convolutional neural networks for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="221" to="231" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Event detection in crowded videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Max-margin action prediction machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="issue">99</biblScope>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>PP</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Discriminative figure-centric models for joint action localization and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning realistic human actions from movies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marszałek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rozenfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Space-time tree ensemble for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Action recognition and localization by hierarchical space-time segments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ikizler-Cinbis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Beyond short snippets: Deep networks for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<idno>abs/1503.08909</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multiple granularity analysis for fine-grained action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">R</forename><surname>Paramathayalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Moulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Dropout improves recurrent neural networks for handwriting recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kermorvant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Louradour</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Poselet key-framing: A model for human activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Raptis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Recognizing fine-grained and composite activities using hand-centric features and script data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Regneri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pinkal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<idno>abs/1502.06648</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Human activity prediction: Early recognition of ongoing activities from streaming videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ryoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Spatiotemporal deformable part models for action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Action recognition with improved trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Video action detection with relational dynamic-poselets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Hidden part models for human action recognition: Probabilistic versus max margin</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1310" to="1323" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Modeling spatial-temporal clues in a hybrid deep learning framework for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<idno>abs/1504.01561</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Every moment counts: Dense detailed labeling of actions in complex videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Discriminative subvolume search for efficient action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
