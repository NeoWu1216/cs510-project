<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learnt Quasi-Transitive Similarity for Retrieval from Large Collections of Faces</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ognjen</forename><surname>Arandjelović</surname></persName>
							<email>ognjen.arandjelovic@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University of St Andrews</orgName>
								<address>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learnt Quasi-Transitive Similarity for Retrieval from Large Collections of Faces</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:43+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We are interested in identity-based retrieval of face sets from large unlabelled collections acquired in uncontrolled environments. Given a baseline algorithm for measuring the similarity of two face sets, the meta-algorithm introduced in this paper seeks to leverage the structure of the data corpus to make the best use of the available baseline. In particular, we show how partial transitivity of inter-personal similarity can be exploited to improve the retrieval of particularly challenging sets which poorly match the query under the baseline measure. We: (i) describe the use of proxy sets as a means of computing the similarity between two sets, (ii) introduce transitivity meta-features based on the similarity of salient modes of appearance variation between sets, (iii) show how quasi-transitivity can be learnt from such features without any labelling or manual intervention, and (iv) demonstrate the effectiveness of the proposed methodology through experiments on the notoriously challenging YouTube database.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The dramatic increase in the capability for large amounts of visual information to be acquired and stored witnessed in the last 10-15 years has effected a profound change on the context in which face recognition algorithms are expected to operate. While the early work on face recognition focused on recognition from a single image using verification and identification protocols on small databases (usually a few dozen people), and at least partly controlled conditions <ref type="bibr" target="#b34">[34,</ref><ref type="bibr" target="#b46">46,</ref><ref type="bibr" target="#b48">48]</ref>, more recent efforts have been directed towards video or image set-based recognition <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b8">8,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b30">30]</ref>, and large databases acquired in highly uncontrolled environments <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b45">45]</ref>.</p><p>Early work on face recognition in the context of large data collections primarily sought to extend existing methods and adapt them for use on low quality images. This includes pose normalization by affine warps <ref type="bibr" target="#b14">[14]</ref> or simplified 3D head models <ref type="bibr" target="#b21">[21]</ref>, illumination normalization by filtering <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> and illumination invariance through the use of local gradient-based features <ref type="bibr" target="#b3">[4]</ref>. Later work has been increasingly oriented towards challenges associated with learning problems which emerge in large data sets <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b45">45]</ref>. Another popular direction involves the use of text information and natural language processing to extract and associate names with detected faces <ref type="bibr" target="#b20">[20,</ref><ref type="bibr" target="#b36">36]</ref>. Concurrently with the research on face recognition in the context of large data collections, there has been much progress in video and setbased recognition <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b15">15]</ref>. Influential contributions include advances in the representation of face sets <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b40">40]</ref>, and in particular manifold-based representations <ref type="bibr" target="#b32">[32,</ref><ref type="bibr" target="#b44">44]</ref>, illumination models <ref type="bibr" target="#b11">[11]</ref>, and similarity measures <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b44">44]</ref>.</p><p>The broad topic of the present paper is that of face set retrieval and its contribution relates both to the previous work on set-based recognition and the work concerned with recognition in the context of large data collections. In contrast to most work in the literature our key interest is neither in the representation of face sets nor the associated similarity measures per se. Rather, given a baseline algorithm for measuring the similarity of two face sets, our work seeks to leverage the structure of the data at the large scale, that of the entire database, to make the best use of the available baseline. In the sense that our method has as an input both data (face image sets) and an algorithm (the baseline), it can be accurately described as a meta-algorithm.</p><p>Problem specification Given a query face set our aim is to retrieve from a large database (gallery), sets of the same person. More specifically, we wish to order the gallery sets in decreasing order of confidence that they match the query in identity. Thus the ideal retrieval has all sets of the query person first ('matches') followed by all others ('nonmatches'). We assume that the gallery is entirely unlabelled and may contain multiple sets of the same person.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Learnt transitive similarity</head><p>In this section we introduce the main contribution of the present paper. In particular, we describe a general framework for face retrieval especially well suited for large collections of face images acquired 'in the wild' i.e. in largely unconstrained imaging conditions, and characterized by highly unbalanced amounts of training data per class (person). We start by motivating the intuition behind our method in the section which follows, and subsequently ex-plain how this intuition can be formalized into a general retrieval framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Motivation and the key idea</head><p>It is useful to consider the motivation behind our idea in the context of related previous work and in particular the recent Matched Background Similarity (MBS) method <ref type="bibr" target="#b45">[45]</ref>. <ref type="bibr">Wolf et al.</ref> argue that in building a classifier which discriminates the appearance of a specific person and all other people, the focus should be on discriminating between this person and those individuals most similar to him/her; improvements in discrimination against very dissimilar people matter less as these individuals are unlikely to be conflated with the person of interest anyway. Our idea can be seen as complementary and builds upon a similarly simple basic principle. Specifically, we make use of the observation that if person A is alike in appearance to person B, and similarly person B to person C, on average persons A and C are more likely to look alike than two randomly chosen individuals. We term this Quasi-Transitive Similarity; 'quasi-' because the stated regularity is a statistical rather than a universal one, as we shall explain shortly.</p><p>As stated in our introduction above, the transitivity of similarity in appearance does not hold universally. It is possible that persons A and B are similar by virtue of one set of physical features, and B and C of another. A useful mental picture can be formed by drawing an analogy from statistics (or geometry): random variables (or vectors) A and B, and B and C may be positively correlated (have a positive dot product), yet A and C may be negatively correlated (have a negative dot product) with one another.</p><p>Lastly it is worth contrasting our work with that of Yin et al. <ref type="bibr" target="#b47">[47]</ref>. Unlike ours, their method necessitates the localization of face parts, which is problematic and highly likely to fail in severe illuminations, extreme poses, or in poor quality images. Their method also needs to extract estimates of pose and illumination, again very much unlike ours which does not have any of the aforementioned bottlenecks -all learning is performed directly from data and without the need for an explicit model at a higher semantic level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Transitivity meta-features</head><p>We have already noted that the observed transitivity of similarity is a statistical rather than a universal phenomenon. In other words, while the similarity of persons A and B, and B and C, on average leads to a greater similarity between A and C, in some instances this will not be the case. This suggests that in addition to inter-personal similarities A-B and B-C, a richer set of features should be used to infer the similarity A-C. Clearly these features should complement the inter-personal similarities in the sense that jointly they should allow for a better estimate of the similarity A-C than just similarities A-B and B-C, or a direct baseline comparison of A and C (i.e. without the use of additional indirect information provided by the relationship of B with A and C).</p><p>To motivate the meta-features that we propose in this paper consider the conceptual illustrations shown in <ref type="figure">Fig 1.</ref> Solid coloured lines depict the range of appearance variation within face sets. Our aim is to estimate the similarity of the query (green) and the set denoted as 'target' (red). The face set marked 'proxy' is a database face set of a person similar in appearance to the 'target', as assessed by the baseline similarity measure; for example, the proxies of a particular target set can be selected as its nearest k p sets in the database. The dotted red line represents the range of possible appearance of the 'target' person which is not actually present in the 'target' face set. For the time being the reader may assume that face sets are represented as sets of actual exemplars and the similarity between two sets is given by the similarity between their most similar members -we will explain how the ideas introduced herein can be generalized in the next section.</p><p>Both in the case shown in <ref type="figure">Fig 1(a)</ref> and that in <ref type="figure">Fig 1(b)</ref>, the baseline similarity measure tells us that 'query' is close to 'proxy', and of course 'proxy' is close to 'target' by design i.e. by the former being a proxy in the first place. The difference between the two cases, illustrated conceptually, lies in the similarity of exemplars f tq and f tp i.e. the exemplars best matching the query and proxy sets. In particular, the observation that the baseline similarity measure deems the proxy set significantly more similar than the query to the target on the one hand, while both similarities are explained by similar target exemplars, informs us that the divergence in query and proxy appearances from the target are of different natures. Thus, even if similarities s 1 , s 2 , and s 3 are the same in Figs 1(a) and 1(b), the information contained in relationships between f tq and f tp , and f pq and f pt tells us that we should infer different query-target similarities in the two cases. Therefore we introduce what we term transitivity meta-features which we use for the said inference. Given a baseline similarity measure and a triplet consisting of query, target, and proxy sets, the corresponding transitivity metafeature v(query,target|proxy) comprises five similaritiess 1 ('query' to 'proxy' similarity), s 2 ('query' to 'target' similarity), s 3 ('proxy' to 'target' similarity), s 4 (similarity between the 'proxy' exemplar most similar to 'query' and the 'proxy' exemplar most similar to 'target'), and s 5 (similarity between the 'target' exemplar most similar to 'query' and the 'target' exemplar most similar to 'proxy'):</p><p>(query,target|proxy) = s1 s2 s3 s4 s5 T (1)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Non-exemplar based representations</head><p>In the preceding discussion we asked the reader to think of appearance variation within each set as being represented using what is probably conceptually the simplest choice of representation: as a collection of exemplars. In other words, each set was a set of representations of individual faces. This was done for pedagogical reasons and we now show that the proposed framework is in no way reliant on this representation.</p><p>In particular, to make the transition of applying the proposed method on the special case in which a face set is represented using a set of directly observed exemplars to the general case in which an arbitrary set representation is employed, we need to explain how the concept of a pair of the most similar exemplars such as those labelled f qp and f pq in <ref type="figure">Fig 1(a)</ref>, as well as the similarity between them (such as that between f pq and f pt ), can be generalized. This is not difficult -all that is required is a slight reframing of the concept. Instead of seeking the nearest pair of specific exemplars, in the general case we are interested in the pair of the most similar modes of variation captured by the representations of two sets (as measured by the baseline similarity measure of course). We illustrate this idea with a few examples.</p><p>If the variation within a set is modelled using a linear subspace and the subspace-to-subspace generalization of the distance from feature space (DFFS) <ref type="bibr" target="#b44">[44]</ref> adopted as the (dis)similarity measure between them, the most similar modes of variation between two sets represented using such subspaces are sub-subspaces themselves. These correspond to different exemplars f xy in <ref type="figure">Fig 1 and</ref> can be compared using the DFFS baseline. If, on the other hand, similarity is measured using the maximum correlation between subspace spans as in <ref type="bibr" target="#b5">[6]</ref>, the most similar modes of variation between two sets are readily extracted as the first pair of the canonical vectors between subspaces <ref type="bibr" target="#b23">[23]</ref> and compared using the cosine similarity measure <ref type="bibr" target="#b35">[35]</ref>. For manifold-to-manifold distances such as that of Lee et al. <ref type="bibr" target="#b30">[30]</ref> the most similar modes of variation are simply the nearest pairs of points on two manifolds, with the similarity of two points on the same manifold readily quantified by the geodesic distance between them.</p><p>The same ideas are readily applied to any of a variety of set representations and similarity measures described in the literature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Learning quasi-transitive similarity</head><p>Given a triplet comprising a query, a target, and a proxy data set, our aim now is to infer the similarity between the query and the target using the corresponding transitivity fea-ture defined in <ref type="bibr" target="#b0">(1)</ref>. Without loss of generality, let us quantify inter-set similarity with a real number in the range [0, 1], where 0 signifies the least and 1 the greatest possible similarity. Then our problem can be stated more formally by saying that we are seeking a mapping m qts :</p><formula xml:id="formula_0">mqts : R 5 → [0, 1],<label>(2)</label></formula><p>with the ideal output of m qts (v(query,target|proxy)) being 0 iff the identities in the query and target sets are different, and 1 iff they are the same. Observe that since we are interested in confidence-based ranking of all sets in a database, the codomain of m qts is not the set {0, 1}, which would make this a binary classification problem, but rather [0, 1] (a range) which makes it a regression task.</p><p>In the types of problem setting in which face recognition is addressed by most of the existing research, obtaining features for training, at least in principle, is simple. Whether it is verification (1-to-1 matching) or identification (1-to-N matching), the database 'known' to the algorithm comprises data which is, it is assumed, correctly partitioned by the identity. The retrieval setting adopted in this work is more challenging in this sense and consequently the learning process needs to be approached with more care. In particular, as described in Sec 1, we assume that our database is entirely unlabelled and that it may contain multiple sets of the same person. We neither know how many individuals there are in the database nor the number of sets of each individual (which can of course vary person to person). Since for any two database sets we cannot know for certain if they belong to the same or different individuals, an obvious corollary is that in the extraction of transitivity features described by (1) both intra-personal and inter-personal training sets may contain incorrect examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.1">Extraction of transitivity features for training</head><p>Given that our data is unlabelled i.e. that we do not know if the two face sets in the database correspond to the same person or not, we cannot extract training transitivity features in the obvious manner by considering different query, target, and proxy triplets, with the query and the target either matching (producing same identity training data) or not (producing differing identities training data). Instead, we describe how training data, albeit corrupted (this issue is dealt with in the next section), can be collected by considering only pairs of sets, that is, all possible database sets and their proxies. We do this for the two baseline set comparison methods adopted from Wolf et al. <ref type="bibr" target="#b45">[45]</ref>: (i) maximum maximorum cosine similarity between sets of exemplars <ref type="bibr" target="#b35">[35]</ref>, and (ii) the maximum correlation between vectors confined to linear subspaces describing within set variability <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b12">12]</ref>.</p><p>Exemplar-based baseline Consider a particular database face set ('reference') used for training and one of its proxies. To extract training transitivity features which correspond to same identity query-target comparisons, we select both query and target data from the reference set (i.e. a single video). In particular, we treat all possible pairs of exemplars in the reference set as possible pairs f qt and f tq . Indeed, for specific choices of possible query and reference sets, any two appearances may present themselves as the nearest exemplars in them. The second element s 2 in the transitivity feature is then simply given by the similarity between the two exemplars. On the other hand the similarity s 1 between the query and the proxy is given by the similarity between the unitary set consisting of the reference set exemplar treated as f qt and the proxy set. The nearest proxy exemplar to f qt is of course f pq . The similarity s 3 is simply computed as the similarity between the reference set and the proxy, which also gives us exemplars f pt and f tp , and allows for a straightforward computation of s 4 (as the similarity between f pq and f pt ) and s 5 (as the similarity between f tq and f tp ). A single pair of reference and proxy sets thus gives us n r (n r − 1) 'positive' training transitivity features, where n r is the number of faces in the reference set.</p><p>The extraction of training transitivity features which correspond to differing identities query-target comparisons is similar. Now we iterate through all exemplar pairs of the proxy set, taking each pair as f qt and f pq in turn. The closest target exemplar to f qt becomes f tq , while f pt and f tp are determined as before, allowing for all transitivity feature entries (exemplar similarities) to be computed as in the case of same identity query-target training data extraction. A single pair of reference and proxy sets thus gives us n p (n p − 1) 'negative' training transitivity features, where n p is the number of faces in the proxy set.</p><p>It is important to observe that the set of 'negative' training transitivity features extracted in the described manner may be corrupt. This is an inherent consequence of the problem setting -since the database is entirely unlabelled we cannot know if the identities of the people in the reference and proxy set are actually different. The proposed process of training the regressor, described in Sec 2.4.2, takes this into account. Nevertheless, the amount of improvement achieved with the proposed method over its baseline is tied to the proportion of 'negative' training data which is incorrect -the improvement inevitably decreases as this proportion is increased. However, if this is so, i.e. if a great proportion of proxies of sets in the database actually represent the same identity as the sets they are proxies to, this by design means that the baseline comparison is very good to start with so no significant improvement can be reasonably expected. Thus, our method is particularly attractive in challenging conditions in which the baseline classifier does not perform well. Subspace-based maximum correlation baseline The extraction of training data for this representation is somewhat simpler than in the previous case. We again extract transitivity feature training data using only face set pairs (rather than triplets) which are now represented by linear subspaces. To extract training transitivity features which correspond to same identity query-target comparisons, we iterate through all reference set exemplars as f qt and obtain f tq and f pq by projecting them to respectively the reference and proxy subspaces. Vectors f pt and f tp are readily obtained using the baseline set comparison as the principal vectors of the subspaces corresponding to reference and proxy subspaces. A single pair of reference and proxy sets thus gives us n r 'positive' training transitivity features.</p><p>The extraction of training transitivity features which correspond to differing identities query-target comparisons proceeds in exactly the same manner, with the difference that it is proxy set exemplars that are iterated through as f qt (as before also taken to be f qp ). A single pair of reference and proxy sets gives us n r 'positive' training transitivity features, where n r is the number of faces in the reference set, and n p 'negative' training transitivity features, where n p is the number of faces in the proxy set. A single pair of reference and proxy sets thus gives us n p 'negative' training transitivity features. The same remarks as before regarding the corruption of the 'negative' training set hold here too. Closing note In Sec 2.1 we remarked that the basic idea behind the proposed method can be seen as complementary to that of MBS <ref type="bibr" target="#b45">[45]</ref>. However when the proposed training scheme is considered it can be seen to contain both conceptually similar elements and complementary elements to MBS. In particular, since the negative training set of quasitransitivity features is extracted by considering elements of the proxy set as the query, our method learns to discriminate precisely between a person and those individuals most similar to him/her (as in MBS), while exploiting the quasitransitivity of similarity (complementary to MBS).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.2">Training the predictor</head><p>We adopt the use of the ǫ support vector (ǫ-SV) regression <ref type="bibr" target="#b41">[41]</ref>. For comprehensive detail of this regression technique the reader is referred to the original work by Vapnik; here we present a brief summary of the ideas relevant to the proposed method. Given training data {(x 1 , y 1 ), . . . , (x l , y l )} ⊂ F × R, where F is the input space (in our case this is R 5 ), ǫ-SVR aims to find a function h(x) which deviates at most ǫ from its targets y. As in other SV-based methods, an implicit mapping of input data x → Φ(x) is performed by employing a Merceradmissible kernel <ref type="bibr" target="#b33">[33]</ref> k(x i , x j ) which allows for the dot products between mapped data to be computed in the input space:</p><formula xml:id="formula_1">Φ(x i ) · Φ(x j ) = k(x i , x j ). The function h(x) of the form h(x) = l i=1 (αi − α * i )k(xi, x) + b<label>(3)</label></formula><p>is then learnt by minimizing</p><formula xml:id="formula_2">l i=1 l j=1 (αi − α * i )(αj − α * j )k(xi, xj) +ǫ l i=1 (αi + α * i ) − l i=1 yi(αi − α * i )<label>(4)</label></formula><p>subject to the constraints</p><formula xml:id="formula_3">l i=1 (α i − α * i ) = 0 and α i , α * i ∈ [0, c].</formula><p>The parameter c can be seen as penalizing prediction errors greater than ǫ i.e. as balancing the trade-off between the smoothness of h(x) and the amount of data predicted with an error greater than ǫ.</p><p>The nature of ǫ-SV regression is particularly well suited to the problem at hand. Specifically, we train the regressor using the value of 1 as the target for same identity transitivity features, and 0 for different identities, allowing for a large prediction error margin of ǫ = 0.4 but severely penalizing greater errors with c = 1000. The large penalty C ensures that it is the outliers in the form of the wrongly labelled training data that define the boundary between the penalized and non-penalized regions of the high-dimensional space, while the wide margin ǫ = 0.4 ensures that the correctly labelled bulk of the training corpus is pushed away from the boundary towards the desired extreme values of 0 and 1. We used the radial basis function kernel k(x i , x j ) = exp{−0.2 x i − x j 2 }.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.3">Retrieval</head><p>Given a query data set we compute its similarity with a target database set by computing the regression-based estimate m qts (v(query,target|proxy)) using each of target's k p proxies, and taking the maximum of these and the baseline similarity between the query and the target. Database sets are then ordered by decreasing similarity with respect to the query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Evaluation</head><p>In this section we report our evaluation of the proposed method and discuss our findings. We start by describing the data set on which the evaluation was performed, consider the measures used to assess performance, summarize the    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Evaluation data</head><p>For evaluation we adopted the YouTube Faces Database <ref type="bibr" target="#b45">[45]</ref> which contains sets of faces extracted from YouTube videos. There are two key reasons which motivated this choice. Firstly, the manner in which this data set was collected and the nature of its contents are representative of the conditions which the present work targets. In particular, the total amount of data is large (3425 videos/sets of 1595 individuals, with the average set size of approximately 181.3 faces or equivalently 620,953 faces in total), it was extracted from videos acquired in unconstrained conditions in which large changes in illumination, pose, and facial expressions are present, and the distribution of data is heterogeneous both with respect to the set sizes (48-6,070) as well as the number of sets <ref type="bibr" target="#b0">(1)</ref><ref type="bibr" target="#b1">(2)</ref><ref type="bibr" target="#b2">(3)</ref><ref type="bibr" target="#b3">(4)</ref><ref type="bibr" target="#b4">(5)</ref><ref type="bibr" target="#b5">(6)</ref> for each person in the database. The second reason lies in the reproducibility of results and the ease of comparison with alternatives in the literature -the database has been widely adopted as a standard benchmark and a number of standard face representations are provided ready for use. Full detail can be found in the original publication <ref type="bibr" target="#b45">[45]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Performance evaluation</head><p>As the cornerstone measure of retrieval performance we adopt the average normalized rank (ANR) <ref type="bibr" target="#b19">[19,</ref><ref type="bibr" target="#b37">37]</ref>. In brief, ANR treats each retrieved datum as either matching or not matching the query and computes the average rank of the former group, normalized to the range [0, 1], with the ANR value of 0 corresponding to the best possible performance (all matching data retrieved before any non-matching) and 1 the worst (all non-matching data retrieved before any matching). Formally:</p><formula xml:id="formula_4">AN R(n, {r1, . . . , rc}) = c i=1 ri − m M − m (5)</formula><p>where n is the database size, {r 1 , . . . , r c } the set of retrieval ranks corresponding to the data of interest (i.e. data matching the query), and m and M respectively the minimum and maximum possible values of the sum of r 1 , . . . , r c :</p><formula xml:id="formula_5">m = c i=1 i = c × (c + 1) 2 (6) M = n i=n+1−c i = c × 2n − c + 1 2 (7)</formula><p>In comparison with other common performance measures, such as the receiver operating characteristic (ROC) curve <ref type="bibr" target="#b22">[22]</ref>, commonly used in verification and identification problems <ref type="bibr" target="#b6">[7]</ref>, the average normalized rank more directly captures the ultimate aim of a retrieval algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Methods</head><p>Motivated by the results reported by Wolf et al. which demonstrate its superiority over a number of alternatives, we adopt the standard local binary pattern (LBP) representation of individual faces <ref type="bibr" target="#b27">[27]</ref>. Using LBP we consider two baseline set representations: (i) a set of LBP exemplars, and (ii) a linear LBP subspace, both of which were also evaluated by Wolf et al. The former simply stores all face exemplars (i.e. the corresponding LBP vectors), while the latter uses PCA to represent the main modes of the observed exemplar variation; previous work suggests that for individual face sets 6-dimensional subspaces produce good results so this is the dimensionality we adopt too.</p><p>We adopt two baseline set similarity measures, again motivated by the reports of their good performance in the existing literature. The first of these is the maximum maximorum ('max-max') cosine similarity between sets of exemplars max f1∈S1,f2∈S2 f T 1 f 2 / f 1 / f 2 which in the experiments of Wolf et al. <ref type="bibr" target="#b45">[45]</ref> outperformed a number of alternatives including by a large margin the pyramid match kernel of Graumanand and Darrell <ref type="bibr" target="#b24">[24]</ref> and the localityconstrained linear coding (LLC) of Wang et al. <ref type="bibr" target="#b43">[43]</ref>. The second baseline comparison which we adopt for the comparison of sets represented as linear subspaces is the algebraic method based on the maximum correlation between pairs of vectors lying in two subspaces. This method too performed well in past experiments <ref type="bibr" target="#b45">[45,</ref><ref type="bibr" target="#b5">6]</ref>. Thus in summary, our two baseline methods are:</p><p>• LBP + maximum maximorum set similarity, and</p><p>• LBP + maximum correlation between subspaces.</p><p>These are used to establish reference performance. They are then employed in the context of several different ways of applying our idea of quasi-transitivity:</p><p>• Simple arithmetic mean-based quasi-transitivity,</p><p>• Simple geometric mean-based quasi-transitivity,</p><p>• Simple quadratic mean-based quasi-transitivity, and</p><p>• Proposed learnt quasi-transitivity (L-QTS).</p><p>The first three methods in the list are simple combination rules. In the first of these, the arithmetic mean-based quasitransitivity, two set similarity of dissimilarity measures ρ QP (query-proxy) and ρ P T (proxy-target) are combined by computing their arithmetic mean i.e. 0.5 × (ρ QP + ρ P T ).</p><p>Similarly, in the geometric and quadratic mean-based methods quasi-transitivity is attempted by computing respectively √ ρ QP × ρ P T and 0.5ρ QP 2 + 0.5ρ P T 2 . The proposed learnt quasi-transitivity (applied atop of both baseline methods) was evaluated using different numbers of proxy sets (1-10) and as detailed in Sec 2.4.2, ǫ-SV regression was learnt using the parameter values ǫ = 0.4 and c = 1000.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Protocol</head><p>We train the ǫ-SV regressor using 200 randomly selected sets and their proxies (which are not necessarily in the random 200). In principle there is no reason why the entire database would not be used (recall that no labelling or manual intervention is used whatsoever) but we found that 200 sets were sufficient to gather sufficient training data. Examples are shown in <ref type="figure" target="#fig_7">Fig 6;</ref> clear patterns are observable both within positive and negative training sets which differ one from another significantly.</p><p>The evaluation of the methods described in the previous section was performed by examining all possible retrievals. In other words, we used every set in our database as the query in turn and evaluated the resulting retrieval. To make this feasible we propose a robust sample selection method so as to reduce the computational demands of the otherwise computationally intensive exemplar-based baseline. Exemplar baseline: robust sample selection It is well established by the existing work on face recognition that the appearance of a face is constrained and thus confined to a region of the image space. Within this region, which is nonlinear, the appearance variation is mostly approximately smooth -this is sometimes somewhat loosely stated as the face appearance being constrained to a nonlinear appearance manifold <ref type="bibr" target="#b32">[32,</ref><ref type="bibr" target="#b44">44]</ref>. That being said, the range of appearance variation of a person's face within a single video typically covers only a portion of the entirety of possible variation. It is a simple yet important observation that even within this range of appearance the underlying manifold is not uniformly sampled, e.g. a person may spend more time in a specific pose than in others. One consequence is that while largely redundant face exemplars of the densely sampled portions of the manifold add little new information about the appearance of the person's face, they can dramatically increase the computational cost of set-based comparisons. This is the case for example for face set-based comparisons which utilize all sample pairs comparisons such as those based on the maximum maximorum similarity (i.e. all pairs maximum similarity) <ref type="bibr" target="#b18">[18]</ref> or the maximum minimorum distance (a variation of the Hausdorff distance <ref type="bibr" target="#b42">[42]</ref>). More worryingly, if a sample voting scheme is used <ref type="bibr" target="#b45">[45]</ref>, redundant exemplars can unduly affect the result even though they carry little additional information.</p><p>We overcome both of the problems described above by employing a robust sample selection scheme. Our starting point is the observation that although the intrinsic dimensionality of the entire face manifold is estimated to be in the range 15-22 <ref type="bibr" target="#b31">[31]</ref>, the appearance variation exhibited in a typical video clip is typically dominated by a single factor such as face yaw changes; the plot in <ref type="figure" target="#fig_1">Fig 2 corroborates</ref> this. Led by this insight we employ kernel principal component analysis (KPCA) <ref type="bibr" target="#b39">[39]</ref> to project the original face exemplars onto their dominant nonlinear principal component, uniformly sample the resulting 1D space between the two projections of the two most extreme exemplars, and finally project them back into the original space. The process is illustrated in <ref type="figure" target="#fig_2">Fig 3.</ref> The plot in <ref type="figure" target="#fig_4">Fig 4 demonstrates</ref> that the proposed sample selection does not greatly affect inter-set similarities; a computational improvement of over 2.5 orders of magnitude (approximately 330 times) was achieved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Results and discussion</head><p>The main set of results of our experiments is summarized in the plots in <ref type="figure" target="#fig_5">Fig 5(a)</ref>   densities of the ANR achieved for the two baseline methods and different quasi-transitivity approaches. Firstly note that the two baseline methods performed approximately equally well, which is consistent with the previous reports in the literature <ref type="bibr" target="#b45">[45]</ref>. The three simple attempts at exploiting quasi-transitivity worsened performance significantly, save for the arithmetic mean-based similarity combination for the subspace-based baseline which effected neither an improvement nor deterioration. This confirmed our expectation expressed in Sec 2.2 that the use of inter-personal similarities only is unlikely to be successful and that a richer set of similarity features is needed instead. This leads us to the proposed method which in both cases effected a major performance improvement over both of the baselines. For example, while the exemplar-based baseline produced retrievals with the ANR less than 0.3 in 54.0% of the cases, the corresponding learnt quasi-transitivity did so in 72.5% of the cases (an improvement of 34%). Similarly, while the subspace-based baseline produced retrievals with the ANR less than 0.3 in 54.9% of the cases, the corresponding learnt quasi-transitivity did so in 72.8% of the cases. It is particularly interesting to observe in how few cases our method produced bad results (i.e. high ANR) -for both baselines our method achieved ANR lower than 0.5 for over 98% of retrievals. In contrast, the 98% quantile of the baseline methods corresponds to the ANR values of 0.92 and 0.88 for the exemplar and subspace-based methods.</p><p>The effect of the number of proxies is summarized in Figs 5(b) and 5(d). For both baselines performance improvement is immediately apparent even using a single proxy per set. Interestingly, while in the case of the exemplar baseline the performance gradually improves up until k p = 5, staying approximately steady thereafter, the improvement using the subspace-based baseline is much more dramatic and reaches its peak (on par with the peak of the exemplar baseline) for k p = 1 already (ANR plots for different k p are virtually indistinguishable). Although we are not sure of the exact mechanism that explains this behaviour, it does appear to be linked to the inherent properties of the subspace-based baseline which is additionally supported by the observation that the within-class variability of the corresponding training meta-features is significantly smaller than for the exemplar-based baseline; see <ref type="bibr">Fig 6.</ref> Let us next turn our attention to the plot in <ref type="figure" target="#fig_8">Fig 7(a)</ref>. It shows the proportion of retrievals which result in at least one correct match being retrieved in the top 100 ranked sets as a function of the total number of target sets in the database which correctly match the query. Plotted as solid blue and red lines are the results obtained using the proposed method (with 10 neighbours used as quasitransitivity proxies) atop of the exemplar-based baseline, and the baseline itself (as expected from <ref type="figure" target="#fig_5">Fig 5,</ref> the results for the subspace-based method are similar and are thus not included to avoid unnecessary repetition). The plots also show predictions based on the methods' performances for queries in which only a single correct match is present in the entire database. Specifically, starting from the estimate of the probability p 1,100 of a correct match being retrieved in the top 100 ranked sets using queries where only a single correct match is possible, if different correct matches are ranked independently when k correct matches exist, the probability of at least a single correct match being retrieved in the top 100 is approximately 1 − (1 − p 1,100 ) k . Since the greatest number of admissible queries (591 individuals in the database have only a single set; these were not meaningful queries for performance evaluation), approximately 48%, has k = 1 this is a reasonable estimate to base the prediction on. <ref type="figure" target="#fig_8">Fig 7(a)</ref> reveals interesting insight into the performance of the proposed method. Specifically, note that unlike the empirical plot of the baseline, the empirical plot of the proposed method grows faster with the number of retrievable sets than the corresponding prediction. This means that the independence assumption underlying the prediction does not hold well, supporting the premise that quasi-transitivity of similarity can be used to improve the retrieval of sets poorly retrieved by the baseline by propagating information from similarly looking individuals or sets of the same person which are acquired in less challenging conditions. <ref type="figure" target="#fig_8">Lastly Fig 7(b)</ref> shows the average number of correct matches retrieved in the top 100 ranked sets as a function of the total number of target sets in the database which correctly match the query. As before the plots also show the corresponding predictions based on the methods' performances for queries in which only a single correct match is present in the entire database. Starting from n 1,100 the average number of correct matches retrieved in the top 100 ranked sets using queries where only a single correct match is possible, if different correct matches are ranked independently when k correct matches exist, the expected number of correct matches in the top 100 is approximately k × n 1,100 . The improvement effected by the proposed method is again consistent and significant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Summary and conclusions</head><p>We introduced a novel framework for improving the performance of retrieval algorithms on large and highly heterogeneous face sets acquired in uncontrolled conditions. In sharp contrast to the previous work, the proposed method learns to benefit from inter-personal similarity using what we term quasi-transitivity. A principled and carefully engineered framework performs learning automatically, with no human intervention whatsoever, making our approach readily employable on large data. Effectiveness was demonstrated on the notoriously challenging YouTube database.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>matching (same identity) query-target set pair, and (b) a non-matching (differing identities) query-target set pair.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>The cumulative distribution function (CDF) of the data energy contained in the 2nd and 3rd nonlinear kernel PCA components relative to the energy of the 1st component, across sets in the YouTube Faces Database. The variation within sets is strongly dominated by the 1st nonlinear principal component.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Conceptual illustration of our robust sample selection: (i) original exemplars are projected onto their 1st kernel principal component, (ii) uniform sampling between the extreme projections is performed in the 1D kernel space, and (iii) the obtained samples are re-projected into the original space (step not shown).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>CDF of the error introduced by our robust sample selection (10 samples were used) in the exemplar-based set method. evaluated baseline set representations, distances and their derivatives, and finally present and comment on the results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>CDF of the average normalized rank obtained using the exemplar-based (a,b) and subspace-based (c,d) methods. (a,c) Comparison of the respective baseline approach, the three simple quasi-transitivity estimation methods, and the proposed learnt quasi-transitivity. (b,d)Comparison of the respective baseline approach and the corresponding proposed method for different numbers of proxies.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 .</head><label>6</label><figDesc>Training data for the exemplar-based (a,b) and subspacebased (c, d) experiments, in the form of intra-class and inter-class transitivity features shown using parallel coordinates.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 .</head><label>7</label><figDesc>Rank-100: (a) probability of a correct match being retrieved, and (b) number of correct matches retrieved, vs. number of matches in the database.</figDesc></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Unfolding a face: from singular to manifold</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Arandjelović</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Asian Conference on Computer Vision</title>
		<meeting>Asian Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="203" to="213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Colour invariants under a non-linear photometric camera model and their application to face recognition from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Arandjelović</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2499" to="2509" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Computationally efficient application of the generic shape-illumination invariant to face recognition from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Arandjelović</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="92" to="103" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Gradient edge map features for frontal face recognition under extreme illumination changes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Arandjelović</surname></persName>
		</author>
		<idno type="DOI">10.5244/C.26.12</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. British Machine Vision Conference</title>
		<meeting>British Machine Vision Conference</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Making the most of the self-quotient image in face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Arandjelović</surname></persName>
		</author>
		<idno type="DOI">10.1109/FG.2013.6553708</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Automatic Face and Gesture Recognition</title>
		<meeting>IEEE International Conference on Automatic Face and Gesture Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Discriminative extended canonical correlation analysis for pattern set matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Arandjelović</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="page" from="353" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A framework for improving the performance of verification algorithms with a low false positive rate requirement and limited training data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Arandjelović</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<idno type="DOI">10.1109/BTAS.2014.6996275</idno>
		<title level="m">IEEE/IAPR International Joint Conference on Biometrics</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Hallucinating optimal high-dimensional subspaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Arandjelović</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2662" to="2672" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Automatic cast listing in feature-length films with anisotropic manifold space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Arandjelović</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1513" to="1520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Face set classification using maximally probable mutual modes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Arandjelović</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IAPR International Conference on Pattern Recognition</title>
		<meeting>IAPR International Conference on Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="511" to="514" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Achieving robust face recognition from video by combining a weak photometric model and a learnt generic face invariant</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Arandjelović</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="9" to="23" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Thermal and reflectance based personal identification methodology in challenging variable illuminations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Arandjelović</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">I</forename><surname>Hammoud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1801" to="1813" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Combined features for face recognition in surveillance conditions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Assaleh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Shanableh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Abuqaaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="503" to="514" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Names and faces in the news</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="848" to="854" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Face recognition using 2-D, 3-D, and infrared: is multimodal better than multisample?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bowyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE</title>
		<meeting>IEEE</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">94</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Similarity metric learning for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision</title>
		<meeting>IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2408" to="2415" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Face recognition by computers and humans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Phillips</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="46" to="55" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Talking pictures: Temporal grouping and dialog-supervised person recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sapp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nagle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Taskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Classification error rate for quantitative evaluation of content-based image retrieval systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IAPR International Conference on Pattern Recognition</title>
		<meeting>IAPR International Conference on Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="505" to="508" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Taking the bite out of automatic naming of characters in TV video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="545" to="559" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Identifying individuals in video by combining generative and discriminative head models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision</title>
		<meeting>IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">An introduction to ROC analysis. Pattern Recognition Letters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fawcett</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="861" to="874" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Consistency of kernel canonical correlation analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fukumizu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="361" to="383" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The pyramid match kernel: Discriminative classification with sets of image features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision</title>
		<meeting>IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1458" to="1465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Combining appearance and motion for face and gender recognition from videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hadid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pietikäinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2818" to="2827" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">A note on the maximization ofR 2 . The American Statistician</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Haitovsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1969" />
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="20" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Description of interest regions with local binary patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Heikkilä</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pietikäinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="425" to="436" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Face recognition with image sets using locally Grassmannian discriminant analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1461" to="1474" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Labeled faces in the wild: A database for studying face recognition in unconstrained environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007-10" />
			<pubPlace>Amherst</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Massachusetts</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report 07-49</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Acquiring linear subspaces for face recognition under variable lighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kriegman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="684" to="698" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Face-space-R: towards a unified account of face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">B</forename><surname>Lewis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Visual Cognition</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="29" to="69" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Grassmann registration manifolds for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">M</forename><surname>Lui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Beveridge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision</title>
		<meeting>European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="44" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Functions of positive and negative type and their connection with the theory of integral equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mercer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Philosophical Transactions of the Royal Society A</title>
		<imprint>
			<biblScope unit="volume">209</biblScope>
			<biblScope unit="page" from="415" to="446" />
			<date type="published" when="1909" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Probabilistic visual learning for object representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Moghaddam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pentland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="696" to="710" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Cosine similarity metric learning for face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">V</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Asian Conference on Computer Vision</title>
		<meeting>Asian Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="709" to="720" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A graph based approach for naming faces in news photos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ozkan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Duygulu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1477" to="1482" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Introduction to Modern Information Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Salton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Mcgill</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1983" />
			<publisher>McGraw Hill</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Learning with kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Advances in Kernel Methods -SV Learning, chapter Kernel principal component analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Müller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<publisher>MIT Press</publisher>
			<biblScope unit="page" from="327" to="352" />
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Person spotting: video shot retrieval for face sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Image and Video Retrieval</title>
		<meeting>IEEE International Conference on Image and Video Retrieval</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="226" to="236" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">The Nature of Statistical Learning Theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
			<publisher>Springer-Verlag</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Robust hausdorff distance measure for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Vivek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sudha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="431" to="442" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Locality-constrained linear coding for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="3360" to="3367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Manifold-manifold distance with application to face recognition based on image set</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Face recognition in unconstrained videos with matched background similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Maoz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="529" to="534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Detecting faces in images: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kriegman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="34" to="58" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">An associate-predict model for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="497" to="504" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Face recognition: A literature survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rosenfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="399" to="458" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
