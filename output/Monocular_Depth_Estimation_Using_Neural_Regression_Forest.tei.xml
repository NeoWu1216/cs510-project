<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Monocular Depth Estimation Using Neural Regression Forest</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirban</forename><surname>Roy</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Oregon State University Corvallis</orgName>
								<orgName type="institution" key="instit2">Oregon State University Corvallis</orgName>
								<address>
									<postCode>97331, 97331</postCode>
									<region>OR, OR</region>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinisa</forename><surname>Todorovic</surname></persName>
							<email>sinisa@eecs.oregonstate.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Oregon State University Corvallis</orgName>
								<orgName type="institution" key="instit2">Oregon State University Corvallis</orgName>
								<address>
									<postCode>97331, 97331</postCode>
									<region>OR, OR</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Monocular Depth Estimation Using Neural Regression Forest</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:43+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents a novel deep architecture, called neural regression forest (NRF), for depth estimation from a single image. NRF combines random forests and convolutional neural networks (CNNs). Scanning windows extracted from the image represent samples which are passed down the trees of NRF for predicting their depth. At every tree node, the sample is filtered with a CNN associated with that node. Results of the convolutional filtering are passed to left and right children nodes, i.e., corresponding CNNs, with a Bernoulli probability, until the leaves, where depth estimations are made. CNNs at every node are designed to have fewer parameters than seen in recent work, but their stacked processing along a path in the tree effectively amounts to a deeper CNN. NRF allows for parallelizable training of all "shallow" CNNs, and efficient enforcing of smoothness in depth estimation results. Our evaluation on the benchmark Make3D and NYUv2 datasets demonstrates that NRF outperforms the state of the art, and gracefully handles gradually decreasing training datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>This paper address one of the basic vision problems, that of estimating depth from a single monocular image. Our goal is to predict the continuous depth values of every pixel. The image may show a natural outdoor scene with continuous depth values ranging from a few to 80 meters, or an indoor cluttered living space with continuous depths ranging between 0 and 10 meters, as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>.</p><p>This problem is challenging, since there may be many distinct 3D scenes depicted in the same 2D image. Also, histograms of depths in typical outdoor and indoor scenes are "peaky" <ref type="figure" target="#fig_0">(Fig. 1)</ref>. Hence, it is very likely that a training set may under-represent or simply not have examples of certain depth values, which could appear in test data.</p><p>To address these challenges, prior work typically uses graphical models for enforcing smoothness and taking into account spatial context in depth estimation <ref type="bibr">[21, 22, 1, 14,</ref>   <ref type="bibr" target="#b22">[22]</ref> (left), and NYUv2 dataset <ref type="bibr" target="#b18">[18]</ref> (right). Depth values are in meters. <ref type="bibr" target="#b12">12</ref>]. More recent work resorts to convolutional neural networks (CNNs) <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b15">15]</ref>. CNNs are appealing as they can efficiently incorporate multiscale contextual cues, in a feedforward manner. However, one of the major bottlenecks of using CNNs for depth estimation is that their training typically requires big data. As <ref type="figure" target="#fig_0">Fig. 1</ref> illustrates, both indoor and outdoor scenes may not have a sufficient number of examples of certain depth values, which makes the training of CNNs challenging.</p><p>In this paper, we present a novel deep architecture, called neural regression forest (NRF), for monocular depth estimation. As shown in <ref type="figure">Fig. 2</ref>, NRF combines CNNs with Regression Forest <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b2">3]</ref> for predicting depths in the continuous domain via regression. Robustness is achieved by processing a data sample with an ensemble of binary regression trees, which we call Convolutional Regression Trees (CRTs). We fuse individual regression results of every CRT into a final depth estimation.</p><p>Our approach scans a window x across every pixel location, and passes x down the CRTs for predicting the depth of the scanning-window's center. At every node of CRT, x is filtered with a CNN associated with that node, and then passed to left and right children nodes (i.e., CNNs) with a Bernoulli probability for further convolutional processing, until x reaches leaves of the tree. The probability that x reaches a particular leaf is equal to the product of all Bernoulli probabilities along the data's path from the root to that leaf. The sample undergoes the same procedure in other CRTs of NRF. Finally, depth estimations made in every leaf are weighted with the corresponding path probabilities for <ref type="bibr">Figure 2</ref>. Neural Regression Forest. (a) A CNN is associated with every node of a binary Convolutional Regression Tree (CRT) for performing the convolutional processing of data samples. The CNN's output is passed to the left and right children nodes with a Bernoulli probability. (b) While our CNNs process data samples as they pass down the CRT, the related deep architecture of <ref type="bibr" target="#b11">[11]</ref> uses a single deep CNN to fully process the data before passing them through a decision tree.</p><formula xml:id="formula_0">estimating depth d of x.</formula><p>NRF has a number of advantages. First, each CRT node solves a binary problem, instead of a multiclass or regression problem. This allows for a robust training of our CNNs, since the left or right routing of data samples may combine a range of depth values, and thus compensate for some underrepresented (or missing) depth examples in the training data. Second, training of all CNNs in CRT is parallelizable. In particular, once a training sample reaches leaf nodes, we can readily compute the regression loss for every node of CRT, and then use these loss functions for simultaneously training the corresponding CNNs. Third, we design CNNs at every node of CRT to have significantly fewer parameters than seen in recent work. Specifically, we use "shallow" architectures with only a few convolutional (1-2) and fully connected levels <ref type="bibr" target="#b0">(1)</ref><ref type="bibr" target="#b1">(2)</ref>. This, in turn, allows for robust training on smaller datasets, as necessary in our application domain <ref type="figure" target="#fig_0">(Fig. 1)</ref>. It is note worthy that results of convolutional filtering at a node are used as input to its children. Hence, all CNN computations along a path from the root to the leaf amount to processing the data sample with a relatively deep CNN. Therefore, despite using "shallow" CNNs at each CRT node, we expect to have the usual benefits of deep processing of data as in related work which advocates the use of deeper architectures.</p><p>In our approach, we also consider more explicit ways of addressing the small number of training examples of certain depths. As illustrated in <ref type="figure">Fig. 3</ref>, depths in a neighborhood of scanning window x i are often smooth. Therefore, for a more reliable depth prediction of x i , we also consider depth estimates of neighboring scanning windows x j . Neighborhood relationships for all pairs of pixels (i, j) are efficiently <ref type="bibr">Figure 3</ref>. The window at pixel i and a window at neighboring pixel j have similar depths, but are different in appearance. Therefore, they could reach distinct NRF leaf nodes with different probabilities. The final depth prediction for the ith pixel is made by combining the Gaussian at the leaf corresponding to pixel i and also the Gaussian at the leaf corresponding to the neighboring pixel j. NRF allows us to effectively increase the (small) number of training samples by considering the pairs of samples and thus improve depth prediction of window xi by additionally considering the NRF leaf reached by neighboring window xj. estimated using the standard bi-lateral filtering <ref type="bibr" target="#b25">[25]</ref>.</p><p>A comparison with the state of the art on the Make3D <ref type="bibr" target="#b22">[22]</ref> and NYU v2 <ref type="bibr" target="#b18">[18]</ref> datasets demonstrates our superior performance in terms of relative error, root mean squared error, and log-10 error. Also, our results show that NRF gracefully handles gradually decreasing training datasets.</p><p>In the following, Sec. 2 reviews prior work, Sec. 3 specifies our NRF, Sec. 4 explains how we enforce smoothness in depth predictions, Sec. 5 derives expressions for learning CNN and CRT leaf parameters, Sec. 6 specifies details of our CNN architecture, and Sec. 7 presents our results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>The section reviews related work on monocular depth estimation, and combining CNNs and Random Forests.</p><p>Depth Estimation. Monocular depth estimation is a long-standing problem <ref type="bibr" target="#b21">[21,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b14">14,</ref><ref type="bibr" target="#b12">12,</ref><ref type="bibr" target="#b7">7,</ref><ref type="bibr" target="#b6">6,</ref><ref type="bibr" target="#b15">15]</ref>. A single CNN has been used for depth estimation <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b15">15]</ref>; however, at a coarser resolution than that of the input image (e.g., 1/4 of the original resolution <ref type="bibr" target="#b6">[6]</ref>). Depth estimation in the continuous domain has also been considered <ref type="bibr" target="#b24">[24,</ref><ref type="bibr" target="#b9">9,</ref><ref type="bibr" target="#b27">27]</ref>. However, these approaches use a pre-constructed 3D shape database of known objects <ref type="bibr" target="#b24">[24]</ref>, or rely on class-specific object keypoints and object segmentations <ref type="bibr" target="#b9">[9]</ref>, whereas our method is aimed at general scenes.</p><p>Random Forests <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b23">23]</ref> have a long-track record in computer vision. Recent work integrates decision trees with multilayer perceptrons for scene labeling <ref type="bibr">[20]</ref>. Each node of a decision tree represents a multilayer perceptron aimed at learning the node's split function given hand-crafted image features. Instead, we use a tree of CNNs to jointly learn the feature representation and split functions for each node in the tree. A single deep CNN has been combined with a random forest for image classification <ref type="bibr" target="#b11">[11]</ref>. Outputs of the top CNN layer are considered as nodes of the decision tree. Prediction loss is computed at each split node of the tree and back-propagated to the network for learning CNN parameters. In contrast, we learn a set of distinct "shallow" CNNs in every node of the decision tree. Importantly, we do not back-propagate the loss of depth prediction bottom-up as in <ref type="bibr">[20]</ref>. Rather, we compute distinct loss for every CNN in the tree, and then use these losses for parallel training of all CNNs in the tree.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Neural Regression Forest</head><p>We consider a regression problem, where for each scanning window of the image, x ∈ X , there is a real-valued depth d ∈ D = [0, d max ]. As the regression model, we use NRF that consists of a set of CRTs, F = {T i }. Each CRT represents a weak regressor. Depth prediction is made by averaging the predictions over the trees as</p><formula xml:id="formula_1">p F (d|x) = 1 |F| T ∈F p T (d|x),<label>(1)</label></formula><p>where p F (·) and p T (·) denote probability distributions. Each T consists of a set of non-leaf decision nodes (also called split nodes), V T = {v}, and a set of leaf nodes for prediction L T = {l}. Each v ∈ V T represents a CNN with parameters w v . The parameters of all CNNs in T form a set W T = {w v : v ∈ V T }. Note that CNNs are not associated with leaf nodes. The architecture of CNNs is explained in Sec. <ref type="bibr" target="#b6">6</ref>.</p><p>A split function f v (x, w v ) : X → [0, 1] is defined for each split node of T . This function is computed by vth CNN with a single output node, which produces the value of the split function through a sigmoid function. Instead of deterministic, we use a stochastic split function, as in <ref type="bibr" target="#b11">[11]</ref>. Specifically, f v (x, w v ) returns the Bernoulli probability of directing x to the left or right child node. In this way, x is passed through T reaching every leaf l ∈ L T with the corresponding probability P (l|x, W T ), where l∈L T P (l|x, W T ) = 1. Following <ref type="bibr" target="#b11">[11]</ref>, we define</p><formula xml:id="formula_2">P (l|x, W T ) = v∈V T f v (x; w v ) L(l,v) (1 − f v (x; w v ) R(l,v) ,<label>(2)</label></formula><p>where L(l, v) and R(l, v) indicate whether l belongs to the left and right subtree of v, respectively. Note that the product is computed by considering all the split nodes in the tree. For split nodes v which do not lie on the path to l, we have L(l, v) = R(l, v) = 0. Therefore, the product in (2) effectively computes the probability of the sample's path from the root to the leaf l.</p><p>Each leaf l ∈ L T holds a Gaussian probability distribution, p(d; θ l ), with parameters θ l = (µ l , σ l ), over depth values D. The set of all Gaussian parameters, Θ T = {θ l : l ∈ L T }, are computed from training samples that reached leaf nodes of T during training.</p><p>When x reaches the leaves in T , prediction for its depth is given by</p><formula xml:id="formula_3">p T (d|x) = l∈L T p(d; θ l )P (l|x, W T ).</formula><p>(3)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Enforcing Smoothness in Depth Predictions</head><p>NRF allows an efficient way of enforcing smoothness in pixel depth predictions across the image. As mentioned in Sec. 1, and illustrated in <ref type="figure">Fig. 3</ref>, neighboring pixels in the image at similar depths, i and j, may define image windows x i and x j with significantly different appearance. When passing x i and x j through T , the convolutional neural pro-</p><formula xml:id="formula_4">cessing in every split node v of T may estimate different values of split functions f v (x i , w v ) and f v (x j , w v ), result- ing in different probabilities of x i and x j reaching a partic- ular leaf l ∈ L.</formula><p>To enforce smoothness in depth predictions, we modify the depth likelihood of every x i given by <ref type="bibr" target="#b2">(3)</ref>. The modification is based on the depth likelihoods of neighboring samples x j , j ∈ N(i), and appearance similarity between x i and x j estimated by bi-lateral filtering <ref type="bibr" target="#b25">[25]</ref>. Thus, when x i reaches the leaves in T , we compute the modified probability distribution of depth d of x i as</p><formula xml:id="formula_5">p T (d|x i ) = j∈N(i) κ ij p T (d|x j ),<label>(4)</label></formula><p>where i's neighborhood N(i) is defined to also include i, i ∈ N(i). κ ij is the weight of bi-lateral filtering, estimated based on the Euclidean distance between the HSV color histograms of windows x i and x j and the Euclidean distance between locations of i and j in the image as</p><formula xml:id="formula_6">κ ij ∝ exp(− z i − z j 2 σ 2 z ) exp(− h i − h j 2 σ 2 h )<label>(5)</label></formula><p>where z i = (x i , y i ) is the pixel location in the image, h i is the HSV color histogram of window x i , and σ z = 3 and σ h = 30 control sensitivity <ref type="bibr" target="#b25">[25]</ref>. Note that for j = i, the weight κ ii has the largest value. We normalize the weights such that j∈N(i) κ ij = 1.</p><p>The effect of using bi-lateral filtering in (4) is that only neighboring pixels of i with high color similarity with i contribute to the depth prediction of i. This means that our bi-lateral filtering is not a post-processing step. Rather, it is integrated with our training and inference. Note that bilateral filtering can be done only once per image. Hence, the above modification of computingp T (d|x i , W, Θ), given by (4), minimally increases our complexity. We use an off-theshelf linear-time implementation for the bi-lateral filtering <ref type="bibr" target="#b19">[19]</ref>. Our results show that this modification in (4) improves depth estimation, producing smoother depth maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Learning</head><p>Our learning of each T estimates the following parameters: 1) Θ -the set of Gaussian parameters in the leaf nodes, and 2) W -the set of CNN parameters of all the split nodes. For simplicity, below, we drop the explicit reference to T in our notation. This learning differs from the traditional supervised setting, because there is no ground truth for directing a training sample to the left or right in the tree. Thus, we learn (Θ, W) by maximizing the log-likelihood of the training data, as in <ref type="bibr">[20,</ref><ref type="bibr" target="#b11">11]</ref>. Given a training set S ⊂ X × D, we define the empirical loss as</p><formula xml:id="formula_7">R(W, Θ; S) = 1 |S| (x,d)∈S L(W, Θ; x, d)<label>(6)</label></formula><p>where L(W, Θ; x, d) is the negative log-likelihood of the depth prediction for x</p><formula xml:id="formula_8">L(W, Θ; x, d) = − log p(d|x),<label>(7)</label></formula><p>where p(d|x) is given by <ref type="formula">(3)</ref>. Note that we also specify an alternative training, when smoothing is enforced in depth estimation. For this training, we modify the definition of the empirical loss toR(W, Θ; S) specified in terms of the negative loglikelihoods of the smoothed depth predictions</p><formula xml:id="formula_9">L(W, Θ; x, d) = − logp(d|x),<label>(8)</label></formula><p>wherep(d|x) is given by <ref type="bibr" target="#b3">(4)</ref>. The learning objective is defined as</p><formula xml:id="formula_10">(W * , Θ * ) = arg min W,Θ R(W, Θ; S),<label>(9)</label></formula><p>and similarly for the case when we useR(W, Θ; S). To solve (9), we alternate the following two steps. In the first step, we fix W, and optimize for Θ, and in the second step, we fix Θ and optimize for W. Learning iterations end after convergence, or when they reach a maximum number of iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Learning CCN Parameters for the Split Nodes</head><p>In this section we describe how to learn the CNN parameters for the split nodes v ∈ V, w v . We compute the gradient of the log-loss of the tree, L(W, Θ; x, d) given by <ref type="formula" target="#formula_8">(7)</ref>, with respect to w v . The gradient is first computed for the final output node of the CNN and then passed to the lower layers of the CNN using the standard backpropagation <ref type="bibr" target="#b13">[13]</ref>. The gradient at the final output node is computed as</p><formula xml:id="formula_11">∂ L(W, Θ; x, d) ∂w v = ∂ L(W, Θ; x, d) ∂f v (x, w v ) f v (x, w v ) ∂w v ,<label>(10)</label></formula><p>where only the first term depends on the tree, and the second term can be computed as the standard derivative of the sigmoid function. The first term can be computed as</p><formula xml:id="formula_12">∂ L(x, d) ∂f v (x) = f v (x) p Lv r (d|x) p T (d|x) + (1 − f v (x)) p Lv l (d|x) p T (d|x) ,<label>(11)</label></formula><p>where L(x, d) and f v (x) are the shorthand notation for L(W, Θ; x, d) and f v (x, w v ), respectively; p T (d|x) is given by <ref type="formula">(3)</ref>, and p Lv r (d|x) and p Lv l (d|x) are analogous to p T (d|x) but defined for the sub-tree rooted at the right and left child of v, v l and v r , respectively: p Lv r (d|x) = l∈Lv r p(d; θ l )P (l|x, W) and p Lv l (d|x) = l∈Lv l p(d; θ l )P (l|x, W). Intuitively, the gradient of a split node is computed by combining the gradients of all the nodes in the subtree rooted at that split node. Note that this gradient computation can be easily extended for the loss function in (8) as the probability distribution in (4) is just a weighted sum of the distributions defined in <ref type="formula">(3)</ref>.</p><p>From <ref type="bibr" target="#b11">(11)</ref>, the gradient ∂L(W,Θ;x,d) ∂fv(x,wv) at v can be recursively computed bottom-up by combining the loss gradients from v's children v l and v r . Thus, our gradient computation starts at the leaf nodes, and gets propagated to the root in a bottom up fashion. The gradient of loss at a leaf node l ∈ L is computed as</p><formula xml:id="formula_13">p(d; θ l )P (l|x, W) p T (d|x) .<label>(12)</label></formula><p>After computing ∂L(W,Θ;x,d) ∂fv(x,wv) at every node v, we proceed with simultaneous training of all CNNs in T .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Learning the Leaf Distribution</head><p>For each leaf, we estimate the probability of the training samples reaching that node P (l|x, W). This probability represents a weight (i.e., relative significance) of training samples collected in the leaf. We use these weights to fit a Gaussian distribution on the weighted depth values of training samples that reached the leaf. Specifically, we follow the <ref type="bibr" target="#b26">[26]</ref> to empirically estimate the Gaussian parameters θ l = (µ l , σ l ) for the leaf as</p><formula xml:id="formula_14">µ l = 1 (xi,di)∈S P (l|x, W) (xi,di)∈S P (l|x i , W) · d i σ l = 1 (xi,di)∈S P (l|x i , W) (xi,di)∈S P (l|x i , W)(d i − µ l ) 2 ,<label>(13)</label></formula><p>where S is the set of training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">CNN Architecture</head><p>We observe that as data samples are passed down the tree, the probability of data reaching a split node decreases. Moreover, the estimated depth distribution becomes more reliable as the data samples are becoming increasingly sorted by passing down the tree. Thus, we expect that it becomes easier to learn the split functions as we go down the tree <ref type="bibr">[20]</ref>.</p><p>To address this observation, we adjust the complexity of the CNN architecture along the tree height. The CNN architecture is determined by the number of: 1) convolution + pooling layers, and 2) fully connected perceptron layers. For the top one third of the tree height, we use CNNs with 2 convolution + pooling layers, and 2 fully connected perceptron layers. For the lower one third of the tree height (closer to the leaf nodes), we use CNNs with 2 convolution + pooling layers and 1 fully connected perceptron layer. Finally, for the bottom third of the tree height, we use CNNs with 1 convolution + pooling layer and 1 connected perceptron layer. We experimentally found this architecture suitable for depth prediction. We present the overview of the tree architecture in the <ref type="figure">Fig. 2</ref>. In our experiments, we use NRF with 100 binary trees, each with height 10.</p><p>A data sample is defined as a pair of an image window at a pixel location and its corresponding depth label. The feature of the window is computed by CNN. In our experiments, we consider two window sizes: 100 × 100 and 150×150 for the Make3D and NYUv2 datasets based on the size and complexity of the dataset. This window around the pixel is used to capture context around the pixel. The convolutional outputs of the CNN at a split node are used as the input window of the CNNs at the children split nodes. Thus an input window passes through a series of convolutions along the path from root to the leaf node. <ref type="figure">Fig. 2a</ref> shows that the input window size decreases for the CNNs at split nodes closer to the leaves. This is due to the convolution and pooling that reduce the size of the input window. This allows us to learn multi-scale features through the CNNs in the split nodes along the data path trough the tree. Multi-scale features are important for depth estimation <ref type="bibr" target="#b22">[22,</ref><ref type="bibr" target="#b21">21]</ref>.</p><p>Unlike [20], we provide only raw color values around the pixel and use convolution kernels to automatically learn the features required to compute the split function. Our tree architecture effectively resembles a deep CNN framework which consists of a set of small CNNs. Our architecture is different from the neural decision forest architecture presented in <ref type="bibr" target="#b11">[11]</ref>, where a decision forest is used on top of a single deep CNN. In this case all the split nodes correspond to the same output layer of the CNN. We use the output of the parent split node as the input for the CNNs at the child split node.</p><p>We use a modified ImageNet initialization of convolution layers by sampling image patches. In our experiments, multiple random initializations produced statistically similar results to those reported in the Sec. 7. Note that the training and inference of the trees can be done in parallel. Inference takes less than 1sec per image and training takes 8-10 hours in our parallel implementation on a standard PC with a nvidia Tesla k80 graphics card.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Results</head><p>Datasets. We evaluate our approach on two benchmark datasets: the Make3D <ref type="bibr" target="#b22">[22]</ref> and the NYUv2 <ref type="bibr" target="#b18">[18]</ref>. These datasets are commonly used by the state of the art approaches for evaluating the depth estimation performance. The Make3D dataset consists of 534 images of outdoor scenes, where 400 images are used for training and 134 images are used for testing. Depth is estimated by a laser scanner and the depth ranges from 0 to 80 meters. The NYUv2 dataset consists of 1449 indoor images where the depth is estimated by a Kinect device. On this dataset, we follow the standard split of 795 training images and 654 test images. The depth ranges from 0 to 10 meters for this dataset.</p><p>Performance metrics. We use three standard error metrics which are commonly used by the state of the art methods <ref type="bibr" target="#b21">[21,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b14">14,</ref><ref type="bibr" target="#b6">6]</ref> to estimate the accuracy of monocular depth prediction. These errors are defined for each pixel and averaged across all the pixels in an image and all images in a dataset. Lets assume d * andd are the ground-truth and predicted depth for a pixel then the errors are defined as 1) relative error (rel): |d * −d|/d * , 2) root mean squared error (rms):</p><p>(d * −d) 2 , and 3) log 10 error (log 10):</p><formula xml:id="formula_15">| log d * − logd|.</formula><p>Baselines. We propose the following baselines and the comparisons to the baselines are presented in Tab. 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CNN -regression forest (CNN-RF):</head><p>In this baseline, we use a similar framework proposed by <ref type="bibr" target="#b11">[11]</ref>. A single CNN is learned for the forest, and its outputs are treated as the split node for the regression trees of the forest. Thus, split functions in every node share the same CNN parameters through a single deep network. We consider same input windows for the pixels as we use for out experiments. NRF outperforms this baseline which suggests that a set of small CNN performs better than a large network for the small datasets. NRF without input forwarding for CNNs (NRFw/oF): In this baseline, for each CNN in the split node, we use only RGB input window instead of convolutional outputs from the parent split node. As suggested by <ref type="bibr">[20]</ref>, the size of input windows for the split nodes is gradually reduced as we go down the tree along its depth. NRF outperforms this baseline, which suggests forwarding the convolutional outputs as the inputs to the lower split nodes can effectively model a better feature learning framework then considering RGB inputs to each split node. NRF without the neighborhood information (NRFw/oN): In this baseline, we predict the depth of a pixel only based on its appearance without considering the information of its neighboring pixels. Specifically, we use (3) instead of (4) for the depth prediction in a tree. In our full approach, for each pixel, we consider 10 neighboring windows for capturing the neighborhood depth information. The results are shown in the Tab. 1. This proves that the neighboring pixels provide important information for depth prediction. Moreover, considering neighboring information results in smoother depth maps ( <ref type="figure">Fig. 5 and 6)</ref>.</p><p>Comparison with state of the art: A comparison with the state of the art is presented in Tab. 2. For fair comparison, we follow the same experimental setup, and use the same standard metrics as done by the state-of-the-art methods. We achieve better performance on both datasets in terms of standard error metrics.</p><p>Robustness against the amount of training data: To evaluate the sensitivity of NRF against the amount of training, we simulate the behavior of our approach on both datasets by gradually reducing the training data. Instead of using the complete training set, we randomly sample a fraction of training data for training. The plots in <ref type="figure" target="#fig_1">Fig. 4</ref> show that the variation of the error values ('rel' and 'log 10') while the amount of training data is gradually reduced. Note that both error metrics increase slowly as the amount of training data is gradually reduced. We believe that this is due to our ensemble architecture of the forest, and accounting for the smoothness of neighboring depths.</p><p>Qualitative results: We present the qualitative results on the Make3D and the NYUv2 datasets in <ref type="figure">Fig. 5</ref> and <ref type="figure">Fig. 6</ref> respectively. We compare our approach with the baseline  which does not consider smoothness is depths of neighboring pixels. We observe that accounting for smoothness produces more accurate depth maps. Recall that the neighborhoods are estimated by bi-lateral filtering. Since the bilateral filter is able to preserve edges, the resulting depth maps tend to respect object boundaries.</p><p>Failure case: In <ref type="figure" target="#fig_2">Fig. 7</ref>, we present a case where our approach fails to produce an accurate depth map for an image in the NYUv2 dataset. As our depth prediction is guided by appearance, regions with high and random texture might create confusion in our prediction. Note that the image is highly textured and cluttered with many different objects. This causes error as shown in <ref type="figure">Fig. 5</ref> and <ref type="figure">Fig. 6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Conclusion</head><p>We have formulated NRF for the problem of monocular depth estimation. A "shallow" CNN with maximum two convolutional layers is associated with every node in the regression trees of NRF. The convolutional output of a CNN is used as input to the left and right children CNNs in the tree, effectively allowing for deep convolutional processing of data samples. In this way, scanning windows <ref type="bibr">Figure 5</ref>. Qualitative results on the Make3D dataset. Note that accounting for smoothness in a neighborhood results in more accurate depth maps which better respect object boundaries. <ref type="figure">Figure 6</ref>. Qualitative results on the NYUv2 dataset. Note that accounting for smoothness in a neighborhood results in more accurate depth maps which better respect object boundaries. extracted from the image are passed down the tree, until the leaves representing weak depth estimators. We have additionally accounted for smoothness in depths across pixel neighborhoods, which in turn are estimated using bilateral filtering. We have evaluated our approach on two benchmark datasets: Make3D and NYUv2. Both datasets provide a relatively small and unbalanced set of training depth examples. Results demonstrate that NRF is able to robustly address these challenges, and outperform the state of the art, due to: (a) its ensemble architecture consisting of the forest of trees, (b) "shallow" CNN architecture with significantly few parameters than seen in recent work, and (c) accounting for smoothness in depths. Also, our experiments show that NRF's performance gracefully downgrades when the size of the training dataset gradually decreases.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Depth histograms extracted from two benchmark datasets: Make3D dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>Variation of 'rel' and 'log 10' errors with respect to the amount of training data on Make3D and NYUv2 datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 7 .</head><label>7</label><figDesc>A failure case for an image from the NYUv2 dataset with highly textured regions and object clutter.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>Table 2. Comparison with the state of the art on Make3D and NYU v2 datasets.</figDesc><table>Make3D 

NYU v2 
rel 
log10 
rms 
rel 
log10 
rms 
CNN-RF 
0.361 
0.148 
15.10 
0.35 
0.131 
1.2 
NRFw/oF 
0.312 
0.128 
13.8 
0.24 
0.09 
0.95 
NRFw/oN 
0.29 
0.126 
13.7 
.22 
0.088 
0.936 
Ours 
0.26 
0.119 
12.40 
0.187 
0.078 
0.744 

Table 1. 

Comparison with the baselines on Make3D and NYUv2 
datasets. 

Make3D 
NYU v2 
rel 
log10 
rms 
rel 
log10 
rms 
[22] 
0.370 
0.187 
-
0.349 
-
1.214 
[1] 
0.362 
0.168 
15.8 
-
-
-
[16] 
0.338 
0.134 
12.60 
0.335 
0.127 
1.06 
[10] 
0.361 
0.148 
15.10 
0.35 
0.131 
1.2 
[12] 
0.364 
0.148 
-
-
-
-
[14] 
0.379 
0.148 
-
-
-
-
[6] 
-
-
-
0.215 
-
0.907 
[15] 
0.307 
0.125 
12.89 
0.230 
0.095 
0.824 
[27] 
-
-
-
0.305 
0.122 
1.04 
Ours 
0.26 
0.119 
12.40 
0.187 
0.078 
0.744 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>This work was supported in part by grant NSF RI 1302700. We would like to thank Jose Roberto Alvarez from Huawei Media Technologies Lab, CA and Bennett Wilburn for fruitful discussions and feedback. A part of the work was developed during Roy Anirban's internship in Huawei Media Technologies Lab, CA.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning the right model: Efficient max-margin learning in laplacian CRFs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Image classification using random forests and ferns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bosch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Munoz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision, 2007. ICCV 2007. IEEE 11th International Conference on</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Random forests. Machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="5" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Segmentation and recognition using structure from motion point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fauqueur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2008</title>
		<imprint>
			<biblScope unit="page" from="44" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Decision forests for computer vision and medical image analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>Springer Science &amp; Business Media</publisher>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Depth map prediction from a single image using a multi-scale deep network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning to be a depth camera for close-range human capture and interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Fanello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Keskin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Izadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sweeney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">B</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Paek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">86</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning hierarchical features for scene labeling. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Farabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Najman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1915" to="1929" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Categoryspecific object reconstruction from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tulsiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Depth transfer: Depth extraction from video using non-parametric sampling. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Karsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">B</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2144" to="2158" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep neural decision forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kontschieder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fiterau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Rota</forename><surname>Bulo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">HC-search for structured prediction in computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Doppa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Todorovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">G</forename><surname>Dietterich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Gradientbased learning applied to document recognition. Proceedings of the IEEE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Single image depth estimation from predicted semantic labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Deep convolutional neural fields for depth estimation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Discrete-continuous depth estimation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="716" to="723" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Indoor segmentation and support inference from RGBD images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">K</forename><surname>Nathan Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
	<note>CVPR 2008. IEEE Conference on</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Neural decision forests for semantic image labelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bulo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kontschieder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="81" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning depth from single monocular images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="1161" to="1168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Make3d: Learning 3D scene structure from a single still image. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Efficient human pose estimation from single depth images. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fitzgibbon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sharp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Finocchio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kipman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2821" to="2840" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Estimating image depth using shape collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">37</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Bilateral filtering for gray and color images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tomasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Manduchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sixth International Conference on</title>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note>Computer Vision</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Locally weighted full covariance gaussian density estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Technical report 1240</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Indoor scene structure analysis for single image depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
