<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">From Keyframes to Key Objects: Video Summarization by Representative Object Proposal Selection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Meng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electrical and Electronic Engineering</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongxing</forename><surname>Wang</surname></persName>
							<email>ihxwang@cqu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Electrical and Electronic Engineering</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Software Engineering</orgName>
								<orgName type="institution">Chongqing University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electrical and Electronic Engineering</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yap-Peng</forename><surname>Tan</surname></persName>
							<email>eyptan@ntu.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="department">School of Electrical and Electronic Engineering</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">From Keyframes to Key Objects: Video Summarization by Representative Object Proposal Selection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:40+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose to summarize a video into a few key objects by selecting representative object proposals generated from video frames. This representative selection problem is formulated as a sparse dictionary selection problem, i.e., choosing a few representatives object proposals to reconstruct the whole proposal pool. Compared with existing sparse dictionary selection based representative selection methods, our new formulation can incorporate object proposal priors and locality prior in the feature space when selecting representatives. Consequently it can better locate key objects and suppress outlier proposals. We convert the optimization problem into a proximal gradient problem and solve it by the fast iterative shrinkage thresholding algorithm (FISTA). Experiments on synthetic data and real benchmark datasets show promising results of our key object summarization apporach in video content mining and search. Comparisons with existing representative selection approaches such as K-mediod, sparse dictionary selection and density based selection validate that our formulation can better capture the key video objects despite appearance variations, cluttered backgrounds and camera motions.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>With videos becoming the biggest big data, there has been increasing need to summarize, index and browse the large corpus of video content. As a common practice, videos are often summarized by keyframes, i.e., a set of representative video frames <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21]</ref>. Although such a keyframe-based summarization can capture the important scenes, it often does not pick out the key objects from less informative backgrounds in a video.</p><p>In this work, we propose to summarize videos into key objects instead of keyframes, as illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>. Comparing with keyframes, summarizing videos into a collection of key objects can be attractive to many applications. For example, the summarized key objects can serve as icons to establish a quick impression of the video by telling what are there. They also provide a small footprint for indexing, browsing and search, e.g., retrieving videos by matching the key objects. Besides object-level summarization and search, as these key objects are essential components of higher level semantics in videos, once identified, they can also be used to recover or help understand more complicated semantics of videos, e.g., tracking candidate objects for spatio-temporal action localization <ref type="bibr" target="#b36">[37]</ref>, constructing story based video summarization by analyzing the interactions among the key objects <ref type="bibr" target="#b25">[26]</ref> and egoncentric analysis <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b23">24]</ref>.</p><p>Motivated by the recent successes of category independent object proposals for detection and weekly supervised learning <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b6">7]</ref>, we propose to summarize videos by object proposals. It has been shown that generating multiple overlapping object proposals in general provides more accurate object regions than segmentation by sidestepping the harder problem of full segmentation <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b0">1]</ref>. Start with a pool of frame-level object proposals produced by a high recall object proposal method <ref type="bibr" target="#b46">[47]</ref>, we formulate video sum-marization as a representative selection problem: selecting a few exemplar object proposals to reconstruct the whole pool. Although representative selection methods have been applied to video keyframe selection <ref type="bibr" target="#b10">[11]</ref>, directly applying them to object-level summarization faces many challenges. First of all, the appearance of the same object may change significantly across frames due to pose and scale variations, partial occlusions, etc. Therefore the key objects do not necessary locate at the densest regions in the feature space. Consequently, classic density based representative selection method may not work well <ref type="bibr" target="#b30">[31]</ref>. Moreover, since object proposals are just candidates of key objects, there may be many irrelevant and noisy proposals in the proposal pool. These outliers may significantly affect the representative selection methods based on sparse reconstruction <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b10">11]</ref>. In such a case, even a further filtering of outliers as postprocessing <ref type="bibr" target="#b10">[11]</ref> may be less effective if most representatives are outliers. Without prior knowledge of the object, it is difficult to locate the key objects accurately.</p><p>To address the above challenges, we propose a new formulation of sparse reconstruction based representative selection, which has the following advantages. First, it can incorporate object proposal priors when selecting the representatives. Therefore, object proposals of high prior weights, e.g., high objectiveness scores, are more likely to be selected as key objects, while background clutters of low weights can be suppressed. Second, our new formulation also considers the local affinity structure of the data samples by introducing a locality prior matrix to regularize the selection matrix. As the outcome, it prefers popular object proposals that appear more frequently while outlier proposals are likely to be suppressed. Third, although complex constraints are introduced, we convert our optimization into a proximal gradient problem and solve it by the fast iterative shrinkage thresholding algorithm (FISTA). As is well known, FISTA has a fast convergence rate, which is O(1/m 2 ) in m iterations <ref type="bibr" target="#b2">[3]</ref>.</p><p>We evaluate our proposed method on both synthetic data and two benchmark video datasets in comparison with existing representative selection approaches such as Kmediod, sparse diction selection <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b5">6]</ref>, and density based selection <ref type="bibr" target="#b30">[31]</ref>. The favourable results validate that our formulation fits better to the key video object summarization problem, and the selected proposals can better capture key video objects despite object appearance variations, background clutters and camera motions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Object-driven Video Summarization. Visual summaries of a video can take many forms such as keyframes <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21]</ref>, skims <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b4">5]</ref>, montages <ref type="bibr" target="#b33">[34]</ref> and dynamic synopses <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30]</ref>. Recently there has been increasing interests in object-driven approaches to produce the above forms of summaries <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b42">43]</ref>. Some object-driven video summarization methods require prior knowledge. For instance, in <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b35">36]</ref>, frame-level labels are required to help identify the object of interest. By learning to predict important object regions in egocentric videos using egocentric and saliency cues, concise visual summaries for egocentric videos can be produced driven by those regions <ref type="bibr" target="#b20">[21]</ref>. In <ref type="bibr" target="#b25">[26]</ref> object-like windows (i.e., object proposals) are taken from each frame as the initial pool of objects, based on which relationships between sub-events are discovered. However, these objects only act as an intermediate to help select sub-shots that construct a story. The ultimate goal is not to summarize videos into key objects and it is not fully unsupervised. Although <ref type="bibr" target="#b43">[44]</ref>  <ref type="bibr" target="#b44">[45]</ref> can discover objects via topical models in an unsupervised way, it relies on image segments instead of object proposals. Also it targets at grouping the segments instead of selecting representative ones. Although <ref type="bibr" target="#b3">[4]</ref> utilizes object proposals to address unsupervised discovery and localization of primary objects with multiple object classes, it targets for noisy image collections instead of a single video clip. Moreover, it only discovers a single object instance per image.</p><p>Representative Selection. Representative selection can be roughly categorized into clustering based methods and subspace learning based methods. As for clustering based methods, K-medoids algorithm is a representative one <ref type="bibr" target="#b17">[18]</ref>, which selects K clustering centroids as representatives to minimize within cluster distances. Instead of using one centroid to represent each sample, Elhamifar et al. improves Kmedoids clustering, so that each sample is able to be represented by multiple centroids <ref type="bibr" target="#b9">[10]</ref>. Representatives are also centroid-like in the methods of affinity propagation <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref> and density peak search <ref type="bibr" target="#b30">[31]</ref>. There has also been recent interest in applying linear subspace learning to find representatives from data, e.g., dictionary learning in <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b24">25]</ref> and dictionary selection in <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b41">42]</ref>. These methods usually require that each sample can be linearly expressed by representatives at a low reconstruction error. To filter outliers, <ref type="bibr" target="#b10">[11]</ref> ranks the representatives based on the norms of the rows in the coefficient matrix and only pick the top ones as final representatives. Another recent work <ref type="bibr" target="#b8">[9]</ref> can find a subset of the source set to efficiently describe the target set, given pairwise dissimilarities between two sets. However, it does not consider the prior weight of the data samples in their applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Problem Formulation</head><p>We formulate video summarization using object proposals as the representative selection problem. Given n object proposals extracted from a video sequence, each of the object proposal can be represented by a feature vector ∈ R d . These feature vectors are arranged as the columns of the data matrix X ∈ R d×n . Our goal is to find a compact subset of the n data points that are representative of X ∈ R d×n .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Preliminaries: sparse dictionary selection [6]</head><p>Sparse dictionary selection was originally proposed for abnormal event detection <ref type="bibr" target="#b5">[6]</ref>. Instead of learning a dictionary of arbitrary atoms, it requires that all atoms of the dictionary must come from the actual data points. In other words, the dictionary is a compact subset of the data matrix X. Denote the selection matrix by S ∈ R n×n , it solves min S∈R n×n</p><formula xml:id="formula_0">1 2 X − XS 2 F + λ 1 S 1,2 ,<label>(1)</label></formula><p>where S 1,2 = n i=1 S i,· 2 , associated with the regularization parameter λ 1 , and S i,· 2 is the l 2 norm of the i th row of S. Once (1) is solved by the proximal gradient method <ref type="bibr" target="#b2">[3]</ref>, the dictionary is constituted by selecting data points whose corresponding S i,· 2 = 0.</p><p>Note that the selected data points can also be seen as representatives of the dataset. Consequently, their corresponding object proposals can be used to summarize the video. Similar to <ref type="bibr" target="#b10">[11]</ref>, we can adapt <ref type="bibr" target="#b5">[6]</ref> to selecting any number of representatives by measuring and ranking the selection confidence of the i th data point x i according to S i,· 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Weighted sparse dictionary selection</head><p>Note that in <ref type="formula" target="#formula_0">(1)</ref>, all data points are treated equally. However, a good video summarization can certainly benefit from prior knowledge from application domain or user specifications. For instance, when summarizing egocentric videos, objects that the subject interact with are usually more important than others <ref type="bibr" target="#b20">[21]</ref>, while in surveillance videos from a fixed camera, moving foreground objects likely carry more weights than static background objects. To better leverage priors, we propose a simple extension to <ref type="bibr" target="#b5">[6]</ref> for representative selection min S∈R n×n</p><formula xml:id="formula_1">1 2 X − XS 2 F + λ 1 n i=1 1 ρ i + ǫ S i,· 2 ,<label>(2)</label></formula><p>where ρ i is the prior selection weight for the i th sample, and ǫ is a tiny number to avoid dividing by zero. Similar to <ref type="bibr" target="#b5">[6]</ref>, the problem of weighted sparse dictionary selection can also be optimized by the proximal gradient iteration <ref type="bibr" target="#b2">[3]</ref>, but needs a proximal decomposition <ref type="bibr" target="#b45">[46]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Locally linear reconstruction (LLR) induced sparse dictionary selection</head><p>As indicated in <ref type="bibr" target="#b10">[11]</ref>, sparse dictionary selection prefers keeping the vertices of convex hull spanned by input data to make sure each sample can be reconstructed at a low cost. It is thus extremely sensitive to noise. To mitigate this issue, we encourage local reconstruction for each sample to improve the robustness of representative selection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Locality prior of linear reconstruction</head><p>Inspired by locally linear embedding <ref type="bibr" target="#b31">[32]</ref>, we build a locality prior matrix W ∈ R n×n of representative selection based on that each sample x i is only allowed to be locally linear reconstructed by its k-NNs, <ref type="formula">(3)</ref> can be solved by a constrained least squares optimization <ref type="bibr" target="#b31">[32]</ref>.</p><formula xml:id="formula_2">N (x i ) \ x i : min W∈R n×n 1 2 n i=1 x i − j:xj ∈N (xi)\xi w ji x j 2 2 , s.t. j:xj ∈N (xi)\xi w ji = 1, w ji = 0, ∀x j / ∈ N (x i ) \ x i . (3) Problem</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">LLR-induced sparse dictionary selection</head><p>To introduce locality information of data for representative selection, we propose a new optimization problem combined with the locality prior martix W in the following:</p><formula xml:id="formula_3">min S∈R n×n 1 2 X−XS 2 F +λ 1 n i=1 1 ρ i + ǫ S i,· 2 +λ 2 S−W 2 F ,<label>(4)</label></formula><p>where the third term regularizes the selection matrix S by W, and λ 2 is a locality regularization parameter. Hence, data samples are preferable to be reconstructed by nearby representatives. Moreover, dense samples are more likely to be selected as representatives than sparse noise, as the former can contribute more to the reconstruction of surrounding samples in comparison with the latter.</p><p>Problem <ref type="formula" target="#formula_3">(4)</ref> is complex due to three optimization terms. But we will show that it can be converted into a proximal gradient optimization and solved by the FISTA method <ref type="bibr" target="#b2">[3]</ref> through a proximal decomposition <ref type="bibr" target="#b45">[46]</ref>, which converges fast with rate O(1/m 2 ) in m iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">Optimization</head><p>To solve our optimization problem (4), we first expand the objective function (O for short) and rewrite it as</p><formula xml:id="formula_4">O = 1 2 tr{X T X − 2 X T X + λ 2 W T S + S T X T X + λ 2 I S + W T W} + λ 1 n i=1 1 ρ i + ǫ S i,· 2 .<label>(5)</label></formula><p>We then let</p><formula xml:id="formula_5">f (S) = 1 2 tr{X T X − 2 X T X + λ 2 W S + S T X T X + λ 2 I S + W T W},<label>(6)</label></formula><p>and</p><formula xml:id="formula_6">g(S) = λ 1 n i=1 1 ρ i + ǫ S i,· 2 .<label>(7)</label></formula><p>Thus, we decompose the objective function O into two convex functions, with f smooth and g nonsmooth, i.e.,</p><formula xml:id="formula_7">O = f (S) + g(S),<label>(8)</label></formula><p>Algorithm 1 LLR-induced Weighted Sparse Dictionary Selection <ref type="bibr" target="#b3">(4)</ref>.</p><formula xml:id="formula_8">Input: X, {ρ i } n i=1 , k, λ 1 , λ 2 Output: S 1: L ← λ 2 + r X T X ⊲ Lipschitz constant (Equation (11)) 2: S ← 0, V ← S, t ← 1 ⊲ Initialization 3: repeat 4: Z ← V + 1 L − X T X + λ 2 W + λ 2 I + X T X V ⊲ Equation (10) 5: U ← S, S i,· ← Z i,· max{(1 − λ 1 L(ρ i +ǫ) Z i,· 2</formula><p>), 0}, i = 1, 2, · · · , n ⊲ Equation <ref type="formula" target="#formula_0">(13)  6</ref>:</p><formula xml:id="formula_9">τ = t − 1, t ← (1 + √ 1 + 4t 2 )/2 7: V ← S + τ (S − U)/t 8: until convergence</formula><p>for which, we can apply the proximal gradient method, FISTA <ref type="bibr" target="#b2">[3]</ref>. It then becomes iteratively solving prox R (Z) = arg min S∈R n×n</p><formula xml:id="formula_10">1 2 S − Z 2 F + 1 L g(S),<label>(9)</label></formula><p>where</p><formula xml:id="formula_11">Z = S − 1 L ∂ ∂S f (S) = S + 1 L − X T X + λ2W + λ2I + X T X S ,<label>(10)</label></formula><p>and L is the smallest Lipschitz constant, which equals to the spectral radius (r(·)) of λ 2 I + X T X, i.e.,</p><formula xml:id="formula_12">L = r λ 2 I + X T X = λ 2 + r X T X .<label>(11)</label></formula><p>We next follow the decomposition tactic in <ref type="bibr" target="#b45">[46]</ref>, then Problem (9) is solvable, and for i = 1, 2, · · · , n,</p><formula xml:id="formula_13">S i,· = arg min s∈R n 1 2 s − Z i,· 2 2 + λ 1 L(ρ i + ǫ) S i,· 2 ,<label>(12)</label></formula><p>After applying soft-thresholding <ref type="bibr" target="#b40">[41]</ref> to the above n group lasso signal approximators, we have, for i = 1, 2, · · · , n,</p><formula xml:id="formula_14">S i,· = Z i,· max{(1 − λ1 L(ρi+ǫ) Z i,· 2 ), 0}.<label>(13)</label></formula><p>We show the representative selection procedure in Algorithm 1, where we integrate a decomposed soft-thresholding strategy into an accelerated proximal gradient procedure, which is known to have a fast convergence rate O(1/m 2 ) in m iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.4">Parameter setting</head><p>Sparsity regularization parameter λ 1 . As in Algorithm 1, we initialize S by a zero matrix. Then according to <ref type="bibr" target="#b9">(10)</ref>, after the first iteration, we have</p><formula xml:id="formula_15">Z = 1 L − X T X + λ 2 W .<label>(14)</label></formula><p>As indicated by the thresholding of Z in <ref type="formula" target="#formula_0">(13)</ref>, when λ 1 is large enough, e.g., λ ≥ λ max 1 , we obtain S = 0, which means we select nothing. Therefore, to avoid an empty selection, we let λ 1 ≤ λ max 1 and solve λ max 1 by substituting S = 0 into (13) as follows:</p><formula xml:id="formula_16">λ max 1 = L max 0≤i≤n {(ρ i + ǫ) Z i,· 2 } = max 0≤i≤n (ρ i + ǫ) x T i X + λ 2 W i,· 2 .<label>(15)</label></formula><p>In our experiments, we let λ 1 = λ max 1 α1 and tune α 1 between the interval <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b29">30]</ref>. Given λ 2 , a smaller α 1 indicates a larger λ 1 , which implies a sparser selection.</p><p>Locality regularization parameter λ 2 . Let us consider the LLR-induced sparse selection in (4). When λ 2 = 0, the problem becomes a weighted sparse dictionary selection as in <ref type="bibr" target="#b1">(2)</ref>. When λ 2 = +∞, it sparsely selects representatives based on the rows of W. Furthermore, as shown in <ref type="formula" target="#formula_0">(10)</ref>, λ 2 balances the contributions of W and XX T to the proximal operation in <ref type="bibr" target="#b8">(9)</ref>. For ease of tuning λ 2 , we let</p><formula xml:id="formula_17">λ 2 ← κ × r(X T X) r(W) ,<label>(16)</label></formula><p>where we set κ = 0.02 × 5 α2 and α 2 between [−3, 1] in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments on Synthetic Data</head><p>We first evaluate the effectiveness of the locality prior in handling outliers on synthetic data, in comparison with the Sparse Dictionary Selection (SDS) <ref type="bibr" target="#b5">[6]</ref> and Sparse Modeling Representative Selection (SMRS) <ref type="bibr" target="#b10">[11]</ref>. We refer to our proposed method as Locally Linear Reconstruction induced Sparse Dictionary Selection (LLR-SDS).</p><p>We consider the noisy data shown in <ref type="figure" target="#fig_1">Fig. 2</ref>, which consists of data points in three clusters and uniform background noise. The top 30 representatives found from the 2, 018 points by each compared method are shown in Figs. 2 (a)-(c). As can be seen, both SDS and SMRS select the outlier points at the border of the convex hull, showing these two methods are sensitive to noise. This is because those points contribute a lower linear reconstruction cost to the dataset than others, which meets the requirement of dictionary selection. As SMRS has a post-processing to filter outliers <ref type="bibr" target="#b10">[11]</ref>, we also run its outlier removal for comparison in <ref type="figure" target="#fig_1">Fig. 2 (d)</ref>. Since most selection of SMRS are outliers as shown in <ref type="figure" target="#fig_1">Fig. 2 (c)</ref>, it is difficult to improve the results by post-processing. In contrast, our proposed LLR-SDS method considers a locality constraint in addition to the linear reconstruction cost for dictionary selection. As a result, it can reject most noisy outliers and select the points in the clusters. For our method, we use k = 5 nearest neighbors to build the locality prior matrix W, and set the sparsity regularization parameter α 1 = 5 for λ 1 , and the locality regularization parameter α 2 = 1 for λ 2 . The same sparsity regularization parameter is used for SDS and SMRS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments on Real Datasets</head><p>Next, we test our approach on two real datasets to evaluate its applicability to topical object discovery and object search in videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Settings</head><p>Although our approach is agnostic to the object proposal method, we use Edge Boxes <ref type="bibr" target="#b46">[47]</ref> with default settings to generate category independent object proposals for a controlled comparison with existing methods on all datasets. In light of the good performance reported using features from the top layers of deep convolutional neural networks (CNNs) for many computer vision tasks such as object detection <ref type="bibr" target="#b13">[14]</ref>, image retrieval <ref type="bibr" target="#b1">[2]</ref> and object instance search <ref type="bibr" target="#b34">[35]</ref>, we choose to represent each object proposal by the CNN feature after dimension reduction. Specifically, we take the 4096-dimensional output from the fully connected layer 6 of Caffe <ref type="bibr" target="#b16">[17]</ref>, using the pre-trained model on ILSVRC 2012 without fine-tuning. The CNN features are then reduced to 256-dimension by PCA and whitening <ref type="bibr" target="#b15">[16]</ref>. The same set of CNN-PCA features are used for comparison with existing work unless noted otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Baseline algorithms</head><p>We compare our proposed LLR-SDS with a variety of different methods for representative selection, including Objectness <ref type="bibr" target="#b46">[47]</ref>, K-medoids <ref type="bibr" target="#b17">[18]</ref>, Density <ref type="bibr" target="#b30">[31]</ref>, Sparse Modeling Representative Selection (SMRS) <ref type="bibr" target="#b9">[10]</ref>, Sparse Dictionary Selection (SDS) <ref type="bibr" target="#b5">[6]</ref>, Locally Linear Reconstruc-tion (LLR) and Latent Dirichelet Allocation with Word Cooccurrence prior (LDA-WCP) <ref type="bibr" target="#b43">[44]</ref>.</p><p>Objectness refers to directly ranking object proposals based on the objectness scores from Edge Boxes. For Density, representatives are ranked and selected according to the parameter γ, which is the product of local density and minimum distance between the point and any other point with higher density <ref type="bibr" target="#b30">[31]</ref>. For SMRS, we follow the authors to tune its parameter α, which is similar to α 1 of our method. For a fair comparison, results reported are without the outlier detection post-processing. SDS refers to ranking and selecting object proposals according to the l 2 norms of the rows of the selection matrix S obtained from the algorithm proposed in <ref type="bibr" target="#b5">[6]</ref> (Sec. 3.1). LLR refers to selecting object proposals according to the l 2 norms of the rows of the LLR prior matrix W (Sec. 3.3.1). Therefore, it favors data points with sufficient density over the outliers. For LDA-WCP, we adapt it to object proposal selection by selecting the highest score segment in the entire video for each topic to summarize the video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Evaluation Metric</head><p>The effectiveness of all representative selection methods are evaluated by the average recall. Denote the set of selected object proposals from a video as P and assume a video contains t different key objects. For the i th key object, denote the set of ground truth bounding boxes in all keyframe as G i , and the best intersection over union (IoU) score S with the i th key object is defined as</p><formula xml:id="formula_18">S(P, G i ) = max p∈P g∈Gi S(p, g) = max p∈P g∈Gi p ∩ g p ∪ g<label>(17)</label></formula><p>The recall of a video is determined by the number of key objects that are recalled, i.e., t i=1 I(S(P,Gi)&gt;=θ) t , where θ is the overlap threshold and I(·) is the indicator function. The average recall is the mean of the recall of all videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Topical Video Object Discovery</head><p>We first demonstrate the effectiveness of the proposed LLR-SDS in summarizing multiple topical objects in videos. We run our experiments on the "multiple" object subset of the topical video object discovery dataset <ref type="bibr" target="#b43">[44]</ref>, which consists of 10 commercial video sequences from YouTube. Each video contains multiple well-defined topical objects such as the product logos and has multiple shots. As in <ref type="bibr" target="#b43">[44]</ref>, keyframes from each video are sampled at two frames per second. For each keyframe, we take the top 100 object proposals according to the objectness score <ref type="bibr" target="#b46">[47]</ref> and summarize from them. The overlap threshold θ is set to 0.5.</p><p>Note that LDA-WCP requires a predefined number of topics for each video, which is set to 8 for all videos in <ref type="bibr" target="#b43">[44]</ref>. Hence, for a fair comparison, we also select 8 object proposals by each of the other methods to summarize a video. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Average Recall@8</head><p>Objectness <ref type="bibr" target="#b46">[47]</ref> 0.173 LLR 0.243 Density <ref type="bibr" target="#b30">[31]</ref> 0.360 LDA-WCP <ref type="bibr" target="#b43">[44]</ref> 0.360 K-medoids 0.410 SDS <ref type="bibr" target="#b5">[6]</ref> 0.417 SMRS <ref type="bibr" target="#b10">[11]</ref> 0.430 LLR-SDS (Ours) 0.430 LLR-wSDS (Ours) 0.547 <ref type="table">Table 1</ref>: Average recall when selecting 8 object proposals. Methods are sorted by increasing average recall@8. Our method with objectness as prior weights (LLR-wSDS) achieves the best average recall among all.</p><p>SMRS is tested on a range of α ∈ {2, 3, 5, 10, 20, 30} and we report the best average recall obtained. For our proposed LLR-SDS, we fix k = 3 nearest neighbors to construct the LLR prior matrix W (Sec. 3.3.1), α 1 = 2 for λ 1 and α 2 = −1 for λ 2 for all videos (Sec. 3.3.4). The same k = 3 is used for LLR and the same α 1 = 2 is used for SDS. We also evaluate the effectiveness of prior weights by simply using the objectness score from Edge Boxes <ref type="bibr" target="#b46">[47]</ref> as ρ i (Eq. 4) for each object proposal. We refer to our method with objectness weights as LLR-wSDS. <ref type="table">Table 1</ref> compares our approach with other methods in terms of the average recall of all videos. Without objectness prior weights, our proposed LLR-SDS and SMRS achieve the same highest average recall of 0.43. Although using the objectness score directly for summarization performs poorest among all, integrating it into our LLR-SDS formulation as prior weights further improves LLR-SDS and SMRS by 27.2%. A close examination of the Objectness results reveals that since object proposals are highly redundant across frames, an object proposal that is scored highest in one frame usually scores the highest in other frames as well. Therefore when selecting few (i.e., top 8) object proposals purely based on the objectness scores, multiple snapshots of one or two dominant object(s) are often picked out but the other topical objects are missed. Note that LDA-WCP is based on quantized SIFT features, while the others except for Objectness are based on CNN features. This could account for the mediocre performance of LDA-WCP. <ref type="figure" target="#fig_2">Fig. 3</ref> shows an example video with representative object proposals selected by different methods. The 8 object proposals selected by each method are displayed in rank order in each row, except for LDA-WCP and K-medoids, which produce no ranks for the selection. There are 5 topical objects in this commercial: the blue juice drink, the green juicy juice, the boy, and 2 toy trucks. Both LLR-SDS and LLR-wSDS capture 4 out of 5 (missing one of the toy trucks) using only 8 proposals. With objectness weights, LLR-wSDS seems to improve the ranking of object proposals with more accurate coverage of the ground truth over LLR-SDS. For instance, the 1 st object proposal selected by LLR-wSDS provides a more accurate coverage of the blue juice drink than the 1 st object selected by LLR-SDS. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Video Summarization for Object Search</head><p>Next we evaluate the applicability of LLR-SDS to visual object search by summarizing shots of a feature movie by object proposals. We take a benchmark dataset for object retrieval, the Groundhog Day <ref type="bibr" target="#b32">[33]</ref>, which consists of 5, 640 keyframes (752 shots) from the full length feature film "Groundhog Day". The ground truth test set consists of 6 object queries. Contrary to the Topical Video Object Discovery dataset, the target objects in this dataset are usually small and the scenes are much cluttered.</p><p>For each keyframe, we extract the top 200 object proposals with an aspect ratio ∈ [0. <ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b3">4]</ref>, and a minimal size of 30×30 pixels. Because object locations in the keyframes are not provided, we manually annotate bounding box locations in all ground truth keyframes. We run our algorithm on all ground truth shots that have ≥ 8 keyframes <ref type="table">(Table 2)</ref>. SDS is tested in a range of α ∈ {2, 3, 5, 10, 15, 20, 30} and α = 10 is selected for comparison because it produces the best average recall. For our LLR-SDS, we fix k = 5 nearest neighbors to construct the LLR prior matrix W and α 1 = 15 for calculating λ 1 . We have also tested the objectness score as prior weights as in Sec. 5.4, which, however, does not boost the performance on this dataset. It is likely due to the scene clutters and generally low resolutions of the target objects in the Groundhog Day dataset. Therefore, we evaluate LLR-SDS using equal weights on this dataset. It is worth noting though that other priors could be effective on this dataset such as object size and scene context. Unless noted otherwise, we set the overlap threshold θ to 0.7 in all following experiments (Eq. 17), as a greater overlap with the ground truth generally leads to a higher matching score and increases the chance for an object to be retrieved.   <ref type="figure" target="#fig_3">Fig. 4</ref> illustrates the average recall on all ground truth shots of the six objects with the selection ratio η ∈ [0.001, 0.2]. It is shown that the proposed LLR-SDS outperforms the other methods and achieves an average recall of 0.74 with as few as 10% of the object proposals. With 14% of the object proposals, it achieves an average recall of 0.78, while the average recall obtained using all object proposals is 0.80. It also improves upon selection by SDS or LLR alone. In addition, <ref type="figure" target="#fig_4">Fig. 5</ref> shows the relationship between the average recall and overlap ratio θ of our proposed LLR-SDS, when η = 0.01.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.1">Results</head><p>To evaluate the sensitivity of α 2 for λ 2 (Sec. 3.3.4), we test LLR-SDS with α 2 ∈ [−3, 1]. <ref type="figure" target="#fig_5">Fig. 6</ref> plots the average recall curves obtained with respect to the two extreme cases discussed in Sec. 3.3.4: i.e., (1) λ 2 = 0 and (2) λ 2 = +∞. The former is equivalent to the sparse dictionary selection (SDS) (Eq. 2), while the latter produces a selection matrix . For each shot, the object proposal with the best IoU with the ground truth (GT) is shown in the yellow bounding box and the GT is in red. Missed shots are highlighted by red rectangles. Overall our method produces summarizations that more accurately capture the GT. Note that given a shot, the best IoU object proposal selected by different methods may come from different keyframes. Best viewed in color and magnification.</p><p>S similar to the LLR prior matrix W, where we rank and select object proposals by the l 2 norms of the rows of W (LLR). We fix α 1 = 15 for LLR-SDS and SDS, and k = 5 for LLR-SDS and LLR. It is observed that when α 2 ≤ −3, the recall curve of LLR-SDS converges to that of SDS. On the other hand, when α 2 0, the recall curve of LLR-SDS almost entirely overlaps with that of LLR. Experimentally, on the Groundhog dataset, LLR-SDS achieves higher average recall than either SDS or LLR when α 2 ∈ [−2, −0.5].</p><p>We further evaluate the effectiveness of LLR-SDS in terms of the percentage of proposals required to provide accurate localization of the ground truth object, in comparison with SDS and LLR. An object proposal is considered to accurately locate the ground truth if its IoU with the ground truth 0.7 or it achieves the best IoU among all object proposals in a video. <ref type="table">Table 2</ref> shows that on average, with as few as 6.60% of all object proposals, LLR-SDS is able to cover the object of interest when summarizing a shot. Except for the Frames Sign, LLR-SDS requires fewer object proposals than both SDS and LLR to ensure accurate localization of the ground truth, while LLR requires the most. <ref type="figure" target="#fig_6">Fig. 7</ref> shows visual comparisons of our results with others on all shots of Microphone, when selecting η = 0.5% of all object proposals. For each shot, we visualize the object proposal that has the highest IoU with the ground truth among all selected. In general, our method produces summarizations that more accurately locate the ground truth than others in all 8 shots of Microphone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>In this work we summarize videos into key objects using object proposals. These key objects can serve as video icons  <ref type="table">Table 2</ref>: Average percentage of object proposals required to cover the ground truth object. and establish a brief impression of the video. By telling what objects appear in each video, we can use these key objects to help search, browse, and index large video volume. To select key objects, we propose a new formulation of sparse dictionary selection to select representative object proposals, i.e., locally linear reconstruction induced sparse dictionary selection (LLR-SDS). The new formulation considers both object proposal priors and locality priors in the feature space thus can better handle outlier proposals when identifying the key objects. Our results on synthetic data and two benchmark real datasets validate the advantages of our approach in comparison with existing representative selection methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Video summarization by representative object proposal selection. A pool of object proposals (middle row) are first generated from video keyframes (top). Representative object proposals (bottom) are selected from the pool to summarize the video.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Data points in three clusters with background uniform noise (blue dots) and the representatives (red circles) found by (a) the proposed LLR-SDS method (b) SDS<ref type="bibr" target="#b5">[6]</ref>, (c) SMRS<ref type="bibr" target="#b10">[11]</ref> and (d) SMRS with outlier removal<ref type="bibr" target="#b10">[11]</ref> .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Video summarization for topical object discovery: example representative objects selected by different methods. Keyframes are shown in the top row. Each following row shows the 8 representative object proposals selected by different methods in rank order (except for K-medoids and LDA-WCP).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Average recall with selection ratio η ∈ [0.001, 0.2].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Evaluation of the relationship between the average recall and overlap ratio θ (η = 0.01).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Average recall when the regularization parameter α 2 ∈ [−3, 1]. A small α 2 (i.e., −3) leads to a small λ 2 , which produces a curve similar to that of SDS; while α 2 = 1 leads to a solution similar to that of LLR (Sec. 3.3.4).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Video summarization for object search: visual results of the 8 shots of Microphone from Groundhog Day (η = 0.5% of all object proposals are selected)</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. This research was carried out at the Rapid-Rich Object Search (ROSE) Lab at the Nanyang Technological University, Singapore. The ROSE Lab is supported by the National Research Foundation, Singapore, under its Interactive Digital Media (IDM) Strategic Research Programme.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Measuring the objectness of image windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Alexe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Neural codes for image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Babenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Slesarev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chigorin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<idno>ECCV. 2014. 5</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A fast iterative shrinkagethresholding algorithm for linear inverse problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Teboulle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIIMS</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Unsupervised object discovery and localization in the wild: Part-based matching with bottom-up region proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Video cosummarization: Video summarization by visual cooccurrence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jaimes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Sparse reconstruction cost for abnormal event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Localizing objects while learning their appearance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Alexe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Decremental sparse modeling representative selection for prototype selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Dornaika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">K</forename><surname>Aldine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="3714" to="3727" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Dissimilarity-based sparse subset selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Elhamifar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Sastry</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1407.6810</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Finding exemplars from pairwise dissimilarities via simultaneous sparse recovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Elhamifar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">See all by looking at a few: Sparse modeling for finding representative objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Elhamifar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Mixture modeling by affinity propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">J</forename><surname>Frey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dueck</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="379" to="386" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Clustering by passing messages between data points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">J</forename><surname>Frey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dueck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">315</biblScope>
			<biblScope unit="issue">5814</biblScope>
			<biblScope unit="page" from="972" to="976" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Video summarization by learning submodular mixtures of objectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gygli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">G L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3090" to="3098" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Negative evidences and cooccurences in image retrieval: The benefit of pca and whitening</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<idno>ECCV. 2012. 5</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<title level="m">Caffe: Convolutional architecture for fast feature embedding</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Clustering by means of medoids</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaufman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rousseeuw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Statistical Data Analysis Based on the L1 Norm and Related Methods</title>
		<editor>Y. Dodge</editor>
		<meeting><address><addrLine>North-Holland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1987" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Largescale video summarization using web-image priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hamid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sundaresan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Joint summarization of large-scale collections of web images and videos for storyline reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Discovering important people and objects for egocentric video summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A hierarchical visual model for video object summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Diversified key-frame selection using structured optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Industrial Informatics</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1736" to="1745" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Personal object discovery in firstperson videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Abnormal event detection at 150 fps in matlab</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Story-driven summarization for egocentric video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Object-centric spatiotemporal pyramids for egocentric activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mccandless</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Category-specific video summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Potapov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Webcam synopsis: Peeking around the world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pritch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rav-Acha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gutman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Peleg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Making a long video short: Dynamic video synopsis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rav-Acha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pritch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Peleg</surname></persName>
		</author>
		<idno>CVPR. 2</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Clustering by fast search and find of density peaks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Laio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Nonlinear dimensionality reduction by locally linear embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">T</forename><surname>Roweis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">290</biblScope>
			<biblScope unit="issue">5500</biblScope>
			<biblScope unit="page" from="2323" to="2326" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Efficient visual search of videos cast as text retrieval. TPAMI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Salient montages from unconstrained videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Taskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Seitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Locality in generic instance search from one example</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014-06" />
			<biblScope unit="page" from="2099" to="2106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Video object discovery and co-segmentation with extremely weak supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
		<idno>ECCV. 2014. 2</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning to track for spatio-temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Image collection summarization via dictionary learning for sparse representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1122" to="1129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Discovering primary objects in videos by saliency fusion and iterative appearance estimation. T-CSVT</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Brandt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Discovering thematic objects in image collections and videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Katsaggelos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2207" to="2219" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Model selection and estimation in regression with grouped variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">RSSSB</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="49" to="67" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">An effective video summarization framework toward handheld devices. Industrial Electronics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Discovering thematic patterns in videos via cohesive sub-graph mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1260" to="1265" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Topical video object discovery from key frames by modeling word co-occurrence prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Topical video object discovery from key frames by modeling word co-occurrence prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5739" to="5752" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Modeling disease progression via fused sparse group lasso</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">A</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Edge boxes: Locating object proposals from edges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
