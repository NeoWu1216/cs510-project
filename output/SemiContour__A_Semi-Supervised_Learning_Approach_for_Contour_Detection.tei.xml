<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SemiContour: A Semi-supervised Learning Approach for Contour Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zizhao</forename><surname>Zhang</surname></persName>
							<email>zizhao@cise.ufl.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Florida</orgName>
								<address>
									<postCode>32611</postCode>
									<settlement>Gainesville</settlement>
									<region>FL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuyong</forename><surname>Xing</surname></persName>
							<email>f.xing@ufl.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Florida</orgName>
								<address>
									<postCode>32611</postCode>
									<settlement>Gainesville</settlement>
									<region>FL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoshuang</forename><surname>Shi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Florida</orgName>
								<address>
									<postCode>32611</postCode>
									<settlement>Gainesville</settlement>
									<region>FL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Yang</surname></persName>
							<email>lin.yang@bme.ufl.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Florida</orgName>
								<address>
									<postCode>32611</postCode>
									<settlement>Gainesville</settlement>
									<region>FL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SemiContour: A Semi-supervised Learning Approach for Contour Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:45+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Supervised contour detection methods usually require many labeled training images to obtain satisfactory performance. However, a large set of annotated data might be unavailable or extremely labor intensive. In this paper, we investigate the usage of semi-supervised learning (SSL) to obtain competitive detection accuracy with very limited training data (three labeled images). Specifically, we propose a semi-supervised structured ensemble learning approach for contour detection built on structured random forests (SRF). To allow SRF to be applicable to unlabeled data, we present an effective sparse representation approach to capture inherent structure in image patches by finding a compact and discriminative low-dimensional subspace representation in an unsupervised manner, enabling the incorporation of abundant unlabeled patches with their estimated structured labels to help SRF perform better node splitting. We re-examine the role of sparsity and propose a novel and fast sparse coding algorithm to boost the overall learning efficiency. To the best of our knowledge, this is the first attempt to apply SSL for contour detection. Extensive experiments on the BSDS500 segmentation dataset and the NYU Depth dataset demonstrate the superiority of the proposed method.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Contour detection is a fundamental but challenging computer vision task. In recent years, although the research of contour detection is gradually shifted from unsupervised learning to supervised learning, unsupervised contour detection approaches are still attractive, since it can be easily adopted into other image domains without the demand of a large amount of labeled data. However, one of the significant limitations is the high computational cost <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b34">35]</ref>. On the other hand, the cutting-edge supervised contour detection methods, such as deep learning, rely on a huge amount of fully labeled training data, which often requires huge human efforts and domain expertise. Semi-supervised learning (SSL) <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b17">18]</ref> is an alternative technique to balance the trade-off between unsupervised learning and supervised Top: At the parent node that contains u-tokens and l-tokens with corresponding structured labels, u-tokens will help l-tokens better estimate the separating plane for the node splitting. This is achieved by mapping tokens into a discriminative low-dimensional subspace and estimating u-tokens' discrete labels. Then the u-tokens are un-mapped to the original high-dimensional space associated with the estimated structured labels. Finally, all tokens will be propagated to child nodes. Bottom: A general view of the node splitting behavior to present how node splitting of SRF enables data with structured labels in the parent node to be categorized in child nodes.</p><p>learning. However, currently there exist no reports on semisupervised learning based contour detection. Supervised contour detection is often based on patchto-patch or patch-to-pixel classification. Contours in local patches (denoted by sketch tokens <ref type="bibr" target="#b19">[20]</ref>) contain rich and well-known patterns, including straight lines, parallel lines, curves, T-junctions, Y-junctions, etc <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b19">20]</ref>. One of the main objectives of the most recent supervised contour detection methods is to classify these patterns using structure learning <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref>, sparse representation <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b34">35]</ref>, convolution neutral network (CNN) <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b35">36]</ref>, etc. In our method, we use unsupervised techniques to capture the patterns of unlabeled image patches, enabling the successful training of the contour detector with a limited number of labeled images. For notation convenience, we denote labeled patches as l-tokens and unlabeled patches as u-tokens.</p><p>The proposed semi-supervised structured ensemble learning approach is built on structured random forests (SRF) <ref type="bibr" target="#b16">[17]</ref>. Inheriting from standard random forests (RF), SRF is popular because its: 1) fast prediction ability for high-dimensional data, 2) robustness to label noise <ref type="bibr" target="#b22">[23]</ref>, and 3) good support to arbitrary size of outputs. However, similar to RF, SRF heavily relies on the number of labeled data <ref type="bibr" target="#b17">[18]</ref>. These properties make SRF a good candidate for SSL.</p><p>In this paper, we propose to train SRF in a novel semisupervised manner, which only requires a few number of labeled training images. By analyzing the learning behaviors of SRF, we observe that improving the node splitting performance for data with structured labels is the key for the successful training. To this end, we incorporate abundant u-tokens into a limited number of l-tokens to guide the node splitting, which is achieved by finding a discriminative lowdimensional subspace embedding using sparse representation techniques to learn a basis dictionary of the subspace in an unsupervised manner.</p><p>In order to solve the sparse coding problem efficiently, we also propose a novel and fast algorithm to boost the overall learning efficiency. In addition, we demonstrate the maxmargin properties of SRF, enabling us to use max-margin learning to dynamically estimate the structured labels for u-tokens inside tree nodes. For better illustration, we explain the idea in <ref type="figure" target="#fig_0">Figure 1</ref>. In the experimental section, we show the vulnerability of other supervised methods to a limited number of labeled images and demonstrate that, with only 3 labeled images, our newly developed contour detector even matches or outperforms these methods which are fully trained over hundreds of labeled images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>Recently, most advanced contour detection methods are based on strong supervision. Ren et al. use sparse code gradients (SCG) <ref type="bibr" target="#b34">[35]</ref> to estimate the local gradient contrast for gPb, which slightly improves the performance of gPb. Maire et al. <ref type="bibr" target="#b23">[24]</ref> propose to learn a reconstructive sparse transfer dictionary to address contour representation. These methods indicate the strong capability of sparse representation techniques to capture the contour structure in image patches. In the ensemble learning family, Lim et al. <ref type="bibr" target="#b19">[20]</ref> propose sketch tokens, a mid-level feature representation, to capture local contour structure, and train a RF classifier to discriminate the patterns of sketch tokens. Dollár et al. <ref type="bibr" target="#b12">[13]</ref> propose a structured edge (SE) detector that outperforms sketch tokens by training a SRF classifier instead. Several variants of SRF are also successfully applied to image patch classification <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b32">33]</ref>. Recently, CNN has shown its strengths in contour detection <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b3">4]</ref>, and its success is attributed to the complex and deep networks with new losses to capture contour structure. One major draw-back of CNN, as well as other supervised learning methods, is its high demand of labeled data.</p><p>Semi-supervised learning (SSL) has been studied to alleviate the aforementioned problems <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b30">31]</ref>. Leistner et al. <ref type="bibr" target="#b17">[18]</ref> treat unlabeled data as additional variables to be jointly optimized with RF iteratively. Liu et al. <ref type="bibr" target="#b22">[23]</ref> instead use unlabeled data to help the node splitting of RF and obtain improved performance. However, it is difficult for these methods to avoid the curse of dimensionality. By contrast, this paper takes advantage of several properties of SRF to achieve an accurate contour detector with very few labeled training images. We address several critical problems to successfully learn SRF in a semi-supervised manner without much sacrificing the training and testing efficiency by 1) estimating the structured labels for u-tokens lying on a complex and high-dimensional space, and 2) preventing noises of extensively incorporated u-tokens from misleading the entire learning process of SRF.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">SSL Overview in Contour Detection</head><p>SSL uses a large number of unlabeled data D U = {x ∈ X } to augment a small number of labeled data D L = {(x, y) ∈ X × Y} and learns a prediction mapping function f : X → Y. In the scenario of contour detection, we denote x as a token, and y as its corresponding structured label of a certain pattern.</p><p>Contour detection performance of supervised methods is not only determined by the number of l-tokens in D L , but also affected by the number of labeled images, from which l-tokens are sampled <ref type="bibr" target="#b11">[12]</ref>. This is because the limited information in l-tokens sampled from a few labeled images is severely biased, which can not lead to a general classification model. On the contrary, sufficient u-tokens in D U sampled from many unlabeled images contain abundant information that is easy to acquire. We apply SSL to take advantage of u-tokens to improve the supervised training of our contour detector. However, u-tokens always have large appearance variations, so it is difficult to estimate their structured labels in the high-dimensional space Y.</p><p>We propose to estimate the structure labels of u-tokens by transferring existing structured labels of l-tokens. Because the patterns of the structured labels are limited and shared from images to images, which can be categorized into a finite number of classes (e.g., straight lines, parallel lines, and T-junctions), the structured labels of l-tokens from a few images are sufficient to approximate the structured labels of massive u-tokens from many images. We demonstrate this in <ref type="figure" target="#fig_2">Figure 2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">SSL via Structured Ensemble Learning</head><p>In this section we describe the proposed semi-supervised ensemble learning approach for contour detection. The method is built on the structured random forests (SRF), which has a similar learning procedure as the standard random forest (RF) <ref type="bibr" target="#b5">[6]</ref>. The major challenge of training SRF is that structured labels usually lie on a complex and high-dimensional space, therefore direct learning criteria for node splitting in RF is not well defined. Existing solutions <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b11">12]</ref> can only handle fully labeled data, and are not applicable in our case that contains both unlabeled and labeled data. We will start by briefly introducing SRF and analyze several favorable properties of SRF for SSL, and then present the proposed SSL based contour detection method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Structured random forest</head><p>SRF is an ensemble learning technique with structured outputs, which ensembles T independently trained decision trees as a forest F = {F t } T t=1 . Robust SRF always has large diversity among trees, which is achieved by bootstrapping training data and features to prevent overfitting. Given a set of training data D, starting from the root node, a decision tree F t attempts to propagate the data from top to bottom until data with different labels are categorized in leaf nodes.</p><p>Specifically, for all data x ∈ D i in node i, a local weak learner h(x, θ) = 1[x k &lt; τ ] propagates x to its left substree if h(·) = 1, and right substree otherwise. θ = (τ, k) is learned by maximizing the information gain I i :</p><formula xml:id="formula_0">θ ⋆ = argmax τ ∈R,k∈Z I i .<label>(1)</label></formula><p>The optimization is driven by the Gini impurity or Entropy <ref type="bibr" target="#b5">[6]</ref>. y ∈ Y = Z m·m is a structured label with the same size as the training tokens. To enable the optimization of I i for structured labels, Dollár et al. <ref type="bibr" target="#b12">[13]</ref> propose a mapping Π : Y → L to project structured labels into a discrete space, l ∈ L = {1, ..., Z}, and then follow the standard way. The training terminates (i.e., leaf nodes are reached) until a stopping criteria is satisfied <ref type="bibr" target="#b5">[6]</ref>. The most representative y (i.e., closet to mean) is stored in the leaf node as its structured prediction, i.e., the posterior p(y|x).</p><p>The overall prediction function of SRF ensembles T predictions from all decision trees, which is defined as</p><formula xml:id="formula_1">argmax y∈Y p(y|x, F) = 1 T T t=1 argmax y∈Y p(y|x, F t ).<label>(2)</label></formula><p>To obtain optimal performance, given a test image, we densely sample tokens in multi-scales so that a single pixel can get m × m × T × (# of scales) predictions in total. The structured outputs force the spatial continuity. The averaged prediction yields soft contour responses, which intrinsically alleviate noise effects and indicate a good sign to performing SSL in SRF. Good features play an important role in the success of SRF. Shen et al. <ref type="bibr" target="#b28">[29]</ref> improve the SE contour detector <ref type="bibr" target="#b12">[13]</ref> by replacing the widely used HoG-like features with CNN features. In fact, this CNN classifier itself is a weak contour detector used to generate better gradient features. Inspired by this idea, we use a limited number of l-tokens from a few labeled images to first train a weak SE contour detector (denoted by Γ) <ref type="bibr" target="#b12">[13]</ref>. Γ produces efficient detection and provides prior knowledge for u-tokens to facilitate SSL. We will see its further usage subsequently. In our method, we use three color channels (Luv), two gradient magnitude (obtained from Γ) and eight orientation channels in two scales, and thus the total feature space is X ∈ R m·m·13 , which is similar to the configuration in <ref type="bibr" target="#b19">[20]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Semi-supervised SRF learning</head><p>In our method, maximizing the information gain I i is achieved by minimizing the Gini impurity measurement G <ref type="bibr" target="#b10">[11]</ref>, which is defined as</p><formula xml:id="formula_2">G(D i ) = Z j=1 p j (l|x k )(1 − p j (l|x k )),<label>(3)</label></formula><p>where p j (l|x k ) denotes the label empirical distribution of class j inD i with respect to the k-th feature dimension. We adopt the mapping function Π <ref type="bibr" target="#b12">[13]</ref> to map structured labels of l-tokens to discrete labels.</p><formula xml:id="formula_3">D i = {(x, l)|(x, y) ∈ D i , l = Π(y)} denotes D i when x is with the discrete label.</formula><p>Intuitively, minimizing G is to find a separating line in the k-th feature dimension (several feature dimensions can be used together to define a separating hyperplane <ref type="bibr" target="#b10">[11]</ref>) to split D i in the whole feature space into the left and right subtrees, so that p j on both sides are maximized <ref type="bibr" target="#b5">[6]</ref>. Proposition 1 proves the close relationship of the Gini impurity to the max-margin learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proposition 1. Given the hinge loss function ξ of maxmargin learning, the Gini impurity function</head><formula xml:id="formula_4">L p j (1 − p j ) is its special case. Proof. Since l(w T x) ≥ 0, if l(w T x) ≤ 1, then we have: ξ(D i ) = 1 |D i | (x,l)∈Di Z j=1 1[l = j] max(0, 1 − l(w T x)) = Z j=1 p j (1 − l(w T x)), where p j = (x,l)∈D i 1[l=j]</formula><p>|Di| . Because p j ∝ l(w T x), the Proposition holds. A generalized theorem is given in <ref type="bibr" target="#b17">[18]</ref>.</p><p>Incorporate Unlabeled Data It is well-known that a limited number of labeled data always lead to biased maxmargin estimation. We incorporate u-tokens into the limited number of l-tokens to improve the max-margin estimation of weak learners in every node. However, p(l|x u ) of u-tokens is unavailable for computing Gini impurity. One solution to address this problem <ref type="bibr" target="#b22">[23]</ref> is to apply a kernel density estimator to obtain p(x u |l) and use the Bayes rule to obtain p(l|x u ). In this approach, a proper selection of bandwidth is not trivial. In addition, it can not handle structure labels and the high-dimensional space, on which utokens lie. In our method, we propose to map tokens into a more discriminate low-dimensional subspace associated with discrete labels using a learned mapping S, and find a hyperplane w to estimate p(l|x u ). In this scenario, the goal is to calculate the bases of the subspace. The data correlation in the subspace is consistent with that in the original space so that the estimated p(l|x u ) will not mislead the weak learners. In Section 5, we demonstrate that this goal can be achieved using sparse representation techniques. SRF Node Splitting Behaviors During the training stage of SRF, tokens with various patterns are chaotic in the top level nodes, and weak learners produce coarse splitting results; while at the bottom level nodes, the splitting becomes more subtle. For example, suppose l ∈ {0, 1}, the weak learner in the root node intends to split foreground and background tokens into the left and right subtrees, respectively. The top level nodes tend to split the straight line and broken line patterns, whereas weak learners tend to split 40 degree and 30 degree straight lines in the bottom level nodes, in which patterns are more pure. Considering this property, we propose a novel dynamic structured label transfer approach to estimate the structured labels for u-tokens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Dynamic structured label transfer</head><p>Because it is challenging to directly estimate highdimensional structured labels for u-tokens, in our method, we transfer existing structured labels of l-tokens to u-tokens. An important concern is to prevent inaccurate structured label estimation for u-tokens from destroying the learning of SRF. Suppose we have mapped tokens in node i into a lowdimensional subspace using S, we first search for a maxmargin hyperplane w using a linear wighted binary support vector machine trained over l-tokens with discrete labels in this node (so the number of discrete labels Z=2 in our case). In this way, for an u-token x u , we can estimate its discrete label (i.e., p(l|x u )) through sigmoid w T S(x u ) .</p><p>To estimate its structured label, we adopt the nearest search to find the best match in the candidate pool of ltokens with the same discrete label as x u . The structured label transfer function H : X → Y is defined as</p><formula xml:id="formula_5">y ⋆ = H(x u ) = argmin (x,y)∈D L i dist(S(x), S(x u )),<label>(4)</label></formula><p>where dist(·, ·) is the cosine metric. In Section 5.2, we will see that S generates very sparse low-dimensional representation for tokens so that the steps of finding the hyperplane and performing the nearest search are computationally efficient. Finally we can easily map u-tokens associated with structure labels back to their original space, and all tokens in node i are propagated to child nodes. The brute-force searching at the top level nodes may yield inaccurate structured label estimation for u-tokens due to the chaotic patterns and coarse discrete labels. In addition, it might lead to unnecessary computations because of redundant structured labels in one class. To tackle these problems, we dynamically update the transferred structured labels during the training of SRF. At the root node, we transfer initial structured labels to u-tokens. As the tree goes deeper, weak learners gradually purify the candidate pool by decreasing the token volume and pattern variety. Therefore, the dynamically estimated structured labels of utokens will become more reliable in the bottom level nodes. Since the number of u-tokens is much larger than that of ltokens, some bottom level nodes might contain less or no l-tokens. We treat u-tokens with high probability as l-tokens when a node does not contain enough l-tokens, less than 10 in our case. In addition, we randomly pick a subset instead of the entire candidate pool to perform the nearest search in each individual node.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Sparse Token Representation</head><p>This section discusses the approach of finding the subspace mapping S mentioned in Section 4.2. We first describe how to learn a token dictionary to construct the bases of the low-dimensional subspace, and then present a novel and fast sparse coding algorithm to accelerate the computation of S.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Sparse token dictionary</head><p>Sparse representation has been proven to be effective to represent local image patches <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b20">21]</ref>. In our method, we pursue a compact set of the low-level structural primitives to describe contour patterns by learning a token dictionary. Specifically, any token </p><p>where · 0 is ℓ 0 -norm, c j 0 = i 1[c ji = 0], to ensure that a sparse code c only has K nonzero entries. · F is </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Fast sparse coding</head><p>Input: A target data x ∈ R d , a dictionary M ∈ R d×V , and a sparsity value K Output:</p><formula xml:id="formula_7">A sparse code c ∈ R V 1: for v = [1, 2, ..., V ] do 2:</formula><p>Obtain the score s v for m v :</p><formula xml:id="formula_8">s ⋆ v = argmin sv x − m v s v 2 2 = (m T v m v ) −1 m T v yy T m v<label>(5)</label></formula><p>3: end for 4: Construct a small size dictionary matrix M s ∈ R d×K using the K bases associated with the first K largest scores in [s 1 , ..., s V ] 5: Solve a constrained leasts-squares problem:</p><formula xml:id="formula_9">c ⋆ s = argmin cs x − M s c s 2 2 + λ c s 2 2 = (M T s M s + λI) −1 M T s x, c s ∈ R K .<label>(6)</label></formula><p>6: Obtain a sparse code c by filling its K entries with c ⋆ s indexed by s 7: return The sparse code c the Frobenius norm. Inspired by <ref type="bibr" target="#b23">[24]</ref>, we adopt MI-KSVD, a variant of the popular K-SVD, to solve Eqn. (7) for better sparse reconstruction <ref type="bibr" target="#b4">[5]</ref>. However, the dictionary M is learned in an unsupervised manner, so it is not task-specific and its learning performance can be influenced by large appearance variances in tokens from different images. In particular, we observe that the cluttered background tokens (i.e., tokens contain no annotated contour inside) may cause unnecessary false positives. To ameliorate these problems, we introduce the prior label knowledge as an extra feature in the dictionary to improve its learning performance.</p><p>Specifically, for an RGB token, we apply Γ (Section 4.1) to generate its corresponding contours as the prior label knowledge, i.e., a patch with detected contours. In this way, the new featured token x will have 4 channels, which is represented as x = [x (r) , x (g) , x (b) , x (e) ] T ∈ R m·m·4 , where x (e) is the contour channel corresponding to the RGB channels (x (r) , x (g) , and x (b) ). We model background with M b and foreground with M f , respectively. <ref type="figure" target="#fig_4">Figure 3</ref> illustrates how the dictionary represents the structure in tokens.</p><p>In our method, both u-tokens and l-tokens are used as the training data for dictionary learning, which are sampled from unlabeled and labeled images, respectively. Foreground tokens are extracted if they straddle any contours indicated by the ground truth. The rest are background tokens. Because the ground truth of u-tokens is unavailable, we use the probability outputs of Γ to help us sample high confident foreground and background u-tokens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Subspace mapping using fast sparse coding</head><p>As we mentioned in Section 4.2, we use the mapping function S to provide a compact and discriminative lowdimensional representation for a token x. Given a learned dictionary M in Section 5.1, the subspace representation of x is defined as</p><formula xml:id="formula_10">S(x) = c ⋆ = argmin c x − M c 2 , s.t. c 0 ≤ K. (8)</formula><p>It is well-known that solving Eqn. <ref type="bibr" target="#b7">(8)</ref> is NP-hard (ℓ 0 -norm). One typical algorithm to solve this problem is orthogonal matching pursuit (OMP) <ref type="bibr" target="#b25">[26]</ref>. Many other algorithms often relax it to the tractable ℓ 1 -norm minimization problem. Yang et al. <ref type="bibr" target="#b36">[37]</ref> show that ℓ 1 -norm provides better classification meaningful information than ℓ 0 -norm. The main reason is that, unlike ℓ 0 -norm that only selects the dictionary bases, ℓ 1 -norm also assigns weights to the selected bases to determine their contributions. Usually, high weights are often assigned to the bases similar to the target data <ref type="bibr" target="#b33">[34]</ref>. In this paper, we propose a novel and fast sparse coding algorithm, which is scalable to a large number of target data. Based on the above observation, we approximate the computation of sparse coding by two steps: 1) basis selection, which measures the similarity score of each basis to the target data individually and then selects the bases with large scores; 2) reconstruction error minimization, which aims to assign weights to selected bases. The details are summarized in Algorithm 1. Given a target data, we first compute a sequence of scores with respect to each basis (steps 1 to 3). Next we select K bases associated with the first K largest scores to construct a small size dictionary M s (step 4). Then we solve a constrained least-squares problem to obtain the coefficient c s and assign weights to the selected bases (step 5). The regularization parameter λ is set to a small value, 10 −4 . Finally, the value of c s is mapped to c as the final sparse code (steps 6 to 7). Unlike many existing methods, our proposed algorithm decouples the sparse coding optimization to problems with analytical solutions, which do not need any iteration. Therefore, our algorithm is faster than others that directly solve ℓ 0 -norm or ℓ 1 -norm problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experimental Results</head><p>In this section, we first evaluate the performance of the proposed method for contour detection on two public datasets, and then compare the efficiency of the proposed sparse coding solver with several state-of-the-arts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Contour detection performance</head><p>We test the proposed approach on the Berkeley Segmentation Dataset and Benchmark (BSDS500) <ref type="bibr" target="#b1">[2]</ref> and the NYUD Depth (NYUD) V2 Dataset <ref type="bibr" target="#b29">[30]</ref>. We measure the contour detection accuracy using several criteria: Fmeasures with fixed optimal threshold (ODS) and perimage threshold (OIS), precision/recall (PR) curves, and average precision (AP) <ref type="bibr" target="#b1">[2]</ref>. In all experiments, we use tokens with a size of m=12 based on an observation that a larger size (e.g., m=30) will significantly reduce the sparse representation performance, while a smaller size (e.g., m=5) can hardly represent rich patterns. This token size is also adopted by SRF to train T =10 trees. The skeleton operation is applied to the output contour images of the proposed SemiContour using the non-maximal suppression for quantitative evaluation. Training Image Settings: We randomly split training images into a labeled set and an unlabeled set 1 . We use a fair and relative large number of training tokens for all comparative methods, i.e., 1 × 10 5 for background and foreground. Tokens (including l-tokens and u-tokens) are evenly sampled from each image in both sets. Γ is trained over the labeled set to sample u-tokens from the unlabeled set. Three tests are performed and average accuracies are reported as the final results. BSDS500: BSDS500 <ref type="bibr" target="#b1">[2]</ref> has been widely used as a benchmark for contour detection methods, including 200 training, 100 validation, and 200 test images. Our method uses 3 labeled training images in the labeled set; the rest 197 images are included in the unlabeled set. <ref type="table">Table 1</ref> and <ref type="figure" target="#fig_6">Figure 5</ref>(a) <ref type="table">Table 1</ref>. Contour detection results on BSDS500. In the first column, from top to down, the first block is the human annotations; the second block is unsupervised methods; the third block is supervised methods; the fourth block is our methods. For supervised methods (third block), we show the performance using both 3 and 200 training images (shown in (·)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ODS</head><p>OIS AP Human</p><p>.80 .80 -Canny <ref type="bibr" target="#b6">[7]</ref> .60 .64 .58 Felz-Hutt <ref type="bibr" target="#b14">[15]</ref> .61 .64 .56 Normalized Cuts <ref type="bibr" target="#b9">[10]</ref> .64 .68 .48 Mean Shift <ref type="bibr" target="#b8">[9]</ref> .64 .68 .56 Gb <ref type="bibr" target="#b18">[19]</ref> .69 .72 .72 gPb-owt-ucm <ref type="bibr" target="#b1">[2]</ref> .73 .76 .70 ISCRA <ref type="bibr" target="#b27">[28]</ref> -(.72) -(.75) -(.46) Sketch Tokens <ref type="bibr" target="#b19">[20]</ref> .64(.73) .66(.75) .58(.78) SCG <ref type="bibr" target="#b34">[35]</ref> .73(.74) .75(.76) .76(.77) SE <ref type="bibr" target="#b12">[13]</ref> .66(.74) .68(.76) .69(.78) SE-Var <ref type="bibr" target="#b11">[12]</ref> . In order to compare with supervised methods, we provide the performance with 3 as well as with all 200 labeled training images (comparative results with 200 images are obtained from the authors' original papers). As we can see, the proposed SemiContour method produces similar results as supervised methods using 200 training images, but outperforms all the unsupervised methods and supervised methods with 3 labeled training images. The performance of all supervised approaches except SCG significantly decreases with only 3 labeled training images. Specifically, compared with the SE-Var (an improved version of SE), our method exhibits 4-point higher ODS and 4-point higher AP. The gPb-owt-ucm and SCG, which merely replaces the local contrast estimation of the former that does not rely on many labeled images, exhibit close performance to ours, but our PR curve still shows higher precision with the same recall rates. In terms of efficiency, our method is hundreds of times faster than these two. For a 420 × 320 image, Semi-Contour runs within 0.89s, while gPb-owt-ucm and SCG require 240s and 280s, respectively. Several qualitative example results are shown in <ref type="figure" target="#fig_5">Figure 4</ref>. In addition, we also  <ref type="figure">Figure 6</ref>. The comparative performance by varying tolerance thresholds (maximized pixel distance allowed when matching the estimated contours to ground-truth). SemiContour slightly underperforms SCG and gPb-owt-ucm at stringent thresholds due to some skewed localizations. However, it outperforms both, especially in AP measurement, with the slack thresholds, which means that SemiContour is less likely to miss real contours.</p><p>show the experimental results of the proposed method using a different number of labeled images in <ref type="table">Table 2</ref>. We find that the estimated structured labels of u-tokens sometimes might cause skewed localization at exact contour position. However, our method is less likely to miss real contours, as shown in <ref type="figure">Figure 6</ref>. Precise contour localization is necessary but less important in applications such as object detection and scene understanding.</p><p>We also test the performance of using the proposed SemiCoutour method for segmentation. After contour detections using SemiContour, multiscale-UCM <ref type="bibr" target="#b2">[3]</ref> is applied onto the generated contour images to generate the segmentation results (denoted as SemiContour-Seg in our experiments). We compare SemiContour-Seg with several stateof-the-art methods. The results are shown in <ref type="figure" target="#fig_5">Figure 4</ref> and <ref type="table">Table 3</ref>. SemiContour-Seg also improves the contour detection performance as shown in <ref type="table">Table 1</ref>. <ref type="table">Table 3</ref>. Segmentation results on BSDS500. Evaluation criteria is described in <ref type="bibr" target="#b1">[2]</ref>. Note that we only use three labeled image to train the proposed SemiContour method.</p><p>Cover PRI ODS OIS ODS OIS red-spectral <ref type="bibr" target="#b31">[32]</ref> .56 .62 .81 .85 gPb-owt-ucm <ref type="bibr" target="#b1">[2]</ref> .59 .65 .83 .86 DC <ref type="bibr" target="#b13">[14]</ref> .58 .63 .82 .85 SemiContour-Seg .59</p><p>.64 .83 .85 <ref type="table">Table 4</ref>. Contour detection results on NYUD. In the first column, from top to bottom, the first block is unsupervised method, the second block is supervised methods, and the third block is our method. For supervised methods (second block), we show the performance using both 10 and 381 training images (shown in (·)).</p><p>ODS OIS AP gPb-owt-ucm <ref type="bibr" target="#b1">[2]</ref> .63 .66 .56 Siberman <ref type="bibr" target="#b29">[30]</ref> -(.65) -(.66) -(.29) SE-Var <ref type="bibr" target="#b12">[13]</ref> . To conduct RGB-D contour detection, we treat the depth image as an extra feature channel, and thus the dictionary basis has five channels, and the feature channels for SRF are increased by 11 <ref type="bibr" target="#b12">[13]</ref>. We use 10 images in the labeled set with the rest 371 images in the unlabeled set. The comparison results are shown in <ref type="table">Table 4</ref> and <ref type="figure" target="#fig_6">Figure 5</ref>(b). We can observe that SemiContour with only 10 training images produces superior results than supervised methods trained with 10 images, and also provides competitive results with supervised methods trained using all 381 labeled data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Cross-dataset generalization results</head><p>One advantage of the proposed SemiContour is that it can improve the generalization ability of contour detection by incorporating unlabeled data from the target dataset domain. To validate this, we perform a cross-dataset experiment on BSDS500 and NYUD. The two datasets exhibit significant visual variations. NYUD contains various indoor scenes under different lighting conditions, and BSDS500 contains outdoor scenes. We use one dataset as the labeled set and another as the unlabeled set. The rest experiment setup is the same as SE-Var <ref type="bibr" target="#b11">[12]</ref>. We compare SemiContour with SE-Var in <ref type="table">Table 5</ref>  <ref type="bibr" target="#b2">3</ref> .</p><p>These experiments validate the strong generalization ability and the robustness of the proposed SemiContour method, which indicates a strong noise resistance of the <ref type="bibr" target="#b2">3</ref>   method even when we incorporate u-tokens from a different image domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Efficiency of the proposed fast sparse coding</head><p>The running time of our novel sparse coding algorithm is determined by the steps of basis selection and reconstruction error minimization. The former step needs O(d·V ) to compute V scores and O(V ·K) to select the K bases, and the latter reconstruction error minimization step needs O(d·K 2 ) with a d × K dictionary. Therefore, the total time complexity is max O(d·V ), O(d·K 2 ) , usually O(d·V ) because K is much smaller than V in practice.</p><p>We compare our fast sparse coding solver with several algorithms in <ref type="figure" target="#fig_8">Figure 7</ref>. Most of existing sparse coding algorithms suffer from computational expensive iterations. We only choose several popular ones to compare with our algorithm, including OMP <ref type="bibr" target="#b25">[26]</ref>, Batch-OMP <ref type="bibr" target="#b25">[26]</ref> and its faster version (Batch-OMP-fast). All of these comparative algorithms contain highly optimized implementations and our algorithm is a simple Matlab implementation. We observe that our fast sparse coding algorithm obtains the same results as the others in terms of the final contour detection accuracy, but it is significantly faster than the others. Since the computation of each target data is independent, an additional benefit is that the proposed algorithm can be easily parallelized. All algorithms are tested on an Intel i7@3.60GHz×6 cores and 32GB RAM machine.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusions</head><p>In this paper, we present a novel semi-supervised structured ensemble learning method for contour detection. Specifically, our approach trains an effective contour detector based on structured random forests (SRF). We take advantage of unlabeled data to conduct better node splitting of SRF using sparse representation techniques, whose procedures are embedded in the overall SRF training. In order to increase the scalability of sparse coding to extensive target data, we have proposed a fast and robust sparse coding algorithm. Compared with many existing literatures, our method provides superior testing results.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>:Figure 1 .</head><label>1</label><figDesc>u-tokens with estimated discrete labels : u-tokens with estimated structured labels : u-tokens Illustration of the proposed method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Examples of mean patterns calculated by clustering the structured labels of tokens sampled from 200 images and 3 images into 150 classes<ref type="bibr" target="#b19">[20]</ref>. The patterns calculated from 3 images are almost identical to the patterns calculated from 200 images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>x can be represented by a linear combination of K bases in a dictionary M = [m 1 , ..., m V ] containing V bases (K ≪ V ). A sparse code c is calculated to select the K bases. Given a set of training tokens X = [x 1 , ..., x n ], the dictionary M , as well as the associated C = [c 1 , ..., c n ], is learned by minimizing the reconstruction error [t. ∀i, m i 2 = 1 and ∀j, ||c j || 0 ≤ K,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 .</head><label>3</label><figDesc>Illustration of the sparse token representation. We empirically set V =256 and K=3 when training M f and M b separately, in which 1 × 10 5 training tokens are used for each. The overall dictionary is M = [M b , M f ] ∈ R (m·m·4)×512 and the sparsity is set as K=6. Left: Examples of some bases with the RGB channels (left) and the contour channel (right). Middle: Average sparse code (512 dimensional vectors) values of foreground tokens (top) and background tokens (bottom) from test images. We can see foreground tokens tend to select bases (i.e., assigning high weights to bases) belonging to M f , while background tokens tend to select bases belonging to M b . Right: Reconstruction representation of input images (top) with the RGB channels (middle) and the contour channel (bottom). Contours are well preserved and background noises are greatly suppressed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>Experimental results on BSDS500. The first two columns show the original image and ground truth. The next three columns show results of comparative methods and our SemiContour. SemiContour produces more clean background and stronger responses on high confident contours as indicated by ground truth. The last two columns show the segmentation results of SemiContour-Seg and DC<ref type="bibr" target="#b13">[14]</ref>. Our method produces more consistent segmentation due to less false positive contour detection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 .</head><label>5</label><figDesc>Precision/recall curves on BSDS500 and NYUD.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>Later on, we conducted an extra experiment to augment 200 labeled training images of BSDS with 100 unlabeled images of NYUD to improve the testing results of BSDS. Our method achieves (.752ODS, .786OIS, .792AP), compared with SE-Var's results (.743ODS, .763OIS, .788AP), both with totally 1 million training tokens.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 .</head><label>7</label><figDesc>Runtime comparison results. The dictionary size is 576× 512 and the sparsity K=6. Our method significantly outperforms the others as the number of target data increases.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>Table 5. Cross-dataset generalization results. The first column indicates the training/testing dataset settings that we used. Semi-Contour outperforms SE-Var on both settings. NYUD: NYUD contains 1449 RGB-D images. We follow<ref type="bibr" target="#b12">[13]</ref> to perform the experiment setup. The dataset is splited into 381 training, 414 validation, and 654 testing images.</figDesc><table>66(.69) .68(.71) .68(.72) 
SemiContour 
.68 
.70 
.69 

ODS OIS AP 

NYUD/BSDS 
SE-Var 
.73 
.74 .77 
SemiContour 
.73 
.75 .78 

BSDS/NYUD 
SE-Var 
.64 
.66 .63 
SemiContour 
.65 
.66 .63 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">To compensate for possible insufficient foreground l-tokens, we duplicated images in the labeled set by histogram matching.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We carefully check every step when re-training their model and keep the other parameters default.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">K-svd: An algorithm for designing overcomplete dictionaries for sparse representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aharon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bruckstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="4311" to="4322" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Contour detection and hierarchical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>PAMI</publisher>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="898" to="916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multiscale combinatorial grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="328" to="335" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Deepedge: A multiscale bifurcated deep network for top-down contour detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.1123</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multipath sparse coding using hierarchical matching pursuit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="660" to="667" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Random forests. Machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="5" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A computational approach to edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Canny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="679" to="698" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zien</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Mean shift: A robust approach toward feature space analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Comaniciu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Meer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<publisher>PAMI</publisher>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="603" to="619" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Spectral segmentation with multiscale graph decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Benezit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1124" to="1131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Decision forests: A unified framework for classification, regression, density estimation, manifold learning and semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Konukoglu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Fast edge detection using structured forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zitnick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Structured forests for fast edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1841" to="1848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Discrete-continuous gradient orientation estimation for faster image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Donoser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Schmalstieg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3158" to="3165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Efficient graphbased image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Huttenlocher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="167" to="181" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Nˆ4-fields: Neural network nearest neighbor fields for image transforms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="536" to="551" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Structured class-labels in random forests for semantic image labelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kontschieder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Bulo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pelillo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2190" to="2197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Semisupervised random forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Leistner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saffari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Santner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="506" to="513" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Generalized boundaries from multiple image interpretations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Leordeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>PAMI</publisher>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1312" to="1324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Sketch tokens: A learned mid-level representation for contour and object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3158" to="3165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Robust tracking using local sparse appearance model and k-selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kulikowsk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1313" to="1320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Robust muscle cell quantification using structured edge detection and hierarchical segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mcgough</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="324" to="331" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Semi-supervised node splitting for random forest construction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="492" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Reconstructive sparse code transfer for contour detection and semantic labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">Y</forename><surname>Stella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="273" to="287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Affordance detection of tool parts from geometric features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Myers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Teo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fermüller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Aloimonos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Orthogonal matching pursuit: Recursive function approximation with applications to wavelet decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">C</forename><surname>Pati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rezaiifar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krishnaprasad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asilomar Conference on Signals, Systems and Computers</title>
		<imprint>
			<date type="published" when="1993" />
			<biblScope unit="page" from="40" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Figure/ground assignment in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="614" to="627" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Image segmentation by cascaded region agglomeration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shakhnarovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2011" to="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deepcontour: A deep convolutional feature learned by positivesharing loss for contour detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3982" to="3991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Indoor segmentation and support inference from rgbd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="746" to="760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Co-tracking using semi-supervised support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Brennan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Towards fast and accurate segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1916" to="1922" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Fast 2d border ownership assignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Teo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fermüller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Aloimonos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5117" to="5125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Locality-constrained linear coding for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="3360" to="3367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Discriminatively trained sparse code gradients for contour detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xiaofeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="584" to="592" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.06375</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">Holistically-nested edge detection. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Beyond sparsity: The role of l 1-optimizer in pattern classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1104" to="1118" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
