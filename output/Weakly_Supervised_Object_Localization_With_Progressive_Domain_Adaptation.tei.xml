<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Weakly Supervised Object Localization with Progressive Domain Adaptation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Illinois</orgName>
								<address>
									<settlement>Urbana-Champaign</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yali</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Merced</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Weakly Supervised Object Localization with Progressive Domain Adaptation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:48+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We address the problem of weakly supervised object localization where only image-level annotations are available for training. Many existing approaches tackle this problem through object proposal mining. However, a substantial amount of noise in object proposals causes ambiguities for learning discriminative object models. Such approaches are sensitive to model initialization and often converge to an undesirable local minimum. In this paper, we address this problem by progressive domain adaptation with two main steps: classification adaptation and detection adaptation. In classification adaptation, we transfer a pre-trained network to our multi-label classification task for recognizing the presence of a certain object in an image. In detection adaptation, we first use a mask-out strategy to collect class-specific object proposals and apply multiple instance learning to mine confident candidates. We then use these selected object proposals to fine-tune all the layers, resulting in a fully adapted detection network. We extensively evaluate the localization performance on the PASCAL VOC and ILSVRC datasets and demonstrate significant performance improvement over the state-of-the-art methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Object localization is an important task for image understanding. It aims to identify all instances of particular object categories (e.g., person, cat, and car) in images. The fundamental challenge in object localization lies in constructing object appearance models for handling large intra-class variations and complex background clutters. The state-of-the-art approaches typically train object detectors from a large set of training images <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b13">14]</ref> in a fully supervised manner. However, this strongly supervised learning paradigm relies on instance-level annotations, e.g., tight bounding boxes, which are time-consuming and laborintensive. In this paper, we focus on the weakly supervised object localization problem where only binary image-level labels indicating the presence or absence of an object category are available for training. <ref type="figure" target="#fig_0">Figure 1</ref> illustrates the prob- <ref type="figure" target="#fig_0">Figure 1</ref>. Weakly supervised object localization problem setting. Given a collection of training images with image-level annotations, our goal is to train object detectors for localizing objects in unseen images. lem setting. This particular setting is important for largescale practical applications because image-level annotations are often readily available from the Internet, e.g., through text tags <ref type="bibr" target="#b14">[15]</ref>, GPS tags <ref type="bibr" target="#b7">[8]</ref>, and image search queries <ref type="bibr" target="#b22">[23]</ref>.</p><p>Most existing methods <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b33">34]</ref> formulate the weakly supervised localization (WSL) task as a multiple instance learning (MIL) problem. Recent efforts include leveraging convolutional neural networks (CNN) to extract discriminative appearance features <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3]</ref> and transferring knowledge from strongly supervised detectors to other categories without bounding box annotations <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b30">31]</ref>. While existing methods have shown promising results, these methods have three main drawbacks. First, it's hard to select correct object proposals because the collection of candidate proposals contains too much noise. Typically, only a few out of several thousands of proposals are actual object instances. Second, many approaches use a pre-trained CNN as a feature extractor and do not adapt the weights from whole-image classification to object detection. Third, existing methods often require either auxiliary strongly annotated data or pre-trained detectors for domain adaptation. <ref type="bibr">Figure 2</ref>. Comparison of our approach with existing object localization methods. Strongly supervised methods use instance-level annotations to train object detectors. Most of the weakly supervised methods use one-step proposal mining to select object instances from a large and noisy candidate pool directly. We propose a two-step progressive domain adaptation approach. We first filter out the noisy object proposal collection and then mine confident candidates for learning discriminative object detectors.</p><p>In this paper, we propose a two-step domain adaptation for weakly supervised object localization: classification adaptation and detection adaptation. <ref type="figure">Figure 2</ref> illustrates the major difference between the proposed algorithm and existing work. Our key observation is that it's hard to train object detectors directly under weak supervisory signals due to the substantial amount of noise in the object proposal collections. Essentially, the main difficulty arises from the large gap between source domain and target domain, as shown in the top-right and bottom-left corner of <ref type="figure">Figure 2</ref>. The goal of our work is to bridge the gap by progressive domain adaptation. In the classification adaptation step, we train a classification network using the given weak image-level labels. We train the classification network to recognize the presence of a certain object category in an image. In the detection adaptation step, we use the classification network to collect class-specific object proposals and apply multiple instance learning to mine confident candidates. We then use the previously selected object candidates to fine-tune all the layers, resulting in a fully adapted detection network.</p><p>The proposed algorithm addresses the drawbacks from prior work in three aspects: (1) Our classification adaptation step fine-tunes the network such that it can collect classspecific object proposals with higher precision. This step aims at removing background clutters and potential confusion from similar objects cases, leading to a purified collection of object candidates for multiple instance learning.</p><p>(2) Detection adaptation uses confident object candidates to optimize the CNN representations for the target domain. This step aims at turning image classifiers into object detectors, providing more discriminative feature representations for localizing generic objects (instead of the presence of them) in an image. (3) Our method learns object detectors from weakly annotated data without any strong labels.</p><p>We make the following three contributions in this work: 1. We propose to use progressive domain adaptation for weakly supervised object localization. We show that this strategy is crucial for good performance. 2. Our classification adaptation helps filter the object proposal collection, and our detection adaptation helps learn discriminative feature representation for the detection task. 3. We present detailed evaluations on the PASCAL VOC and ILSVRC datasets. Experimental results demonstrate that our progressive domain adaptation algorithm performs favorably against the state-of-the-art methods. Our detector achieves 39.5% mAP on VOC 2007, surpassing the second best performing algorithm by 8 points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Weakly supervised learning. Existing methods often treat WSL as an MIL problem <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b36">37]</ref>.</p><p>In an MIL framework, each image is considered as a bag of potential object instances. Positive images are assumed to contain at least one object instance of a certain object category and negative images do not contain object instances from this category. Using this weak supervisory signal, WSL methods often alternate between (1) selecting the positive object instances from positive images and (2) learning object detectors. However, this results in a non-convex optimization problem. Due to the non-convexity, these methods are sensitive to model initialization and prone to getting trapped into local extrema. Although many efforts have been made to overcome the problem via seeking better initialization models <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b32">33]</ref> and optimization strategies <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b1">2]</ref>, the localization performance is still limited. We observe that previous MIL-based methods attempt to train object detectors directly from the large and noisy collection of object candidates. In this work, we apply MIL <ref type="bibr" target="#b35">[36]</ref> to mine confident candidates. However, unlike existing methods, we apply MIL on a cleaner collection of class-specific object proposals (instead of on a large, noisy, category-independent proposals).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Convolutional neural networks for object localization.</head><p>Recently, convolutional neural networks have achieved great success on various visual recognition tasks <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b12">13]</ref>. The key ingredient for the success lies in end-to-end training CNN in a fully supervised fashion. In object detection problems, these methods <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b12">13]</ref> require instance-level supervision. Moving beyond strong supervision, recent work focuses on applying off-the-shelf CNN features <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b1">2]</ref>, learning from weak labels <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b23">24]</ref> or noisy labels <ref type="bibr">[26,</ref><ref type="bibr" target="#b37">38]</ref>. Our classification adaptation step is related to the method by Oquab et al. <ref type="bibr" target="#b23">[24]</ref> in the formulation of multi-label classification. We use a different a multi-label loss that allows us to incorporate negative images during training. Also, we focus on detecting the locations and spatial supports of objects while the method by Oquab et al. <ref type="bibr" target="#b23">[24]</ref> only predicts approximate locations of objects. Our work resembles the work by Bazzani et al. <ref type="bibr" target="#b0">[1]</ref>. We use a similar mask-out strategy to collect class-specific object proposals. The main differences are three-fold. Domain adaptation. Some recent approaches use domain adaptation to help learn object detectors or features <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b30">31]</ref>. Shi et al. <ref type="bibr" target="#b30">[31]</ref> learn a mapping relationship between the bounding box overlap and the appearance similarity, and then transfer it to the target domain. Hoffman et al. <ref type="bibr" target="#b15">[16]</ref> learn the difference between classification and detection tasks and transfer this knowledge to convert classifiers to detectors using weakly annotated data. Also, MIL is incorporated for joint learning of representation and detector <ref type="bibr" target="#b16">[17]</ref>. Rochan et al. <ref type="bibr" target="#b26">[27]</ref> transfer existing appearance models of the familiar objects to the unseen object. Existing domain adaptation methods often use strongly annotated source data to improve recognition performance for weakly supervised object localization. Our work differs from these approaches in that we focus on object localization in a weakly supervised manner, i.e., we do not require any instance-level annotation and do not borrow additional strongly annotated data or outside detectors.</p><p>Progressive and self-paced learning. Our work is also related to several approaches in other problem contexts. Examples include visual tracking <ref type="bibr" target="#b38">[39]</ref>, pose estimation <ref type="bibr" target="#b11">[12]</ref>, image search <ref type="bibr" target="#b19">[20]</ref>, and object discovery <ref type="bibr" target="#b21">[22]</ref>. Progressive methods can decompose complex problems into simpler ones. We find that progressive adaptation is particularly important for the weakly supervised object localization problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Classification Adaptation</head><p>In this section, we introduce the classification adaptation step. This step aims to train the whole-image classifica- <ref type="figure">Figure 3</ref>. Classification adaptation step. We use the AlexNet architecture <ref type="bibr" target="#b20">[21]</ref> and replace the softmax loss layer with the proposed multi-label loss layer. We set the number of nodes in the last fullyconnected layer to 2C (C is number of object categories). These 2C entries are divided into C pairs for representing the presence and absence of each object category. See Section 3 for details. tion network such that the adapted network is sensitive to the object categories of interest. The original AlexNet <ref type="bibr" target="#b20">[21]</ref> is trained for multi-class classification with a softmax loss layer by assuming that only one single object exists per image. In our adapted network, we replace the last classification layer with a multi-label loss layer. Unlike the problem in ImageNet classification, we address a more general multi-label classification problem where each image may contain multiple objects from more than one category.</p><p>Assuming that the object detection dataset has C categories and a total of N training images, we denote the weakly labeled training image set as I = {(I (1) , y (1) ), . . . , (I (N ) , y (N ) )}, where I is the image data and y = [y 1 , . . . , y c , . . . , y C ] ⊤ ∈ {0, 1} C , c ∈ {1, . . . , C} is the C-dimensional label vector of I, in which each entry can be 1 or 0 indicating whether at least one specific object instance exists in the image. In the weakly object localization setting, one image may contain objects from different categories, i.e., more than one entry in y can be 1. In this case, conventional softmax loss cannot be used for this multi-label classification problem. We thus introduce a multi-label loss to handle this problem.</p><p>First, we transform the original training label to a new label t ∈ {0, 1} 2C , where</p><formula xml:id="formula_0">t 2c−1 = 1, y c = 1 0, y c = 0 and t 2c = 0, y c = 1 1, y c = 0 .<label>(1)</label></formula><p>In other words, each odd entry of t represents whether the image contains the corresponding object. Similarly, each even entry represents whether the image does not contain the corresponding object. We then introduce our new loss layer for multi-label classification. We denote the CNN as a function p(·) that maps an input image I to a 2C dimensional output p(I) ∈ R 2C . The odd entry p 2c−1 (I) represents the probability that the image contains at least one object instance of c-th category. . Detection adaptation step. We first use a mask-out strategy to collect class-specific object proposals and apply multiple instance learning to mine confident candidates. We then use these selected object proposals to fine-tune all the layers (marked magenta), resulting in a network that is fully adapted for detection. See Section 4 for details.</p><p>Similarly, the even entry p 2c (I) indicates the probability that the image does not contain objects of c-th category. We compute the probabilities using a sigmoid for each object class and thus we have p 2c−1 (I) + p 2c (I) = 1.</p><p>We can define negative logarithmic classification loss L c (I) of one image for category c as, L c (I) = −(t 2c−1 log p 2c−1 (I) + t 2c log p 2c (I)).</p><p>(</p><p>We obtain the final loss function L by summing up all the training samples and losses for all the categories,</p><formula xml:id="formula_2">L = N i=1 C c=1 L c (I (i) ) = − N i=1 t (i) log p(I (i) ). (3)</formula><p>Here log(·) is the element-wise logarithmic function. The sum over different categories is done by dot product.</p><p>In the classification adaptation network, we substitute the conventional softmax loss layer with our new multilabel loss layer and adjust the number of nodes in the last fully-connected layer to 2C. We use mini-batch Stochastic Gradient Descent (SGD) for training the CNN. We initialize all the layers except the last layer using the pre-trained parameters on ILSVRC 2012 <ref type="bibr" target="#b5">[6]</ref>. We randomly initialize the weights of the modified classification layer. We describe the implementation details in Section 5.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Detection Adaptation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Class-specific proposal mining</head><p>The goal of detection adaptation step is to transfer image classifiers to object detectors. To train object detectors, we first collect confident object proposals. We use a mask-out strategy to collect class-specific object proposals and apply multiple instance learning to mine confident candidates. The mining procedure offers two key benefits:</p><p>• Compared with generic object proposals, class-specific proposals remove substantial noise and potential confusion from similar objects. This helps MIL avoid converging to an undesirable local minimum and reduce computational complexity. • More precise object proposals can be mined using MIL. These confident object proposals allow us to further fine-tune the network for object detection. The adapted classification network recognizes whether an image contains a certain object. We use a mask-out strategy to collect object proposals for each class based on the adapted classification network. The idea of masking out the input of CNN has been previously explored in <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b0">1]</ref>. Intuitively, if the mask-out image by a region causes a significant drop in classification score for the c-th class, the region can be considered discriminative for the c-th class. Inspired by <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b0">1]</ref>, we investigate the contrastive relationship between a selected region and its mask-out image.</p><p>Without loss of generality, we take mining object proposals for the c-th category as an example. First, for the image I labeled with y c = 1, we apply Edge Boxes <ref type="bibr" target="#b45">[46]</ref> to generate the initial collection of object proposals. The set of initial proposals is marked asB c . For an initial bounding box regionb, we denote its corresponding image as I in (b) and its mask-out image as I out (b). We generate the mask-out image by replacing the pixel values withinb with the fixed mean pixel values pre-computed on ILSVRC 2012. We feed the region image I in (b) and mask-out image I out (b) to the adapted classification network. We can then compute the contrastive score for bounding box regionb of image I as,</p><formula xml:id="formula_3">s c (I,b) = p 2c−1 (I in (b)) − p 2c−1 (I out (b)).<label>(4)</label></formula><p>Here, if the value of s c (I,b) is large, it indicates that the regionb is likely an object of the c-th category. Note that our mask-out strategy differs from <ref type="bibr" target="#b0">[1]</ref>, which compute the score difference between the whole image and mask-out image.</p><p>With classification adaptation, a bounding box region can achieve higher confidence than the whole image for classification. In <ref type="figure">Figure 5</ref>, we show top 10 class-specific proposals using our mask-out strategy. According to (4), top M (M =50 in our experiments) object proposals of image I are selected for the c-th category. That is, selected proposals are category-specific. We mark the top-ranked proposals as B c .</p><p>Since we set a loose criteria in the previous mask-out step, the top-ranked proposals are still coarse and may contain many false positives. We apply MIL to mine confident candidates for training object detector. In MIL, the label of object candidate is set as a latent variable. During the training, the label is iteratively updated. For candidates set B c , we set up latent variable z k c ∈ {0, 1} M , k, c ∈ 1, . . . , C, in which each entry represents whether the corresponding proposal is an object of the k-th category. We make two assumptions for solving z k=c c .</p><p>• For image I with y c = 1, at least one proposal in B c belongs to the c-th category, i.e., 1 ⊤ · z k=c c ≥ 1 where 1 is an M -dimensional all-one vector.</p><p>• For image I ′ with y c = 0, none of proposals in B ′ c ′ =c belongs to the c-th category, i.e., 1 ⊤ · z k=c c ′ =c = 0. Under the two assumptions, we can treat each image with y c = 1 as a positive bag and treat each image with y c = 0 as a negative bag. We then cast the task of solving z k=c c as an MIL problem. Note that multiple positive instances can be collected according to the scores of the MIL classifier for each class.</p><p>We use the smoothed hinge loss function in <ref type="bibr" target="#b35">[36]</ref>. Note that the initialization step in <ref type="bibr" target="#b35">[36]</ref> is carried out via a submodular clustering method from the initial object proposals. The noisy collection of proposals limits the performance of clustering process. Also, the initialization step is time-consuming as the similarity measures among all the proposals in all the images need to be computed. Our classspecific proposals not only help filter the object proposal collection but also reduce the training time of MIL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Object detector learning</head><p>In this step, we aim at adapting the network from multilabel image classification for object detection. We jointly train the detection network with C object classes and a background class instead of training each object detector independently. Similar to <ref type="bibr" target="#b12">[13]</ref>, we replace the 2Cdimensional classification layer (for image-level classifica- <ref type="figure">Figure 5</ref>. Examples of the mined object proposals using the maskout strategy. We show top 10 proposals for each category (different colors indicate mined proposals for different categories). Note that the mined object proposals are class-specific. tion) with a randomly initialized (C+1)-dimensional classification layer (for instance-level classification with C object classes and background). We take the top-scoring proposals given by MIL as positive samples for each object category. We collect background samples from object proposals that have a maximum IoU ∈ [0.1, 0.5) overlap with the mined object proposals by MIL. For data augmentation, we also treat all the proposals that have IoU ≥ 0.5 overlap with a mined object as positive samples.</p><p>Given a test image, we first generate object proposals using Edge Boxes <ref type="bibr" target="#b45">[46]</ref> and use the adapted detection network to score each proposal. We then rank all the proposals and use non-maximum suppression to obtain final detections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Implementation details</head><p>For multi-label image classification training, we use the AlexNet <ref type="bibr" target="#b20">[21]</ref> as our base CNN model, initialized with the parameters pre-trained on ImageNet dataset. We train the network with SGD at a learning rate of 0.001 for 10,000 mini-batch iterations. We set the size of mini-batch to 500. For class-specific proposal mining, we use Edge Boxes <ref type="bibr" target="#b45">[46]</ref> to generate 2,000 initial object proposals and select top M =50 proposals as the input for multiple instance learning. For object detector training, we use AlexNet <ref type="bibr" target="#b20">[21]</ref> and VG-GNet <ref type="bibr" target="#b31">[32]</ref> as our base models. Similar to Fast-RCNN <ref type="bibr" target="#b12">[13]</ref>, we set the maximum number iterations to 40k.</p><p>We implement our network using Caffe <ref type="bibr" target="#b18">[19]</ref>. For the PASCAL VOC 2007 trainval set, fine-tuning the AlexNet for classification and detection adaptation takes about 10 hours and 1 hours with a Tesla K40 GPU, respectively. With our mined class-specific proposals, it takes about 3 hours to mine confident object samples on PC wBazzani:WACV16ith a 4.0 GHz Intel i7 CPU and 16 GB memory. The source code, as well as the pre-trained models, are available at the project webpage 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Datasets and evaluation metrics</head><p>Datasets. We extensively evaluate the proposed method on the PASCAL VOC 2007, 2010, 2012 datesets <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b8">9]</ref> and ILSVRC 2013 detection dataset <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b27">28]</ref>. For VOC 2007, we use both train and val splits as the training set and test split as our test set. For VOC 2010 and 2012, we use train split as the training set and val split as the test set. For the ILSVRC detection dataset, we follow the RCNN <ref type="bibr" target="#b13">[14]</ref> in splitting the val data into val 1 and val 2 . We use val 1 for training object detectors and val 2 for validating the localization performance. Note that we do not use any instance-level annotations (i.e., object bounding boxes) for training in all the datasets.</p><p>Evaluation metrics. We use two metrics to evaluate localization performance. First, we compute the fraction of positive training images in which we obtain correct localization (CorLoc) <ref type="bibr" target="#b6">[7]</ref>. Second, we measure the performance of object detectors using average precision (AP) in the test set. For both metrics, we consider that a bounding box is correct if it has an intersection-over-union (IoU) ratio of at least 50% with a ground-truth object instance annotation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Comparison to the state-of-the-art</head><p>We compare the proposed algorithm with state-of-theart methods for weakly supervised object localization, including the MIL-based methods <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3]</ref>, topic model <ref type="bibr" target="#b29">[30]</ref>, and latent category learning <ref type="bibr" target="#b40">[41]</ref>. For fair comparisons, we do not include methods that use strong labels for training. <ref type="table" target="#tab_0">Table 1</ref> shows performance comparison in terms of Cor-Loc on the PASCAL VOC 2007 trainval set. Our method achieves 52.4% of average CorLoc for all the 20 categories, outperforming all the state-of-the-art algorithms. Compared to the MIL-based approaches <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4]</ref>, we achieve significant improvements by 10 to 20 points. While these approaches use sophisticated model initialization or optimization strategies for improving MIL, the inevitable noise in the initial collection of category-independent proposals limits the performance of trained object detectors during MIL iterations. Compared to the topic model <ref type="bibr" target="#b29">[30]</ref>, we incorporate inter-class relations by jointly training CNN with all object classes and background class while they rely on handcrafted features. Wang et al. <ref type="bibr" target="#b40">[41]</ref> use a pre-trained CNN for feature extraction. In contrast, we learn feature representations with our classification and detection adaptation, boosting the performance of CorLoc by 3.9 points. <ref type="table">Table 2</ref> shows the detection average precision (AP) performance on the PASCAL VOC 2007 test set. Our method achieves 39.5% mAP, outperforming the state-of-the-art approaches by 8 points. Our method using the AlexNet achieves comparable performance with the second best method <ref type="bibr" target="#b40">[41]</ref>, 31.6% (ours) vs. 31.0% <ref type="bibr" target="#b40">[41]</ref>. Most of existing methods <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37]</ref> use pre-trained networks to extract features for object detector learning and do not finetune the network. In contrast, we progressively adapt the network from whole-image classification to object detection. Such domain adaptation helps learn better object de-tectors from weakly annotated data. Unlike previous work that relies on noisy and class-independent proposals to select object candidates, we mine purer and class-specific proposals for MIL training, which can discard background clutters and confusion with similar objects. <ref type="table" target="#tab_1">Table 3</ref> shows our detection performance in terms of mean average precision on the PASCAL VOC 2010 and 2012 and ILSVRC 2013 datasets 2 . Using the VGGNet, our method achieves better localization performance. We include the full results in the supplementary materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Ablation studies</head><p>To quantify the relative contribution of each step, we examine the performance of our approach using different configurations.</p><p>• OM: Using mask-out strategy to mine top M =50 classspecific object proposals. • MIL: Using MIL to mine confident objects.</p><p>• FT: Using the mined object candidates to fine-tune the detection network. The last four rows of <ref type="table" target="#tab_0">Table 1</ref> show our CorLoc performance on the PASCAL VOC 2007 trainval set. We achieve average CorLoc of 31.8% by directly using topranked class-specific object proposals. Using MIL for selecting confident objects, we obtain 41.2% with around 10 points improvement. The result demonstrates that MIL iterations help to select better object proposals. The performance boost comes from: (1) the mined object proposals are less noisy and can discard background clutters, and (2) the mined object proposals are class-specific and can discard confusion with similar objects. Furthermore, adding detection network fine-tuning, we obtain 49.8% performance using the AlexNet and 52.4% using a deeper VG-GNet <ref type="bibr" target="#b31">[32]</ref>. Such network training further boosts the performance by another 10 points. In detection adaptation, we collect confident object proposals and use them to train all the layers. This fine-tuning step helps turn image classifiers to object detectors for modeling object appearance.</p><p>The last five rows of <ref type="table">Table 2</ref> show our detection AP performance on the PASCAL VOC 2007 test set. We refer Song et al. <ref type="bibr" target="#b35">[36]</ref> as our MIL baseline. A straightforward approach to train detector uses proposals selected by MIL. However, the simple combination only gives marginal performance improvement from 22.7% to 23.0% because the selected proposals by MIL are too noisy for training object detection network effectively without object mining. Using the top-ranked object proposals based on the adapted classification network, we achieve significant improvement from 23.0% to 31.0%, highlighting the importance of progressive adaptation. Using a deeper network VGGNet <ref type="bibr" target="#b31">[32]</ref>, we can achieve a large improvement from 26.2% to 39.5%. In ad-  dition, we evaluate the performance using the best proposal (M =1) mined by the mask-out strategy for detection adaptation. The OM+FT method achieves 19.5% mAP using AlexNet and 20.5% using VGGNet. Without the MIL step, the results are poor due to noisy training samples. These experimental results validate the importance of the progressive adaptation steps proposed in this work. <ref type="table" target="#tab_2">Table 4</ref> shows results using different mask-out strategies. Similar to the top 5 error evaluation for the ImageNet classification protocol, we compute the percentage of positive images in which an object is correctly located by at least one from top M proposals. When M =1, this metric reduces to the commonly used CorLoc. These results show our contrastive score In-Out strategy outperforms Whole-Out. Only using classification score of the region itself can also collect good proposals because classification adaptation step trains the network to be sensitive to object categories of target datasets. As the classification network is fine-tuned using the whole image, the mask-out image provides additional discriminative power for ranking the object proposals. Considering the trade-off between recall and precision, we set M =50 throughout the experiments. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Error analysis</head><p>In <ref type="figure" target="#fig_3">Figure 7</ref>, we apply the detector error analysis tool from Hoiem et al. <ref type="bibr" target="#b17">[18]</ref> to analyze errors of our detector. Comparing the first and third columns, we achieve significant improvement of localization performance by detection adaptation. Fine-tuning the network for object-level detection helps learn discriminative appearance model for object categories, particularly for animals and furniture classes. Comparing the second and third columns, the importance of class-specific proposal mining step is clear. We attribute the performance boost to the classification adaptation that finetunes the network from 1000-way single-label classification (source) to 20-way multi-label classification task (target).</p><p>From the error analysis plots, the majority of errors comes from inaccurate localization. We show a few sample results in <ref type="figure" target="#fig_4">Figure 8</ref>. Our model often detects the correct category of an object instance but fails to predict a sufficiently tight bounding box, e.g., IoU ∈ [0.1, 0.5). For example, we may detect a human face and a partial train and claim to detect a person or a train. The error analysis suggests that the learned model makes sensible errors. We believe that we can further improve the performance of our model  by incorporating techniques for addressing the inaccurate localization issues <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b44">45]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We present a progressive domain adaptation approach to tackle the weakly supervised object localization problem. In classification adaptation, we transfer the classifiers from source to target domains using a multi-label loss function for training a multi-label classification network. In detection adaptation, we transfer adapted classifiers to object detectors. We first use a mask-out strategy to generate classspecific object proposals and apply MIL to mine confident candidates. We then use the selected object proposals to fine-tune all the layers for object detection. Experimental results demonstrate that our algorithm significantly outperforms the state-of-the-art methods. We achieve 39.5% mAP on VOC 2007, surpassing the second best approach by 8 points.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>( 1 )</head><label>1</label><figDesc>Our classification adaptation transfers the source classification domain (1000 single-label classes for ILSVRC 2012) to the target classification domain (20 multilabel classes for PASCAL VOC). (2) We use a contrastbased mask-out strategy for ranking proposals. (3) Instead of training a classifier over pre-trained CNN features, we fine-tune the parameters of all the CNN layers for training object detectors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4</head><label>4</label><figDesc>Figure 4. Detection adaptation step. We first use a mask-out strategy to collect class-specific object proposals and apply multiple instance learning to mine confident candidates. We then use these selected object proposals to fine-tune all the layers (marked magenta), resulting in a network that is fully adapted for detection. See Section 4 for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 6 .</head><label>6</label><figDesc>Sample detection results. Green boxes indicate ground-truth instance annotation. Yellow boxes indicate correction detections (with IoU ≥ 0.5). For all the testing results, we set threshold of detection as 0.8 and use NMS to remove duplicate detections.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 7 .</head><label>7</label><figDesc>Detector error analysis. The detections are categorized into five types of correct detection (Cor), false positives due to poor localization (Loc), confusion with similar objects (Sim), confusion with other VOC objects (Oth), and confusion with background (BG). Each plot shows types of detection as top detections increase. Line plots show recall as function of the number of objects by IoU ≥ 0.5 (solid) and IoU ≥ 0.1 (dash). The results of "MIL+FT" and "OM+MIL+FT" are obtained using the VGGNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 8 .</head><label>8</label><figDesc>Sample results of detection errors due to imprecise localization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 .</head><label>1</label><figDesc>Quantitative comparison in terms of correct localization (CorLoc) on the PASCAL VOC 2007 trainval set. Table 2. Quantitative comparison in terms of detection average precision (AP) on the PASCAL VOC 2007 test set. OM + MIL + FT-AlexNet 49.7 33.6 30.8 19.9 13 40.5 54.3 37.4 14.8 39.8 9.4 28.8 38.1 49.8 14.5 24.0 27.1 12.1 42.3 39.7 31.0 OM + MIL + FT-VGGNet 54.5 47.4 41.3 20.8 17.7 51.9 63.5 46.1 21.8 57.1 22.1 34.4 50.5 61.8 16.2 29.9 40.7 15.9 55.3 40.2 39.5</figDesc><table>Methods 
aero bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tv Avg. 
Siva et al. [33] 
45.8 21.8 30.9 20.4 5.3 37.6 40.8 51.6 7.0 29.8 27.5 41.3 41.8 47.3 24.1 12.2 28.1 32.8 48.7 9.4 30.2 
Shi et al. [30] 
67.3 54.4 34.3 17.8 1.3 46.6 60.7 68.9 2.5 32.4 16.2 58.9 51.5 64.6 18.2 3.1 20.9 34.7 63.4 5.9 36.2 
Cinbis et al. [4] 
56.6 58.3 28.4 20.7 6.8 54.9 69.1 20.8 9.2 50.5 10.2 29.0 58.0 64.9 36.7 18.7 56.5 13.2 54.9 59.4 38.8 
Bilen et al. [3] 
66.4 59.3 42.7 20.4 21.3 63.4 74.3 59.6 21.1 58.2 14.0 38.5 49.5 60.0 19.8 39.2 41.7 30.1 50.2 44.1 43.7 
Wang et al. [41] 
80.1 63.9 51.5 14.9 21.0 55.7 74.2 43.5 26.2 53.4 16.3 56.7 58.3 69.5 14.1 38.3 58.8 47.2 49.1 60.9 48.5 

OM 
50.4 30 34.6 18.2 6.2 39.3 42.2 57.3 10.8 29.8 20.5 41.8 43.2 51.8 24.7 20.8 29.2 26.6 45.6 12.5 31.8 
OM + MIL 
64.3 54.3 42.7 22.7 34.4 58.1 74.3 36.2 24.3 50.4 11.0 29.2 50.5 66.1 11.3 42.9 39.6 18.3 54.0 39.8 41.2 
OM + MIL + FT-AlexNet 77.3 62.6 53.3 41.4 28.7 58.6 76.2 61.1 24.5 59.6 18.0 49.9 56.8 71.4 20.9 44.5 59.4 22.3 60.9 48.8 49.8 
OM + MIL + FT-VGGNet 78.2 67.1 61.8 38.1 36.1 61.8 78.8 55.2 28.5 68.8 18.5 49.2 64.1 73.5 21.4 47.4 64.6 22.3 60.9 52.3 52.4 

Methods 
aero bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tv mAP 
Cinbis et al. [4] 
35.8 40.6 8.1 7.6 3.1 35.9 41.8 16.8 1.4 23.0 4.9 14.1 31.9 41.9 19.3 11.1 27.6 12.1 31.0 40.6 22.4 
Song et al. [36] 
27.6 41.9 19.7 9.1 10.4 35.8 39.1 33.6 0.6 20.9 10.0 27.7 29.4 39.2 
9.1 19.3 20.5 17.1 35.6 7.1 22.7 
Song et al. [37] 
36.3 47.6 23.3 12.3 11.1 36.0 46.6 25.4 0.7 23.5 12.5 23.5 27.9 40.9 14.8 19.2 24.2 17.1 37.7 11.6 24.6 
Bilen et al. [2] 
42.2 43.9 23.1 9.2 12.5 44.9 45.1 24.9 8.3 24.0 13.9 18.6 31.6 43.6 
7.6 20.9 26.6 20.6 35.9 29.6 26.4 
Bilen et al. [3] 
46.2 46.9 24.1 16.4 12.2 42.2 47.1 35.2 7.8 28.3 12.7 21.5 30.1 42.4 
7.8 20.0 26.8 20.8 35.8 29.6 27.7 
Wang et al. [41] 
48.9 42.3 26.1 11.3 11.9 41.3 40.9 34.7 10.8 34.7 18.8 34.4 35.4 52.7 19.1 17.4 35.9 33.3 34.8 46.5 31.6 

OM + MIL 
37.2 35.7 25.8 13.8 12.7 36.2 42.4 22.3 14.3 24.2 9.4 13.1 27.9 38.9 
3.7 18.7 20.1 16.3 36.1 18.4 23.4 
OM + FT-AlexNet 
30.4 22.4 15.0 3.5 2.8 26.6 27.3 46.8 0.8 10.8 13.1 34.7 35.8 38.7 12.6 8.4 8.8 12.8 33.6 4.6 19.5 
MIL + FT-AlexNet 
17.5 50.2 22.5 4.0 9.9 38.8 48.7 39.3 0.3 22.1 10.1 19.8 22.4 49.9 
3.4 15.5 32.1 10.8 40.0 1.9 23.0 
OM+ FT-VGGNet 
30.4 25.3 11.1 6.3 1.5 31.3 29.4 49.1 1.0 10.6 12.6 42.0 38.7 36.7 12.8 10.8 10.3 10.3 34.1 5.0 20.5 
MIL + FT-VGGNet 
25.6 58.5 25.3 1.8 11.7 43.5 53.4 35.7 0.2 32.3 10.7 19.3 32.8 56.5 
1.8 15.6 37.3 16.0 43.6 2.9 26.2 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 3 .</head><label>3</label><figDesc>Object detection performance (mAP) on the PASCAL VOC 2010 and 2012 and ILSVRC 2013 datasets.</figDesc><table>Methods 
VOC 2010 VOC 2012 ILSVRC 2013 
Cinbis et al. [4] 
18.5 
-
-
Wang et al. [41] 
-
-
6.0 
OM + MIL + FT-AlexNet 
21.4 
22.4 
7.7 
OM + MIL + FT-VGGNet 
30.7 
29.1 
10.8 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 4 .</head><label>4</label><figDesc>Different mask-out strategies in terms of average correct localization from top M proposals.</figDesc><table>Mask-out strategy 
M =1 
M =10 
M =50 
M =100 
In-Out 
31.8 
73.8 
82.9 
84.2 
Whole-Out 
29.6 
64.9 
76.0 
78.5 
In 
32.7 
71.0 
79.9 
81.8 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://sites.google.com/site/lidonggg930/wsl</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">The result of Cinbis et al.<ref type="bibr" target="#b3">[4]</ref> is obtained on the VOC 2010 test set. The result of Wang et al.<ref type="bibr" target="#b40">[41]</ref> is obtained on the ILSVRC 2013 val set.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Self-taught object localization with deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bazzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bergamo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Weakly supervised object detection with posterior regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pedersoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Weakly supervised object detection with convex clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pedersoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multi-fold mil training for weakly supervised object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">G</forename><surname>Cinbis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning to localize detected objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Weakly supervised localization and learning with generic knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Alexe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="275" to="293" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">What makes paris look like paris?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
		<idno>2012. 1</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The pascal visual object classes challenge: A retrospective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="98" to="136" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Zisserman. The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained part-based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Progressive search space reduction for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marin-Jimenez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Tagprop: Discriminative metric learning in nearest neighbor models for image auto-annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Guillaumin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Lsda: Large scale detection through adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">S</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Detector discovery in the wild: Joint multiple instance and representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Diagnosing error in object detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chodpathumwan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Easy samples first: Self-paced reranking for zero-example multimedia search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mitamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning the easy things first: Self-paced visual category discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Harvesting mid-level visual concepts from large-scale internet images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Is object localization for free?-weakly-supervised learning with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deepid-net: multi-stage and deformable deep convolutional neural networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<title level="m">Training deep neural networks on noisy labels with bootstrapping</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Weakly supervised localization of novel objects using appearance transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rochan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ImageNet Large Scale Visual Recognition Challenge. IJCV</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Overfeat: Integrated recognition, localization and detection using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Le-Cun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Bayesian joint topic modelling for weakly supervised object localisation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Transfer learning by ranking for weakly supervised object annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Siva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Mary</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">In defence of negative mining for annotating weakly labelled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Siva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Looking beyond the image: Unsupervised learning for object saliency and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Siva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Agapito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Weakly supervised object detector learning with model drift detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Siva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">On learning to localize objects with minimal supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">O</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Weakly-supervised discovery of visual pattern configurations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">O</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning from noisy labels with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Self-paced learning for long-term tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Supancic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Weakly supervised object localization with latent category learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.5726</idno>
		<title level="m">Cnn: Single-label to multi-label</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Deep multiple instance learning for image classification and auto-annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Improving object detection with deep convolutional networks via bayesian optimization and structured prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Edge boxes: Locating object proposals from edges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
