<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">POD: Discovering Primary Objects in Videos Based on Evolutionary Refinement of Object Recurrence, Background, and Primary Object Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yeong</forename><forename type="middle">Jun</forename><surname>Koh</surname></persName>
							<email>yjkoh@mcl.korea.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Korea University</orgName>
								<orgName type="institution" key="instit2">Korea University</orgName>
								<orgName type="institution" key="instit3">Korea University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Won-Dong</forename><surname>Jang</surname></persName>
							<email>wdjang@mcl.korea.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Korea University</orgName>
								<orgName type="institution" key="instit2">Korea University</orgName>
								<orgName type="institution" key="instit3">Korea University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang-Su</forename><surname>Kim</surname></persName>
							<email>changsukim@korea.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Korea University</orgName>
								<orgName type="institution" key="instit2">Korea University</orgName>
								<orgName type="institution" key="instit3">Korea University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">POD: Discovering Primary Objects in Videos Based on Evolutionary Refinement of Object Recurrence, Background, and Primary Object Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:44+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A primary object discovery (POD) algorithm for a video sequence is proposed in this work, which is capable of discovering a primary object, as well as identifying noisy frames that do not contain the object. First, we generate object proposals for each frame. Then, we bisect each proposal into foreground and background regions, and extract features from each region. By superposing the foreground and background features, we build the object recurrence model, the background model, and the primary object model. We develop an iterative scheme to refine each model evolutionarily using the information in the other models. Finally, using the evolved primary object model, we select candidate proposals and locate the bounding box of a primary object by merging the proposals selectively. Experimental results on a challenging dataset demonstrate that the proposed POD algorithm extracts primary objects accurately and robustly.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Discovering a primary object is an essential task in computer vision, since a repeatedly appearing object in multiple images or videos conveys useful information about those signals. For example, the segmentation of a common object across frames in a video facilitates the video summarization. Also, object discovery techniques can be used for collecting objects of the common class from many images to train an object detector. Without those techniques, the collection would demand a lot of human efforts. Moreover, in a content-based image retrieval system, object discovery techniques can identify noisy frames, which do not contain a target object, and exclude them from the system.</p><p>Many techniques have been developed to discover a primary object. They can be classified into three categories: object discovery, cosegmentation, and video object segmentation. In object discovery <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b32">33]</ref>, objects of the common class are localized in a set of images or videos. In cosegmentation <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b30">31]</ref>, assuming that an identical object appears in multiple images, the object is delineated at the pixel-level in each image. In video object segmentation, objects are separated from its background <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b33">[34]</ref><ref type="bibr" target="#b34">[35]</ref><ref type="bibr" target="#b35">[36]</ref>, or dense object segments are determined based on motion clustering <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b27">28]</ref>. However, the primary object discovery (POD) is still a challenging problem due to a wide variety of difficulties, such as cluttered and diverse backgrounds, object appearance variations, interrupting objects, and noisy frames.</p><p>In this work, we propose a novel POD algorithm, which discovers an identical object in a video sequence. The proposed algorithm has the following main advantages:</p><p>• It discovers a primary object efficiently in a single video, whereas most conventional object discovery techniques assume a large set of images or videos. • It provides robust performance even when a primary object exhibits abrupt motions or is interfered by other moving objects. This is because the proposed algorithm does not depend on motion information. • While discovering a primary object, it also identifies noisy frames that do not contain the object.</p><p>To this end, we first generate object proposals for each frame. Then, we divide each proposal into foreground and background regions and extract superpixel-based features from each region. By superposing the foreground and background features, we construct three models: object recurrence, background, and primary object models. We iteratively update each model by exploiting the other models. During the iteration, we also detect noisy frames. Finally, we choose candidate proposals using the primary object model and discover a primary object by merging the proposals selectively. Experimental results on an extensive dataset demonstrate that the proposed POD algorithm discovers primary objects effectively and robustly. The rest of this paper is organized as follows: Section 2 reviews conventional techniques, related to POD. Section 3 describes the proposed POD algorithm. Section 4 discusses experimental results. Section 5 concludes this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Object Discovery</head><p>Object discovery or co-localization is a process to find objects of the same class over multiple images or videos. To achieve this goal over videos, Prest et al. <ref type="bibr" target="#b22">[23]</ref> extracted spatiotemporal tubes based on the motion segmentation <ref type="bibr" target="#b3">[4]</ref>, and jointly selected one tube in each video. Tang et al. <ref type="bibr" target="#b28">[29]</ref> proposed the image-box strategy to determine the common object and to identify noisy images without the object. Joulin et al. <ref type="bibr" target="#b12">[13]</ref> selected object proposals, containing the common object, by employing the Frank-Wolfe algorithm. Even from an image pool containing multi-class objects, Cho et al. <ref type="bibr" target="#b6">[7]</ref> discovered an object by combining the partbased region matching with the foreground localization.</p><p>To achieve pixel-level object co-localization, several algorithms have been proposed to perform object discovery and segmentation simultaneously. Rubinstein et al. <ref type="bibr" target="#b25">[26]</ref> developed a saliency-driven object discovery algorithm using pixel correspondences between images. Chen et al. <ref type="bibr" target="#b5">[6]</ref> divided a set of images into subcategories, and then trained the object model and detector for each subcategory to delineate objects. Wang et al. <ref type="bibr" target="#b32">[33]</ref> proposed an energy minimization scheme for simultaneous object discovery and segmentation.</p><p>These object discovery techniques collect objects of the same class, and thus are useful for training an object detector. However, since most discovery techniques <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b32">33]</ref> require a large dataset of images or videos, they are unsuitable for finding a primary object in a single video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Cosegmentation</head><p>The objective of cosegmentation is to discover an identical object within a set of images. However, note that sometimes cosegmentation techniques are used for pixel-level object co-localization, i.e., for delineating objects of the same class, instead of the same object. Rother et al. <ref type="bibr" target="#b24">[25]</ref> first proposed a cosegmentation algorithm using a generative Markov random field (MRF) model. Instead of the generative model, Mukherjee et al. <ref type="bibr" target="#b18">[19]</ref> built a successive model to constrain foreground histograms to be similar to one another. To delineate an identical object, Hochbaum and Singh <ref type="bibr" target="#b10">[11]</ref> optimized an MRF model and maximized the similarity between the foregrounds simultaneously.</p><p>Joulin et al. <ref type="bibr" target="#b11">[12]</ref> applied a discriminative clustering technique to the cosegmentation problem. Chang et al. <ref type="bibr" target="#b4">[5]</ref> introduced a co-saliency prior to locate the common object. Inspired by the anisotropic heat diffusion, Kim et al. <ref type="bibr" target="#b13">[14]</ref> proposed a scalable cosegmentation algorithm. Vicente et al. <ref type="bibr" target="#b30">[31]</ref> determined similar object proposals from multiple images by learning a classifier. To match objects among images, Rubio et al. <ref type="bibr" target="#b26">[27]</ref> exploited a region matching technique, and solved pixel-level and region-level energy mini-mization problems. Wang et al. <ref type="bibr" target="#b31">[32]</ref> extracted cyclic functional maps and generated segmentation functions to indicate the foreground probability of each superpixel. Lee et al. <ref type="bibr" target="#b14">[15]</ref> proposed the notion of multiple random walkers on a graph, and applied it to the image cosegmentation using the repulsive restart rule.</p><p>Although the cosegmentation techniques <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32]</ref> can delineate the same object in a set of images, they are not effective for segmenting objects in a video since they do not consider noisy frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Video Object Segmentation</head><p>In video object segmentation, a primary object in a video sequence is separated from its background. Shi and Malik <ref type="bibr" target="#b27">[28]</ref> constructed a spatiotemporal graph and partitioned it using the normalized cuts. Brox and Malik <ref type="bibr" target="#b3">[4]</ref> traced point trajectories and clustered them. Ochs and Brox <ref type="bibr" target="#b19">[20]</ref> converted clusters of sparse trajectories into dense object segments using a diffusion process. Ochs and Brox <ref type="bibr" target="#b20">[21]</ref> also applied the spectral clustering to a hypergraph representing point trajectories. However, these motion segmentation algorithms <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b27">28]</ref> do not provide the priority of a segment and thus cannot identify a primary object.</p><p>Lee et al. <ref type="bibr" target="#b15">[16]</ref> delineated a key object by selecting a hypothesis, composed of object proposals across frames. Ma and Latecki <ref type="bibr" target="#b17">[18]</ref> extracted a primary object, by determining a maximum weight clique in a graph of object proposals. Zhao et al. <ref type="bibr" target="#b35">[36]</ref> discovered objects of interest in a video by employing the latent Dirichlet allocation <ref type="bibr" target="#b2">[3]</ref>. Li et al. <ref type="bibr" target="#b16">[17]</ref> formed segment tracks from a pool of figureground segments, and refined the segmentation using the composite statistical inference. Zhang et al. <ref type="bibr" target="#b34">[35]</ref> introduced a layered graph of object proposals, and selected a node for each frame using dynamic programming. Papazoglou and Ferrari <ref type="bibr" target="#b21">[22]</ref> estimated motion boundaries to discover moving objects. Faktor et al. <ref type="bibr" target="#b8">[9]</ref> presented the non-local consensus voting scheme, which used saliency maps as initial likelihood to find primary objects. Wang et al. <ref type="bibr" target="#b33">[34]</ref> detected saliency maps using geodesic distances to discover a salient object. Giordano et al. <ref type="bibr" target="#b9">[10]</ref> performed video object segmentation by exploiting the continuity of superpixels in consecutive frames. Taylor et al. <ref type="bibr" target="#b29">[30]</ref> delineated objects in a video, by identifying occluders and determining occlusion relations. However, most of these algorithms <ref type="bibr">[9, 10, 16-18, 22, 30, 34-36]</ref> assume that a primary object has distinct motions from the background. Moreover, they cannot identify an primary object across different shots, when those shots have diverse scene contents and contain different non-primary objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Algorithm</head><p>Our goal is to discover the bounding boxes that trace a primary object in a sequence of video frames V =  <ref type="figure">Figure 1</ref>. An overview of the proposed POD algorithm. In the evolutionary refinement of the three models, high and low weights are indicated by coloured boundaries and , respectively.</p><formula xml:id="formula_0">{I 1 , . . . , I T }.</formula><p>We assume that a primary object appears in most frames, but it need not be in all frames. Hence, while detecting a primary object, we also identify noisy frames that do not contain the object. <ref type="figure">Figure 1</ref> shows an overview of the proposed POD algorithm. First, we generate object proposals for each frame and extract foreground and background features from each proposal. Second, for each frame, we build the object recurrence model and the background model using those features. Third, using the two base models, we combine the foreground features linearly to construct the primary object model, which is finally used to discover a primary object. Note that the three models are iteratively constructed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Modelling a Primary Object</head><p>Object Proposal Generation: For each frame, we obtain a set of object proposals by employing the Alexe et al.'s algorithm <ref type="bibr" target="#b1">[2]</ref>. Let O t = {o t,1 , o t,2 , . . . , o t,m } be the set of proposals at frame t, where m is the number of proposals that is set to 20 in this work. From each proposal o t,i , we extract the foreground feature p t,i and the background feature q t,i . Specifically, we first divide each proposal into foreground and background regions using the GrabCut algorithm <ref type="bibr" target="#b23">[24]</ref>, which was designed to bisect a manually annotated boxed region. Note that a proposal (or the corresponding box) is automatically generated by <ref type="bibr" target="#b1">[2]</ref>, and thus the manual annotation is not performed in this work. Then, we extract the foreground and background features, p t,i and q t,i , from the foreground and background regions, respectively. For the feature representation, we adopt the bag-ofvisual-words approach. Given a video sequence, we oversegment each frame into 1,000 superpixels using the SLIC algorithm <ref type="bibr" target="#b0">[1]</ref>, and then encode the average LAB colors of all superpixels into 100 codewords. Then, the foreground feature p t,i is obtained by recording the histogram of the codewords for the foreground region, and the background feature q t,i is obtained in a similar manner. Both p t,i and q t,i are normalized, and thus they can be regarded as prob-ability distributions.</p><p>Object Recurrence Model: A primary object occurs repeatedly in a video sequence. This recurrence property implies that some of the object proposals contain a whole or part of the primary object. Thus, by mixing the foreground features of the proposals, we can approximate the features of the primary object. Based on this observation, we define the object recurrence model R</p><formula xml:id="formula_1">(θ) t (Γ t ) at frame t at iteration θ as R (θ) t (Γ t ) = m i=1 γ t,i p t,i .<label>(1)</label></formula><p>where Γ t = (γ t,1 , . . . , γ t,m ) denotes the set of the recurrence weights such that 0 ≤ γ t,i ≤ 1 and i γ t,i = 1.</p><p>Each recurrence weight γ t,i indicates the likelihood that the corresponding feature p t,i comes from the primary object. Notice that, in the object recurrence model, only the foreground features p t,i are used and the background features q t,i are not considered.</p><p>To obtain Γ t , we use the primary object model for each frame, which will be discussed later in this section. The primary object model is a refinement of the object recurrence model by using both foreground and background features. In this work, we update the object recurrence models and the primary object models iteratively. Let</p><formula xml:id="formula_2">P (θ) = (P (θ) 1 , . . . , P (θ)</formula><p>T ) denote the set of the primary object models at iteration θ. At the start of iterations, the primary object model P (0) t at frame t is initialized to the average of the foreground features, given by</p><formula xml:id="formula_3">P (0) t = 1 m m i=1 p t,i .<label>(2)</label></formula><p>Given the primary object models P (θ−1) at the previous iteration θ − 1, we compute the recurrence weights Γ t to make the object recurrence model R (θ) t (Γ t ) approximate the feature of the primary object. To this end, we define (a) Frame t the global primary object model P (θ−1) as</p><formula xml:id="formula_4">(b) θ = 1 (c) θ = 2 (d) θ = 3 (e) θ = 4 (f) θ = 5 (g) θ = 6</formula><formula xml:id="formula_5">P (θ−1) = T t=1 c (θ−1) t P (θ−1) t T t=1 c (θ−1) t<label>(3)</label></formula><p>where the binary indicator c (θ−1) t is 0 if frame t is detected as noisy at the previous iteration θ − 1, and 1 otherwise. Thus, P (θ−1) is averaged over only the frames containing the primary object. Note that, if we construct the recurrence model at frame t using the primary object model at the corresponding frame only, each recurrence model may describe a different object. We hence use the global model to make all recurrence models represent an identical object. Consequently, the object recurrence model R</p><formula xml:id="formula_6">(θ)</formula><p>t (Γ t ) should be similar to the global primary object model P (θ−1) . We adopt the Kullback-Leibler divergence to measure the dissimilarity between R (θ) t (Γ t ) and P (θ−1) ,</p><formula xml:id="formula_7">D(R (θ) t (Γ t )||P (θ−1) ).<label>(4)</label></formula><p>Note that the Kullback-Leibler divergence or relative entropy</p><formula xml:id="formula_8">D(u||v) = i u i log u i v i<label>(5)</label></formula><p>is often used to measure the distance between two probability distributions u and v, and it is convex in terms of both u and v <ref type="bibr" target="#b7">[8]</ref>. We then estimate the optimal set of the recurrence weights Γ * t by</p><formula xml:id="formula_9">Γ * t = arg min Γt D(R (θ) t (Γ t )||P (θ−1) )<label>(6)</label></formula><formula xml:id="formula_10">subject to 0 ≤ γ t,i ≤ 1, i γ t,i = 1.<label>(7)</label></formula><p>Because of the convexity of relative entropy <ref type="bibr" target="#b7">[8]</ref>, the constrained optimization in <ref type="formula" target="#formula_9">(6)</ref> is a convex optimization problem, which can be easily solved. Finally, using the optimal Γ * t , we obtain the object recurrence model R (θ) t (Γ * t ) at frame t. Note that the optimal Γ * t makes the object recurrence model R at frame t at iteration θ using the background features of the proposals as</p><formula xml:id="formula_11">B (θ) t = m i=1 λ t,i q t,i m i=1 λ t,i<label>(8)</label></formula><p>where λ t,i is the weight for the background feature q t,i of the ith proposal. The background model should be distinguishable from the object recurrence model. Therefore, we assign a higher weight λ t,i , when the feature q t,i is more</p><formula xml:id="formula_12">dissimilar from R (θ) t (Γ * t ).</formula><p>In other words, we set the weight λ t,i as</p><formula xml:id="formula_13">λ t,i = d χ (q t,i , R (θ) t (Γ * t ))<label>(9)</label></formula><p>where d χ (·, ·) denotes the chi-square distance.</p><p>Primary Object Model: Even though the object recur- <ref type="formula" target="#formula_1">(1)</ref> and <ref type="formula" target="#formula_9">(6)</ref> roughly represents the feature of the primary object, it does not fully exploit the background information in <ref type="bibr" target="#b7">(8)</ref>. Therefore, we attempt to obtain a more refined model of the primary object. We define the primary object weight ω t,i for the foreground feature p t,i of the ith proposal as</p><formula xml:id="formula_14">rence model R (θ) t (Γ * t ) in</formula><formula xml:id="formula_15">ω t,i = d χ (p t,i , B (θ) t ) d χ (p t,i , R (θ) t (Γ * t ))</formula><p>.</p><p>A high ω t,i indicates that the foreground feature p t,i is similar to the object recurrence model but dissimilar from the background model. We select the top-5 proposals according to the primary object weights. Then, we determine the primary object model P (θ) t at frame t at the current iteration θ, by superposing the foreground features of the selected proposals,</p><formula xml:id="formula_17">P (θ) t = i∈It ω t,i p t,i i∈It ω t,i<label>(11)</label></formula><p>where I t is the index set of the top-5 proposals at frame t. Note that these primary object models P (θ) = (P (θ) 1 , . . . , P</p><p>T ) are, in turn, used to update the object recurrence models in (1) and (6) at the next iteration θ + 1.</p><p>Noisy Frame Detection: After obtaining the primary object models for all frames, we compute the distance between the global model P (θ−1) in (3) and each model P Iterative Modelling: We update the object recurrence models, the background models, and the primary object models iteratively, until d(P (θ) , P (θ−1) ) converges to zero. <ref type="figure">Figure 2</ref> illustrates how the three models evolve as the iteration goes on. Initially, at θ = 1, the primary object model expresses the green features of a vegetable. However, after the convergence at θ = 6, the primary object model faithfully represents the features of the primary object, i.e., the monkey. For most sequences, the proposed algorithm converges after 5 to 10 iterations. <ref type="figure">Figure 3</ref> shows examples of primary object models after the convergence. </p><formula xml:id="formula_19">(a) o t,δ , F δ (b) o t,i , F i (c) F i ∪ F δ (d) F i \ F δ (e) Merged box (f) Final box</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Discovering a Primary Object</head><p>After the convergence of the global primary object model P = P (θ−1) = P (θ) in Section 3.1, we discover the primary object through the video sequence.</p><p>For each frame t, we have the index set I t of the top-5 proposals in <ref type="bibr" target="#b10">(11)</ref>. Among those proposals, we choose the main proposal that has the minimum distance from the global model P, whose index is given by</p><formula xml:id="formula_20">δ = arg min i∈It d χ (p t,i , P).<label>(12)</label></formula><p>We then merge the main proposal o t,δ with each candidate proposal o t,i , where i ∈ I t and i = δ, by employing a score function Ψ(o t,i , o t,δ ). Let F δ and F i denote the foreground regions of a main proposal o t,δ and a candidate proposal o t,i , respectively, extracted by the GrabCut algorithm <ref type="bibr" target="#b23">[24]</ref>. The boxes and the foreground regions of the main and candidate proposals are illustrated in <ref type="figure" target="#fig_3">Figure 4</ref>(a) and (b), respectively. We consider the union region F i ∪ F δ in <ref type="figure" target="#fig_3">Figure 4</ref>(c) and the difference region F i \ F δ in <ref type="figure" target="#fig_3">Figure 4</ref>(d). Then, we extract the features p i∪δ and p i\δ from the union region and the difference region, respectively. Using these features, we evaluate the score function as</p><formula xml:id="formula_21">Ψ(o t,i , o t,δ ) = d χ (p t,δ , P) d χ (p i∪δ , P) 1 − d χ (p i\δ , p t,δ ) . (13)</formula><p>Suppose that the foreground region F i of the candidate proposal o t,i mostly covers the primary object and includes none or only a little part of the background. Then, the feature p i∪δ of the union region should be similar to the global primary object model P, as the feature p t,δ of the main proposal is. In other words, the ratio dχ(p t,δ ,P) dχ(p i∪δ ,P) should be close to 1. Moreover, the difference region should have a similar feature to the main proposal, and d χ (p i\δ , p t,δ ) should be for each frame t do <ref type="bibr">5:</ref> Compute the object recurrence model R Compute the primary object model P Merge the proposals selectively ⊲ (13) 15: end for Output: Bounding boxes of the primary object small. Note that the chi-square distance ranges from 0 to 1. Thus, if Ψ(o t,i , o t,δ ) is larger than a threshold 0.4, we merge o t,i to the main proposal o t,δ . After the mergence, we put the bounding box that encloses the two foreground regions, as in <ref type="figure" target="#fig_3">Figure 4</ref>(e). This merging process is repeatedly performed with each candidate proposal o t,i , where i ∈ I t and i = δ. <ref type="figure" target="#fig_3">Figure 4</ref>(f) shows that the final bounding box discovers the kite panda effectively.</p><p>Algorithm 1 summarizes the proposed POD algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results</head><p>We test the proposed algorithm on the primary object video (POV) dataset and YouTube-Objects dataset <ref type="bibr" target="#b22">[23]</ref>. As in the object discovery and localization techniques in <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b28">29]</ref>, we adopt the correct localization (CorLoc) metric, which measures the percentage of images correctly localized according to the PASCAL criterion: an estimated box B p is correct as compared with the ground-truth box B gt , when the intersection over union (IoU) overlap ratio |Bp∩Bgt| |Bp∪Bgt| is larger than 0.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">POV Dataset</head><p>We organize a dataset of 20 video sequences, whose durations vary from one to four minutes. Each video contains an identical primary object. In <ref type="table" target="#tab_1">Table 1</ref>, we classify the 20 videos into four categories according to scene types and object motions: "Simple," "Static object," "Multi-objects," and "Animation." The "Simple" category consists of relatively easy videos, each of which contains a single object, few scene changes, and few noisy frames. In each "Static object" video, a primary object remains stationary in many frames without fast motions. Each "Multi-objects" video contains other objects in addition to a primary object. Each "Animation" video includes many noisy frames, has frequent scene changes and diverse backgrounds, and contains multiple objects. To reduce the amount of manual efforts for annotating ground truths, we sampled every 12th frame to obtain the temporally decimated sequences, and then annotated the bounding boxes for the sampled frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparative Performance Evaluation</head><p>In <ref type="table" target="#tab_1">Table 1</ref>, we compare the proposed algorithm with the previous algorithms on cosegmentation <ref type="bibr" target="#b14">[15]</ref>, object discovery <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b28">29]</ref>, and video object segmentation <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b34">35]</ref>. We obtain the results of the previous algorithms using the source codes, provided by the respective authors. Every attempt has been made to make a fair comparison. However, as detailed below, experimental conditions are slightly different according to the implementation issues of each algorithm.</p><p>Cosegmentation: We compare the proposed POD algorithm with the Lee et al.'s cosegmentation algorithm <ref type="bibr" target="#b14">[15]</ref>. Note that <ref type="bibr" target="#b14">[15]</ref> constructs a graph by connecting all superpixels in all frames. Thus, it requires a huge amount of memory that is proportional to the number of frames. To overcome this issue, we execute <ref type="bibr" target="#b14">[15]</ref> on 10 randomly selected frames, repeat the test ten times, and report the average performance. <ref type="bibr" target="#b14">[15]</ref> provides good performances on simple videos, which have temporally consistent backgrounds and no noisy frames. However, it is vulnerable to noisy frames or diverse backgrounds.</p><p>Object Discovery: The object discovery algorithms <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b28">29]</ref> are tested under the same condition as the proposed algorithm. On all the video sequences, the proposed POD algorithm significantly outperforms these object discovery algorithms, which often produce large bounding boxes, containing background regions as well. This is because <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b28">29]</ref> depend on the similarity of foreground proposals. However, background proposals also may be similar to one another, degrading the performances of these algorithm. Especially, the Cho et al.'s algorithm <ref type="bibr" target="#b6">[7]</ref> assumes that background regions in different frames are dissimilar. However, this assumption is often invalid within a video sequence, and the background cannot be effectively discriminated from the foreground. Therefore, <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b28">29]</ref> cannot deal with background clutters effectively for the video sequence in <ref type="figure" target="#fig_7">Figure 5</ref>(a).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Video Object Segmentation:</head><p>We also compare the proposed POD algorithm with the video object segmentation algorithms in <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b34">35]</ref>. Since these previous algorithms provide pixel-level segmentation results, we fit a bounding box to the largest connected component of a segmentation result, as done in <ref type="bibr" target="#b21">[22]</ref>. Also, although the dataset is constructed by sampling every 12th frame, we triple the sampling rate and sample every 4th frame for the Papazoglou and Ferrari's algorithm <ref type="bibr" target="#b21">[22]</ref>. This is because <ref type="bibr" target="#b21">[22]</ref> exploits the motion information and often benefits from a higher sampling rate. However, the CorLoc metric is applied to every 12th frame only, for which the ground truth is available. On the other hand, Zhang et al.'s algorithm <ref type="bibr" target="#b34">[35]</ref> uses the same sampling rate as the proposed algorithm. Except for the "Helicoptor" and "Polarbear" sequences, the proposed algorithm outperforms both the conventional algorithms <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b34">35]</ref> significantly. For the "Simple" category, both algorithms provide sufficiently good performances. For the "Static object" category, the Papazoglou and Ferrari's algorithm <ref type="bibr" target="#b21">[22]</ref> is vulnerable to static objects as shown in <ref type="figure" target="#fig_7">Figure 5</ref>(b), since it is highly dependent on motion information. For the "Multi-object" category, both algorithms <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b34">35]</ref> often fail to identify primary objects, and thus detect non-primary objects. For example, another kite, instead of the panda kite, in <ref type="figure" target="#fig_7">Figure 5</ref>(c) and a vegetable in <ref type="figure" target="#fig_7">Figure 5</ref>(d) are regarded as primary objects due to their distinct colors and motions. In <ref type="figure" target="#fig_7">Figure 5</ref>(e), the conventional algorithms fail to locate the dog, which is occluded by obstacles, such as the fence. For the "Animation" category, the conventional algorithms suffer from noisy frames, frequent scene changes, and multi-objects. Therefore, they almost always fail to detect primary objects and identify noisy frames, as shown in <ref type="figure" target="#fig_7">Figure 5</ref>(f). In contrast, the proposed POD algorithm provides good performances on all categories, using the robust primary object models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">YouTube-Objects Dataset</head><p>YouTube-Objects <ref type="bibr" target="#b22">[23]</ref> is a large dataset, containing videos for 10 object classes. Many videos in this dataset contain identical primary objects, respectively. However, there are a few 'ambiguous' videos, in which it is not obvious to pick primary objects. This is because these ambiguous videos contain many kinds of objects whose appearance frequencies are almost the same. The dataset provides ground truth bounding boxes for a selected set of frames that enclose the most distinct objects. Since each video is composed of several shots, we sample 10 frames from each shot and apply the CorLoc metric to the frames whose ground truths are available. Also, as done in <ref type="bibr" target="#b22">[23]</ref>, we evaluate the discovery performance only for the training videos in the dataset.</p><p>In <ref type="table">Table 2</ref>, we compare the proposed POD algorithm with the Papazoglou and Ferrari's algorithm <ref type="bibr" target="#b21">[22]</ref> and the Prest et al.'s algorithm <ref type="bibr" target="#b22">[23]</ref>. The results of <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref> are from the respective papers. The proposed algorithm yields relatively low CorLoc scores on the "cat," "horse," and "train" POD M. Cho <ref type="bibr" target="#b6">[7]</ref> A. Papazoglou <ref type="bibr" target="#b21">[22]</ref> K. Tang <ref type="bibr" target="#b28">[29]</ref> D. Zhang <ref type="bibr" target="#b34">[35]</ref> (a) Cat -Simple classes, which contain relatively many ambiguous videos. On the other hand, most videos in the "boat," "car," "dog," and "motorbike" classes contain clear primary objects, thus POD provides good performances on these classes. On average, as compared with the state-of-the-art algorithm in <ref type="bibr" target="#b21">[22]</ref>, POD improves the accuracy by 10.1%. Due to the page limitation, we provide primary object discovery results in the supplemental materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>We proposed the POD algorithm for a single video. We first generated object proposals for each frame. Then, we divided each proposal into foreground and background regions, and extracted superpixel-based feature from each region. By combining the foreground and background fea-tures, we constructed the object recurrence, background, and primary object models, and iteratively updated each model using the information in the other models. Also, we detected noisy frames during the iteration. Finally, we selected candidate proposals using the primary object model, and localized a primary object by merging the candidate proposals selectively. Experimental results confirmed that the proposed POD algorithm effectively discovers primary objects in the challenging and extensive dataset.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .tFigure 3 .</head><label>23</label><figDesc>Iterative primary object modeling on the "Monkey" sequence. An input frame and its ground-truth box at frame t are shown in (a). At each iteration θ in (b)∼(g), from top to bottom, the object recurrence model R , the foreground regions of proposals are linearly superposed using weights γ * t,i and ωt,i, respectively. Similarly, for B(θ) t , the background regions are superposed using weights λt,i . More examples of primary object models after the convergence. The top row shows ground-truth bounding boxes, and the bottom row are the corresponding primary object models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>frame t as noisy if d χ (P (θ) t , P (θ−1) ) is larger than a threshold 0.7. This information is recorded in the indicator vector c (θ) frame t is noisy, and 1 otherwise.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>An example of the primary object discovery. Since a candidate proposal ot,i yields a high score Ψ(ot,i, o t,δ ), we merge ot,i to o t,δ and update the box in (e). After merging with other proposals, we obtain the final box in (f). Coloured boundaries and regions depict bounding boxes and foreground regions, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 .</head><label>5</label><figDesc>Performance comparison of the proposed algorithm with<ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b34">35]</ref>. Results of each algorithm are depicted by bounding boxes of a different color. The small boxes in the left top corner are the ground-truth boxes. A box with a red cross depicts a noisy frame.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 .</head><label>1</label><figDesc>Performance comparison of the proposed algorithm with the conventional algorithms on the POV dataset using the CorLoc metric. The best results are boldfaced.Table 2. Performance comparison of the proposed algorithm with the conventional algorithms<ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref> on the YouTube-Objects dataset using the CorLoc metric. The best results are boldfaced.</figDesc><table>Cosegmentation 
Object discovery 
Video object segmentation 
Proposed 
Category 
Video (No. of frames) 
[15] 
[29] 
[7] 
[22] 
[35] 
POD 

Simple 

Baby (528) 
71.00 
14.84 
25.24 
76.30 
76.47 
82.96 
Helicopter (200) 
26.03 
12.50 
67.00 
77.50 
61.37 
76.50 
Rabbit (459) 
93.31 
64.54 
73.13 
82.16 
71.48 
92.73 
Cat (299) 
93.00 
31.44 
49.83 
49.83 
68.29 
92.98 
Babypanda (361) 
73.09 
52.15 
53.58 
82.52 
37.25 
94.18 

Static object 

Yellow Bear (147) 
3.47 
27.59 
40.00 
22.76 
85.52 
99.32 
Raccoon (586) 
49.25 
38.83 
45.02 
27.32 
70.88 
72.18 
RC Car (287) 
78.78 
28.92 
70.04 
51.22 
44.27 
88.50 
Polarbear (247) 
95.89 
55.42 
44.58 
29.58 
99.17 
84.58 
Fox (393) 
87.00 
58.93 
16.07 
58.93 
36.74 
88.30 

Multi-objects 

Monkey (131) 
58.00 
30.53 
36.64 
29.77 
45.80 
80.92 
Panda (271) 
53.72 
42.74 
41.91 
21.16 
49.80 
95.44 
Car (322) 
66.08 
40.52 
66.34 
33.01 
34.58 
66.99 
Baby2 (349) 
71.00 
10.32 
34.10 
52.15 
70.59 
87.97 
Peacock (217) 
64.14 
24.17 
45.02 
48.34 
51.89 
74.41 
Dog (355) 
20.56 
17.39 
10.44 
19.42 
40.29 
61.13 

Animation 

Pooh (387) 
42.00 
14.21 
3.88 
6.98 
5.67 
93.80 
Yellow Larva (280) 
4.00 
33.57 
6.43 
26.07 
21.43 
87.50 
Frozen (434) 
9.00 
10.00 
9.07 
57.91 
34.19 
64.88 
Dooly (450) 
54.00 
11.33 
16.22 
32.22 
30.67 
83.56 

Average 
54.28 
31.00 
35.90 
43.96 
51.87 
83.44 

aeroplane 
bird 
boat 
car 
cat 
cow 
dog 
horse 
motorbike 
train 
avg 

[23] 
51.7 
17.5 
34.4 
34.7 
22.3 
17.9 
13.5 
26.7 
41.2 
25.0 
28.5 
[22] 
65.4 
67.3 
38.9 
65.2 
46.3 
40.2 
65.3 
48.4 
39.0 
25.0 
50.1 
POD 
64.3 
63.2 
73.3 
68.9 
44.4 
62.5 
71.4 
52.3 
78.6 
23.1 
60.2 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">SLIC superpixels compared to state-of-the-art superpixel methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lucchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Susstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2274" to="2282" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Measuring the objectness of image windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Alexe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2189" to="2202" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Latent Dirichlet allocation. JMLR</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Object segmentation by long term analysis of point trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="282" to="295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">From co-saliency to co-segmentation: An efficient and fully unsupervised energy minimization model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-H</forename><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2129" to="2136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Enriching visual knowledge bases via object discovery and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2035" to="2042" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Unsupervised object discovery and localization in the wild: Part-based matching with bottom-up region proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1201" to="1210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Elements of Information Theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Cover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Thomas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>Wiley</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Video segmentation by non-local consensus voting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Faktor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Superpixel-based video object segmentation using perceptual organization and location prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Giordano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Murabito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Palazzo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Spampinato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4814" to="4822" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">An efficient algorithm for co-segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Hochbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="269" to="276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Discriminative clustering for image co-segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1943" to="1950" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Efficient image and video co-localization with Frank-Wolfe algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Distributed cosegmentation via submodular optimization on anisotropic diffusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="169" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multiple random walkers and their application to image cosegmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-D</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y.</forename><surname>Sim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-S</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Key-segments for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Video segmentation by tracking many figure-ground segments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Humayun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2192" to="2199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Maximum weight cliques with mutex constraints for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Latecki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="670" to="677" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Half-integrality based algorithms for cosegmentation of images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2028" to="2035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Object segmentation in video: A hierarchical variational approach for turning point trajectories into dense regions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ochs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1583" to="1590" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Higher order motion models and spectral clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ochs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="614" to="621" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fast object segmentation in unconstrained video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Papazoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1777" to="1784" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning object class detectors from weakly annotated video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Prest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Leistner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Civera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">GrabCut: Interactive foreground extraction using iterated graph cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kolmogorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graphics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Cosegmentation of image pairs by histogram matchingincorporating a global constraint into MRFs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Minka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kolmogorov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="993" to="1000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Unsupervised joint object discovery and segmentation in internet images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rubinstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1939" to="1946" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Unsupervised co-segmentation through region matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Rubio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Serrat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>López</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Paragios</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="749" to="756" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Motion segmentation and tracking using normalized cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1154" to="1160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Co-localization in real-world images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1464" to="1471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Causal video object segmentation from persistence of occlusions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Karasev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4268" to="4276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Object cosegmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vicente</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kolmogorov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="2217" to="2224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Image co-segmentation via consistent functional maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="849" to="856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Video object discovery and co-segmentation with extremely weak supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="640" to="655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Saliency-aware geodesic video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3395" to="3402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Video object segmentation through spatially accurate and temporally dense extraction of primary object regions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Javed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="628" to="635" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Topical video object discovery from key frames by modeling word co-occurrence prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1602" to="1609" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
