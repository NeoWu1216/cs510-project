<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Automated 3D Face Reconstruction from Multiple Images using Quality Measures</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcel</forename><surname>Piotraschke</surname></persName>
							<email>piotraschke@nt.uni-siegen.de</email>
							<affiliation key="aff0">
								<orgName type="department">Institute for Vision and Graphics</orgName>
								<orgName type="institution">University of Siegen</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volker</forename><surname>Blanz</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute for Vision and Graphics</orgName>
								<orgName type="institution">University of Siegen</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Automated 3D Face Reconstruction from Multiple Images using Quality Measures</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:37+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Automated 3D reconstruction of faces from images is challenging if the image material is difficult in terms of pose, lighting, occlusions and facial expressions, and if the initial 2D feature positions are inaccurate or unreliable. We propose a method that reconstructs individual 3D shapes from multiple single images of one person, judges their quality and then combines the best of all results. This is done separately for different regions of the face. The core element of this algorithm and the focus of our paper is a quality measure that judges a reconstruction without information about the true shape. We evaluate different quality measures, develop a method for combining results, and present a complete processing pipeline for automated reconstruction.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Algorithms that reconstruct 3D faces from images by fitting a deformable face model, such as a 3D Morphable Model (3DMM), rely on a relatively precise initial positioning of the face <ref type="bibr" target="#b6">[7]</ref> or on a set of feature point coordinates <ref type="bibr" target="#b7">[8]</ref>. For an automated procedure, it is straight-forward to combine these algorithms with automatic face and landmark detection, such as the algorithm by Zhu and Ramanan <ref type="bibr" target="#b32">[33]</ref> or other feature detectors <ref type="bibr" target="#b10">[11]</ref>. In practice, however, this combination has turned out to be more challenging than expected, posing a number of fundamental questions. The feature point detection is a non-trivial task, especially if the image material includes complex lighting, facial expressions, wrinkles, eye glasses or facial hair. Therefore, the features may be inaccurate, and some may even be outliers. Moreover, the optimal set of features for 3DMM fitting includes points that are not easy to detect, such as the facial silhouette and the ears. Those points are necessary for the 3DMM to converge to the correct pose angle, and this in turn affects the shape estimate.</p><p>Therefore, a simple combination of existing methods <ref type="figure">Figure 1</ref>: A segment-based, weighted linear combination is used to create the final head shape. The weight decreases with the rank. Implausible segments are discarded. Note that each facial segment is handled separately. Optionally the texture can be extracted from one of the input images.</p><p>produces results that are substantially worse than those obtained with manually labeled features. Attempts to make 3DMM fitting more robust <ref type="bibr" target="#b9">[10]</ref> are promising but still not sufficient. Instead, we argue that in many real-world applications more than one image of a person is available, so an automated algorithm can exploit redundant data from multiple images to gain robustness and reliability. Our algorithm outperforms existing methods of simultaneous 3D reconstruction from multiple images <ref type="bibr" target="#b6">[7]</ref> significantly, which may be due to the fact that outliers in feature positions adversely affect the simultaneous least squares solution.</p><p>In contrast, our algorithm calculates separate reconstructions from each input image, and then combines them to an optimal overall solution. We propose a method that selects the most plausible reconstructions, operates on different region of the face separately, and merges them into a single 3D face.</p><p>The key component of our algorithm is a new measure for the visual quality of 3D reconstructions, based on surface normals. Automated assessment of visual quality in computer graphics and vision is a fundamental challenge. Simple image comparisons are insufficient because they are insensitive to small but important errors and artifacts. Euclidean distance in 3D overrates global shape deformations that would be irrelevant to human observers. Mahalanobis distance is also inconsistent with the quality ratings of humans. In an experimental comparison with quality ratings from human subjects, our new, normal based measure outperforms these existing criteria.</p><p>In summary, the contributions of this paper are:</p><p>• a general measure of the quality (naturalness) of a shape reconstruction, • an algorithm for selecting and combining reconstructions of different facial regions (segments) from different input images into a single 3D face, • an automated algorithm that produces 3D shape reconstructions from multiple images of a person, which goes beyond a simple combination of landmark detection and 3DMM fitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Although several approaches have been published related to high quality 3D reconstructions of faces from 2D images, automated reconstruction still remains a challenging task, especially with facial expressions. It is often difficult to find images with a neutral expression, as most people tend to smile in portraits.</p><p>In the literature on face modeling several different approaches can be found. For high quality 3D reconstructions of faces which are used in computer games and movies, the state of the art techniques still require 3D scans of the person using laser scanners or multi-view camera setups <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b1">2]</ref>. Additionally, substantial post processing is required to combine the generated 3D data and to morph between different facial expressions and visemes to realistically animate the subjects face.</p><p>Approaches like the one presented in this paper try to obviate the need for special equipment. Instead, they make use of data that can be easily produced with standard equipment or that is already available, such as photo or video data. Multi-view geometry <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b12">13]</ref> is a common procedure to reconstruct 3D shapes from several single images or video frames. Although these algorithms are quite flexible in usage for different scenarios varying from the reconstruction of buildings, smaller objects and even faces, they cannot sufficiently handle non-rigid transformations (facial expressions) within a series of input images.</p><p>Other recent publications have shown promising results by aligning a 3D face to single or multiple images as well as to videos frames. The approaches by Park et. al <ref type="bibr" target="#b21">[22]</ref>, Aldrian and Smith <ref type="bibr" target="#b0">[1]</ref> and Dou et. al <ref type="bibr" target="#b23">[24]</ref> reconstruct the 3D shape from a single image. Wang et al. <ref type="bibr" target="#b29">[30]</ref> extract the silhouette from several input images to reconstruct the 3D shape, while Roth et. al <ref type="bibr" target="#b26">[27]</ref> use an image collection for photometric stereo-based normal estimation which iteratively optimizes the surface reconstruction. By estimating the pose and computing the optical flow, a high detail refinement of the 3D shape is performed, resulting in a 3D to 2D correspondence <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b28">29]</ref>. Suwajanakorn et. al <ref type="bibr" target="#b28">[29]</ref> even captured fine details like wrinkles and in <ref type="bibr" target="#b18">[19]</ref> Kemelmacher-Shlizerman and Seitz showed that also 'faces in the wild' can be handled properly. But these approaches lack an additional 3D to 3D correspondence. In this paper we address 3D to 2D as well as 3D to 3D correspondence.</p><p>To reconstruct a 3D shape of a face from a 2D image, Blanz and Vetter <ref type="bibr" target="#b6">[7]</ref> introduced the 3DMM. With the Basel Face Model <ref type="bibr" target="#b22">[23]</ref>, a 3DMM has been made available to the public and Zhu et. al <ref type="bibr" target="#b33">[34]</ref> presented a discriminative 3DMM based on local features that provides accurate reconstructions. A common and significant drawback of the 3DMM is its lack of robustness in the case of 'faces in the wild', especially if the facial landmarks are not perfectly detected. Although Breuer et al. <ref type="bibr" target="#b10">[11]</ref> propose to use a Support Vector Machine for automatic 3D face reconstruction and in <ref type="bibr" target="#b9">[10]</ref> an idea is presented to correct misplaced landmarks to some extent, both implementations were not robust enough to handle difficult scenarios caused by facial expressions or complex lighting conditions. With the approach in this paper, we aim to overcome the previous drawbacks of the 3DMM.</p><p>Additionally there are approaches which are not aiming at the reconstruction of faces directly, but provide a strong foundation for further processing by detecting faces, estimating poses, localizing feature points or aligning face geometries <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b16">17]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">3D Morphable Model</head><p>The 3D Morphable Model <ref type="bibr" target="#b6">[7]</ref> is a vector space of 3D shapes and textures, S i = (X 1 , Y 1 , Z 1 , X 2 , . . . , Z n ) T and T i = (r 1 , g 1 , b 1 , r 2 , . . . , b n ) T , with X, Y, Z coordinates and r, g, b colors of n = 113 753 vertices. In our experiments, the 3DMM is constructed from 3D scans of 200 individuals and from 35 additional scans that show facial expressions of a single individual <ref type="bibr" target="#b5">[6]</ref>. On the individual shapes, the expressions and the textures, a PCA defines eigenvectors s i , u i and t i , respectively, and average shapes and textures s and t. In this basis, new faces can be approximated by linear combinations</p><formula xml:id="formula_0">S = s + m i=1 α i s i + p i=1 γ i u i T = t + m i=1 β i t i . (1)</formula><p>We use m = 100 eigenvectors for individual variations and p = 4 for the most important degrees of freedom of facial expressions, with a focus on mouth movements.</p><p>Please note that a high percentage of images, for example those in the database 'faces in the wild', involve non-neutral facial expressions, so our approach of combining multiple images only makes sense with this additional degree of freedom. We use separate PCAs and basis vectors for shape and expression in order to be able to give the 3D faces neutral expressions (γ i = 0) after fitting.</p><p>3D shape reconstruction by fitting the model to an image is essentially a minimization of the image distance</p><formula xml:id="formula_1">d image = u,v I input (u, v) − I model (u, v) 2<label>(2)</label></formula><p>in all 3 color channels, with respect to the linear coefficients α i , γ i , β i and some imaging parameters ρ i that control pose, lighting and other parameters (for details see <ref type="bibr" target="#b6">[7]</ref>).</p><p>Overfitting is avoided by a regularization term that is the Mahalanobis distance from the starting conditions,</p><formula xml:id="formula_2">d maha = i α 2 i σ 2 S,i + i γ 2 i σ 2 S,i + i β 2 i σ 2 T,i + i (ρ i − ρ i ) 2 σ 2 R,i ,<label>(3)</label></formula><p>where ρ i denotes the starting values of the rendering parameters, and σ are the standard deviation from PCA.</p><p>A stochastic newton optimization algorithm minimizes the weighted sum of d image , d maha and an additional term</p><formula xml:id="formula_3">d f eatures = j x j y j − P x (X kj , Y kj , Z kj ) P y (X kj , Y kj , Z kj ) 2</formula><p>(4) which is the sum of squared distances between 2D feature positions x j , y j and the projected positions of the corresponding vertex k j , with a perspective projection P <ref type="bibr" target="#b7">[8]</ref>. d f eatures is only for initialization, with a weight that decreases as the fitting proceeds.</p><p>Unlike earlier work on 3DMM fitting <ref type="bibr" target="#b7">[8]</ref>, we use a feature detection algorithm by Zhu and Ramanan <ref type="bibr" target="#b32">[33]</ref> for an automated process. The reduced precision of these features and the suboptimal choice of features (silhouettes, ears) affect the quality of the output significantly. In the remainder of this paper we describe how to select the most successful reconstructions based on a given set of images of a person, and how to combine these to a 3D face.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Quality Measures</head><p>For a meaningful quality measure, it is important to be independent of facial expression. Therefore, we use "neutralized" facial expressions (with γ i = 0) in this section except for image distance. Because the image distance compares the rendered reconstruction with the original input image, it needs to be as close as possible to the original face. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Image Distance</head><p>In contrast to all others distance functions that are discussed in this paper, the image distance d image Eq. <ref type="formula" target="#formula_1">(2)</ref> is the only one that penalizes differences between the original face and the reconstruction. The other distance measures will only estimate the plausibility of naturalness of reconstructed faces. <ref type="figure" target="#fig_0">Fig. 2</ref> illustrates one major drawback of this error function: it is not possible to penalize the fact that the projected face does not occlude the complete face in the input image. This is the case for Obama's right ear. In I input − I model , the image distance for most pixels of the right ear is zero and therefore the error is quite small. The reconstructed ear is rendered on the cheek, but due to the similar color, this has also little effect on d image . In general, d image fails to capture small but relevant errors and artifacts in the reconstruction.</p><p>On the other hand, d image can also be high even though the faces look similar, for example when the overall color tone is wrong or the face is slightly shifted. All of these problems are caused by the fact that d image is a sum of all pixels and that many small errors count more than a few large errors.</p><p>Even though d image turns out to be suboptimal for rating the quality or plausibility of the 3D reconstruction, as we will demonstrate in Section 5, it makes sense to use d image in the fitting procedure because, unlike the following criteria, it measures the distance from the input face, and it is easy to compute.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Mahalanobis Distance</head><p>Mahalanobis distance measures the distance of the current solution from the average face using PCA, taking into account the standard deviations observed in the training data. It is directly related to the multivariate Gaussian probability density function which is estimated by PCA. Just as the image distance, the Mahalanobis distance is already integrated in the 3DMM fitting procedure. For the experiments in Section 5, where we only want to rate the quality of the reconstructed shape, we simplified Eq. <ref type="formula" target="#formula_2">(3)</ref> to</p><formula xml:id="formula_4">d maha = i α 2 i σ 2 S,i ,<label>(5)</label></formula><p>so we measure only the distance of the neutral face shape from the average face, while expressions, texture and rendering parameters are omitted. The motivation is that, unlike neutral shape, the texture and expression of a successful reconstruction may be far from the average if the input image is unusual (hair, facial hair, eye glasses, smile).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Euclidean Distance</head><p>A more direct measure for the distance of a 3DMM shape from the average face is the Euclidean distance between the reconstructed shape vector (with neutralized expression) S, and the average vector s:</p><formula xml:id="formula_5">d eucl (S, s) = 3n i=1 (S i − s i ) 2 = ||S − s|| 2 .<label>(6)</label></formula><p>Please note that d eucl is sensitive to rigid transformations of the faces. The 3DMM shape vectors are, by construction, aligned in a least-squares sense. In 3DMM fitting, rigid transformations are applied to these externally, and captured by rendering parameters ρ i (Section 3). Still, a general drawback of d eucl remains with respect to simple, global transformations, e.g. anisotropic scaling, which does not affect naturalness or shape similarity, but has significant effect on d eucl .</p><p>Equation <ref type="formula" target="#formula_5">(6)</ref> tends to overrate outlier vertices in the sum of squared distances. For the evaluation (Section 5), we also considered a modified distance which is the sum of 3D vertex distances (square root on a per-vertex level). But we found no improvement, so Section 5 will refer to Equation (6) only.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Normal Distance</head><p>We have observed that local or even global distortions of the surface are a common feature of failed 3D face reconstructions. This is true for most or perhaps all 3DMM algorithms (see Section 2) and -in a different contexteven for 3D shape capture setups such as scanners or stereo and multiview techniques. For shape fitting algorithms, it is unlikely that a failed reconstruction is misaligned and still close to the average, because misalignments tend to have undesired effects on the cost functions of the fitting algorithm and therefore lead away from the set of plausible faces. In our context, misalignments may be caused by inaccurate initial feature positions. Also, other potential reasons for failed reconstructions, such as lighting effects, occlusions or extreme facial expressions, tend to lead the algorithm far away from the average, and a very sensitive measure for this is the deviation of surface normals from the average.</p><p>We would like to point out that regularization mechanisms, such as Equation <ref type="formula" target="#formula_2">(3)</ref>   <ref type="figure">Figure 3</ref>: The Normal distance is determined by computing the angle between the normal of the average <ref type="figure">(Fig. 3a</ref>) and the reconstructed face <ref type="figure">(Fig. 3b</ref>) per corresponding vertex pair (see <ref type="figure">Fig. 3c</ref>). These values are averaged per segment (see <ref type="figure" target="#fig_2">Fig. 4a</ref>) or face to obtain a global distance value.</p><p>solution close to the average. Still, for practical purposes, we have observed that (1) if the weight of the regularization is too large, it implies suboptimal results on images that would otherwise be reconstructed successfully, so there is a fundamental tradeoff between quality and robustness, and (2) the regularizer Eq. <ref type="formula" target="#formula_2">(3)</ref> is not a reliable measure of plausibility of faces, as we will see in Section 5.</p><p>Based on the dense point-to-point correspondence between vertices i of the 3DMM, the new distance measure d normal analyzes the difference between the surface normals n i of the reconstructed face, and the normals n ′ i of the average face:</p><formula xml:id="formula_6">d normal = 1 n n i=1 arccos n i · n ′ i n i n ′ i .<label>(7)</label></formula><p>The idea of this Normal distance is illustrated in <ref type="figure">Fig. 3</ref>. Note that, unlike d eucl , d normal is insensitive to scaling and shifting. By segmenting the full face into distinct facial regions (eyes, mouth, nose and surrounding region, see <ref type="figure" target="#fig_2">Fig.  4a</ref>), separate distances d normal can be defined that reflect the plausibilities of regions separately. We will use this idea in Section 6.</p><p>In human faces, the normals in some vertices on the nose, the eyes or the lips vary more than others. We have analyzed the original 200 3D scans of the 3DMM and created different weight maps ω (see <ref type="figure" target="#fig_2">Fig. 4b</ref>) which account for these local differences by scaling regions with high normal variation either up (considering them most diagnostic) or down (normalization). In a first step, we computed the average deviation angle φ i of the normal n i from the average normal n ′ i in each vertex i across all 200 faces. Then, we found the best weight map to be defined byω i = 1 − φ i −φ min φ max −φ min , and the weighted Normal distance is</p><formula xml:id="formula_7">d normalW = 1 n n i=1ω i arccos n i · n ′ i n i n ′ i ,<label>(8)</label></formula><p>for which we obtained experimental results that are summarized in the next section. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Evaluating the Distance Measures</head><p>The goal of this evaluation is to find out which quality measure is closest to the ratings that human observers would assign to different reconstructions. For humans, quality may mean how natural and plausible the 3D face looks, but also how similar it is to the person in the image. For failed reconstructions, both criteria are usually violated at the same time, so the distance measures from Section 4 are good candidates even though most do not measure similarity to the input face.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Evaluation 1</head><p>The first ranking was performed on 24 3D reconstructions from pictures of Barack Obama based on automatically detected landmarks. The automatic detection of landmarks is based on the approach of Zhu and Ramanan <ref type="bibr" target="#b32">[33]</ref>. An additional set of 24 reconstructions was created by using manually selected landmarks on the same input images. Again the algorithmic distance measures introduced in Section 4 were used to perform a ranking. All reconstructions were created from a single image as described in Section 3.</p><p>We asked four naive participants to create a ranking in each of the two sets of 24 reconstructions, based on the perceived quality of the reconstruction. The individual user rankings were combined to define an overall ranking list, which was compared to the ranking of each distance measure. As can be seen in <ref type="table">Table 1</ref>, the mean and max errors (difference of ranks assigned to each reconstruction) of Mahalanobis and Normal distance are much less than the ones based on Euclidean and image distance. Furthermore, based on the numbers for d normalW (see Eq. 8), it can be noted that the influence of the weight map is not very strong compared to the ranking based on d normal (see Eq. 7).</p><p>In <ref type="figure" target="#fig_3">Fig. 5</ref> the correlation of each distance measure is visualized: The horizontal axis describes the average user ranking, while each distance measure is mapped to the vertical axis. If a distance measure correlates perfectly with the user ranking, the dots of the scatter diagram are aligned along the diagonal. As can be seen in <ref type="figure" target="#fig_3">Fig. 5a</ref>, for the image distance the dots are widely scattered. The same can be observed for the Euclidean distance in <ref type="figure" target="#fig_3">Fig. 5b</ref>. Consequently, both measures are not useful to distinguish plausible from implausible reconstructions in a way that correlates to the opinion of users. For the Mahalanobis (see <ref type="figure" target="#fig_3">Fig. 5c</ref>) and the Normal distance (see <ref type="figure" target="#fig_3">Fig. 5d</ref>), the correlation between the user rating and the rating based on the algorithmic distance measures is clearly visible. Especially the Normal distance predicts much of the quality judgments of our participants. The correlations for all 2 · 24 reconstructions (automatic and manual) are for d image : 0.27, for d euclidean : 0.27, for d maha : 0.85 and for d normal : 0.94.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Evaluation 2</head><p>In a second evaluation, 3D reconstructions based on images of Obama (24 images), Lawrence (32 images), Annan  <ref type="table">Table 1</ref>: Mean and max difference of ranks for 24 reconstructions with automatically and 24 with manually selected landmarks based on the perceived quality of four naive participants (see Section 5.1).</p><p>(32 images), Watson (46 images) and Carell (28 images) were rated. Again two distinct sets were created, but this time only automatically selected landmarks were utilized. The first set was created by fitting to a single image, while for the second set a simultaneous fit to two images was performed by applying the multifit approach of Blanz and Vetter <ref type="bibr" target="#b6">[7]</ref>. A fixed reference image was selected and was then combined with each other image of the collection for the person. Please note that the facial landmarks differ from to the ones in Section 5.1. Thus, although the same input images are used for the Obama dataset, the reconstructions are different.</p><p>For each dataset, the distance measures were used to create a ranking list. Then we asked seven naive participants to rate each 3D reconstruction. Possible ratings were 'very good', 'good', 'acceptable' or 'failed'. The individual ratings were averaged and then used to create a ranking. Many reconstructions obtained the same average ratings and therefore many positions in the ranking are shared. This implies higher discrepancies between the rank list derived from humans, and the rank list from distance measures than in Evaluation 1, where we asked participants to create a unique ranking directly. Still, the Normal distance matches the user rating best, as can be seen in <ref type="table" target="#tab_2">Table 2</ref>    <ref type="table">Table 3</ref>: Mean and max (in brackets) difference of ranks for reconstructions from multiple images based on the perceived quality of seven naive participants (see Section 5.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Weighted Linear Combination per Segment</head><p>The automated 3D reconstruction that we propose in this paper compensates the reduced precision and reliability of automatically detected feature positions by using more than a single image of the face. Note that, unlike stereo and multiview algorithms, we allow for nonrigid deformations due to facial expressions, and large differences in the (unknown) imaging conditions. <ref type="figure">Figure 6</ref>: Plausibility rating of the single image based reconstructions using Normal distance with subsequent ordering.</p><p>Our strategy is to apply single image 3DMM fitting (Section 3) on each of the input images of the person separately, based on landmarks detected by the algorithm by Zhu and Ramanan <ref type="bibr" target="#b32">[33]</ref>, select the m best results ( <ref type="figure">Fig. 1 and 6</ref>) on each segment <ref type="figure" target="#fig_2">(Fig. 4a</ref>) using d normal , compute weighted linear combinations of these and merge them into a single 3D face.</p><p>The shape for each segment is determined by a weighted linear combination of corresponding segments based on the ranking list order. The weight decreases with the the rank. Thus the combined shape for each individual segment</p><formula xml:id="formula_8">S seg = m−1 i=0 α i S seg,i<label>(9)</label></formula><p>is determined by m individual reconstructions of corresponding segments S seg,i weighted by</p><formula xml:id="formula_9">α i = 1 − (i · 1 m ) m−1 c=0 1 − (c · 1 m ) .<label>(10)</label></formula><p>The algorithm is summarized in <ref type="figure">Fig. 1</ref>. Note that for illustration, <ref type="figure">Fig. 6</ref> and 1 refer to the shape of the entire face, and not for separate segments as in our algorithm.</p><p>An important element of our algorithm is to define a threshold quality value that determines which reconstructions are considered in the weighted sum. Based on the data from Section 5, we estimated a threshold that separates plausible from implausible reconstructions. In Evaluation 1, participants were also asked which faces are still plausible and which are not. In Evaluation 2, the threshold is supposed to be between ratings "acceptable" and "failed". For both data sets, we estimated Gaussian Distributions p 0 (u) and p 1 (u) for plausible and implausible reconstructions using the arithmetic mean and the estimated standard deviations of d normal in either set.</p><p>In a maximum likelihood approach, the threshold equals the intersection point of the Gaussian distributions p 0 (u) and p 1 (u). Based on our data, this threshold equals u = 11 as is shown in <ref type="figure">Fig. 7</ref> and can be computed by solving p 0 (u) = p 1 (u).  <ref type="figure">Figure 7</ref>: Gaussian distribution of d normal for plausible (blue) and implausible (red) 3D reconstructions. For Evaluation 1 <ref type="figure">(Fig. 7a</ref>) the intersection is in u = 11.08 and for Evaluation 2 <ref type="figure">(Fig. 7b)</ref> it is in u = 10.95. Now, all segments with Normal distances larger than this threshold are discarded, as illustrated in <ref type="figure">Fig. 1</ref>. After the shape for each segment has been reconstructed using a weighted linear combination based on the ranking order for the remaining segments, all independent segments are combined to build the shape of the complete face using the method described in <ref type="bibr" target="#b6">[7]</ref>. One of the input images, for example the one with minimum d normal , can be used for texture transfer as in <ref type="bibr" target="#b6">[7]</ref>, so the texture is not just a linear combination of all input images, but captured from a single input image with inverse projection and lighting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Results</head><p>We compared our approach with an existing method of simultaneous 3D reconstruction from multiple images <ref type="bibr" target="#b6">[7]</ref>. To that end we used sets of 8 to 15 images showing the same face from different angles. A subset of these images is shown in the second column of <ref type="figure" target="#fig_5">Fig. 8 and 9</ref>. The first column in each of these figures show the results of the existing approach, whereas the results of our approach are presented in Col. 3 with a uniform color and in Col. 4 with the combined texture colors. Therefore the textures of each individual segment have been linearly combined in exactly the same way as has been described for the shape in Section 6. In respect of shape estimation it outperforms the existing method if a fully automated approach is demanded and if the landmark locations may not fit the input image perfectly.</p><p>While the input images in <ref type="figure" target="#fig_5">Fig. 8 and 9</ref> are taken under controlled lighting conditions and lack facial expressions, we also tested our approach with images from the Labeled Faces in the Wild <ref type="bibr" target="#b15">[16]</ref> database. Here pose, expression and lighting differ in each image and the resolution is only 250*250px. The results are shown in <ref type="figure">Fig 10.</ref> From left to right the columns contain one image per dataset and person and two views of the reconstructed shape. At first with an uniform coloring and then with the extracted texture from the input image on the left. To retain a fully automated process, the input image belongs to the most plausible single image reconstruction <ref type="figure">(Fig. 6)</ref>. As is shown in <ref type="figure">Fig. 1</ref>, any other input image can also be used for texture transfer, because the 3DMM enables a 2D to 3D correspondence between the image and each reconstruction as well as a 3D to 3D correspondence between all reconstructions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Conclusions</head><p>We have proposed an algorithm that reconstructs a 3D face from a set of arbitrary images of a person. The core idea is to perform separate reconstructions on each image and combine the best of all reconstructions into the final shape. An important element of our work is to evaluate dif- <ref type="figure">Figure 9</ref>: Two face reconstructions from multiple images using the existing 3DMM approach (Col. 1), the proposed method with Normal distance (Col. 3) plus combined color (Col. 4). A subset of the input images is shown in Col. 2. ferent quality measures of 3D reconstructions. Combined with a feature point detector, we obtain an automated algorithm for 3D reconstruction that accounts for errors in the feature coordinates. Our method is modular, scalable and flexible, and it overcomes some of the problems that have restricted 3DMMs so far.</p><p>On a more fundamental level, it is the combination of results (multiple images, multiple segments) which makes our algorithm robust, and this is an alternative strategy to combining all input data into a single optimization problem.</p><p>It is a non-trivial result that multiple suboptimal 3D faces can be combined into a single, much more appealing one, and that this result is not just the average face. Another non-trivial result is that the reconstruction quality can be assessed without knowing the ground truth shape. <ref type="figure">Figure 10</ref>: 3D face reconstructions for image sets from the LFW <ref type="bibr" target="#b15">[16]</ref> database. From left to right the columns contain an example image from the image set, two views of the reconstructed shape and two views with extracted textures.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>The image distance is computed by subtracting the input image with a modified version where the reconstructed face is rendered on top of the original face.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Fig. 4ashows the different face segments. The 3DMM based weight map is shown inFig. 4b.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Visualization of the correlation between the average user ranking and each distant measure (100 = very good, 0 = very bad) for reconstructions based on automatic (red) and manual (yellow) landmark selection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 :</head><label>8</label><figDesc>Two face reconstructions from multiple images using the existing 3DMM approach (Col. 1), the proposed method with Normal distance (Col. 3) plus combined color (Col. 4). A subset of the input images is shown in Col. 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>and 3.</figDesc><table>Obama 
Lawrence 
Annan 
Watson 
Carell 
d image 
5.29 (13) 7.56 (20) 9.59 (30) 13.15 (29) 8.04 (18) 
d eucl 
8.38 (20) 8.38 (22) 10.84 (23) 13.54 (35) 8.82 (20) 
d maha 
5.79 (13) 7.00 (18) 5.22 (16) 11.94 (27) 3.46 (9) 
d normal 
5.21 (13) 5.44 (14) 4.97 (12) 11.50 (27) 2.61 (8) 
d normalW 5.21 (13) 5.31 (14) 4.97 (12) 11.41 (27) 2.46 (8) 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 2 :</head><label>2</label><figDesc>Mean and max (in brackets) difference of ranks for reconstructions from a single image based on the perceived quality of seven naive participants (see Section 5.2).</figDesc><table>Obama 
Lawrence 
Annan 
Watson 
Carell 
d image 
6.75 (14) 8.34 (23) 8.75 (24) 10.80 (33) 6.32 (19) 
d eucl 
5.58 (16) 8.47 (24) 6.56 (19) 10.02 (34) 9.32 (24) 
d maha 
4.33 (11) 5.47 (14) 5.25 (19) 10.07 (28) 6.82 (19) 
d normal 
4.08 (10) 4.41 (14) 4.31 (15) 7.85 (25) 4.96 (17) 
d normalW 4.08 (10) 4.41 (14) 4.31 (15) 7.80 (25) 4.96 (17) 

</table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A Linear Approach to Face Shape and Texture Recovery using a 3D Morphable Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Aldrian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<publisher>BMVA Press</publisher>
			<date type="published" when="2010" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="75" to="76" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fyffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Busch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ichikari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Debevec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jimenez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Danvoye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Antionazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Eheler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kysela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Von</forename><surname>Der Pahlen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Javier. Digital Ira: Creating a Real-Time Photoreal Digital Actor. ACM SIGGRAPH Posters</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">The Digital Emily Project: Photoreal Facial Modeling and Animation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lambeth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Debevec</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>ACM SIGGRAPH Courses</publisher>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">High-Quality Single-Shot Capture of Facial Geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Beeler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Beardsley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sumner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
		<idno>40:1-40:9</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">High-quality Passive Facial Performance Capture using Anchor Frames</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Beeler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bradley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Beardsley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gotsman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">W</forename><surname>Sumner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
		<idno>75:1-75:10</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Reanimating Faces in Images and Video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Blanz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Basso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum (EU-ROGRAPHICS)</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="641" to="650" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A Morphable Model for the Synthesis of 3D Faces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Blanz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGGRAPH</title>
		<meeting>SIGGRAPH</meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="187" to="194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Face Recognition Based on Fitting a 3D Morphable Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Blanz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI)</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1063" to="1074" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">High Resolution Passive Facial Performance Capture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bradley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Heidrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Popa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sheffer</surname></persName>
		</author>
		<idno>41:1-41:10</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Breuer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Blanz</surname></persName>
		</author>
		<title level="m">Self-Adapting Feature Layers. European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="299" to="312" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Automatic 3D Face Reconstruction from Single Images or Video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Breuer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-I</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kienzle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Blanz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Automatic Face and Gesture Recognition (FG)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multiview Face Capture using Polarized Spherical Gradient Illumination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fyffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tunwattanapong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Busch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Debevec</surname></persName>
		</author>
		<idno>129:1-129:10</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Multi-View Stereo for Community Photo Collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Goesele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hoppe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Multiple View Geometry in Computer Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
	<note>2nd edition</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Viewing Real-World Faces in 3D</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3607" to="3614" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Labeled Faces in the Wild: A Database for Studying Face Recognition in Unconstrained Environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Dense 3D Face Alignment from 2D Videos in Real-Time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Jeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Internet-based Morphable Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kemelmacher-Shlizerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Face Reconstruction in the Wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kemelmacher-Shlizerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 International Conference on Computer Vision, ICCV &apos;11</title>
		<meeting>the 2011 International Conference on Computer Vision, ICCV &apos;11<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1746" to="1753" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kemelmacher-Shlizerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
		<title level="m">Collection Flow. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1792" to="1799" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">High-Detail 3D Capture and Non-sequential Alignment of Facial Performance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Klaudiny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hilton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3D Imaging, Modeling, Processing, Visualization and Transmission (3DIMPVT)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="17" to="24" />
		</imprint>
	</monogr>
	<note>Second International Conference on</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">3D Face Reconstruction from a Single 2D Face Image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingu</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savvides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops (CVPR Workshops)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A 3D Face Model for Pose and Illumination Invariant Face Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Paysan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Knothe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Amberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Romdhani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Advanced Video and Signal Based Surveillance</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="296" to="301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Robust 3D Face Shape Reconstruction from Single Images via Two-Fold Coupled Structure Learning and Off-the-Shelf Landmark Detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shishir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><forename type="middle">A</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kakadiaris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<publisher>BMVA Press</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">3D Facial Landmark Detection under Large Yaw and Expression Variations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Passalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Theoharis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">A</forename><surname>Kakadiaris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1552" to="1564" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Self-Calibration and Metric Reconstruction Inspite of Varying and Unknown Intrinsic Camera Parameters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="7" to="25" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Unconstrained 3D Face Reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Multi-View Stereo via Graph Cuts on the Dual of an Adaptive Tetrahedral Mesh</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">N</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mordohai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Suwajanakorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kemelmacher-Shlizerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
		<title level="m">Total Moving Face Reconstruction. European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="796" to="812" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Face Reconstruction Across Different Poses and Arbitrary Illumination Conditions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Samaras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lecture Notes in Computer Science</title>
		<imprint>
			<biblScope unit="page" from="91" to="101" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Supervised Descent Method and Its Applications to Face Alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>De La Torre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="532" to="539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Exemplar-based Graph Matching for Robust Facial Landmark Localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1025" to="1032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Face Detection, Pose Estimation, and Landmark Localization in the Wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2879" to="2886" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<title level="m">Discriminative 3D Morphable Model Fitting. Automatic Face and Gesture Recognition (FG)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
