<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CNN-RNN: A Unified Framework for Multi-label Image Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Baidu Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Baidu Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhua</forename><surname>Mao</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of California at Los Angles</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Facebook Speech 4 Horizon Robotics</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Huang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Baidu Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">CNN-RNN: A Unified Framework for Multi-label Image Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:38+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>While deep convolutional neural networks (CNNs) have shown a great success in single-label image classification, it is important to note that real world images generally contain multiple labels, which could correspond to different objects, scenes, actions and attributes in an image. Traditional approaches to multi-label image classification learn independent classifiers for each category and employ ranking or thresholding on the classification results. These techniques, although working well, fail to explicitly exploit the label dependencies in an image. In this paper, we utilize recurrent neural networks (RNNs) to address this problem. Combined with CNNs, the proposed CNN-RNN framework learns a joint image-label embedding to characterize the semantic label dependency as well as the image-label relevance, and it can be trained end-to-end from scratch to integrate both information in a unified framework. Experimental results on public benchmark datasets demonstrate that the proposed architecture achieves better performance than the state-of-the-art multi-label classification models.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Every real-world image can be annotated with multiple labels, because an image normally abounds with rich semantic information, such as objects, parts, scenes, actions, and their interactions or attributes. Modeling the rich semantic information and their dependencies is essential for image understanding. As a result, multi-label classification task is receiving increasing attention <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b35">36]</ref>. Inspired by the great success from deep convolutional neural networks in single-label image classification in the past few years <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b31">32]</ref>, which demonstrates the effectiveness of end-to-end frameworks, we explore to learn a unified framework for multi-label image classification.</p><p>A common approach that extends CNNs to multi-label classification is to transform it into multiple single-label classification problems, which can be trained with the ranking loss <ref type="bibr" target="#b8">[9]</ref> or the cross-entropy loss <ref type="bibr" target="#b11">[12]</ref>. However, when treating labels independently, these methods fail to model * This work was done when the authors are at Baidu Research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Airplane</head><p>Great Pyrenees Archery Sky, Grass, Runway Dog, Person, Room Person, Hat, Nike <ref type="figure">Figure 1</ref>. We show three images randomly selected from ImageNet 2012 classification dataset. The second row shows their corresponding label annotations. For each image, there is only one label (i.e. Airplane, Great Pyrenees, Archery) annotated in the Im-ageNet dataset. However, every image actually contains multiple labels, as suggested in the third row. the dependency between multiple labels. Previous works have shown that multi-label classification problems exhibit strong label co-occurrence dependencies <ref type="bibr" target="#b38">[39]</ref>. For instance, sky and cloud usually appear together, while water and cars almost never co-occur.</p><p>To model label dependency, most existing works are based on graphical models <ref type="bibr" target="#b38">[39]</ref>, among which a common approach is to model the co-occurrence dependencies with pairwise compatibility probabilities or co-occurrence probabilities and use Markov random fields <ref type="bibr" target="#b12">[13]</ref> to infer the final joint label probability. However, when dealing with a large set of labels, the parameters of these pairwise probabilities can be prohibitively large while lots of the parameters are redundant if the labels have highly overlapping meanings. Moreover, most of these methods either can not model higher-order correlations <ref type="bibr" target="#b38">[39]</ref>, or sacrifice computational complexity to model more complicated label relationships <ref type="bibr" target="#b19">[20]</ref>. In this paper, we explicitly model the label dependencies with recurrent neural networks (RNNs) to capture higher-order label relationships while keeping the computational complexity tractable. We find that RNN significantly improves classification accuracy.</p><p>For the CNN part, to avoid problems like overfitting, previous methods normally assume all classifiers share the same image features <ref type="bibr" target="#b35">[36]</ref>. However, when using the same image features to predict multiple labels, objects that are small in the images are easily get ignored or hard to recognize independently. In this work, we design the RNNs framework to adapt the image features based on the previous prediction results, by encoding the attention models implicitly in the CNN-RNN structure. The idea behind it is to implicitly adapt the attentional area in images so the CNNs can focus its attention on different regions of the images when predicting different labels. For example, when predicting multiple labels for images in <ref type="figure">Figure 1</ref>, our model will shift its attention to smaller ones (i.e. Runway, Person, Hat) after recognizing the dominant object (i.e. Airplane, Great Pyrenees, Archery). These small objects are hard to recognize by itself, but can be easily inferred given enough contexts.</p><p>Finally, many image labels have overlapping meanings. For example, cat and kitten have almost the same meanings and are often interchangeable. Not only does exploiting the semantic redundancies reduce the computational cost, it also improves the generalization ability because the labels with duplicate semantics can get more training data.</p><p>The label semantic redundancy can be exploited by joint image/label embedding, which can be learned via canonical correlation analysis <ref type="bibr" target="#b9">[10]</ref>, metric learning <ref type="bibr" target="#b18">[19]</ref>, or learning to rank methods <ref type="bibr" target="#b36">[37]</ref>. The joint image/label embedding maps each label or image to an embedding vector in a joint low-dimensional Euclidean space such that the embeddings of semantically similar labels are close to each other, and the embedding of each image should be close to that of its associated labels in the same space. The joint embedding model can exploit label semantic redundancy because it essentially shares classification parameters for semantically similar labels. However, the label co-occurrence dependency is largely ignored in most of these models.</p><p>In this paper, we propose a unified CNN-RNN framework for multi-label image classification, which effectively learns both the semantic redundancy and the co-occurrence dependency in an end-to-end way. The framework of the proposed model is shown in <ref type="figure" target="#fig_0">Figure 2</ref>. The multi-label RNN model learns a joint low-dimensional image-label embedding to model the semantic relevance between images and labels. The image embedding vectors are generated by a deep CNN while each label has its own label embedding vector. The high-order label co-occurrence dependency in this low-dimensional space is modeled with the long short term memory recurrent neurons, which maintains the information of label context in their internal memory states. The RNN framework computes the probability of a multilabel prediction sequentially as an ordered prediction path, where the a priori probability of a label at each time step can be computed based on the image embedding and the output of the recurrent neurons. During prediction, the multilabel prediction with the highest probability can be approximately found with beam search algorithm. The proposed CNN-RNN framework is a unified framework which combines the advantages of the joint image/label embedding and label co-occurrence models, and it can be trained in an end-to-end way. Compared with state-of-the-art multi-label image classification methods, the proposed RNN framework has several advantages:</p><p>• The framework employs an end-to-end model to utilize the semantic redundancy and the co-occurrence dependency, both of which is indispensable for effective multi-label classifications.</p><p>• The recurrent neurons is more compact and more powerful model of high-order label co-occurrence dependency than other label co-occurrence models in this task.</p><p>• The implicit attention mechanism in the recurrent neurons adapt the image features to better predict small objects that need more contexts.</p><p>We evaluate the proposed CNN-RNN framework with exhaustive experiments on public multi-label benchmark datasets inlcuding NUS-WIDE, Microsoft COCO, and PASCAL VOC 2007. Experimental results demonstrate that the proposed method achieves significantly better performance compared to the current state-of-the-art multi-label classification methods. We also visualize the attentional regions of the RNN framework with the Deconvolutional net-works <ref type="bibr" target="#b39">[40]</ref>. Interestingly, the visualization shows that the RNN framework can focus on the corresponding image regions when predicting different labels, which is very similar to humans' multi-label classification process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>The progress of image classification is partly due to the creation of large-scale hand-labeled datasets such as Ima-geNet <ref type="bibr" target="#b4">[5]</ref>, and the development of deep convolutional neural networks <ref type="bibr" target="#b16">[17]</ref>. Recent work that extends deep convolutional neural networks to multi-label classification achieves good results. Deep convolutional ranking <ref type="bibr" target="#b8">[9]</ref> optimizes a top-k ranking objective, which assigns smaller weights to the loss if the positive label. Hypotheses-CNN-Pooling <ref type="bibr" target="#b35">[36]</ref> employs max pooling to aggregate the predictions from multiple hypothesis region proposals. These methods largely treat each label independently and ignore the correlations between labels.</p><p>Multi-label classification can also be achieved by learning a joint image/label embedding. Multiview Canonical Correlation Analysis <ref type="bibr" target="#b9">[10]</ref> is a three-way canonical analysis that maps the image, label, and the semantics into the same latent space. WASABI <ref type="bibr" target="#b36">[37]</ref> and DEVISE <ref type="bibr" target="#b6">[7]</ref> learn the joint embedding using the learning to rank framework with WARP loss. Metric learning <ref type="bibr" target="#b18">[19]</ref> learns a discriminative metric to measure the image/label similarity. Matrix completion <ref type="bibr" target="#b0">[1]</ref> and bloom filter <ref type="bibr" target="#b2">[3]</ref> can also be employed as label encodings. These methods effectively exploit the label semantic redundancy, but they fall short on modeling the label co-occurrence dependency.</p><p>Various approaches have been proposed to exploit the label co-occurrence dependency for multi-label image classification. <ref type="bibr" target="#b27">[28]</ref> learns a chain of binary classifiers, where each classifier predicts whether the current label exists given the input feature and the already predicted labels. The label co-occurrence dependency can also be modeled by graphical models, such as Conditional Random Field <ref type="bibr" target="#b7">[8]</ref>, Dependency Network <ref type="bibr" target="#b12">[13]</ref>, and co-occurrence matrix <ref type="bibr" target="#b38">[39]</ref>. Label augment model <ref type="bibr" target="#b19">[20]</ref> augments the label set with common label combinations. Most of these models only capture pairwise label correlations and have high computation cost when the number of labels is large. The low-dimensional recurrent neurons in the proposed RNN model are more computationally efficient representations for high-order label correlation.</p><p>RNN with LSTM can effectively model the long-term temporal dependency in a sequence. It has been successfully applied in image captioning <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b34">35]</ref>, machine translation <ref type="bibr" target="#b30">[31]</ref>, speech recognition <ref type="bibr" target="#b10">[11]</ref>, language modeling <ref type="bibr" target="#b29">[30]</ref>, and word embedding learning <ref type="bibr" target="#b17">[18]</ref>. We demonstrate that RNN with LSTM is also an effective model for label dependency.  <ref type="figure">Figure 3</ref>. A schematic illustration of a LSTM neuron. Each LSTM neuron has an input gate, a forget gate, and an output gate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>Since we aim to characterize the high-order label correlation, we employ long short term memory (LSTM) neurons <ref type="bibr" target="#b14">[15]</ref> as our recurrent neurons, which has been demonstrated to be a powerful model of long-term dependency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Long Short Term Memory Networks (LSTM)</head><p>RNN <ref type="bibr" target="#b14">[15]</ref> is a class of neural network that maintains internal hidden states to model the dynamic temporal behaviour of sequences with arbitrary lengths through directed cyclic connections between its units. It can be considered as a hidden Markov model extension that employs nonlinear transition function and is capable of modeling long term temporal dependencies. LSTM extends RNN by adding three gates to an RNN neuron: a forget gate f to control whether to forget the current state; an input gate i to indicate if it should read the input; an output gate o to control whether to output the state. These gates enable LSTM to learn long-term dependency in a sequence, and make it is easier to optimize, because these gates help the input signal to effectively propagate through the recurrent hidden states r(t) without affecting the output. LSTM also effectively deals with the gradient vanishing/exploding issues that commonly appear during RNN training <ref type="bibr" target="#b25">[26]</ref>.</p><formula xml:id="formula_0">x t = δ(U r .r(t − 1) + U w w k (t)) i t = δ(U ir r(t − 1) + U iw w k (t)) f t = δ(U fr r(t − 1) + U fw w k (t)) o t = δ(U or r(t − 1) + U ow w k (t)) r(t) = f t ⊙ r(t − 1) + i t ⊙ x t o(t) = r(t) ⊙ o(t)<label>(1)</label></formula><p>where δ(.) is an activation function, ⊙ is the product with gate value, and various W matrices are learned parameters. In our implementation, we employ rectified linear units (ReLU) as the activation function <ref type="bibr" target="#b3">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Model</head><p>We propose a novel CNN-RNN framework for multilabel classification problem. The illustration of the CNN-RNN framework is shown in <ref type="figure" target="#fig_1">Fig. 4</ref>. It contains two parts: The CNN part extracts semantic representations from images; the RNN part models image/label relationship and label dependency.</p><p>We decompose a multi-label prediction as an ordered prediction path. For example, labels "zebra" and "elephant" can be decomposed as either ("zebra", "elephant") or ("elephant", "zebra"). The probability of a prediction path can be computed by the RNN network. The image, label, and recurrent representations are projected to the same lowdimensional space to model the image-text relationship as well as the label redundancy. The RNN model is employed as a compact yet powerful representation of the label cooccurrence dependency in this space. It takes the embedding of the predicted label at each time step and maintains a hidden state to model the label co-occurrence information. The a priori probability of a label given the previously predicted labels can be computed according to their dot products with the sum of the image and recurrent embeddings. The probability of a prediction path can be obtained as the product of the a-prior probability of each label given the previous labels in the prediction path.</p><p>A label k is represented as a one-hot vector e k = [0, . . . 0, 1, 0, . . . , 0], which is 1 at the k-th location, and 0 elsewhere. The label embedding can be obtained by multiplying the one-hot vector with a label embedding matrix U l . The k-th row of U l is the label embedding of the label k.</p><formula xml:id="formula_1">w k = U l .e k .<label>(2)</label></formula><p>The dimension of w k is usually much smaller than the number of labels. The recurrent layer takes the label embedding of the previously predicted label, and models the co-occurrence dependencies in its hidden recurrent states by learning nonlinear functions:</p><formula xml:id="formula_2">o(t) = h o (r(t−1), w k (t)), r(t) = h r (r(t−1), w k (t)) (3)</formula><p>where r(t) and o(t) are the hidden states and outputs of the recurrent layer at the time step t, respectively, w k (t) is the label embedding of the t-th label in the prediction path, and h o (.), h r (.) are the non-linear RNN functions, which will be described in details in Sec. 3.1.</p><p>The output of the recurrent layer and the image representation are projected into the same low-dimensional space as the label embedding.</p><formula xml:id="formula_3">x t = h(U x o o(t) + U x I I),<label>(4)</label></formula><p>where U x o and U x I are the projection matrices for recurrent layer output and image representation, respectively. The number of columns of U x o and U x I are the same as the label embedding matrix U l . I is the convolutional neural network image representation. We will show in Sec 4.5 that the learned joint embedding effectively characterizes the relevance of images and labels.</p><p>Finally, the label scores can be computed by multiplying the transpose of U l and x t to compute the distances between x t and each label embedding.</p><formula xml:id="formula_4">s(t) = U T l x t .<label>(5)</label></formula><p>The predicted label probability can be computed using softmax normalization on the scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Inference</head><p>A prediction path is a sequence of labels (l 1 , l 2 , l 3 , · · · , l N ), where the probability of each label l t can be computed with the information of the image I and the previously predicted labels l 1 , · · · , l t−1 . The RNN model predicts multiple labels by finding the prediction path that maximizes the a priori probability. l 1 , · · · , l k = arg max l1,··· ,l k P (l 1 , · · · , l k |I) = arg max l1,··· ,l k P (l 1 |I) × P (l 2 |I, l 1 ) · · · P (l k |I, l 1 , · · · , l k−1 )</p><p>. <ref type="formula">(6)</ref> Since the probability P (l k |I, l 1 , · · · , l k−1 ) does not have Markov property, there is no optimal polynomial algorithm to find the optimal prediction path. We can employ the greedy approximation, which predicts labell t = arg max lt P (l t |I, l 1 , · · · , l t−1 ) at time step t and fix the label predictionl t at later predictions. However, the greedy algorithm is problematic because if the first predicted label is wrong, it is very likely that the whole sequence cannot be correctly predicted. Thus, we employ the beam search algorithm to find the top-ranked prediction path.</p><p>An example of the beam search algorithm can be found in <ref type="figure" target="#fig_2">Figure 5</ref>. Instead of greedily predicting the most probable label, the beam search algorithm finds the top-N most probable prediction paths as intermediate paths S(t) at each time step t. S(t) = {P 1 (t), P 2 (t), · · · , P N (t)}</p><p>At time step t + 1, we add N most probable labels to each intermediate path P i (t) to get a total of N × N paths. The N prediction paths with highest probability among these paths constitute the intermediate paths for time step t + 1.</p><p>The prediction paths ending with the END sign are added to the candidate path set C. The termination condition of the beam search is that the probability of the current intermediate paths is smaller than that of all the candidate paths. It indicates that we cannot find any more candidate paths with greater probability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Training</head><p>Learning CNN-RNN models can be achieved by using the cross-entropy loss on the softmax normalization of score softmax(s(t)) and employing back-propagation through time algorithm. In order to avoid the gradient vanishing/exploding issues, we apply the rmsprop optimization algorithm <ref type="bibr" target="#b32">[33]</ref>. Although it is possible to fine-tune the convolutional neural network in our architecture, we keep the convolutional neural network unchanged in our implementation for simplicity.</p><p>One important issue of training multi-label CNN-RNN models is to determine the orders of the labels. In the experiments of this paper, the label orders during training are determined according to their occurrence frequencies in the training data. More frequent labels appear earlier than the less frequent ones, which corresponds to the intuition that easier objects should be predicted first to help predict more difficult objects. We explored learning label orders by iteratively finding the easiest prediction ordering and order ensembles as proposed in <ref type="bibr" target="#b27">[28]</ref> or simply using fixed random order, but they do not have notable effects on the performance. We also attempted to randomly permute the label orders in each mini-batch, but it makes the training very difficult to converge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In our experiments, the CNN module uses the 16 layers VGG network <ref type="bibr" target="#b28">[29]</ref> pretrained on ImageNet 2012 classification challenge dataset <ref type="bibr" target="#b4">[5]</ref> using Caffe deep learning framework <ref type="bibr" target="#b15">[16]</ref>. The dimensions of the label embedding and of LSTM RNN layer are 64 and 512, respectively. We employ weight decay rate 0.0001, momentum rate 0.9, and dropout <ref type="bibr" target="#b3">[4]</ref> rate 0.5 for all the projection layers.</p><p>We evaluate the proposed method on three benchmark multi-label classification datasets: NUS-WIDE, Microsoft COCO, and VOC PASCAL 2007 datasets. The evaluation demonstrates that the proposed method achieves superior performance to state-of-the-art methods. We also qualitatively show that the proposed method learns a joint label/image embedding and it focuses its attention in different image regions during the sequential prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Evaluation Metric</head><p>The precision and recall of the generated labels are employed as evaluation metrics. For each image, we generate k 1 highest ranked labels and compare the generated labels to the ground truth labels. The precision is the number of correctly annotated labels divided by the number of generated labels; the recall is the number of correctly annotated labels divided by the number of ground-truth labels.</p><p>We also compute the per-class and overall precision (C-P and O-P) and recall scores (C-R and O-R), where the average is taken over all classes and all testing examples, respectively. The F1 (C-F1 and O-F1) score is the geometrical average of the precision and recall scores. We also compute the mean average precision (MAP)@N measure <ref type="bibr" target="#b33">[34]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">NUS-WIDE</head><p>NUS-WIDE dataset <ref type="bibr" target="#b1">[2]</ref> is a web image dataset that contains 269,648 images and 5018 tags from Flickr. There are a total of 1000 tags after removing noisy and rare tags. These images are further manually annotated into 81 concepts by   human annotators. An example of the annotations and predictions for both of label set is shown in the left side of <ref type="figure" target="#fig_3">Fig. 6</ref>. The quality of 81-tag annotations is relatively high, while the 1000-tag annotations are very noisy. In <ref type="figure" target="#fig_3">Fig. 6</ref>, we can find some tags with duplicate semantics, such as "cloud" and "clouds", some completely wrong tags, such as "photographer", and some tags that are too general to have specific meanings, such as "beautiful". We first evaluate the proposed method on less noisy 81 concepts labels. We compare the proposed method with state-of-the-art methods including K nearest neighbor search <ref type="bibr" target="#b1">[2]</ref>, softmax prediction, WARP method <ref type="bibr" target="#b8">[9]</ref>, metric learning <ref type="bibr" target="#b18">[19]</ref>, and joint embedding <ref type="bibr" target="#b37">[38]</ref> in <ref type="table">Table 1</ref>. Since there is less noise in 81 concepts labels, all methods achieve fairly good performance. Although we do not fine-tune our convolutional neural network image representation, the proposed RNN framework outperforms the state-of-the-art methods. In particular, we find the CNN-RNN framework achieves 8% higher precision, because it is capable of exploiting the label correlation to filter out the labels that can not possibly exist together.</p><p>We also compare RNN model with softmax, DSLR <ref type="bibr" target="#b21">[22]</ref>, and WARP models on the more challenging 1000-tag label set in <ref type="table">Table 2</ref>. The prediction accuracy of all the methods are very low, because the labels on this dataset are very noisy, but the proposed method still outperforms all the baseline methods. We find the proposed method cannot  distinguish gender-related labels such as "actor" and "actress", because our convolutional neural network is trained on ImageNet, which does not have the annotation for this task. More multi-label prediction examples can be found in the supplemental materials.</p><formula xml:id="formula_6">Method C-P P-R C-F1 O-P O-R O-F1</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Microsoft COCO</head><p>Microsoft COCO (MS-COCO) dataset <ref type="bibr" target="#b20">[21]</ref> is an image recognition, segmentation, and captioning dataset. It contains 123 thousand images of 80 objects types with perinstance segmentation labels. Among those images, 82783 images are utilized as training data, and 40504 images are employed as testing data. We utilize the object annotations as the labels. An example of the annotations and predictions of the MS-COCO dataset is shown in the right of <ref type="figure" target="#fig_3">Fig. 6</ref>. An interesting property of the MS-COCO dataset is that most images in this dataset contain multiple objects, and these objects usually have strong co-occurrence dependencies. For example, "baseball glove" and "sport ball" have high co-occurrence probability, while "zebra" and "cat" never appear together.</p><p>We compare the softmax, multi-label binary cross entropy, and WARP <ref type="bibr" target="#b8">[9]</ref> models with the CNN-RNN model in <ref type="table" target="#tab_4">Table 3</ref>. Since the number of the objects per image varies considerably in this dataset, we do not set the minimum length of the prediction path during beam search. It can be observed that the proposed method achieves much better performance both in terms of overall precision and recall. It has a slightly lower per-class recall because it may output less than k labels for an image and it usually chooses not to predict the small objects that have little co-occurrence dependencies with other larger objects. We also replace the recurrent layer with a linear embedding layer in the proposed architecture and evaluate the performance. We find that removing the recurrent layer significantly affects the recall of the multi-label classification.</p><p>The per-class precision and recall of the proposed frame-   work is shown in <ref type="figure" target="#fig_4">Fig. 7</ref>. We find the proposed framework performs very well on large objects, such as "person", "zebra", and "stop sign", and the objects with high dependencies with other objects or the scene, such as "sports bar" and "baseball glove". It performs very poorly on small objects with little dependencies with other objects, such as "toaster" and "hair drier", because the global convolutional neural network image features have limited discriminative ability to recognize small objects. There is zero prediction for "toaster" and "hair drier", resulting in zero precision and recall scores for these two labels. The relationship between recall and bounding box area on Microsoft COCO dataset is shown in <ref type="figure" target="#fig_5">Fig. 8</ref>. We can observe that the recall is the generally higher as the object is larger, unless the object is so large that it almost fill the whole image, where some important information might lose during the image cropping process in CNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">PASCAL VOC 2007</head><p>PASCAL Visual Object Classes Challenge (VOC) datasets <ref type="bibr" target="#b5">[6]</ref> are widely used as the benchmark for multilabel classification. VOC 2007 dataset contains 9963 images divided into train, val and test subsets. We conduct our experiments on trainval/test splits (5011/4952 images). The evaluation is Average Precision (AP) and mean of AP (mAP). The comparison to state-of-the-art methods is shown in <ref type="table" target="#tab_6">Table 4</ref>. INRIA <ref type="bibr" target="#b13">[14]</ref> is based on the transitional feature extraction-coding-pooling pipeline. CNN-SVM <ref type="bibr" target="#b26">[27]</ref> directly applies SVM on the OverFeat features pre-trained on ImageNet. I-FT <ref type="bibr" target="#b35">[36]</ref>   proposal information to fine-tune the CNN features pretrained on ImageNet 1000 dataset, and ac hives better performance than the methods that do not exploit region information. The proposed CNN-RNN method outperforms the I-FT method by a large margin, and it also performs better than HCP-1000C method, although the RNN method does not take the region proposal information into account.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Label embedding</head><p>In addition to being able to generate multiple labels, the CNN-RNN model also effectively learns a joint label/image embedding. The nearest neighbors of the labels in the embedding space for NUS-WIDE and MS-COCO, are shown in <ref type="table" target="#tab_7">Table 5</ref>. We can see that a label is highly semantically related to its nearest-neighbor labels. <ref type="figure" target="#fig_6">Fig. 9</ref> shows the nearest neighbor labels for images on NUS-WIDE 1000-tag dataset computed according to label embedding w k and image embedding U x I I. In the joint embedding space, an image and its nearest neighbor labels are semantically relevant. Moreover, we find that compared to the top-ranked labels predicted by classification model, the nearest neighbor labels are usually more fine-grained. For example, the nearest neighbor labels "hawk" and "glacier" are more fine-grained than "bird" and "landscape".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Original Image</head><p>Initial Attention Attention after first word <ref type="figure">Figure 10</ref>. The attentional visualization for the RNN multi-label framework. This image has two ground-truth labels: "elephant" and "zebra". The bottom-left image shows the framework's attention in the beginning, and the bottom-right image shows its attention after predicting "elephant".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Attention Visualization</head><p>It is interesting to investigate how the CNN-RNN framework's attention changes when predicting different labels. We visualize the attention of the RNN multi-label model with Deconvolutional networks <ref type="bibr" target="#b39">[40]</ref> in <ref type="figure">Fig. 10</ref>. Given an input image, the attention of the RNN multilabel model at each time step is the average of the synthesized image of all the label nodes at the softmax layer using Deconvolutional network. The ground truth labels of this image are "elephant" and "zebra". (Notice that the visualization of attention does not utilize the ground truth labels) At the beginning, the attention visualization shows that the model looks over the whole image and predicts "elephant". After predicting "elephant", the model shifts its attention to the regions of zebra and predicts "zebra". The visualization shows that although the RNN framework does not learn an explicit attentional model, it manages to steer its attention to different image regions when classifying different objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion and Future Work</head><p>We propose a unified CNN-RNN framework for multilabel image classification. The proposed framework combines the advantages of the joint image/label embedding and label co-occurrence models by employing CNN and RNN to model the label co-occurrence dependency in a joint image/label embedding space. Experimental results on several benchmark datasets demonstrate that the proposed approach achieves superior performance to the state-of-theart methods.</p><p>The attention visualization shows that the proposed model can steer its attention to different image regions when predicting different labels. However, predicting small objects is still challenging due to the limited discriminativeness of the global visual features. It is an interesting direction to not only predict the labels, but also predict the segmentation of the objects by constructing an explicit attention model. We will investigate that in our future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>An illustration of the CNN-RNN framework for multilabel image classification. The framework learns a joint embedding space to characterize the image-label relationship as well as label dependency. The red and blue dots are the label and image embeddings, respectively, and the black dots are the sum of the image and recurrent neuron output embeddings. The recurrent neurons model the label co-occurrence dependencies in the joint embedding space by sequentially linking the label embeddings in the joint embedding space. At each time step, the probability of a label is computed based on the image embedding and the output of the recurrent neurons. (best viewed in color)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>The architecture of the proposed RNN model for multilabel classification. The convolutional neural network is employed as the image representation, and the recurrent layer captures the information of the previously predicted labels. The output label probability is computed according to the image representation and the output of the recurrent layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 .</head><label>5</label><figDesc>An example of the beam search algorithm with beam size N = 2. The beam search algorithm finds the best N paths with the highest probability, by keeping a set of intermediate paths at each time step and iteratively adding labels these intermediate paths.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 .</head><label>6</label><figDesc>One example image from NUS-WIDE (left) and MS-COCO(right) datasets, the ground-truth annotations and our model's predictions. Method C-P P-R C-F1 O-P O-R O-F1 MAP@10</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 .</head><label>7</label><figDesc>The per-class precision and recall of the RNN model on MS-COCO dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 .</head><label>8</label><figDesc>The relationship between recall and bounding box area on Microsoft COCO dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 .</head><label>9</label><figDesc>Nearest neighbor labels and top classification predictions by softmax model for three query images of 1000 label set on NUS-WIDE dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 .</head><label>3</label><figDesc>Comparisons on MS-COCO Dataset for k = 3.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 4 .</head><label>4</label><figDesc>Classification results (AP in %) comparison on PASCAL VOC 2007 dataset.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 5 .</head><label>5</label><figDesc>employs the squared loss function on the shared CNN features on PASCAL VOC for multi-label classification. HCP-1000C<ref type="bibr" target="#b35">[36]</ref> employs region Nearest neighbors for label embeddings of 1k labels of NUS-WIDE and MS-COCO datasets</figDesc><table>Label 

Nearest Neighbors 

glacier 
arctic, norway, volcano, tundra, lakes 
sky 
nature, blue, clouds, landscape, bravo 
sunset 
sun, landscape, light, bravo, yellow 
rail 
railway, track, locomotive, tracks, steam 
cat 
dog, bear, bird, hair drier, toaster 
cow 
horse, sheep, bear, zebra, elephant 

hawk, eagle, fauna, 
wind, bird 

glacier, volcano, cli , 
arctic, lakes 

portraits, costume, 
female, asian, hat 

portraits,people, street, 
hospital, woman 

landscape, mountain, 
nature, mountains, bravo 

bird, hwak, nature, 
bravo, birds 

Images 

KNN 

Classi-
cation 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">For RNN model, we set the minimum prediction length during beam search to ensure that at least k labels are predicted.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Matrix completion for multi-label image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Cabral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Torre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Costeira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bernardino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="190" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Nus-wide: a real-world web image database from national university of singapore</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM international conference on image and video retrieval</title>
		<meeting>the ACM international conference on image and video retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Robust bloom filters for large multilabel classification tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Artieres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gallinari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1851" to="1859" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Improving deep neural networks for lvcsr using rectified linear units and dropout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note>CVPR 2009. IEEE Conference on</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Devise: A deep visual-semantic embedding model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2121" to="2129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Collective multi-label classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ghamrawi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th ACM international conference on Information and knowledge management</title>
		<meeting>the 14th ACM international conference on Information and knowledge management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="195" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.4894</idno>
		<title level="m">Deep convolutional ranking for multilabel image annotation</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A multi-view embedding space for modeling internet images, tags, and their semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Speech recognition with deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A.-R</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="6645" to="6649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Tagprop: Discriminative metric learning in nearest neighbor models for image auto-annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Guillaumin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 12th International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="309" to="316" />
		</imprint>
	</monogr>
	<note>Computer Vision</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Multi-label classification using conditional dependency networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI Proceedings-International Joint Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Combining efficient object localization and image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Harzallah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jurie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 12th International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="237" to="244" />
		</imprint>
	</monogr>
	<note>Computer Vision</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<title level="m">Caffe: Convolutional architecture for fast feature embedding</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Compositional distributional semantics with long short term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuidema</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02510</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A distributed approach toward discriminative distance metric learning. Neural Networks and Learning Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Rui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Multi-label image classification with a probabilistic label enhancement model. UAI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2014</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Image tag completion via dual-view linear sparse reconstructions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Ge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Image Understanding</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">124</biblScope>
			<biblScope unit="page" from="42" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Unified tag analysis with multi-edge graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the international conference on Multimedia</title>
		<meeting>the international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="25" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A new baseline for image annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Makadia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pavlovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2008</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="316" to="329" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep captioning with multimodal recurrent neural networks (m-rnn)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1211.5063</idno>
		<title level="m">On the difficulty of training recurrent neural networks</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Cnn features off-the-shelf: an astounding baseline for recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Razavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Azizpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Carlsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="512" to="519" />
		</imprint>
	</monogr>
	<note>2014 IEEE Conference on</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Classifier chains for multi-label classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Read</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Pfahringer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Frank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Lstm neural networks for language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sundermeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schlüter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.4842</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">Going deeper with convolutions. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COURSERA: Neural Networks for Machine Learning</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">User performance versus precision measures for simple search tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Turpin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Scholer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 29th annual international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="11" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Show and tell: A neural image caption generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.4555</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.5726</idno>
		<title level="m">Cnn: Single-label to multi-label</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Wsabie: Scaling up to large vocabulary image annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Cross-modal learning to rank via latent joint representation. Image Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1497" to="1509" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Correlative multi-label multi-instance image annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deconvolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="2528" to="2535" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
