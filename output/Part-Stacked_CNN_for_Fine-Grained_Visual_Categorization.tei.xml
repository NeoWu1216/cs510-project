<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Part-Stacked CNN for Fine-Grained Visual Categorization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoli</forename><surname>Huang</surname></persName>
							<email>shaoli.huang@student.</email>
							<affiliation key="aff0">
								<orgName type="department">Centre for Quantum Computation &amp; Intelligent Systems and Faculty of Engineering and Information Technology</orgName>
								<orgName type="institution">University of Technology Sydney</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Centre for Quantum Computation &amp; Intelligent Systems and Faculty of Engineering and Information Technology</orgName>
								<orgName type="institution">University of Technology Sydney</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Cooperative Medianet Innovation Center</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
							<email>dacheng.tao@uts.edu.au</email>
							<affiliation key="aff0">
								<orgName type="department">Centre for Quantum Computation &amp; Intelligent Systems and Faculty of Engineering and Information Technology</orgName>
								<orgName type="institution">University of Technology Sydney</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ya</forename><surname>Zhang</surname></persName>
							<email>yazhang@sjtu.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="department">Cooperative Medianet Innovation Center</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Part-Stacked CNN for Fine-Grained Visual Categorization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:44+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In the context of fine-grained visual categorization, the ability to interpret models as human-understandable visual manuals is sometimes as important as achieving high classification accuracy. In this paper, we propose a novel Part-Stacked CNN architecture that explicitly explains the finegrained recognition process by modeling subtle differences from object parts. Based on manually-labeled strong part annotations, the proposed architecture consists of a fully convolutional network to locate multiple object parts and a two-stream classification network that encodes object-level and part-level cues simultaneously. By adopting a set of sharing strategies between the computation of multiple object parts, the proposed architecture is very efficient running at 20 frames/sec during inference. Experimental results on the CUB-200-2011 dataset reveal the effectiveness of the proposed architecture, from multiple perspectives of classification accuracy, model interpretability, and efficiency. Being able to provide interpretable recognition results in realtime, the proposed method is believed to be effective in practical applications.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Fine-grained visual categorization aims to distinguish objects at the subordinate level, e.g., different species of birds <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b3">4]</ref>, pets <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b29">30]</ref>, flowers <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b0">1]</ref> and cars <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b25">26]</ref>. It is a highly challenging task due to the small inter-class variance caused by highly similar subordinate categories, and the large intra-class variance by nuisance factors such as pose, viewpoint and occlusion. Inspiringly, huge progress has been made over the last few years <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b48">49]</ref>, making fine-grained recognition techniques a large step closer to practical use in various applications, such as wildlife observation and surveillance systems.</p><p>Whilst numerous attempts have been made to boost the * These authors contributed equally to this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>California Gull Ring billed Gull</head><p>The class has its beak mostly different from the class <ref type="figure">Figure 1</ref>. Overview of the proposed approach. We propose to classify fine-grained categories by modeling the subtle difference from specific object parts. Beyond classification results, the proposed PS-CNN architecture also offers human-understandable instructions on how to classify highly similar object categories explicitly.</p><p>classification accuracy of fine-grained visual categorization <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b45">46]</ref>, we argue that another important aspect of the problem has yet been severely overlooked, i.e., the ability to generate a human-understandable "manual" on how to distinguish fine-grained categories in detail. For example, volunteers for ecological protection may certainly benefit from an algorithm that could not only classify bird species accurately, but also provide brief instructions on how to distinguish a category from its most similar subspeciese.g., a salient difference between a Ringed-billed gull and a California gull lies in the pattern on their beaks ( <ref type="figure">Figure 1</ref>)with some intuitive illustration examples. Existing finegrained recognition methods that aim to provide a visual field guide mostly follow the routine of "part-based onevs-one features" (POOFs) <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4]</ref> or employ human-inthe-loop methods <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b40">41]</ref>. Since the data size has been increasing drastically, a method that simultaneously implements and interprets fine-grained visual categorization using the latest deep learning methods <ref type="bibr" target="#b18">[19]</ref> is therefore highly advocated.</p><p>It is widely acknowledged that the subtle difference between fine-grained categories mostly resides in the unique properties of object parts <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b52">53]</ref>. Therefore, a practical solution to interpret classification results as human-understandable manuals is to discover classification criteria from object parts. Some of existing finegrained datasets have provided detailed part annotations including part landmarks and attributes <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b25">26]</ref>. However, they are usually associated with a large number of object parts, which poses heavy computational burden for both part detection and classification. From this perspective, one would like to seek a method that follows the object-partaware strategy to provide interpretable predicting criteria, while requiring minimum computational effort to deal with a possibly large number of parts.</p><p>In this paper, we propose a new part-based CNN architecture for fine-grained visual categorization that models multiple object parts in a unified framework with high efficiency. Similar with previous fine-grained recognition approaches, the proposed method consists of a localization module to detect object parts ("where pathway") and a classification module to classify fine-grained categories at the subordinate level ("what pathway"). In particular, we employ a fully convolutional network (FCN) to perform object part localization. The inferred part locations are fed into the classification network, in which a two-stream architecture is proposed to analyze images in both object-level (bounding boxes) and part-level (part landmarks). The computation of multiple parts is first conducted via a shared feature extraction route, then separated directly on feature maps through a part crop layer, concatenated, and then fed into a shallower network to perform object classification. Except for categorical predictions, the proposed method also generates interpretable classification instructions based on object parts. Since the proposed architecture employs a sharing strategy that stacks the computation of multiple parts together, we call it Part-Stacked CNN (PS-CNN).</p><p>The contributions of this paper include: 1) we present a novel and efficient part-based CNN architecture for finegrained recognition; 2) our architecture adopts an FCN to localize object parts, which has seldom been studied before in the context of object recognition; 3) our classification network follows a two-stream structure that captures both object-level and part-level information, in which a new share-and-divide strategy is presented on the computation of multiple object parts. As a result, the proposed architecture is very efficient, with a capacity of 20 frames/sec 1 on a Tesla K80 to classify images at test time using 15 object parts; 4) The proposed method provides effective model interpretation for fine-grained object recognition, while being able to run in real-time. This is a much preferred property for practical applications, such as surveillance systems. The effectiveness of the proposed method is demonstrat-ed through systematic experiments on the Caltech-UCSD Birds-200-2011 <ref type="bibr" target="#b43">[44]</ref> dataset, in which we achieved 76% classification accuracy. We also present practical examples of human-understanding manuals generated by the proposed method for the task of fine-grained visual categorization.</p><p>The rest of the paper is organized as follows. Section 2 summarizes related works. The proposed architecture including the localization network and the classification network is described in Section 3. Detailed performance studies and analysis are conducted in Section 4. Section 5 concludes the paper and proposes discussions on the application scenarios of the proposed PS-CNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Fine-Grained Visual Categorization. A number of methods have been developed to classify object categories at the subordinate level. Recently, the best performing methods mostly sought for improvement brought by the following three aspects: more discriminative features including deep CNNs for better visual representation <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b36">37]</ref>, explicit alignment approaches to eliminate pose displacements <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b13">14]</ref>, and part-based methods to study the impact of object parts <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b54">55]</ref>. Another line of research explored human-in-the-loop methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b44">45]</ref> to identify the most discriminative regions for classifying finegrained categories. Although such methods provided direct references of how people perform fine-grained recognition in real life, they were impossible to scale for large systems due to the need of human interactions at test time.</p><p>Current state-of-the-art methods for fine-grained recognition are part-based R-CNN by Zhang et al. <ref type="bibr" target="#b50">[51]</ref> and Bilinear CNN by Lin et al. <ref type="bibr" target="#b21">[22]</ref>, which both employed a twostage pipeline of part detection and part-based object classification. The main idea of the proposed PS-CNN is largely inherited from <ref type="bibr" target="#b50">[51]</ref>, who first detected the location of two object parts and then trained an individual CNN based on the unique properties of each part. Compared to part-based R-CNN, the proposed method is far more efficient in both detection and classification phrases. As a result, we are able to employ much more object parts than that of <ref type="bibr" target="#b50">[51]</ref>, while still being significantly faster at test time.</p><p>On the other hand, Lin et al. <ref type="bibr" target="#b21">[22]</ref> argued that manually defined parts were sub-optimal for the task of object recognition, and thus proposed a bilinear model consisting of two streams whose roles were interchangeable as detectors or features. Although this design enjoyed the data-driven nature that could possibly lead to optimal classification performance, it also made the resultant model hard to interpret. On the contrary, our method tries to balance the need of both both classification accuracy and model interpretability in fine-grained recognition systems.  Fully Convolutional Networks. Fully convolutional network (FCN) is a fast and effective approach to produce dense prediction with convolutional networks. Successful examples can be found on tasks including sliding window detection <ref type="bibr" target="#b33">[34]</ref>, semantic segmentation <ref type="bibr" target="#b22">[23]</ref>, and human pose estimation <ref type="bibr" target="#b39">[40]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Part-Stacked CNN</head><p>We present the model architecture of the proposed Part-Stacked CNN in this section. In accordance with the common framework for fine-grained recognition, the proposed architecture is decomposed into a Localization Network (Section 3.1) and a Classification Network (Section 3.2). We adopt CaffeNet <ref type="bibr" target="#b15">[16]</ref>, a slightly modified version of the standard seven-layer AlexNet <ref type="bibr" target="#b18">[19]</ref> architecture, as the basic structure of the network; deeper networks could potentially lead to better recognition accuracy, but may also result in lower efficiency.</p><p>A unique design in our architecture is that the message transferring operation from the localization network to the classification network, i.e. using detected part locations to perform part-based classification, is conducted directly on the conv5 output feature maps within the process of data forwarding. It is a significant difference compared to the standard two-stage pipeline of part-based R-CNN [51] that con-secutively localizes object parts and then trains part-specific CNNs on the detected regions. Based on this design, a set of sharing schemes are performed to make the proposed PS-CNN fairly efficient for both learning and inference. <ref type="figure" target="#fig_0">Figure  2</ref> illustrates the overall network architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Localization Network</head><p>The first stage of the proposed architecture is a localization network that aims to detect the location of object parts. We employ the simplest form of part landmark annotations, i.e. a 2D key point is annotated at the center of each object part. Assume that M -the number of object parts labeled in the dataset, is sufficient large to offer a complete set of object parts on which fine-grained categories are usually different from each other. Motivated by recent progress of human pose estimation <ref type="bibr" target="#b22">[23]</ref> and semantic segmentation <ref type="bibr" target="#b39">[40]</ref>, we adopt a fully convolutional network (FCN) <ref type="bibr" target="#b27">[28]</ref> to generate dense output feature maps for locating object parts.</p><p>Fully convolutional network. A fully convolutional network is achieved by replacing the parameter-rich fully connected layers in standard CNN architectures by convolutional layers with kernels in spatial size of 1 × 1. Given an input RGB image, the output of a fully convolutional network is a feature map in reduced dimension compared to  <ref type="figure">Figure 3</ref>. Demonstration of the localization network. Training process is denoted inside the dashed box. For inference, a Gaussian kernel is then introduced to remove noise. The results are M 2D part locations in the 27 × 27 conv5 feature map. the input. The computation of each unit in the feature map only corresponds to pixels inside a region with fixed size in the input image, which is called its receptive field. FCN is preferred in our framework due to the following three reasons: 1) feature maps generated by FCN can be directly utilized as the part locating results in the classification network, which will be detailed in Section 3.2; 2) results of multiple object parts can be obtained simultaneously using an FCN; 3) FCN is very efficient in both learning and inference.</p><p>Learning. We model the part localization process as a multi-class classification problem on dense output spatial positions. In particular, suppose the output of the last convolutional layer in the FCN is in the size of h × w × d, where h and w are spatial dimensions and d is the number of channels. We set d = M + 1. Here M is the number of object parts and 1 denotes for an additional channel to model the background. To generate corresponding ground-truth labels in the form of feature maps, units indexed by h × w spatial positions are labeled by their nearest object part; units that are not close to any of the labeled parts (with an overlap &lt; 0.5 with respect to receptive field) are labeled as background.</p><p>A practical problem here is to determine the model depth and the size of input images for training the FCN. Generally speaking, layers at later stages carry more discriminative power and thus are more likely to generate promising localization results; however, their receptive fields are also much larger than those of previous layers. For example, the receptive field of conv5 layer in CaffeNet has a size of 163 × 163 compared to the 227 × 227 input image, which is too large to model an object part. We propose a simple trick to deal with this problem, i.e., upsampling the input images so that the fixed-size receptive fields denoting object parts become relatively smaller compared to the whole object, while still being able to use layers at later stages to guarantee enough discriminative power.</p><p>The localization network in the proposed PS-CNN is illustrated in <ref type="figure">Figure 3</ref>. The input of the FCN is a boundingbox-cropped RGB image, warped and resized into a fixed size of 454 × 454. The structure of the first five layers is identical to those in CaffeNet, which leads to a 27×27×256 output after conv5 layer. Afterwards, we further introduce a 1×1 convolutional layer with 512 output channels as conv6, and another 1 × 1 convolutional layer with M + 1 outputs termed conv7 to perform classification. By adopting a spatial preserving softmax that normalizes predictions at each spatial location of the feature map, the final loss function is a sum of softmax loss at all 27 × 27 positions:</p><formula xml:id="formula_0">L = − 27 h=1 27 w=1 log σ(h, w,ĉ),<label>(1)</label></formula><p>where</p><formula xml:id="formula_1">σ(h, w,ĉ) = exp(f conv7 (h, w,ĉ)) M c=0 exp(f conv7 (h, w, c))</formula><p>.</p><p>Here,ĉ ∈ [0, 1, ..., M ] is the part label of the patch at location (h, w), where the label 0 denotes background. f conv7 (h, w, c) stands for the output of conv7 layer at spatial position (h, w) and channel c.</p><p>Inference. The inference process starts from the output of the learned FCN, i.e., (M +1) part-specific heat maps in the size of 27 × 27, in which we introduce a Gaussian kernel G to remove isolated noise in the feature maps. The final output of the localization network are M locations in the 27 × 27 conv5 feature map, each of which is computed as the location with the maximum response for one object part. Meanwhile, considering that object parts may be missing in some images due to varied poses and occlusion, we set a threshold µ that if the maximum response of a part is below µ, we simply discard this part's channel in the classification network for this image. Let g(h, w, c) = σ(h, w, c) * G, the inferred part locations are given as:</p><formula xml:id="formula_2">(h * c , w * c ) = argmax h,w g(h, w, c) if g(h * c , w * c , c) &gt; µ, (−1, −1) otherwise.<label>(2)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Classification network</head><p>The second stage of the proposed PS-CNN is a classification network with the inferred part locations given as an input. It follows a two-stream architecture with a Part Stream and a Object Stream to capture semantics from multiple levels. A sub-network consisting of three fully connected layers is then performed as an object classifier, as shown in <ref type="figure" target="#fig_0">Figure 2</ref>.</p><p>Part stream. The part stream acts as the core of the proposed PS-CNN architecture. To capture object-partdependent differences between fine-grained categories, one can train a set of part CNNs, each one of which conducts classification on a part separately, as proposed by Zhang et al. <ref type="bibr" target="#b50">[51]</ref>. Although such method worked well for <ref type="bibr" target="#b50">[51]</ref> who only employed two object parts, we argue that it is not applicable when the number of object parts is much larger in our case, because of the high time and space complexity.</p><p>In PS-CNN, we introduce two strategies to improve the efficiency of the part stream. The first one is model parameter sharing. Specifically, model parameters of the first five convolutional layers are shared among all object parts, which can be regarded as a generic part-level feature extractor. This strategy leads to less parameters in the proposed architecture and thus reduces the risk of overfitting.</p><p>Other than model parameter sharing, we also conduct a computational sharing strategy. The goal is to make sure that the feature extraction procedure of all parts only requires one pass through the convolutional layers. Analogous to the localization network, the input images of the part stream are in doubled resolution 454 × 454 so that the respective receptive fields are not too large to model object parts; forwarding the network to conv5 layer generates output feature maps of size 27 × 27. By far, the computation of all object parts is completely shared.</p><p>After performing the shared feature extraction procedure, the computation of each object part is then partitioned through a part crop layer to model part-specific classification cues. For each part, the part crop layer extracts a local neighborhood region centered at the detected part location. Features outside the cropped region are simply dropped. In practice, we crop 6 × 6 neighborhood regions out of the 27 × 27 conv5 feature maps to match the output size of the object stream. The resultant receptive fields for the cropped feature maps has a width of 243, given the receptive field size of conv5 layers and the respective stride.</p><p>Object stream. The object stream utilizes bounding-boxlevel supervision to capture object-level semantics for finegrained recognition. It follows the general architecture of CaffeNet, in which the input of the network is a 227 × 227 RGB image and the output of pool5 layer are 6 × 6 feature maps.</p><p>We find the design of the two-stream architecture in PS-CNN analogous to the famous Deformable Part-based Models <ref type="bibr" target="#b11">[12]</ref>, in which object-level features are captured through a root filter in a coarser scale, while detailed part-level information is modeled by several part filters at a finer scale. We find it critical to measure visual cues from multiple semantic levels in an object recognition algorithm.</p><p>Dimension reduction and fully connected layers. The aforementioned two-stream architecture generates an individual feature map for each object part and bounding box. When conducting classification, they serve as an over-complete set of CNN features from multiple scales. Following the standard CaffeNet architecture, we employ a DNN including three fully connected layers as object classifiers. The first fully connected layer fc6 now becomes a part concatenation layer whose input is generated by stacking the output feature maps of the part stream and the object stream together. However, such a concatenating process requires M + 1 times more model parameters than the original fc6 layer in CaffeNet, which leads to a huge memory cost.</p><p>To reduce model parameters, we introduce a 1×1 convolutional layer termed conv5 1 in the part stream that projects the 256 dimensional conv5 output to 32-d. It is identical to a low-rank projection of the model output and thus can be initialized through standard PCA. Nevertheless, in our experiments, we find that directly initializing the weights of the additional convolution by PCA in practice worsens the performance. To enable domain-specific fine-tuning from pre-trained CNN model weights, we train an auxiliary CN-N to initialize the weights for the additional convolutional layer.</p><p>Let X c ∈ R N ×M ×6×6 be the c th 6 × 6 cropped region around the center point (h * c , w * c ) from conv5 1 feature maps</p><formula xml:id="formula_3">X ∈ R N ×M ×27×27 , where (h * c , w * c )</formula><p>is the predicted location for part c and N is the number of output feature maps. The output of part concatenation layer fc6 can be formulated as:</p><formula xml:id="formula_4">f out (X) = σ( M c=1 (W c ) T X c ),<label>(3)</label></formula><p>where W c is the model parameters for part c in fc6 layer, and σ is an activation function. We conduct the standard gradient descent method to train the classification network. The most complicated part for computing gradients lies in the dimension reduction layer due to the impact of part cropping. Specifically, the gradient of each cropped part feature map (in 6×6 spatial resolution) is projected back to the original size of conv5 (27 × 27 feature maps) according to the respective part location and then summed up. Note that the proposed PS-CNN is implemented as a two stage framework, i.e. after training the FCN, weights of the localization network are fixed when training the classification network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We present experimental results and analysis of the proposed method in this section. Specifically, we will evaluate the performance through four different aspects: localization accuracy, classification accuracy, inference efficiency, and model interpretation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Dataset and implementation details</head><p>Experiments are conducted on the widely used finegrained classification benchmark the Caltech-UCSD Bird-   <ref type="bibr" target="#b15">[16]</ref>. Specifically, bounding-box cropped input images are warped to a fixed size of 512 × 512, randomly cropped into 454 × 454, and then fed into the localization network and the part stream in the classification network as input. We employ a pooling layer in the object stream that downsamples the 454 × 454 input to 227 × 227 to guarantee synchronization between the two streams in the classification network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Localization results</head><p>As the localization results in our method are directly delivered to the classification network at feature-map-level, we do not intend to achieve accurate keypoint localization at pixel-level but instead focus on a rougher correctness measure. The localization correctness is quantitatively assessed using APK (Average Precision of Key points) <ref type="bibr" target="#b49">[50]</ref>. Following <ref type="bibr" target="#b23">[24]</ref>, we consider a key point to be correctly predicted if the prediction lies within a Euclidean distance of α times the maximum of the bounding box width and height compared to the ground truth. We set α = 0.1 in all the analysis below.</p><p>The adopted FCN architecture in PS-CNN achieves a reasonably inspiring 86.6% APK on the test set of CUB-200-2011 for <ref type="bibr" target="#b14">15</ref>  1 convolutional layer and the employed Gaussian smoothing kernel delivers 1.5% and 2% improvements over the results using standard five convolutional layers in AlexNet, respectively. Furthermore, we present per part APKs in <ref type="table" target="#tab_2">Table 1</ref>. An interesting phenomenon here is that parts residing near the head of the birds tend to be located more accurately. It turns out that the birds' head has relatively more stable structure with less deformations and lower probability to be occluded. On the contrary, parts that are highly deformable such as wings and legs get lower APK values. <ref type="figure" target="#fig_1">Figure 4</ref> shows typical localization results of the proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Classification results</head><p>We begin the analysis of classification results by a study on the discriminative power of each object part. Each time we select one object part as the input and discard the computation of all other parts. Different parts reveal significantly different classification results. The most discriminative part crown itself achieves a quite impressive accuracy of 57%, while the lowest accuracy is only 10% for part beak. Therefore, to obtain better classification results, it may be beneficial to find a rational combination or order of object parts instead of directly ran the experiments on all parts altogether.</p><p>We therefore introduce a strategy that incrementally adds object parts to the whole framework and iteratively trains the model. Specifically, starting from a model trained on bounding-box supervision only, which is also the baseline of the proposed method, we iteratively insert object parts into the framework and re-finetune the PS-CNN model. The number of parts inserted in each iteration increases exponentially, i.e., in the i th iteration, 2 i parts are selected and inserted. When starting from an initialized model with relatively high performance, introducing a new object part into the framework does not require to run a brand new classification procedure based on this specific part alone; ideally only the classification of highly confusing categories that may be distinguished through the new part will be impacted and amended. As a result, this procedure overcomes the drawback raised by the existence of object parts with lower discriminative power. In our implementation, the ordering of part inclusion is determined by its discriminative power measured by the classification accuracy using each part only (see Supplementary for details). <ref type="table">Table 2</ref> reveals that as the number of object parts increases from 0 to 8, the classification accuracy improves gradually and then becomes saturated. Further increasing the part number does not lead to a better accuracy; however, it does provide more resources for performing explicit model interpretation. <ref type="table">Table 3</ref> shows the performance comparison between PS-CNN and existing fine-grained recognition methods. Since the CNN architecture has a large impact on the recognition performance, for fair comparison, we only compare results reported on the standard seven-layer architecture. Deeper models could surely lead to better accuracy, but also result in less efficiency. The complete PS-CNN model with a bounding-box and 15 object parts achieves 76% accuracy, which is comparable with part-based R-CNN <ref type="bibr" target="#b50">[51]</ref>, while being slightly lower than several most recent state-of-theart methods <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b34">35]</ref> due to the effectiveness-efficiency tradeoff. In particular, our model is over two orders of magnitude faster than <ref type="bibr" target="#b50">[51]</ref>, requiring only 0.05 seconds to perform end-to-end classification on a test image. This number is quite inspiring, especially considering the number of parts used in the proposed method. The efficiency makes it possible for the proposed method to be conducted in realtime, leading to potential applications in video domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Model interpretation</head><p>One of the most prominent features of the proposed Part-Stacked CNN (PS-CNN) method is that it can produce human-understandable interpretation manuals for finegrained recognition. Here we detail the algorithm we use to perform interpretation using the proposed method.</p><p>Different from <ref type="bibr" target="#b1">[2]</ref> who directly conducted one-on-one classification on object parts, the interpretation process of the proposed method is conducted in a relatively indirec-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Train Anno. Test Anno. Acc. Constellation <ref type="bibr" target="#b35">[36]</ref> n/a n/a 68.5 Attention <ref type="bibr" target="#b47">[48]</ref> n/a n/a 69.7 Bilinear-CNN <ref type="bibr" target="#b21">[22]</ref> n/a n/a 74.2 Weak FGVC <ref type="bibr" target="#b53">[54]</ref> n/a n/a 75.0 CNNaug <ref type="bibr" target="#b30">[31]</ref> BBox t way. Considering that using each object part by itself cannot lead to convincing classification results, we perform the analysis for interpretation on a combination of bounding box supervision and each single object part. The analysis is performed in two ways: a "one-versus-rest" comparison for denoting the most discriminative part to classify a subcategory from all other classes, and a "one-versus-one" comparison to find out the classification criteria of a subcategory with its most similar classes.</p><p>• The "one-versus-rest" manual for an object category k. For every part p, we compute the summation of prediction scores of the category's positive samples. The most discriminative part is then captured as the one with the largest accumulated score:</p><formula xml:id="formula_5">p * k = argmax p i,yi=k S (p) ip .<label>(4)</label></formula><p>• The "one-versus-one" manual obtained by computing as the part which results in the largest difference of prediction scores on two categories k and l. We first pick up the respective two rows in the score matrix S, and re-normalize it using the binary classification criterion as S ′ . Afterwards, the most discriminative part is given as:  <ref type="figure">Figure 5</ref>. Example of the prediction manual generated by the proposed approach. Given a test image, the system reports its predicted class label with some typical exemplar images. Part-based comparison criteria between the predicted class and its most similar classes are shown in the right part of the image. The number in brackets shows the confidence of classifying two categories by introducing a specific part. We present top three object parts for each pair of comparison. For each of the parts, three part-center-cropped patches are shown for the predicted class (upper rows) and the compared class (lower rows) respectively.</p><formula xml:id="formula_6">p * k→l = argmax p ( i,yi=k S ′(p) ip + j,yj =l S ′(p) jp )<label>(5</label></formula><p>The model interpretation routine is demonstrated in <ref type="figure">Figure 5</ref>. When a test image is presented, the proposed method first conducts object classification through the PS-CNN architecture. The predicted category is presented by a set of images in the dataset that are closest to the test image according to conv5 1 outputs. Except for classification results, the proposed method also presents classification criteria for distinguishing the predicted category from its most similar neighbor classes based on object parts. Again we use the output of conv5 1 layer but after performing part cropping to retrieve nearest neighbor part patches of the input test image. The procedure described above provides an intuitive visual guide for distinguishing fine-grained categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we proposed a novel model for fine-grained recognition called Part-Stacked CNN. The model exploited detailed part-level supervision, in which object parts were first located by a fully convolutional network, following by a two-stream classification network that explicitly captured object-level and part-level information. Experiments on the CUB-200-2011 dataset revealed the effectiveness and efficiency of PS-CNN, especially the impact of introducing object parts on fine-grained visual categorization tasks. Meanwhile, we have presented human-understandable interpretations of the proposed method, which can be used as a visual field guide for studying fine-grained categorization.</p><p>We have discussed the application of the proposed Part-Stacked CNN on fine-grained visual categorization with strong supervision. In fact, PS-CNN can be easily generalized for varied applications. Examples include: 1) Discarding the requirement of strong supervision. Instead of introducing manually-labeled part annotations for generating human-understandable visual guides, one can also exploit unsupervised part discover methods <ref type="bibr" target="#b17">[18]</ref> to define object parts automatically, which requires far less human labeling effort.</p><p>2) Attribute learning. The application scenario of PS-CNN is not restricted to FGVC. For instance, performance of online shopping <ref type="bibr" target="#b24">[25]</ref> could definitely benefit from clothing attribute analysis from local parts provided by PS-CNN.</p><p>3) Context-based CNN. The role of local "parts" in PS-CNN is interchangeable with global contexts, especially for objects that are small in size and have no obvious object parts, such as volleyballs or tennis balls.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Network architecture of the proposed Part-Stacked CNN model. The model consists of: 1) a fully convolutional network for part landmark localization; 2) a part stream where multiple parts share the same feature extraction procedure, while being separated by a novel part crop layer given detected part locations; 3) an object stream with lower spatial-resolution input images to capture bounding-box level supervision; and 4) three fully connected layers to achieve the final classification results based on a concatenated feature map containing information from all parts and the bounding box.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>Typical localization results on CUB-200-2011 test set. We show 6 of the 15 detected parts here. They are: beak (red), belly (green), crown (blue), right eye (yellow), right leg (magenta), tail (cyan). Better viewed in color. s dataset (CUB-200-2011) [44]. The dataset contains 200 bird categories with roughly 30 training images per category. In the training phase we adopt strong supervision available in the dataset, i.e. we employ 2D key point part annotations of altogether M = 15 object parts together with image-level labels and object bounding boxes. The proposed Part-Stacked CNN architecture is implemented using the open-source package Caffe</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 1 .</head><label>1</label><figDesc>APK for each object part in the CUB-200-2011 test set in descending order.</figDesc><table>part 

throat 
beak 
crown 
forehead right eye 
nape 
left eye 
back 
APK 0.908 0.894 
0.894 
0.885 
0.861 
0.857 
0.850 
0.807 
part 
breast belly right leg 
tail 
left leg 
right wing left wing overall 
APK 0.799 0.794 
0.775 
0.760 
0.750 
0.678 
0.670 
0.866 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head></head><label></label><figDesc>object parts. Specifically, the additional 1×</figDesc><table>BBox only +2 part +4 part +8 part +15 part 
69.08 
73.72 
74.84 
76.63 
76.41 

Table 2. The effect of increasing the number of object parts on the 
classification accuracy. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head></head><label></label><figDesc>Table 3. Comparison with state-of-the-art methods on the CUB-200-2011 dataset. To conduct fair comparisons, for all the methods using deep features, we report their results on the standard seven-layer architecture (mostly ALexNet except VGG-m for<ref type="bibr" target="#b21">[22]</ref>) if possible. Note that our method achieves comparable results with state-of-the-art while running in real-time.</figDesc><table>BBox 
61.8 
Alignment [13] 
BBox 
BBox 
67.0 
No parts [18] 
BBox 
BBox 
74.9 
Bilinear-CNN [22] 
BBox 
BBox 
80.4 
Part R-CNN [51] 
BBox+Parts 
n/a 
73.9 
PoseNorm CNN [6] 
BBox+Parts 
n/a 
75.7 
POOF [2] 
BBox+Parts 
BBox 
56.8 
DPD+DeCAF[11] 
BBox+Parts 
BBox 
65.0 
Deep LAC [21] 
BBox+Parts 
BBox 
80.2 
Multi-proposal [35] 
BBox+Parts 
BBox 
80.3 
Part R-CNN [51] 
BBox+Parts 
BBox 
76.4 
PS-CNN (this paper) BBox+Parts 
BBox 
76.6 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">For reference, a single CaffeNet runs at 50 frames/sec under the same experimental setting.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>The work is partially supported by Australian Research Council Projects DP-140102164, FT-130101457, and LE-140100061, the High Technology Research and Development Program of China 2015AA015801, NSFC 61221001, STCSM 12DZ2272600, and the 111 Project B07022.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Image segmentation for large-scale subcategory flower recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Angelova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Applications of Computer Vision (WACV), 2013 IEEE Workshop on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="39" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Poof: Part-based one-vs.-one features for fine-grained categorization, face verification, and attribute estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Belhumeur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">How do you tell a blackbird from a crow?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">N</forename><surname>Belhumeur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV), 2013 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="9" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Birdsnap: Large-scale fine-grained visual categorization of birds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">N</forename><surname>Belhumeur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2019" to="2026" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Kernel descriptors for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="244" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Bird species categorization using pose normalized deep convolutional nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Van Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.2952</idno>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The ignorant led by the blind: A hybrid humanmachine vision system for fine-grained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Van Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="3" to="29" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Visual recognition with humans in the loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Babenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2010</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="438" to="451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Symbiotic segmentation and part localization for fine-grained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="321" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Fine-grained crowdsourcing for fine-grained recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2013 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Decaf: A deep convolutional activation feature for generic visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1310.1531</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained partbased models. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1627" to="1645" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Fine-grained categorization by alignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Smeulders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV 2013</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1713" to="1720" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Local alignments for fine-grained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Smeulders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="191" to="212" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Actions and attributes from wholes and parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2470" to="2478" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Conference on Multimedia</title>
		<meeting>the ACM International Conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Novel dataset for fine-grained image categorization: Stanford dogs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jayadevaprakash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR Workshop on Fine-Grained Visual Categorization (FGVC)</title>
		<meeting>CVPR Workshop on Fine-Grained Visual Categorization (FGVC)</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Fine-grained recognition without part annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="5546" to="5555" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Leafsnap: A computer vision system for automatic plant species identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">N</forename><surname>Belhumeur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Biswas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Kress</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">C</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">V</forename><surname>Soares</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2012</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="502" to="516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep lac: Deep localization, alignment and classification for fine-grained recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1666" to="1674" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Bilinear cnn models for fine-grained visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roychowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1449" to="1457" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Do convnets learn correspondence?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1601" to="1609" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Where to buy it: Matching street clothing photos in online shops</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Hadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xufeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Svetlana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tamara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV), 2015 IEEE International Conference on</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Fine-grained visual classification of aircraft</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rahtu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blaschko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1306.5151</idno>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Part and attribute discovery from relative annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shakhnarovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="82" to="96" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Multidigit recognition using a space displacement neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Matan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Burges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Le</forename><surname>Cun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Automated flower classification over a large number of classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-E</forename><surname>Nilsback</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision, Graphics &amp; Image Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="722" to="729" />
		</imprint>
	</monogr>
	<note>ICVGIP&apos;08. Sixth Indian Conference on</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Cats and dogs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3498" to="3505" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Cnn features off-the-shelf: an astounding baseline for recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Razavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Azizpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Carlsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="806" to="813" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rosch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">B</forename><surname>Mervis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">D</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Boyes-Braem</surname></persName>
		</author>
		<title level="m">Basic objects in natural categories. Cognitive psychology</title>
		<imprint>
			<date type="published" when="1976" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="382" to="439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Fisher vectors for fine-grained visual categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FGVC Workshop in IEEE Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Overfeat: Integrated recognition, localization and detection using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6229</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Part localization using multi-proposal consensus for fine-grained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mallya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.06332</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Neural activation constellations: Unsupervised part model discovery with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rodner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1143" to="1151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Fine-grained categorization for 3d scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Pepik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="1543" to="1552" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno>arX- iv:1409.4842</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Joint training of a convolutional network and a graphical model for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1799" to="1807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Building a bird recognition app and large scale dataset with citizen scientists: The fine print in fine-grained dataset collection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Van Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Farrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Haber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Barry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ipeirotis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="595" to="604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Understanding objects in detail with finegrained attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mahendran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tsogkas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rahtu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">B</forename><surname>Blaschko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3622" to="3629" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Multiclass recognition and part localization with humans in the loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2524" to="2531" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">The caltech-ucsd birds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Similarity comparisons for interactive finegrained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Van Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="859" to="866" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Multiple granularity descriptors for fine-grained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2399" to="2406" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Caltech-ucsd birds 200</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">The application of two-level attention models in deep convolutional neural network for fine-grained image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="842" to="850" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Augmenting strong supervision using web data for fine-grained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV), 2015 IEEE International Conference on</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Articulated human detection with flexible mixtures of parts. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2878" to="2890" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Partbased r-cnns for fine-grained category detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2014</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2003" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Panda: Pose aligned networks for deep attribute modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1637" to="1644" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Fused one-vs-all mid-level features for fine-grained visual categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Conference on Multimedia</title>
		<meeting>the ACM International Conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="287" to="296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Weakly supervised fine-grained image categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V.-A</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">N</forename><surname>Do</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.04943</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Deepm: A deep partbased model for object detection and semantic part localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07131</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
