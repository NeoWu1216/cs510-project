<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DeepCAMP: Deep Convolutional Action &amp; Attribute Mid-Level Patterns</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Diba</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><forename type="middle">Mohammad</forename><surname>Pazandeh</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamed</forename><surname>Pirsiavash</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Maryland Baltimore</orgName>
								<address>
									<addrLine>County 4 CVL</addrLine>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">ETH Zurich</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><forename type="middle">Van</forename><surname>Gool</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esat-Psi</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">U</forename><surname>Leuven</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sut</surname></persName>
						</author>
						<title level="a" type="main">DeepCAMP: Deep Convolutional Action &amp; Attribute Mid-Level Patterns</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:39+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The recognition of human actions and the determination of human attributes are two tasks that call for fine-grained classification. Indeed, often rather small and inconspicuous objects and features have to be detected to tell their classes apart. In order to deal with this challenge, we propose a novel convolutional neural network that mines mid-level image patches that are sufficiently dedicated to resolve the corresponding subtleties. In particular, we train a newly designed CNN (DeepPattern) that learns discriminative patch groups. There are two innovative aspects to this. On the one hand we pay attention to contextual information in an original fashion. On the other hand, we let an iteration of feature learning and patch clustering purify the set of dedicated patches that we use. We validate our method for action classification on two challenging datasets: PASCAL VOC 2012 Action and Stanford 40 Actions, and for attribute recognition we use the Berkeley Attributes of People dataset. Our discriminative mid-level mining CNN obtains state-of-theart results on these datasets, without a need for annotations about parts and poses.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Mimicking the human capability to understand the actions and attributes of people is very challenging. Lately, deep neural networks have strongly increased the capacity of computers to recognize objects, yet the analysis of human actions and attributes is lagging behind in terms of performance. These are a kind of fine-grained classification problems, where on the one hand possibly small patches that correspond to crucial appearance features of objects interacted with as well as, on the other hand, the global context of the surrounding scene contain crucial cues. The paper presents a newly designed CNN to extract such information by identifying informative image patches.</p><p>The idea of focusing on patches or parts definitely is not * A. Diba and A.M.Pazandeh contributed equally to this work new in computer vision, also not when it comes to human actions or attributes <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b34">35]</ref>. <ref type="bibr" target="#b27">[28]</ref> show that a good solution to human action classification can be achieved without trying to obtain a perfect pose estimation and without using body part detectors. Indeed, an alternative is to capture discriminative image patches. Mining such patches for the cases of actions and attributes is the very topic of this paper. After deriving some initial discriminative patch clusters for each category of action or attribute, our deep pattern CNN puts them into an iterative process that further optimizes the discriminative power of these clusters. <ref type="figure" target="#fig_1">Fig. 2</ref> sketches our CNN and will be explained further in the upcoming sec-tions. At the end of the training, the CNN has become an expert in detecting those image patches that distinguish human actions and attributes. The CNN comes with the features extracted from those patches. Our experiments show that we obtain better performance for action and attribute recognition than top scoring, patch-based alternatives for object and scene classification <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b31">32]</ref>. The latter do not seem to generalize well to the action and attribute case because these tasks need more fine-grained mid-level visual elements to make discrimination between similar classes.</p><p>The rest of the paper is organized as follows. Related work is discussed in section 2. Section 3 describes our framework and new CNN for the mining and detection of discriminative patches for human action and attribute classification. Section 4 evaluates our method and compares the results with the state-of-the-art. Section 5 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>This section first discusses action and attribute recognition in the pre-CNN era. It then continues with a short description of the impact that CNNs have had in the action and attribute recognition domain. Finally, we focus on the mid-level features that this paper shows to further improve performance.</p><p>Action and Attribute Recognition. Action and attribute recognition has been approached using generic image classification methods <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b18">19]</ref>, but with visual features extracted from human bounding boxes. Context cues are based on the objects and scene visible in the image, e.g. the mutual context model <ref type="bibr" target="#b33">[34]</ref>. The necessary annotation of objects and human parts is substantial. Discriminative part based methods like DPM <ref type="bibr" target="#b9">[10]</ref> have been state-of-theart for quite a while. Inspired by their performance, human poselet methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref> try to capture ensembles of body and object parts in actions and attributes. Maji et al. <ref type="bibr" target="#b22">[23]</ref> trained dedicated poselets for each action category. In the domain of attributes the work by Parikh et al. <ref type="bibr" target="#b24">[25]</ref> has become popular. It ranks attributes by learning a function to do so. Berg et al. <ref type="bibr" target="#b1">[2]</ref> proposed automatic attribute pattern discovery by mining unlabeled text and image data sampled from the web. Thus, also before the advent of CNNs some successes had been scored.</p><p>CNN powered Approaches. Convolutional neural networks (CNN) have since defined the state-of-the-art for many tasks, including image classification and object detection <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b10">11]</ref>. Many researchers proposed new CNN architectures or innovative methods on top of a CNN. Girshick et al. <ref type="bibr" target="#b11">[12]</ref> proposed a novel state-of-the-art ob-ject detection scheme (RCNN) by extracting CNN features from object region proposals. Gkioxari et al. <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b13">14]</ref> used a scheme similar to RCNN for action classification and detection, and for pose estimation. Zhang et al. <ref type="bibr" target="#b36">[37]</ref> used HOGposelets to train a part-based CNN model for attribute classification. They achieved a nice gain over previous work. There also is recent work that trains models based on parts and poses <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b12">13]</ref>. Zhang et al. <ref type="bibr" target="#b35">[36]</ref> obtained a good performance with a part-based RCNN for bird species classification. The part-based RCNN can discriminate birds by learned models of their parts, by fine-tuning a CNN trained on ImageNet. <ref type="bibr" target="#b4">[5]</ref> trained a deep CNN with prepared HOG poselets as training data and detected humans based on the resulting deep poselets. Recently Gkioxari et al. <ref type="bibr" target="#b12">[13]</ref> proposed to train human body part detectors, e.g. for the head and torso, based on CNN pool5 feature sliding window search and combined them with the whole body box to train a CNN jointly. They showed that for the task of action and attribute classification, performance can be improved by adding such deep body part detectors to the holistic CNN. This work therefore suggests that adding dedicated patch analysis is beneficial.</p><p>Discriminative Mid-level feature learning Mid-level visual learning aims at capturing information at a level of complexity higher than that of typical visual words. Mining visual elements in a large number of images is difficult since one needs to find similar discriminate patterns over a very large number of patches. The fine-grained nature of our action and attribute tasks further complicates this search. <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b26">27]</ref> describe methods for extracting clusters of mid-level discriminative patches. Doersch et al. <ref type="bibr" target="#b7">[8]</ref> proposed such a scheme for scene classification, through an extension of the mean-shift algorithm to evaluate discriminative patch densities. Naderi et al. <ref type="bibr" target="#b25">[26]</ref> introduce a method to learn part-based models for scene classification which a joint training alternates between training part weights and updating parts filter. One of the state-of-the-art contributions in mid-level element mining is <ref type="bibr" target="#b21">[22]</ref>, which applies pattern mining to deep CNN patches. We have been inspired by the demonstration that the mining improves results.</p><p>To the best of our knowledge, the use of CNN mid-level elements for action and attribute classification, as is the case in this paper, is novel. Moreover, given the fine-grained nature of these challenges, we propose a new method to get more discriminative mid-level elements. The result is a performance better than that of competing methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>In this section, we go through all our new framework for finding discriminative patch clusters and also our convolutional neural network for precise describing of patches. In the first part of this section we talk about the motivation and  give an overview of solution. Second part describes our proposed pipeline of mid-level patch mining and its contained blocks. Third part of the section introduces our proposed deep convolutional network for patch learning and the idea behind it. And in the final part we summarize that how we use mid-level visual elements in actions and attributes classspecific classifiers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Approach overview</head><p>We address an approach using mid-level deep visual patterns for actions and attributes classification which are fine-grained classification tasks. Applying discriminative patches or mid-level pattern mining state-of-the-arts like <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b21">22]</ref> to these tasks can not perform very promising as much as in the more generic classification tasks like scenes or object recognition (as we show in the experiments Sec 4). The pattern mining algorithm <ref type="bibr" target="#b21">[22]</ref> maps all data points to an embedding space to performs the association rule based clustering. For the embedding space, it fine-tunes AlexNet <ref type="bibr" target="#b17">[18]</ref> for action or attribute recognition and uses its fc7 layer to extract deep feature embedding. Our main insight in this paper is that a better embedding can improve the quality of clustering algorithm. We design an iterative algorithm where in each iteration, we improve the embedding by training a new CNN to classify cluster labels obtained in the previous iteration. In addition, we believe that aggregating the information and context from whole human body with specific action or attribute label with patches can improve the clusters of mid-level elements. Hence, we modify the architecture of AlexNet to concatenate features from both patch and the whole human bounding box in an intermediate layer <ref type="figure">(Fig.3)</ref>. We show that learning the embedding using this new architecture outperforms the original AlexNet fine-tuned using patch images alone. Moreover, in each it-eration, we purify the clusters by removing the patches that are scored poorly in the clustering. Subsequently, to classify actions and attributes by discriminative patches, we use a similar representation in <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b0">1]</ref> which more details about it come in Sec 3.4. In the next part, we reveal more about the components of our pipeline. Finally, we show that the newly learned clusters produce better representations that outperform state-of-the-art when used in human action and attribute recognition. Our contributions are two-fold: (1) designing an iterative algorithm contains an expert patch CNN to improve the embedding, <ref type="bibr" target="#b1">(2)</ref> proposing new patch CNN architecture training to use context in clustering the patches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Pipeline Detailes</head><p>As shown both in <ref type="figure" target="#fig_1">Fig.2</ref> and Algorithm.1, our iterative algorithm consists of four blocks which are described in more details in this section.</p><p>Initial feature extraction and clustering The first block clusters image patches discriminatively using Mid-Level Deep Pattern Mining (MDPM) algorithm <ref type="bibr" target="#b21">[22]</ref>. Given, a set of training images annotated with humans' actions and their bounding boxes, it extracts a set of patches from the person bounding box and learns clusters that can discriminate between actions. The MPDM method, building on the wellknown association rule mining which is a popular algorithm in data mining, proposes a pattern mining algorithm, to solve mid-level visual element discovery. This approach in MDPM makes it an interesting method because the specific properties of activation extracted from the fully-connected layer of a CNN allow them to be seamlessly integrated with association rule mining, which enables the discovery of category-specific patterns from a large number of image patches. This method proves that the association rule mining can easily fulfill two requirements of mid-level visual elements, representativeness and discriminativeness. After defining association rule patterns, MDPM creates many mid-level elements cluster based on shared patterns in each category and then applying their re-clustering and merging algorithm to have discriminative patch cluster. We use the MDPM block to have initial mid-level elements clusters to move further on our method.</p><p>Training patch clusters CNN Our main insight is that the representation of image patches plays an important role in clustering. Assuming that the initial clustering is reasonable, in this block, we train a new CNN to improve the representation. The new CNN is trained so that given patch images, it predicts their cluster label. This is in contrast to the initial CNN that was learned to classify bounding box images to different action categories. We believe learning this fine-grained classification using discriminative patch cluster CNN results in a better representation for clustering. Updating clusters Now that a representation is learned by a newly trained CNN, we can update the clusters again using MDPM to get a better set of clusters that match the new representation. Since populating mid-level clusters in MDPM is time consuming, we freeze the first level of clustering and update the clusters by repeating re-clustering and merging using the new representations. This results in better clusters. Finally, we train new set of LDA classifiers to detect the clusters. The modification to MDPM to do reclustering is described in Section 4.1.</p><p>Harvesting patches In order to improve the purity of clusters, we clean the clusters by removing patches that do not fit well in any cluster. We do this by thresholding the confidence value that LDA classifiers produce for each cluster assignment. Finally, we pass the new patches with associate cluster labels to learn a new CNN based representation. In the experiments, do cross validation, and stop the iterations when the performance on the validation set stops improving.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Mid-level Deep Patterns Network</head><p>In updating the representation, we train a CNN to predict the cluster labels for given image patches. This is a challenging task for the network since clusters are defined to be action or attribute specific, so they can discriminate between actions. However, the patch image may not have enough information to discriminate between actions. Hence, to increase the discrimination power of the representation, we modify the network architecture to add the human bounding box image as extra contextual information in the network input <ref type="figure">(Fig.3)</ref>. Following AlexNet architecture, we pass both patch image and the whole bounding box image to the network and concatenate the activations in conv5 layer to form a larger convolutional layer. To train our mid-level deep patterns CNN, we try fast RCNN <ref type="bibr" target="#b10">[11]</ref>. In training process of fast RCNN for patch learning, we push two regions: patch and the croped image of person. An adaptive max pooling layer takes the output of the last convolutional layer and a list of regions as input. We concatenate the ROI-pooled conv5 features from two regions and then pass this new conv5(concatenated) through the fully connected layers to make the final prediction. Using fast RCNN helps us to have an efficient, fast and computationally low cost CNN layers calculations, since convolutions are applied at an image-level and are afterward reused by the ROI-specific operations. Our network is using a pretrained CNN model on ImageNet-1K with Alex-Net <ref type="bibr" target="#b17">[18]</ref> architecture, to perform fine-tuning and learn patch network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Action and attribute classifiers</head><p>After learning the mid-level pattern clusters, we use them to classify actions and attributes. Given an image, we extract all patches and find the best scoring one for each cluster. To construct the image representation for action or attribute on each image, we use the idea of mid-level elements for object detection <ref type="bibr" target="#b0">[1]</ref>, by taking the max score of all patches per mid-level pattern detectors per region encoded in a 2-level (1 * 1 and 2 * 2) spatial pyramid. This feature vector represents occurrence confidence of elements in the image. This results in a rich feature for action and attribute classification since the clusters are learned discriminatively for this task. Finally, we pass the whole bounding box through overall CNN trained on action or attribute la-</p><formula xml:id="formula_0">… Conv1 Conv4 Patch Conv5 Fc6 Fc7 Softmax Action1_Cluster1 Action1_Cluster2 . . Action2_Cluster1 Action2_Cluster2 .</formula><p>. .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="256">512</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Human Conv5</head><p>256 <ref type="figure">Figure 3</ref>. Overview of proposed Mid-level Deep Pattern Network. To train this CNN for mid-level discriminative patches, we concatenate the conv5 layers of patch and the person regions to abstract the visual distinctive information of the patch with holistic clue of the person who is performing an action or has a specific attribute.</p><p>bels and append it's fc7 activations to obtained feature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We evaluate our algorithm on two tasks: action classification and attribute classification in still images. In both tasks, we are folowing the stantdard PASVAL VOC <ref type="bibr" target="#b8">[9]</ref> setting that the human bounding box is given in the inference time. The first section of our evaluations are on the PAS-CAL VOC <ref type="bibr" target="#b8">[9]</ref> and Stanford 40 <ref type="bibr" target="#b34">[35]</ref> action datasets, and the second part is on the Berkeley attributes of people dataset <ref type="bibr" target="#b3">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Setup</head><p>Common properties of the networks All of the networks have been trained using the caffe CNN training package <ref type="bibr" target="#b16">[17]</ref> with back-propagation. We use weights of the trained Network on ImageNet dataset <ref type="bibr" target="#b6">[7]</ref> as initial weights and fine-tune our networks on specific datasets and with different properties according to the task. We set the learning rate of CNN training to 0.0001, the batch size to 100.</p><p>Initial feature extraction network The fine-tuning of the network is done on the cropped images of each person as input and the Action or Attribute label of images as output of the network. Then we use fc7 feature vector of body image or extracted patches as input of the MDPM (Midlevel Deep Pattern Mining) <ref type="bibr" target="#b21">[22]</ref> block.</p><p>Mid-level deep patterns network Input images of this network are patches that extracted from cropped body image in 3 different scales (128*128, 160*160, 192*192 patches from a resized image with stride of 16). The output layer of this network is cluster labels that computed by MDPM block.</p><p>Mid-level deep pattern mining block. We use MPDM block with the mentioned properties in <ref type="bibr" target="#b21">[22]</ref> for the initial feature extraction and clustering block. While updating clusters in our iterative patch clusters training, we use a part of MPDM algorithm which tries to merge and reconfigure clusters. The new obtained CNN representations for patches help updating clusters to be performed more precise. We apply MDPM patch mining with 50 cluster per each category.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Action Classification.</head><p>For the action Classification task, we evaluate our midlevel pattern mining pipeline and proposed patch CNN network performances on PASCAL VOC and Stanford 40 action datasets.</p><p>Dataset. The PASCAL VOC action dataset <ref type="bibr" target="#b8">[9]</ref> includes 10 different action classes including Jumping, Phoning, Playing Instrument, Reading, Riding Bike, Riding Horse, Running, Taking Photo, Using Computer, Walking, and an Other class consists of images of persons, which has no action label. The dataset has 3 splits of training, validation and test set.</p><p>The Stanford 40 action dataset <ref type="bibr" target="#b34">[35]</ref> contains total of 9532 images and 40 classes of actions, split into train set containing 4000, and test set containing 5532 instances. Implementation detailes. The training and fine-tuning of the initial CNN and pattern CNN, have been done only on PASCAL VOC dataset. It means to evaluate on Stan-ford40, we just use convolutional networks of action and patch clusters, which are trained on PASCAL and afterward run the MDPM cluster mining and configure clusters for Stanford40.</p><p>In the test time we will evaluate the results on both PAS-CAL VOC and Stanford 40 datasets. The reason of train-  ing patch CNN networks on a dataset with less classes than the test dataset is to evaluate discrimination power of our proposed method's extracted patches. In the results section we show that our method achieves state-of-the-art on the both of PASCAL VOC and Stanford 40 dataset, which consequently with results on Stanford40, the discrimination power of extracted patches has been proved.</p><p>Results. We report the result of our baseline, and proposed method on the PASCAL VOC validation set in Table 1. The baseline 'CNN' in the first row of table is AlexNet trained on PASCAL VOC dataset using SVM on the fc7 layer features. The second row which is the output of our initial feature extraction and clustering block, named 'CNN+MDPM' reports the result of SVM training on the concatenated feature vector of convolutional network fc7 and the 2500 dimensional feature vector output of MDPM block (50 cluster * 10 category * 5 spatial pyramid region). The next three rows of the table with names of 'Ours AlexNet iter1-3' show the result of performing the pipeline using the convolutional neural network architecture of AlexNet and with 1 to 3 iterations. Finally the last three rows are same as previous ones with 1 to 3 iterations applying our proposed pattern CNN architecture. We can conclude from the table that our proposed iterative pipeline and newly proposed CNN architecture can improve the result independently, so the combined method outperform either one alone.</p><p>The results of our final proposed method in comparison with results of the following methods, Poselets <ref type="bibr" target="#b22">[23]</ref> , Oquab et al <ref type="bibr" target="#b23">[24]</ref> ,Hoai <ref type="bibr" target="#b15">[16]</ref> , and Gkioxari et al <ref type="bibr" target="#b12">[13]</ref> on the test set of PASCAL VOC has been shown in <ref type="table" target="#tab_2">Table 2</ref>. As we can see in the table the mean accuracy of our proposed method with the proposed PatternNet outperforms all the previous 8 layer CNN network based methods. The important point in this improvement in result is that most of the mentioned methods were using part detectors based on the part and pose annotations of the datasets which limits the number of annotated training data because of the hardness of pose annotating. In contrast the proposed method does not use AP(%) is male has long hair has glasses has hat has t-shirt has long sleeves has shorts has jeans has long pants mAP CNN att 88.6 82.  any annotation more than action labels and bounding box of person. As we mentioned in the implementation details we evaluate the Stanford40 actions dataset using our final pipeline mid-level patterns 'CNN -PatternNet iter3' which is trained on PASCAL VOC, and report the results in <ref type="table">Table  3</ref>. The result shows that our method improved the results of the previous methods in action classification on this human action dataset. Some of the visual results on PASCAL images are shown in <ref type="figure" target="#fig_3">Fig.4.</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Attribute Classification</head><p>In this section we report implementation details and results of our method on the Berkeley Attributes of people dataset. We need to train all the networks on the new dataset.</p><p>Dataset. The Berkeley attributes of people dataset contains 4013 training and 4022 test examples of people, and 9 Attributes classes, is male, has long hair, has glasses, has hat, has t-shirt, has long sleeves, has shorts, has jeans, has long pants. Each of the classes labeled with 1, -1 or 0, as present, absent and un-specified labels of the attribute. Implementation details. In contrast to action classification task in attributes classification, more than one label can be true for each instance, it means that classes in attribute classification do not oppose each other. Therefore instead of using the softmax function as the loss function in the last layer of the initial convolutional network, which forces the network to have only a true class for each instance, we use cross entropy function for the task of attributes classification.</p><p>The other block with the same assumption in opposition of classes is MDPM block which try to find some cluster for each class such that instances of other classes labels as negative to maximize the discrimination of clusters. In the other hand, attribute classes do not oppose each other, so a modification is needed in the MDPM block. We extract discriminative clusters of each class separately, using the positive and negative labels of that class.</p><p>Results. We evaluate our method on the Berkeley attributes of people dataset and compare our results on the test set with Gkioxari et al <ref type="bibr" target="#b12">[13]</ref> and PANDA <ref type="bibr" target="#b36">[37]</ref> methods in <ref type="table">Table 4</ref>. As we show in the table, our baselines, 'CNN att ' and 'CNN att +MDPM' did not improve the results of previous methods. Even our proposed pipeline with the AlexNet architecture couldn't outperform <ref type="bibr" target="#b12">[13]</ref> which uses trained deep body parts detectors. However, our proposed pipeline with the proposed PatternNet architecture improves the result of attribute classification in comparison with all previous methods. <ref type="table">Table 4</ref> shows that although our method have significant improvements in action classification, the method does not have the same margin with the state-ofthe-arts in classifying attributes. We believe this is due to the importance of part annotations in training attribute classifier, which is not available in our setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work, we have addressed human action and attribute classification using mid-level discriminative visual elements. We proposed a novel framework to learn such elements using a Deep Convolutional Neural Network which also has a new architecture. The algorithm explores a huge number of candidate patches, covering human body parts as well as scene context. We validated our method on the PASCAL VOC 2012 action, the Stanford40 actions, and the Berkeley Attributes of People datasets. The results are good, both qualitatively and quantitatively, reaching the state-of-the-art, but without using any human pose or part annotations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Mid-level visual elements: discriminative descriptors of human actions and attributes. Our method discovers visual elements which make discrimination between human body parts or attributes or interacted objects . (a) shows the scores and classification results in the action classification task (b) shows discrimination scores of elements in the attributes classification task by color and shows final results of classification.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Full Pipeline of the proposed method for training mid-level deep visual elements in action and attribute. All the modules are explained in Sec. 3. The first box, which is the baseline of our work, initially cluster patches. The second box propose the introduced iterative process, and contains 3 main blocks. The final block takes trained classifiers and patch features of the second box after convergence, and classify images based on their action or attribute.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Input: Image set (I , L) Extract dense patche: P i j (jth patch of ith image) Extract initial features F i j and initial cluster labels C i j while Convergence do CN N P atch = Train CNN(P,C) F ← Extract CNN Feature(CN N P atch ,P) C ← Update Cluster(F,C) W = Train Patch Classifier(F,C) S= Compute Score(W,F) for all patches do if S i j &lt; th then Eliminate P i j end end end Output: Mid-Level Pattern Clusters (C) Algorithm 1: Iterative mid-level deep pattern learning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Explored deep mid-level visual patterns of different categories of actions and samples detected from top scored pattern and aggregated scores over all image from PASCAL VOC 2012 action dataset. In each block of figure, small patches are representatives from most discriminative patches.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 2 .</head><label>2</label><figDesc>Average Precision on the PASCAL VOC dataset test set and comparison with previous methods. The first two rows are our baselines which reported on the test set, the next rows of the above part are previous methods based on 8 layer convolutional network, same as ours. The ours Alex and ours PatternNet are the results of testing our proposed pipeline with Alex-Net and our Pattern-Net architectures, on the test set of PASCAL VOC, until the convergence of iteration (3 iterations).</figDesc><table>Method 
AP(%) 
Object bank [21] 
32.5 
LLC [31] 
35.2 
SPM [19] 
34.9 
EPM [28] 
40.7 
CNN AlexNet 
46 
CNN+MDPM 
46.8 
Ours AlexNet 
49 
Ours PatternNet 
52.6 

Table 3. Average Precision on the Stanford40 action dataset. The 
used initial CNN and patch CNNs in this section are trained on 
the PASCAL VOC dataset, and we use these networks to extract 
patches form Images of Stanford40 dataset. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head></head><label></label><figDesc>Table 4. Average Precision on the Berkeley Attributes dataset and comparison with previous methods. The CNNatt and CNNatt+MDPM are the baselines of the work, which their convolutional networks trained on train set of Berkeley attributes dataset. The results of PANDA method with 5 layer network and 8 layer network results of Gkioxari et al is reported in last rows of above part. The bottom of the table shows the results of our proposed pipeline using both Alex-Net and Our Patch-Net until the convergence of the iteration process (3 iterations).</figDesc><table>2 
50.1 
83.2 
60.1 
86.2 
88.3 
88.6 
98.2 
80.6 
CNN att +MDPM 
88.8 
84.2 
54.1 
83.4 
64.3 
86.4 
88.5 
88.8 
98.3 
81.9 
PANDA [37] 
91.7 
82.7 
70 
74.2 
49.8 
86 
79.1 
81 
96.4 
79 
Gkioxari et al [13] 
91.7 
86.3 
72.5 
89.9 
69 
90.1 
88.5 
88.3 
98.1 
86 
Ours AlexNet 
90.8 
84.2 
61.4 
88.9 
67.1 
88.1 
89.2 
89.3 
98.3 
84.1 
Ours-PatternNet 
91.8 
88.4 
71.1 
88.9 
70.7 
91.8 
88.7 
89.3 
98.9 
86.6 

jumping 
playing instrument 
reading 

riding bike 
riding horse 

running 

taking photo 
using computer 
walking 

phoning 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was supported by DBOF PhD scholarship, KU Leuven CAMETRON project.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Mid-level elements for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.07284</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Automatic attribute discovery and characterization from noisy web data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2010</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="663" to="676" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Detecting people using mutually consistent poselet activations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2010</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="168" to="181" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Describing people: A poselet-based approach to attribute classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV), 2011 IEEE International Conference on</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1407.0717</idno>
		<title level="m">Deep poselets for human detection</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Recognizing human actions in still images: a study of bag-of-features and partbased representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Delaitre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC 2010-21st British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Mid-level visual element discovery as discriminative mode seeking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The pascal visual object classes challenge: A retrospective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="98" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained partbased models. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1627" to="1645" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Actions and attributes from wholes and parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Contextual action recognition with r*cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Rcnns for pose estimation and action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.5212</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Regularized max pooling for image categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hoai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference</title>
		<meeting>the British Machine Vision Conference</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Conference on Multimedia</title>
		<meeting>the ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Gradientbased learning applied to document recognition. Proceedings of the IEEE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Object bank: A high-level image representation for scene classification &amp; semantic feature sparsification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1378" to="1386" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Mid-level deep pattern mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR&apos;15)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Action recognition from a distributed representation of pose and appearance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="3177" to="3184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning and transferring mid-level image representations using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1717" to="1724" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Relative attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV), 2011 IEEE International Conference on</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="503" to="510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Automatic discovery and optimization of parts for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">N</forename><surname>Parizi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Felzenszwalb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Dataset fingerprints: Exploring image collections through data mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rematas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Dellaert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4867" to="4875" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Expanded parts model for human attribute and action recognition in still images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jurie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2013 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Unsupervised discovery of mid-level discriminative patches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2012</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="73" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning discriminative part detectors for image classification and cosegmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV), 2013 IEEE International Conference on</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3400" to="3407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Locality-constrained linear coding for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="3360" to="3367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Max-margin multiple-instance dictionary learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 30th International Conference on Machine Learning</title>
		<meeting>The 30th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="846" to="854" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Grouplet: A structured image representation for recognizing human and object interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="9" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Modeling mutual context of object and human pose in human-object interaction activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="17" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Human action recognition by learning bases of action attributes and parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV), 2011 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="1331" to="1338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Partbased r-cnns for fine-grained category detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2014</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="834" to="849" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Panda: Pose aligned networks for deep attribute modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
			</analytic>
	<monogr>
		<title level="m">IEEE Conference on</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1637" to="1644" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
