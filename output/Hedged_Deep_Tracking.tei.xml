<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Hedged Deep Tracking</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuankai</forename><surname>Qi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Harbin Institute of Technology ♭ Institute of Computing Technology</orgName>
								<orgName type="institution" key="instit2">Chinese Academy of Sciences</orgName>
								<orgName type="institution" key="instit3">University of Chinese Academy of Sciences</orgName>
								<orgName type="institution" key="instit4">Hanyang University</orgName>
								<orgName type="institution" key="instit5">University of California at Merced</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengping</forename><surname>Zhang</surname></persName>
							<email>s.zhang@hit.edu.cnqinlei@ict.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Harbin Institute of Technology ♭ Institute of Computing Technology</orgName>
								<orgName type="institution" key="instit2">Chinese Academy of Sciences</orgName>
								<orgName type="institution" key="instit3">University of Chinese Academy of Sciences</orgName>
								<orgName type="institution" key="instit4">Hanyang University</orgName>
								<orgName type="institution" key="instit5">University of California at Merced</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Qin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Harbin Institute of Technology ♭ Institute of Computing Technology</orgName>
								<orgName type="institution" key="instit2">Chinese Academy of Sciences</orgName>
								<orgName type="institution" key="instit3">University of Chinese Academy of Sciences</orgName>
								<orgName type="institution" key="instit4">Hanyang University</orgName>
								<orgName type="institution" key="instit5">University of California at Merced</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongxun</forename><surname>Yao</surname></persName>
							<email>h.yao@hit.edu.cnqmhuang@jdl.ac.cnjlim@hanyang.ac.krmhyang@ucmerced.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Harbin Institute of Technology ♭ Institute of Computing Technology</orgName>
								<orgName type="institution" key="instit2">Chinese Academy of Sciences</orgName>
								<orgName type="institution" key="instit3">University of Chinese Academy of Sciences</orgName>
								<orgName type="institution" key="instit4">Hanyang University</orgName>
								<orgName type="institution" key="instit5">University of California at Merced</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Harbin Institute of Technology ♭ Institute of Computing Technology</orgName>
								<orgName type="institution" key="instit2">Chinese Academy of Sciences</orgName>
								<orgName type="institution" key="instit3">University of Chinese Academy of Sciences</orgName>
								<orgName type="institution" key="instit4">Hanyang University</orgName>
								<orgName type="institution" key="instit5">University of California at Merced</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongwoo</forename><surname>Lim</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Harbin Institute of Technology ♭ Institute of Computing Technology</orgName>
								<orgName type="institution" key="instit2">Chinese Academy of Sciences</orgName>
								<orgName type="institution" key="instit3">University of Chinese Academy of Sciences</orgName>
								<orgName type="institution" key="instit4">Hanyang University</orgName>
								<orgName type="institution" key="instit5">University of California at Merced</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Harbin Institute of Technology ♭ Institute of Computing Technology</orgName>
								<orgName type="institution" key="instit2">Chinese Academy of Sciences</orgName>
								<orgName type="institution" key="instit3">University of Chinese Academy of Sciences</orgName>
								<orgName type="institution" key="instit4">Hanyang University</orgName>
								<orgName type="institution" key="instit5">University of California at Merced</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Hedged Deep Tracking</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:41+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In recent years, several methods have been developed to utilize hierarchical features learned from a deep convolutional neural network (CNN) for visual tracking. However, as features from a certain CNN layer characterize an object of interest from only one aspect or one level, the performance of such trackers trained with features from one layer (usually the second to last layer) can be further improved. In this paper, we propose a novel CNN based tracking framework, which takes full advantage of features from different CNN layers and uses an adaptive Hedge method to hedge several CNN based trackers into a single stronger one. Extensive experiments on a benchmark dataset of 100 challenging image sequences demonstrate the effectiveness of the proposed algorithm compared to several state-of-theart trackers.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Visual tracking has become a topic of increasing interest over the past couple of decades due to its importance in numerous applications, such as intelligent video surveillance, vehicle navigation, and human-computer interaction. Despite significant efforts put into developing algorithms <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b34">35]</ref> and benchmark evaluations <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b26">27]</ref> for visual tracking, it is still a challenging task due to complicated interfering factors like heavy illumination changes, shape deformation, partial and full occlusion, large scale variations, in-plane and out-of-plane rotations, and fast motion, to name a few.</p><p>Most existing tracking approaches focus on either designing effective decision models <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b39">40]</ref> or extracting robust features <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b0">1]</ref>. Recently, inspired by the success of deep convolutional neural networks (CNNs) in object recognition and detection <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b10">11]</ref>, several CNN based trackers <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b24">25]</ref> have been developed. Empirical studies using a large object tracking benchmark show that the performance of CNN based trackers surpasses <ref type="bibr">Figure 1</ref>. Tracking results of using CNN features from different convolutional layers on a representative frame of four sequences with diverse challenges. The best tracking results are obtained using layers 12, 16, 10, and 10 on four sequences, respectively. that of hand-crafted features such as HOG <ref type="bibr" target="#b5">[6]</ref>, SIFT <ref type="bibr" target="#b23">[24]</ref>, and color histogram <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b0">1]</ref>.</p><p>Despite achieving state-of-the-art performance, existing CNN based trackers still have some limitations. Most of these methods represent target objects only using features from very last layers (e.g., fully-connected layers) of C-NNs, which capture rich category-level semantic information, and therefore are useful for object classification. However, features from last layers are not optimal for visual tracking as they do not capture spatial details of the tracked target. These details, captured by first layers, are crucial to visual tracking, as they allow for accurate localization of targets, as shown in the last two rows of <ref type="figure">Figure 1</ref>. On the other hand, as features from first layers are more generic than discriminative as ones from last layers, methods based on features from first layers are likely to fail in challenging scenarios, as shown in the first two rows of <ref type="figure">Figure 1</ref>. To achieve better tracking performances, it is imperative to combine features from different layers to best represent and separate foreground objects from the background clutters.</p><p>In this paper, we propose a novel CNN based tracking algorithm, which first builds weak trackers from convolutional layers by applying correlation filters on the layer output, and then hedges all weak trackers into a single stronger one using an online decision-theoretical Hedge algorithm. Specifically, we treat each weak tracker as an expert and compute weights for all experts as their decision confidences. The tracking result in the current frame is the weighted decisions of all experts, which combines advantages of all the considered CNN layers. Since the tracked target moves a small offset between consecutive frames and undergoes appearance variance gradually, an expert that performs well in previous frames has a higher probability to perform well in the current frame. By factoring in historical performance of experts to make decisions, we propose an improved Hedge algorithm to update the weights of all experts, which is more suitable for real-world tracking tasks.</p><p>The contributions of this paper are summarized below:</p><p>• We propose a novel tracking algorithm that combines weak CNN based trackers from various convolutional layers into a single stronger tracker. • We develop an improved Hedge algorithm for visual tracking by considering historical performance of weak trackers. • We carry out extensive experiments on a large-scale benchmark dataset <ref type="bibr" target="#b33">[34]</ref> with 100 challenging sequences to demonstrate the effectiveness of the proposed algorithm in comparisons to the state-of-the-art trackers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>We give a brief review of tracking methods closely related to this work. Comprehensive reviews on visual tracking approaches can be found in <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b26">27]</ref>.</p><p>Correlation filters based trackers. Correlation filters are introduced into visual tracking for its computational efficiency <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17]</ref>. These methods approximate the dense sampling scheme by generating a circulant matrix, of which each row denotes a vectorized sample. As such, its regression model can be computed in the Fourier domain, which brings a large speed improvement in both training and testing stages. Bolme et al. <ref type="bibr" target="#b3">[4]</ref> develop the Minimum Output Sum of Squared Error (MOSSE) method to learn the filters, and use intensity features for object representation. In <ref type="bibr" target="#b15">[16]</ref>, Henriques et al. propose a tracking method based on correlation filters by introducing kernel methods and employing ridge regression. Subsequently a method that extends the input features from a single channel to multiple channels (e.g., HOG) is presented <ref type="bibr" target="#b16">[17]</ref>. <ref type="bibr">Danelljan et al. [7]</ref> propose an algorithm that searches over scale space for correlation filters to handle large variation in object size. However, all the above mentioned works use only one correlation filter, which limits the power of trackers based on correlation filters. In this work, we exploit the computational efficiency of correlation filters to construct an ensemble tracker where each component tracker is based on features extracted from one convolutional layer of a CNN.</p><p>CNN based trackers. Hierarchical features learned from CNNs have been shown to be effective for numerous vision tasks, e.g., classification and recognition <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b14">15]</ref> in recent years. Numerous methods have since been proposed to exploit CNN features <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b17">18]</ref> for visual tracking. In <ref type="bibr" target="#b8">[9]</ref>, Fan et al. utilize a pre-trained deep network for human tracking. Wang and Yeung <ref type="bibr" target="#b29">[30]</ref> design an autoencoder network to learn representative features for generic objects. Hong et al. <ref type="bibr" target="#b17">[18]</ref> construct a discriminative model with features from the first fully-connected layer of R-CNN <ref type="bibr" target="#b10">[11]</ref> and a generative model with saliency map for visual tracking. While this method is effective for visual tracking, its computational complexity is high. We note that the aforementioned methods do not exploit features from different layers adequately. As shown in <ref type="figure">Figure 1</ref>, features from different layers are effective in different scenarios. Based on these observations, we use an ensemble of multiple CNN based trackers where each one is trained with features from one convolutional layer. We regard each one as a weak expert and hedge them adaptively for visual tracking.</p><p>Ensemble trackers. Ensemble approaches have been developed to combine multiple component trackers for visual tracking. Several ensemble tracking methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b2">3]</ref> have been proposed using hand-crafted features. For example, ensemble methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b2">3]</ref> under the boosting framework <ref type="bibr" target="#b9">[10]</ref> incrementally train each component weak tracker to classify the training samples that previous trackers misclassified. In <ref type="bibr" target="#b30">[31]</ref>, Wang and Yeung use a conditional particle filter to infer the target position and the reliability of each component tracker. Different from these works, we consider visual tracking as a decision-theoretic online learning task <ref type="bibr" target="#b4">[5]</ref> that infers the tracked target using decisions from multiple expert trackers. That is, in every round each expert makes a decision and the final decision is determined by the weighted decisions of all experts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Algorithmic Overview</head><p>As shown in <ref type="figure" target="#fig_0">Figure 2</ref>, the proposed approach consists of three steps: extracting CNN features, constructing weak trackers, and hedging weak trackers. The pre-trained VGG-Net <ref type="bibr" target="#b25">[26]</ref> is used to extract feature maps of convolutional layers from image regions, which represent the tracked target at different resolutions and semantic levels. Each feature map is then convolved by correlation filters to generate response maps, from which a weak tracker is constructed with </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Proposed Algorithm</head><p>In this section, we first present the technical details of the proposed algorithm and then describe the online update scheme.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Deep CNN features</head><p>CNN models, such as AlexNet <ref type="bibr" target="#b20">[21]</ref>, R-CNN <ref type="bibr" target="#b10">[11]</ref>, CaffeNet <ref type="bibr" target="#b18">[19]</ref>, and VGG-Net <ref type="bibr" target="#b25">[26]</ref>, have been developed for large-scale image classification and object recognition tasks. The proposed method is based on the VGG-Net, as it has a much deeper architecture (up to 19 weight layers) and hence can provide much richer features compared to most CNNs which usually have 5 or 7 layers. The VGG-Net is trained with 1.3 million images of the ImageNet dataset and achieves the state-of-the-art results on classification challenges <ref type="bibr" target="#b25">[26]</ref>.</p><p>Different from classification tasks which only require the extracted features to capture more category-level semantic information, visual tracking also requires the extracted features to have precise localization ability since a small drift from the tracked target to its surrounding background causes gradual degradation in tracking performance and eventual failure. The deep VGG-Net facilitates features extracted from different layers to describe target objects with greater details. However, tracking methods using CNN features from any layer alone are less effective (see <ref type="figure">Figure 1</ref> for example of tracking failures).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Weak CNN based trackers</head><p>In this work, a module that makes use of correlation filters on CNN features extracted from one layer is used to build a weak tracker. Trackers based on correlation filters <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b15">16]</ref> exploit the circulant structure of training and testing samples to greatly accelerate the training and testing processes with negligible precision loss. Let X k ∈ R P ×Q×D denote the feature map extracted from the k-th convolutional layer and Y ∈ R P ×Q be the gaussian shape label matrix, which is subject to a 2D Gaussian distribution with zero mean and standard deviation proportional to the target size. Let</p><formula xml:id="formula_0">X k = F(X k ), Y = F(Y ), where F(·) denotes the discrete Fourier transformation (DFT).</formula><p>The k-th filter can be modeled in the Fourier domain by</p><formula xml:id="formula_1">W k = arg min W Y − X k · W 2 F + λ W 2 F ,<label>(1)</label></formula><p>where</p><formula xml:id="formula_2">X k · W = D d=1 X k * , * ,d ⊙ W * , * ,d ,<label>(2)</label></formula><p>and the symbol ⊙ denotes the element-wise product. The optimization problem in (1) has a simple closed form solution, which can be efficiently computed in the Fourier domain by</p><formula xml:id="formula_3">W k * , * ,d = Y X k · X k + λ ⊙ X k * , * ,d .<label>(3)</label></formula><p>Given the testing data T k from the output of the k-th layer, we first transform it to the Fourier domain T k = F(T k ), and then the responses can be computed by</p><formula xml:id="formula_4">S k = F −1 (T k · W k ),<label>(4)</label></formula><p>where F −1 denotes the inverse of DFT. The k-th weak tracker outputs the target position with the largest response</p><formula xml:id="formula_5">(x k , y k ) = arg max x ′ ,y ′ S k (x ′ , y ′ ).<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Hedging CNN based trackers</head><p>The standard parameter-free Hedge algorithm <ref type="bibr" target="#b4">[5]</ref> is proposed to tackle decision-theoretic online learning problems in a multi-expert multi-round setting. Given the initial confidence weights of all experts, in the current round, a final decision is made based on the weighted decisions of all experts. The weights of all the experts are then updated to reflect each expert's decision loss. In the visual tracking scenario, it is natural to treat each CNN based tracker as an expert and then predict the target position in the t-th frame by</p><formula xml:id="formula_6">(x * t , y * t ) = K k=1 w k t · (x k t , y k t ),<label>(6)</label></formula><p>where w k t is the weight of expert k and K k=1 w k t = 1. Once the ultimate target position is predicted, each expert will incur a loss.</p><p>The loss of expert k at frame t is computed as</p><formula xml:id="formula_7">ℓ k t = max(S k t ) − S k t (x * t , y * t ),<label>(7)</label></formula><p>where max(·) operates on a matrix and returns the largest element of the matrix and S(x, y) denotes the element at position (x, y) of matrix S. The standard parameter-free Hedge algorithm generates a new weight distribution on all experts by introducing a regret measure defined by</p><formula xml:id="formula_8">r k t =l k t − ℓ k t ,<label>(8)</label></formula><p>where the weighted average loss among all experts is computed asl k t = K k=1 w k t ℓ k t . By minimizing the cumulative regret</p><formula xml:id="formula_9">R k t = t τ =1 r k τ ,<label>(9)</label></formula><p>to any expert k, for any round of t, the new weights w 1 t+1 , · · · , w K t+1 are generated. Although the standard parameter-free Hedge algorithm performs well in the simulated one-dimension tracking experiment, where the target stays stationary or moves in a constant velocity <ref type="bibr" target="#b4">[5]</ref>, it is less effective for the real-world tracking tasks since it does not consider two crucial factors: (i) The target appearance usually changes at irregular pace (sometimes gradually and sometimes rapidly). This means that the proportion of the historic regret R k t−1 should vary with time t to better reflect the current state for visual tracking. (ii) Since each expert captures a different aspect of the target, it is not effective to fix the ratio of the cumulative regret for all the experts. To address these issues, we propose an adaptive Hedge algorithm, which considers the difference of historic regrets over time t and expert k simultaneously.</p><p>As the object appearance usually does not change significantly at least in a short time period, we model the loss of each expert ℓ k during the time period ∆t via a Gaussian distribution with mean µ k t and standard variance σ k</p><formula xml:id="formula_10">t µ k t = 1 ∆t t τ =t−∆t+1 ℓ k τ ,<label>(10)</label></formula><formula xml:id="formula_11">σ k t = 1 ∆t − 1 t τ =t−∆t+1 (ℓ k τ − µ k t ) 2 .<label>(11)</label></formula><p>We then measure the stability of expert k at time t using</p><formula xml:id="formula_12">s k t = |ℓ k t − µ k t | σ k t .<label>(12)</label></formula><p>A smaller s k t indicates that this expert tends to be more stable than the one with a larger s k t . Therefore, we prefer a larger proportion on its current regret. In contrast, a larger s k t means this expert varies greatly, and therefore we compute its cumulative regret mainly depending on its historic information. Based on this principle, we obtain the following adaptive cumulative regret</p><formula xml:id="formula_13">R k t = (1 − α k t )R k t−1 + α k t r k t ,<label>(13)</label></formula><formula xml:id="formula_14">α k t = min (g, exp (−γs k t )),<label>(14)</label></formula><p>where γ is a scale factor and g defines a maximum ratio on the current regret to avoid that no historic information is considered. We validate the effectiveness of the proposed adaptive Hedge compared to the original one in Section 5.4. Since our adaptive Hedge algorithm adheres to the framework of the standard one, the solution to minimize the cumulative regret (13) has the same form,</p><formula xml:id="formula_15">w k t+1 ∝ [R k t ] + c t exp ([R k t ] + ) 2 2c t ,<label>(15)</label></formula><p>where [R k t ] + denotes max {0, R k t }, and c t servers as a scale parameter like in <ref type="bibr" target="#b4">[5]</ref>, which is determined by solving</p><formula xml:id="formula_16">1 K K k=1 exp( ([R k t ] + ) 2 2ct ) = e.</formula><p>Algorithm 1: Hedged deep tracking 1 Input: initial weights w 1 1 , · · · , w K 1 ; target position (x1, y1) in the 1st frame; VGG-Net19; R k 1 = 0, ℓ k 1 = 0; 2 Crop interested image region; 3 Initiate K weak experts using (3); 4 for t = 2, 3, · · · do <ref type="bibr" target="#b4">5</ref> Exploit the VGG-Net19 to obtain K representations; <ref type="bibr" target="#b5">6</ref> Compute correlation filter responses using (4); <ref type="bibr" target="#b6">7</ref> Find target position predicted by each expert using <ref type="formula" target="#formula_5">(5)</ref> Compute experts' losses using <ref type="formula" target="#formula_7">(7)</ref>; <ref type="bibr" target="#b13">14</ref> Update stability models using <ref type="bibr" target="#b9">(10)</ref> and <ref type="formula" target="#formula_1">(11)</ref>; <ref type="bibr" target="#b14">15</ref> Measure each expert's stability using (12); <ref type="bibr" target="#b15">16</ref> Compute adaptive proportion of historic regret for each expert using <ref type="formula" target="#formula_1">(14)</ref>; <ref type="bibr" target="#b16">17</ref> Update cumulative regret of each expert using (13); <ref type="bibr" target="#b17">18</ref> Update weights for each expert using <ref type="bibr" target="#b14">(15)</ref> and normalize them to have a sum of 1;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="19">end</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Model update</head><p>Since the feature maps of VGG-Net have up to 512 channels, retraining the ridge regression models with the newly collected samples is impractical, especially when the amount of the training data becomes extremely large over time. In practice, we adopt an incremental update similar to that in <ref type="bibr" target="#b6">[7]</ref>, which only uses new samplesX k in the current frame to partially update the previous models,</p><formula xml:id="formula_17">Z k * , * ,d = Ȳ X k ·X k + λ ⊙X k * , * ,d ,<label>(16)</label></formula><formula xml:id="formula_18">W k t = (1 − η)W k t−1 + ηZ k t .<label>(17)</label></formula><p>Algorithm 1 summarizes the main steps of the proposed tracking method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experimental Results</head><p>In this section, we present extensive experimental evaluations on the proposed hedged deep tracker (HDT). We first discuss the implementation details and the evaluation protocol. We then present two sets of experimental evaluations: one compared to several state-of-the-art trackers and the other one to several baseline trackers including component weak trackers and the hedged strong tracker using the standard parameter-free Hedge method <ref type="bibr" target="#b4">[5]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Implementation details</head><p>For feature extraction, we crop an image patch with 2.2 times the size of the target bounding box and then resize it  to 224×224 pixels for the VGG-Net with 19 layers (16 convolutional layers and 3 fully-connected layers). After the forward propagation, we use the outputs from six convolutional layers (10th∼12th, 14th∼16th) as six types of feature maps and all feature maps are resized to the same size. This setting simultaneously takes the feature diversities and the computational cost into consideration. Since VGG-Net adopts very small convolutional filters (3×3 pixel size), the feature maps from first layers (i.e., less than 10) have limited representation power (see Section 5.4). We implement our algorithm in MATLAB, and utilize the MatConvNet toolbox <ref type="bibr" target="#b28">[29]</ref> in this work. Our implementation runs at 10 frames per second on a computer with an Intel I7-4790K <ref type="figure">Figure 5</ref>. Attribute-based evaluation on 100 sequences. We also put the overall performance here (the last one) for comparison convenience facing a single challenge and their combination. <ref type="bibr" target="#b3">4</ref>.00 GHz CPU, 16GB RAM, and a GeForce GTX780Ti G-PU card which is only used to compute the CNN features. We make MATLAB code available to the public (http:// faculty.ucmerced.edu/mhyang/pubs.html).</p><p>All the following experiments are carried out using the following fixed parameters: the tradeoff parameter in (1) is set to λ = 10 −4 ; the time window in (10) is set to ∆t = 5; the truncate threshold in <ref type="formula" target="#formula_1">(14)</ref> is set to g = 0.97; the learning rate in <ref type="formula" target="#formula_1">(17)</ref> is set to η = 0.01; and the initial weights of the six weak experts are empirically set to (1, 0.2, 0.2, 0.02, 0.03, 0.01).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Evaluation protocol</head><p>To fully assess our method, we use one-pass evaluation (OPE), temporal robustness evaluation (TRE), and spatial robustness evaluation (SRE) metrics on a large object tracking benchmark dataset <ref type="bibr" target="#b33">[34]</ref> which contains 100 image sequences. These sequences involve 11 tracking challenges, such as illumination changes, camera shake, scale variation, pose variation, partial or full occlusion, and rotation, to name a few. Experimental results are reported using overlap success plots and center location error plots. The compared trackers are ranked in terms of area under the curve and distance precision at a threshold of 20 pixels, respectively. For completeness, we also report the results on the benchmark <ref type="bibr" target="#b32">[33]</ref>, which is a subset of <ref type="bibr" target="#b33">[34]</ref>. More results and videos are presented in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Comparisons to state-of-the-art trackers</head><p>We compare our algorithm to 8 recent state-of-theart trackers: DLT <ref type="bibr" target="#b29">[30]</ref>, CNN-SVM <ref type="bibr" target="#b17">[18]</ref>, KCF <ref type="bibr" target="#b16">[17]</ref>, MEEM <ref type="bibr" target="#b35">[36]</ref>, Struck <ref type="bibr" target="#b13">[14]</ref>, CXT <ref type="bibr" target="#b7">[8]</ref>, TLD <ref type="bibr" target="#b19">[20]</ref>, and SCM <ref type="bibr" target="#b40">[41]</ref>. DLT and CNN-SVM are based on deep learning; KCF is one of the best correlation filters based trackers; and the remaining trackers rank top 5 on the benchmark <ref type="bibr" target="#b33">[34]</ref>.</p><p>Quantitative evaluation. <ref type="figure" target="#fig_2">Figure 3</ref> shows the OPE, TRE, and SRE results on 100 image sequences. It should be noted that the results of CNN-SVM are not included for fair comparisons as the source code is not available. We also provide a comparison on 50 sequences in <ref type="figure" target="#fig_3">Figure 4</ref> with OPE results for CNN-SVM taken from <ref type="bibr" target="#b17">[18]</ref>. <ref type="figure" target="#fig_2">Figure 3</ref> and <ref type="figure" target="#fig_3">Figure 4</ref> show that our HDT performs favorably against the state-of-the-art methods on all the three evaluation metrics. We note that HDT performs better in terms of tracking precision (than success rate), which indicates that HDT is able  to track target objects well but gives a less accurate bounding box since, for computational efficiency, HDT does not search over scales to determine the best one.</p><p>Attribute-based evaluation. To thoroughly evaluate the robustness of the proposed HDT in various scenes, we present tracking performance in terms of each tracking challenge on 100 image sequences in <ref type="figure">Figure 5</ref>. As illustrated in <ref type="figure">Figure 5</ref>, our algorithm performs well against other meth-ods in almost all tracking challenges. In particular, HDT outperforms other methods by a huge margin in handling low resolution, which can be attributed to CNN features with rich spatial details from first layers and features with semantics from last layers. In contrast, DLT only takes advantage of last layers' features, and hence its performance is suffered. We also observed that HDT does not perform well in handling out-of-view challenge, as HDT does not search for the target in a whole frame in order to reduce the computational load. Therefore, HDT may lose the target, even if it reappears somewhere else.</p><p>Qualitative evaluation. We present several tracking results from the evaluated methods in <ref type="figure" target="#fig_4">Figure 6</ref>. For presentation clarity, only results from the top six performing methods are shown. Overall, our tracker is able to localize the targets more precisely. However, almost all the other methods are unable to handle these complicated scenarios. The MEEM tracker performs well in presence of illumination variations, occlusion, and in-plane-rotation (shaking, coke, and trellis), as MEEM simultaneously maintains several target snapshots from different times. However, it tends to fail when similar objects appear, such as in sequence bolt2 or football, since the features are not discriminative enough. When the background is cluttered, as in sequences diving or ironman, most of the compared methods are apt to lose the target. Although DLT adopts a deep autoencoder network, it usually fails on these challenging sequences. This is because its deep network has no shared weights and it is trained with small amount of data. Since our HDT hedges several weak CNN based trackers that perform well in different environments, it can overcome these challenges much better than other trackers. In addition, we note that in the diving, human9, and trellis sequences, even though HDT tracks targets accurately, some of its bounding boxes are not tight to the target since it does not search for the best scale as previously discussed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Comparisons to baseline trackers</head><p>To evaluate the effectiveness of the proposed adaptive Hedge method, we compare the HDT against its component CNN based trackers denoted by VGG-10, VGG-11, VGG-12, VGG-14, VGG-15, and VGG-16, as well as the hedged CNN based tracker using the standard parameter-free Hedge <ref type="bibr" target="#b4">[5]</ref>, denoted by HDT-SH, on the benchmark <ref type="bibr" target="#b32">[33]</ref>. <ref type="figure" target="#fig_6">Figure 8</ref> shows the tracking results. When features from each convolutional layer are used solely for tracking, the performance generally increases as the depth of the layer is increased. But even the best component CNN based tracker VGG-16 still does not perform as good as CNN-SVM. This is because CNN-SVM takes advantages of both a R-CNN feature based discriminative model (features of f c 6 being used) and a back-project saliency map based generative model. Note that the performance of VGG-10 is far behind that of MEEM which is based on hand-crafted features. This explains why we train weak trackers only using convolutional features from the layers after the 10th layer. When combining these six component CNN based trackers using the standard Hedge, the tracking performance is below the best performed component tracker. In contrast, the proposed HDT achieves the best results, which demonstrates the effectiveness of the adaptive Hedge method.</p><p>To further explore the difference between the proposed adaptive and the standard parameter-free Hedge methods, we present a comparison of them on a typical frame at running time in <ref type="figure" target="#fig_5">Figure 7</ref>. <ref type="figure" target="#fig_5">Figure 7</ref> shows that the proposed adaptive Hedge allocates more desirable weights to weak C-NN based trackers than the standard one. The reason mainly lies in the computation of the accumulative regret R. The standard Hedge uses a fixed proportion of historical information R t−1 for all weak trackers at any time t. In contrast, we adaptively compute the proportion of historical information R t−1 , i.e., we introduce a dynamic parameter α in <ref type="bibr" target="#b12">(13)</ref> and model the α with a Gaussian distribution in a time window ∆t. As demonstrated in <ref type="figure" target="#fig_6">Figure 8</ref>, the hedged tracker using the adaptive scheme performs better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we propose a novel CNN based tracking framework which uses an adaptive online decision learning algorithm to hedge weak trackers, obtained by correlation filters on CNN feature maps, into a stronger one to achieve better results. To the best of our knowledge, the proposed algorithm is the first to adaptively hedge features from different CNN layers in an online manner for visual tracking. Extensive experimental evaluations on a large-scale benchmark dataset demonstrate the effectiveness of the proposed hedged deep tracking algorithm.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Main steps of the proposed algorithm. The proposed algorithm consists of three components: 1) extracting CNN features from different convolutional layers using the pre-trained VGG-Net (Section 4.1); 2) constructing weak trackers using correlation filters where each one is trained with CNN features from one layer (Section 4.2); 3) hedging weak trackers into a stronger one using an improved Hedge algorithm (Section 4.3). moderate performance. All weak trackers are finally hedged into a stronger one using the proposed adaptive Hedge algorithm for visual tracking, which exploits the strength of all CNN layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Evaluation results on 100 sequences.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Evaluation results on 50 sequences.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>Bounding box comparison on several challenging image sequences (from left to right and top to down are bolt2, coke, diving, dragonBaby, football, human2, human9, ironman, shaking, and trellis, respectively).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .</head><label>7</label><figDesc>Tracking results on the 12-th frame of the skiing sequence. We illustrating how the weights are assigned to the CNN based trackers by the proposed adaptive and the standard parameter-free Hedge methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 .</head><label>8</label><figDesc>Comparison among our HDT and several baselines: all its constituent CNN based trackers and the one combined by standard parameter-free Hedge. For completeness, we also include two state-of-the-art methods, CNN-SVM and MEEM, in the plots.</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Acknowledgments</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Robust fragmentsbased tracking using the integral histogram</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rivlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Shimshoni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Ensemble tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Avidan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="261" to="271" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Randomized ensemble tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Betke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Monnier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Visual object tracking using adaptive correlation filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Bolme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Beveridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Draper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">M</forename><surname>Lui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A parameter-free hedging algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Freund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Accurate scale estimation for robust visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Häger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Context tracker: Exploring supporters and distracters in unconstrained environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">B</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">G</forename><surname>Medioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Human tracking using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TNN</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1610" to="1623" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A desicion-theoretic generalization of on-line learning and an application to boosting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Freund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational learning theory</title>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Real-time tracking via on-line boosting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Grabner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Grabner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Semi-supervised on-line boosting for robust tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Grabner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Leistner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Struck: Structured output tracking with kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saffari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Exploiting the circulant structure of tracking-by-detection with kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caseiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Batista</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Highspeed tracking with kernelized correlation filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caseiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Batista</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>TPAMI</publisher>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Online tracking by learning discriminative saliency map with convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM Multimedia</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">P-N learning: Bootstrapping binary classifiers by structural constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Treat samples differently: Object tracking with semi-supervised online covboost</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A survey of appearance models in visual object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Dick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM TIST</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">58</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Object recognition from local scale-invariant features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Hierarchical convolutional features for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition. CoRR, abs/1409.1556</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Visual tracking: An experimental survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W M</forename><surname>Smeulders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cucchiara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Calderara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dehghan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1442" to="1468" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Improved object tracking algorithm based on new HSV color probability model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISNN</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Matconvnet: Convolutional neural networks for MATLAB</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lenc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning a deep compact image representation for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Ensemble-based tracking: Aggregating crowdsourced structured time Series data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Jots: Joint online tracking and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Online object tracking: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<title level="m">Object tracking benchmark. TPAMI</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Bounding multiple gaussians uncertaninty with application to object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Perina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Murino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">MEEM: robust tracking via multiple experts using entropy minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Online dictionary learning on symmetric positive definite manifolds with vision applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kasiviswanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Yuen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Harandi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Sparse coding based visual tracking: Review and experimental comparison. Pattern Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="1772" to="1788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Robust visual tracking using structurally random projection and weighted least squares</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Techn</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1749" to="1760" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Robust visual tracking via multi-task sparse learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Robust object tracking via sparsity-based collaborative model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
