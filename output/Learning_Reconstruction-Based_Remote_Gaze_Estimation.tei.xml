<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Reconstruction-based Remote Gaze Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Northwestern University</orgName>
								<address>
									<addrLine>2145 Sheridan Road</addrLine>
									<postCode>60208</postCode>
									<settlement>Evanston</settlement>
									<region>IL</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahuan</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Northwestern University</orgName>
								<address>
									<addrLine>2145 Sheridan Road</addrLine>
									<postCode>60208</postCode>
									<settlement>Evanston</settlement>
									<region>IL</region>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Wu</surname></persName>
							<email>yingwu@eecs.northwestern.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Northwestern University</orgName>
								<address>
									<addrLine>2145 Sheridan Road</addrLine>
									<postCode>60208</postCode>
									<settlement>Evanston</settlement>
									<region>IL</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Reconstruction-based Remote Gaze Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:42+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>It is a challenging problem to accurately estimate gazes from low-resolution eye images that do not provide fine and detailed features for eyes. Existing methods attempt to establish the mapping between the visual appearance space to the gaze space. Different from the direct regression approach, the reconstruction-based approach represents appearance and gaze via local linear reconstruction in their own spaces. A common treatment is to use the same local reconstruction in the two spaces, i.e., the reconstruction weights in the appearance space are transferred to the gaze space for gaze reconstruction. However, this questionable treatment is taken for granted but has never been justified, leading to significant errors in gaze estimation. This paper is focused on the study of this fundamental issue. It shows that the distance metric in the appearance space needs to be adjusted, before the same reconstruction can be used. A novel method is proposed to learn the metric, such that the affinity structure of the appearance space under this new metric is as close as possible to the affinity structure of the gaze space under the normal Euclidean metric. Furthermore, the local affinity structure invariance is utilized to further regularize the solution to the reconstruction weights, so as to obtain a more robust and accurate solution. Effectiveness of the proposed method is validated and demonstrated through extensive experiments on different subjects.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Visual sense is the most informative one among all human perceptions. Gaze estimation, which infers visual attention, has wide applications in many areas, e.g., humancomputer interfaces can utilize estimated gaze as an auxiliary input to aid disabled people.</p><p>Eye gaze tracking techniques can be divided into two categories, intrusive and non-intrusive eye gaze trackers <ref type="bibr" target="#b11">[12]</ref>. Intrusive eye gaze trackers provide high accuracy and reliability, but require dedicated hardware. One example uses contact lens, and the gaze directions are estimated by mea-suring the change of voltage in the lens.</p><p>Non-intrusive eye gaze trackers are mostly vision-based. They utilize the information from remote cameras, thus are easier and suitable to use for a long period of time. These techniques can be feature-based or appearance-based. Feature-based methods track eye features such as iris contour, pupil location and etc. One popular method uses infrared light source to create glints on cornea.</p><p>Since the feature-based methods still need expensive devices, appearance-based methods, which only need one simple web camera to capture eye appearances, is simpler and more attractive in practice. Such methods attempt to construct the mapping between the visual appearance of the eyes and the gaze. In <ref type="bibr" target="#b16">[17]</ref>  <ref type="bibr" target="#b8">[9]</ref>, the same weights that reconstruct the visual appearance are used to reconstruct the gaze. These reconstruction-based methods are used as the base estimation when handling the head movements and simplifying the calibration process <ref type="bibr" target="#b7">[8]</ref> [10] <ref type="bibr" target="#b15">[16]</ref>, and they have shown promising performance and considerable simplicity.</p><p>However, there is one fundamental problem in such reconstruction-based methods that has not been well studied yet. The using of the same reconstruction weights in the two different spaces (i.e. the appearance space and the gaze space) is taken for granted, without justification or guarantee. In fact, they may not be the same, and can even be significantly different. Without addressing this issue, regardlessly using the same reconstruction weights leads to inaccurate results. This issue has not been investigated in the literature.</p><p>To address this issue, we propose a new solution that explicitly decomposes the gaze estimation error of the reconstruction-based methods into two terms. The first term is the matching error between the reconstructed appearance and the query appearance. The second one is the error incurred by using the same reconstruction weight in the appearance space and the gaze space, which has never been investigated before. We reduce the second error term via exploring the relationship between the reconstruction weights in the two spaces by analyzing their structures. Two novel methods, learning a global distance metric in training phase and minimizing the local space structure discrepancy in estimation phase, are proposed to align the two spaces, so as to reduce the second error term. The effectiveness of these two new methods is validated through their consistently and significantly superior performances in our experiments on different subjects. The average estimation error of our proposed method outperforms existing methods by at least 11%. The proposed method also shows resilience to pose and subject variation in cross pose and cross subject experiments.</p><p>Our proposed method has following significant differences from existing methods:</p><p>1. This is the first work to investigate the difference of the reconstruction weights in appearance space and gaze space, and the estimation error incurred by this difference.</p><p>2. Two novel methods are proposed to reduce this error term by aligning the two spaces. The first one is used in training phase as a global distance metric learning. The second one is conducted in the estimation phase by minimizing the discrepancy of local space structure between the query appearance and the estimated gaze.</p><p>This paper is organized as follows. Section 2 briefly describes the related works. Section 3 describes our proposed method. Section 4 assesses the proposed method, and the baseline methods, with different settings. Section 5 concludes this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Gaze estimation methods can be categorized from two perspectives. In term of the visual features used, there are feature-based methods that utilize specific eye features, and appearance-based methods that treat eye images as high dimensional features. From the perspective of computing, there are model-based methods that reconstruct the 3-D eye model from the input features, and regression methods that learn a mapping directly from the input feature to the gaze. Comprehensive surveys can be found in <ref type="bibr" target="#b6">[7]</ref>  <ref type="bibr" target="#b11">[12]</ref>.</p><p>Feature-based method: Feature based methods extract specific eye features such as Pupil Center Corneal Reflection (PCCR) <ref type="bibr">[</ref>  <ref type="bibr" target="#b1">[2]</ref>. Zhu and Ji obtained a head mapping function to compensate head movements <ref type="bibr" target="#b23">[24]</ref>. In <ref type="bibr" target="#b24">[25]</ref>, support vector regression (SVR) is utilized. In <ref type="bibr" target="#b22">[23]</ref>  <ref type="bibr" target="#b21">[22]</ref>, Yoo et al. and subsequently Yoo and Chung proposed to use five infra red light sources and cross ratio to allow head motion. In <ref type="bibr" target="#b2">[3]</ref>, Chen and Ji proposed to utilize saliency map to incrementally learn a distribution of the person dependent parameters and the gaze. A detailed study of the methods using PCCR can be found in <ref type="bibr" target="#b5">[6]</ref>. All of above methods have to use infra red camera to capture cornea glint. In <ref type="bibr" target="#b17">[18]</ref>, Wang et al. proposed to extract the iris circle from natural image. In <ref type="bibr" target="#b1">[2]</ref>, Chen and Ji used facial feature points. Although the feature-based methods may provide accurate estimation of gaze, they require infra red devices or high resolution imagery, which limits their adaptability to outdoor situation and/or low resolution camera.</p><p>Appearance-based method: Appearance-based methods simply use eye images as high dimensional input features, which relieves gaze estimation from relying on dedicated hardware. In <ref type="bibr" target="#b0">[1]</ref>  <ref type="bibr" target="#b20">[21]</ref>, neural network was applied to learn the regression. To reduce the number of training samples, Tan et al. proposed to estimate gaze using Locally Linear Reconstruction (LLR) <ref type="bibr" target="#b16">[17]</ref>  <ref type="bibr" target="#b12">[13]</ref>. This method assumes an appearance manifold, where the k-nearest neighbors (k-NN) of one appearance vector are used to form the Delaunay triangulation topology of the corresponding gaze positions. Such a reconstruction is assumed to be the same in both the appearance manifold and the gaze manifold. Lu et al. proposed Adaptive Linear Regression (ALR), leveraging the sparsity to choose supporting training samples <ref type="bibr" target="#b8">[9]</ref>. In <ref type="bibr" target="#b19">[20]</ref>, Williams et al. proposed a sparse semisupervised Gaussian process regression model. To handle the head movements, Sugano et al. proposed an incremental learning method <ref type="bibr" target="#b15">[16]</ref>. Lu et al. proposed a two stage solution, i.e., the initial gaze estimation under a fixed head pose and the compensation of the estimation bias caused by the head movement <ref type="bibr" target="#b7">[8]</ref>. Furthermore, Lu et al. proposed a synthesis approach to generate training images of unseen head poses <ref type="bibr" target="#b9">[10]</ref>. Depth camera was utilized by Funes and Odobez <ref type="bibr" target="#b10">[11]</ref>. To simplify the training procedure, Sugano et al. proposed to use saliency map <ref type="bibr" target="#b13">[14]</ref>. For the issue of cross subject estimation, Sugano et al. applied the learning-by-synthesis approach and the redundant regression forests <ref type="bibr" target="#b14">[15]</ref>.</p><p>The proposed method is appearance-based, but focuses on studying the relation and alignment between the appearance space and the gaze space. The unclear relation between the two spaces is a significant factor leading to the estimation error in the reconstruction-based methods, but this issue has not been addressed before.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The proposed approach</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Baseline methods</head><p>Appearance-based gaze estimation is to infer the gaze position from eye appearances through learning a mapping f : α → β, where α is the space of appearance features a ∈ R n , and β is the space of gaze position vectors y ∈ R d . A set of training pairs, or called exemplars, (a i , y i ), i = 1 . . . N is needed. Without losing generality, we represent a gaze y by a 2-D position on the target plane.</p><p>One solution is to learn a global regression function, e.g. Support Vector Regression (SVR), that constructs the direct mapping from a to y. However, such regression relationship between a and y can be very complicated, and very difficult to capture, especially when the number of training exemplars is small.</p><p>Unlike the direct regression methods that do not use the structure information of the appearance space α, an alternative approach takes advantage of the local structures. It reconstructs the query appearance feature based on the training appearance exemplars (e.g., using the k-nearest neighbors of the query appearance):</p><formula xml:id="formula_0">ω * α = arg min ω a − Aω 2 2 s.t. 1 T ω = 1 (1)</formula><p>where a is query appearance feature, and A = {a 1 , . . . , a k } is the set of the related exemplars in the training set. They can be simply the nearest neighbors <ref type="bibr" target="#b16">[17]</ref>, or those that give the most sparse reconstruction <ref type="bibr" target="#b8">[9]</ref>. Assuming that such reconstruction holds the same in both the appearance space and the gaze space, this approach uses the same weight ω * α to interpolate the gaze by the corresponding gaze exemplars:</p><formula xml:id="formula_1">y = Bω * α (2)</formula><p>where B = {y 1 , . . . , y k }, and each y i is the corresponding gaze of its appearance a i in A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Decomposing the estimation error</head><p>By minimizing the objective function in Equ. 1, those traditional methods seek a best reconstructed appearance in the appearance space to best match the query appearance a, i.e. minimizing the matching error D(a, Aω). In <ref type="bibr" target="#b8">[9]</ref> [17], D(a, Aω) = a−Aω 2 . This seems to be plausible due to the correspondences between appearance a and gaze position y, hoping the true gaze to be recovered via transferring the reconstruction. Unfortunately, such correspondences do not automatically imply or guarantee that the reconstruction weights are the same in the two spaces. Regardlessly using the same reconstruction of the query appearance to reconstruct the estimated gaze will introduce significant estimation error.</p><p>In fact, the reconstruction in the gaze space β should be given by:</p><formula xml:id="formula_2">ω * β = arg min ω y − Bω 2 2 s.t. 1 T ω = 1<label>(3)</label></formula><p>where y is the true gaze of query appearance a. Previous works all took for granted that ω * β = ω * α , which is in fact not true in practice. As shown in Equ. 1, weight ω * α depends on the distance between different appearance feature vectors D(a i , a j ) that exhibits the structure of the appearance space α. However, as illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>, the structure of the appearance space α and the gaze position space β may be very different. Because of this discrepancy between the structure of the two spaces, simply using ω * α to reconstruct the estimated gaze will inevitably introduce error in gaze estimation. Therefore, in the reconstruction-based methods, the estimation error comes from two sources, and we explicitly decompose it as:</p><formula xml:id="formula_3">E(ω) = E D (ω) + E R (ω)<label>(4)</label></formula><p>where E(ω) is the total estimation error using ω to reconstruct estimated gaze, E D (ω) is the matching error between the reconstructed appearance using ω and the query appearance, and E R (ω) indicates the error introduced by using the appearance reconstruction weight ω to reconstruct the gaze. In previous works of appearance based gaze estimation, only the appearance reconstruction error E D (ω) is considered. E R (ω) term has not been investigated before, but is critical to bridging the gap of ω between the appearance space α and the gaze space β.</p><p>In the following sections, we propose two novel methods to reduce E R (ω) from two different perspectives. The first one is to find a global alignment between the appearance space α and the gaze space β via learning a new distance metric in Sec. 3.3. The second one is locally regularizing the discrepancy of space structure related to query appearance a and estimated gaze Bω in Sec. 3.4. The first one is conducted in training phase, while the second one is conducted in estimation phase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Reducing E R by learning global distance metric</head><p>In <ref type="bibr" target="#b16">[17]</ref>  <ref type="bibr" target="#b8">[9]</ref>, only E D (ω) term is considered, and it is modeled as E D (ω) = a − Aω 2 , the Euclidean distance between reconstructed appearance and query appearance. However, as analyzed in Sec. 3.2, the space structure of the two spaces, if simply defined by Euclidean distance, will not be the same, resulting in different reconstruction weights in the two spaces.</p><p>In order to reduce the error E R (ω) caused by this discrepancy, instead of pursuing different weights ω for the two spaces, we propose to find an optimal distance metric D * for the appearance space α, such that the structure of α approximates the Euclidean structure of β, i.e., D * (a i , a j ) ∼ y i − y j 2 2 . We call it the global distance metric, since it is learned from the space structures defined by all training appearance-gaze pairs, and applies to all appearance.</p><p>Our method seeks the optimal distance metric D * :</p><formula xml:id="formula_4">D * = arg min D ǫ(α, β : D),<label>(5)</label></formula><p>where ǫ(α, β : D) refers to the discrepancy between the structure of space α subject to the metric D, and the structure of space β subject to the Euclidean distance metric. α and β are the appearance space and the gaze space, respectively. With this learned metric, our E D (ω) term is</p><formula xml:id="formula_5">E D (ω) = D * (a, Aω).<label>(6)</label></formula><p>The E R (ω) term has been reduced through the process of learning distance metric D * . The weight to reconstruct gaze is found through:</p><formula xml:id="formula_6">ω * = arg min ω D * (a, Aω) s.t 1 T ω = 1<label>(7)</label></formula><p>where A = {a 1 , . . . , a k } is the k-NN of a with respect to the distance metric D * . This is our first proposed approach, and the detail of the learning of this metric is described in Sec. 3.3.1 and the gaze estimation is given in Sec. 3.3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Learning a global distance metric</head><p>To obtain the distance metric, we propose to learn a linear projection C that transforms the appearance space, such that the transformed appearance space shares the same Euclidean structure of the gaze space. The distance metric of the transformed appearance space is</p><formula xml:id="formula_7">D(a i , a j ) = (Ca i − Ca j ) T (Ca i − Ca j ) = (a i − a j ) T S(a i − a j )<label>(8)</label></formula><p>where S = C T C is the transformation kernel. S is a positive semidefinite (PSD) matrix. This is equivalent to learning a Mahalanobis distance metric with kernel S for the original eye appearance space.</p><p>We model the structure of the appearance space α, the gaze space β, and the transformed appearance space γ. Then we need to learn a transform kernel S : α → γ, such that the structure of space γ approximates the structure of space β. This structure representation can have various choices, as long as it conveys the distance information between different data pairs. Without losing generality, in this paper we represent the structure of a space by its affinity matrix.</p><p>Given the training set of appearance-gaze exemplar pairs (a i , y i ), i = 1 . . . N , the affinity matrix of the target space</p><formula xml:id="formula_8">β is U [u ij ] u ij = exp(− (y i − y j ) T (y i − y j ) 2σ 1 ).<label>(9)</label></formula><p>Since the scale of the distance among data points does not change the estimation, normalization is performed on u ij and we get P [p ij ]</p><formula xml:id="formula_9">p ij = u ij k =i u ik , p ii = 0.<label>(10)</label></formula><p>The affinity matrix of transformed appearance space γ is modeled in the same way. The affinity matrix</p><formula xml:id="formula_10">V S [v S ij ] v S ij = exp(− (a i − a j ) T S(a i − a j ) 2σ 2 ).<label>(11)</label></formula><formula xml:id="formula_11">By normalizing matrix V S , we get matrix Q S [q S ij ] q S ij = v S ij k =i v S ik , q S ii = 0.<label>(12)</label></formula><p>We use KL divergence to model this difference between the matrices P and Q S .</p><formula xml:id="formula_12">KL[P|Q S ] = ij KL[p ij |q S ij ].<label>(13)</label></formula><p>Hence, the objective function of learning the metric is:</p><formula xml:id="formula_13">arg min S f (S) = ij KL[p ij |q S ij ], S ∈ PSD<label>(14)</label></formula><p>Equ. 14 can be solved by alternating between gradient descent step and projection to PSD cone. Noticing that:</p><formula xml:id="formula_14">f (S) = ij p ij log p ij − ij p ij log q S ij<label>(15)</label></formula><p>The gradient of f (S) w.r.t. S is</p><formula xml:id="formula_15">∇f (S) = 1 2σ 2 ij (p ij − q S ij )(a i − a j )(a i − a j ) T (16)</formula><p>For t-th iteration, gradient descent is performed with step length η to get S of next iteration.</p><formula xml:id="formula_16">S t+1 = S t − η∇f (S)<label>(17)</label></formula><p>To ensure that S is PSD, we project the matrix S t+1 to PSD cone. We first perform EVD on S t+1</p><formula xml:id="formula_17">S t+1 = k λ k u k u T k (18)</formula><p>where λ k is the eigenvalue of matrix S, u k is its corresponding eigenvector. Then we eliminate the components corresponding to negative eigenvalue.</p><formula xml:id="formula_18">S t+1 = k max(0, λ k )u k u T k<label>(19)</label></formula><p>Alternate the process of gradient descent and projection on PSD cone until S converges.</p><p>Compared with similar metric learning methods in <ref type="bibr" target="#b4">[5]</ref> [4], the proposed method is different in following aspects. Firstly, the proposed method is for regression problems, rather than classification. Secondly, our objective is to align the structure of the two spaces (i.e., the visual appearance space and the gaze space), not to collapse all the feature points in the same class to a single point.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Gaze estimation with learned distance metric</head><p>With the transformation kernel S = C T C, the new objective of gaze reconstruction corresponding to Eq. 7 is:</p><formula xml:id="formula_19">arg min ω a − Aω 2 C s.t.1 T ω = 1,<label>(20)</label></formula><p>where a is the input query appearance feature, C is the transform matrix, and A = (a 1 , . . . , a k ) is the collection of the k-nearest neighbors of the query appearance. Note that here the k-nearest neighbors are chosen based on the Mahalanobis distance with kernel S.</p><p>It is easy to obtain the closed-form solution to this constrained least squares problem:</p><formula xml:id="formula_20">ω * = D † 1 1 T D † 1<label>(21)</label></formula><p>where D = (a1 T − A) T S(a1 T − A), D † is the pseudoinverse of matrix D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Reducing E R by local regularization</head><p>In Sec. 3.3, the reduction of E R term is conducted in training phase with a global scope. In this section, we consider the possibility to further decrease E R in the estimation phase with a local scope.</p><p>In the estimation part in Sec. 3.3.2, Equ. 20 minimizes the matching error between the reconstructed appearance Aω and the query appearance a with the learned distance metric D * . It only considers the space structure of the training set of appearance and gaze pairs (a i , y i ), i = 1 . . . N .</p><p>Nevertheless, when the gaze y corresponding to the query appearance a is estimated (i.e., not included in the training set), we actually obtain another pair of appearance and gaze, (a, Bω), where a is the query appearance, Bω is the reconstructed or estimated gaze position. Therefore, the optimal weight ω * has impact on the mutual information between the true gaze and the estimated gaze Bω. However, this is missing in the estimation in Equ. 20, which will introduce additional error part of E R (ω) to the final estimation.</p><p>This part of E R (ω) can be further reduced in the estimation phase. We denote by β ′ the reconstructed gaze space (i.e., via Bω). Note that when ǫ(α, β : D * ) is small, ǫ(α, β ′ : D * ), the discrepancy of the structure of the appearance space α using distance metric D * and space β ′ with Euclidean distance, should also be small. We use this constraint to further regularize this space structure discrepancy.</p><p>This E R (ω) = ǫ(α, β ′ : D * ) term is local because it only considers the local space structure, i.e. only the local data related to the query appearance a and the estimated gaze Bω is influenced by the estimated ω. Only this local space structure discrepancy will contribute to E R (ω). By penalizing this local E R (ω) term, ω is not only trying to find the best matched reconstructed appearance Aω, but also trying to minimize the error caused in reconstructing the estimated gaze Bω by using this ω. ǫ(α, β ′ : D * ) is analogous to the energy function in energy-based classification in <ref type="bibr" target="#b18">[19]</ref>, but with different objective function. ǫ(α, β ′ : D * ) denotes the discrepancy between the structure of two spaces, while the energy term in <ref type="bibr" target="#b18">[19]</ref> is hinge loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">Modeling the local space structure</head><p>To describe the structure of extended gaze space, we use</p><formula xml:id="formula_21">u i = exp(− (Bω − y i ) T (Bω − y i ) 2σ 1 ),<label>(22)</label></formula><p>where Bω is the estimated gaze, B = {y 1 , . . . , y k } that is the collection of the corresponding gazes of the k-nearest neighbor of the query appearance. We normalize it:</p><formula xml:id="formula_22">p i = u i k u k<label>(23)</label></formula><p>The extended appearance space is subject to the Mahalanobis kernel S, and its structure can be modeled as:</p><formula xml:id="formula_23">v S i = exp(− (a − a i ) T S(a − a i ) 2σ 2 ).<label>(24)</label></formula><p>After normalization, we have:</p><formula xml:id="formula_24">q S i = v S i k v S k .<label>(25)</label></formula><p>The discrepancy of the local structure of spaces α and β ′ can still be modeled by the KL divergence KL[p|q S ], where vector q S [q S i ] models the structure of appearance space α related to the query appearance a, vector p [p i ] models the structure of the gaze position space β ′ related to the estimated gaze position Bω. Similar to Sec. 3.3.1,</p><formula xml:id="formula_25">KL[p|q S ] = i p i log p i − i p i log q S i .<label>(26)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2">Estimation with local regularization</head><p>The final objective function of our proposed method is:</p><formula xml:id="formula_26">arg min ω f (ω) = a − Aω 2 C +λ KL[p|q S ] s.t.1 T ω = 1 (27)</formula><p>where λ is a balancing factor.</p><formula xml:id="formula_27">f (ω) = a − Aω 2 C +λ( i p i log p i − i p i log q S i ),<label>(28)</label></formula><p>whose gradient w.r.t. ω is:</p><formula xml:id="formula_28">−2A T S(a−Aω)+ λ σ 1 ( i p i log p i q S i (B T y i − j p j B T y j )).</formula><p>(29) Gradient-based method can be easily applied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>This section evaluates the performance of the proposed method. Experiments have been performed on the benchmark dataset provided by Sugano et al. in <ref type="bibr" target="#b14">[15]</ref>. Same as in <ref type="bibr" target="#b8">[9]</ref>, the estimation error is measured in degree, and calculated as</p><formula xml:id="formula_29">Error = arctan( ŷ − y 2 d ),<label>(30)</label></formula><p>whereŷ is the estimated gaze position, y is the ground truth gaze position, and d is the distance between experiment subject and the screen on which the gaze position resides. Baseline comparisons include k-NN, SVR, Local Linear Reconstruction (LLR), the triangle region based Local Linear Reconstruction (LLR-TRI) <ref type="bibr" target="#b16">[17]</ref> and Adaptive Linear Regression (ALR) <ref type="bibr" target="#b8">[9]</ref>. The LLR method is a simplified version of <ref type="bibr" target="#b16">[17]</ref>, where the k-NN is chosen only based on the Euclidean distance without considering the topological information as in LLR-TRI. To investigate the influence of the regularization term in Sec. 3.4, the proposed method has been evaluated under two conditions: without the regularization term, denoted as Ours  The proposed methods and baseline methods were evaluated in three scenarios, fixed-subject and fixed-pose setting, cross-pose setting, and cross-subject setting. Influence of the balancing parameter λ, and the dimension of appearance will be discussed in following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Appearance feature</head><p>We extract the eye appearance feature in following steps:</p><p>1. Histogram equalization is performed on eye images.</p><p>2. Follow method in <ref type="bibr" target="#b8">[9]</ref> to detect the right and left eye corners, align the eye images with respect to the two eye corners to the template eye images.</p><p>3. The image region bounded by two eye corners is cropped using a fixed aspect ratio.</p><p>4. Eye images are downsampled to a lower resolution, e.g. 3 × 5, after antialiasing filtering. Normalize and concatenate appearance feature a l and a r of the left eye and the right eye to one appearance feature, a = ( </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Result on UT multi-view gaze dataset</head><p>Experiments are performed on a recently published dataset, UT Multi-view Gaze Dataset <ref type="bibr" target="#b14">[15]</ref>, which contains 50 subjects, 160 gaze directions, 8 cameras and 144 synthetic head poses for each subject. The proposed methods and baseline methods were evaluated under three settings,  <ref type="figure">Figure (a), (b)</ref>, (c) shows the affinity matrix of appearance feature space, gaze position space and the transformed appearance feature space after metric learning. In fixed-pose and fixed-subject setting, for each subject, training and testing images are from the same camera. The training and testing gaze split pattern follows <ref type="bibr" target="#b8">[9]</ref>, as shown in <ref type="figure" target="#fig_2">Fig. 3</ref>. One camera is sufficient for fixed pose and subject setting. We randomly choose camera No.5. In total, there are 3750 training samples and 3750 testing samples. For each subject, there are 75 training samples and 75 testing samples. Feature extraction follows Sec. 4.1. Note that ALR method requires the feature dimension to be lower than the number of training samples, the dimension of appearance feature of each eye must be lower than 37. On the contrary, our method does not have such constraint. For a fair comparison, 30-dimensional appearance features are constructed, denoted by 30-D, with 3×5 dimension for each eye image, the same as <ref type="bibr" target="#b8">[9]</ref>. k for k-nearest neighbor is set to 8. The dimension of the appearance feature is later increased to 64, denoted by 64-D, 4 × 8 for each eye image, to investigate its influence on estimation accuracy.</p><p>In cross-pose setting, we use synthesized images of each subject for training, and actual recorded images of the same subject for testing. The testing data has 8 different poses (cameras). All of 160 gaze positions of each pose are included in corresponding training or testing set, with no split. k for k-NN is increased to 10 as more training data are included. Note that the synthesized poses may also include the actual poses, we exclude from training set the synthetic poses whose distance to the testing pose is less than 5 degrees. This setting is not investigated in <ref type="bibr" target="#b14">[15]</ref> as they use all synthetic poses as training. Note that because of normalization, the poses of left eye and right eye are different for the same actual camera. Therefore, we can not use appearance which combines two eye images as in Sec. 4.1 since training poses are chosen based on testing poses. Left eye and right eye were evaluated separately. Appearance feature is extracted from each eye and with 9 × 15 dimension as in <ref type="bibr" target="#b14">[15]</ref>.</p><p>Cross-subject setting follows the design in <ref type="bibr" target="#b14">[15]</ref>. Training images include all gaze positions of 144 synthesized poses of 33 different subjects. Testing images include all 160 gaze positions of 8 actual cameras. k of k-NN is set to 10. Appearance feature is single eye image of 9 × 15 dimension. <ref type="figure" target="#fig_4">Fig. 4</ref> shows the affinity structure of the original appearance space, gaze space and the transformed appearance space of subject 00 with fixed-pose and fixed-subject setting. S is initialized as identity matrix. It is clear that the structure of the original appearance space in <ref type="figure" target="#fig_4">Fig. 4 (a)</ref> is deviated from the gaze space in <ref type="figure" target="#fig_4">Fig. 4 (b)</ref>. While the gaze space exhibits a strong periodic structure, the original appearance feature space structure is cluttered and irregular. As shown in <ref type="figure" target="#fig_4">Fig. 4</ref> (c), it is clear that the affinity structure of the transformed appearance feature space better approximates the structure of the gaze space. <ref type="figure" target="#fig_5">Fig. 5</ref> shows how the average estimation error changes as the balancing factor λ varies, under fixed-pose and fixedsubject setting. When λ is 0, the estimation error is large since the objective function degenerates to Equ. 20, which only considers the reconstruction error with learned distance metric but overlooking E R (ω). When λ is too large, the estimation is also inferior since the E D (ω) may be large, i.e. the reconstructed appearance is deviated from the query one, so is the estimated gaze. As shown in <ref type="figure" target="#fig_5">Fig. 5</ref>, the curve has only one extreme point, which makes it easy to find the optimal λ via binary search and cross validation on the same training set. In fixed-pose and fixed-subject setting, λ is fixed for different gaze directions. In cross-pose setting and cross-subject setting, it is estimated from k-NN training features of the query feature, which will be introduced in Sec. 4.2.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Metric learning result</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Influence of λ</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Result without pose and subject variation</head><p>This section investigates the fixed-pose and fixed-subject setting. Average estimation error in degree and standard deviation of error have been calculated for evaluation. To assess statistical significance of our proposed method, the Wilcoxon signed rank test has been conducted between each baseline method and Ours-R. A p value less than 0.05 indicates that the difference between the estimation error by Ours-R and the estimation error by one baseline method is statistically significant. The average estimation error across all subjects is summarized in table 1 in the form of mean ± std. The best estimation error is marked by bold face, while the second best one is in blue color. With 30-D appearance feature, in average of all subjects, Ours-R outperforms baseline methods relatively by 11.7% to 56.6%, Ours by 5.2%. With 64-D appearance feature, Ours-R outperforms baseline methods relatively by 11.8% to 62.7%, Ours by 3.2%. Ours-R also achieves the lowest standard deviation. In average of all subjects, p value of each baseline is less than 0.05</p><p>This experimental result shows that as the feature dimension increases, the average estimation error for all methods decreases. With 64-D appearance feature, our proposed methods still outperforms baseline methods by a significant margin similar to the 30-D feature. Moreover, since ALR method requires appearances with dimension lower than the number of training exemplars, it cannot use the higher dimensional feature, while our proposed methods can.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.4">Result with pose and subject variation</head><p>In this section, we show that the proposed method is actually robust to pose and subject variations. With pose and subject variations, the visual appearance space structure is more complex than that under fixed-pose and fixed-subject  <ref type="table">Table 2</ref>. Estimation error with pose and subject variation.</p><p>setting. A simple linear distance metric may not be able to align the entire appearance space to the gaze space. Our method indeed performs both the global alignment (i.e., learning a global metric) and local alignments conditioned on the query (i.e., a local metric). At the offline training, a rough global metric is learned on the entire appearance space. Then for a given query, we further learn a fine metric using the k-NN of this query, where k-NN exemplars are selected based on the global metric, as described in Sec. 3.3.1. λ is estimated using these k-NN exemplars.</p><p>Experimental results are summarized in table 2. Ours-R achieves the best estimation error with p values less than 0.05. Our methods produce obvious performance improvement over LLR. It clearly shows that the proposed method is resilient to the head pose and subject variations. In addition to average cross-pose error, we also investigate how the estimation error changes w.r.t. the distance between training and testing poses. Due to limited space, the result is provided in supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>This paper investigates a fundamental issue in reconstruction-based gaze estimation, i.e., under what condition the reconstruction in the appearance space can be transferred to the gaze space for gaze estimation. This is the first of its kind to study this important issue. Without a proper metric adjustment in either space, the direct transfer of the reconstruction from one space to the other is questionable. This paper proposes an effective metric learning method to identify a new metric for the appearance space. In addition, this paper also presents a novel method to solve for the reconstruction by local regularization based on the affinity structure of the appearance space and the reconstructed gaze space. These two methods have shown consistently and significantly superior performances on public benchmark dataset across different settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledgement</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Illustration of the difference of structure between the appearance feature space and the gaze position space. The length of connection lines indicates the Euclidean distance between two connected components. Figure (a) describes the Euclidean distances between the appearance bounded by black box and its 5nearest neighbors. Figure (b) describes the distances in the gaze position space, where each triangle indicates the gaze position point corresponding to the appearance bounded by the box with the same color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>(Sec. 3.3.2), and with regularization term, denoted as Ours-R (Sec. 3.4.2). Illustration of feature extraction process</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Gaze position patterns. Each blue cross denotes one gaze position in the training set. Each red cross is one gaze position in the testing set. Each green cross denotes one excluded gaze position in UT multi-view gaze dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Result of metric preservation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Regularized estimation errors with different λ including fixed-pose &amp; fixed-subject, cross-pose, crosssubject.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>23±1.34 1.11±1.24 LLR-TRI 1.27±1.60 1.14±1.53 ALR 1.23±1.35 1.06±1.75 Ours 1.14±1.20 0.95±1.10 Ours-R 1.08±1.15 0.92±1.08 Table 1. Estimation error without pose and subject variation.</figDesc><table>Error 30-D Error 64-D 
k-NN 
2.49±1.84 2.47±1.80 
SVR 
1.32±1.32 1.05±1.26 
LLR 
1.</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported in part by National Science Foundation grant IIS-1217302 and ARO W911NF-15-1-0472.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Non-intrusive gaze tracking using artificial neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baluja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pomerleau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">3d gaze estimation with a single camera without ir illumination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Probabilistic gaze estimation without active personal calibration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="609" to="616" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Metric learning by collapsing classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Globerson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">T</forename><surname>Roweis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="451" to="458" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Goldberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">T</forename><surname>Roweis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="513" to="520" />
		</imprint>
	</monogr>
	<note>Neighbourhood components analysis</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">General theory of remote gaze estimation using the pupil center and corneal reflections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Guestrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Eizenman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Biomedical Engineering</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1124" to="1133" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">In the eye of the beholder: A survey of models for eyes and gaze</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ji</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>PAMI</publisher>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="478" to="500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A head pose-free approach for appearance-based gaze estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Okabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sugano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Inferring human gaze from appearance via adaptive linear regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sugano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Okabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="153" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Head pose-free appearance-based gaze sensing via eye image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sugano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Okabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1008" to="1011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Geometric generative gaze estimation (g3e) for remote rgb-d cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">A F</forename><surname>Mora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-M</forename><surname>Odobez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1773" to="1780" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Eye gaze tracking techniques for interactive applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Morimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Mimica</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="page" from="4" to="24" />
		</imprint>
		<respStmt>
			<orgName>CVIU</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Nonlinear dimensionality reduction by locally linear embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">T</forename><surname>Roweis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">290</biblScope>
			<biblScope unit="issue">5500</biblScope>
			<biblScope unit="page" from="2323" to="2326" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Appearance-based gaze estimation using visual saliency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sugano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Matsushita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="329" to="341" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning-bysynthesis for appearance-based 3d gaze estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sugano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Matsushita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">An incremental learning method for unconstrained gaze estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sugano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Matsushita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Koike</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="656" to="667" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Appearance-based eye gaze estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-H</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kriegman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Eye gaze estimation from a single image of one eye</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Venkateswarlu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="136" to="143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Distance metric learning for large margin nearest neighbor classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="1473" to="1480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Sparse and semisupervised visual mapping with the s 3 gp</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="230" to="237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A novel approach to real-time non-intrusive gaze finding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Machin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sheppard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">A novel non-intrusive eye gaze estimation using cross-ratio under large head motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Chung</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="page" from="25" to="51" />
		</imprint>
		<respStmt>
			<orgName>CVIU</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Noncontact eye gaze tracking system by mapping of corneal reflections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">R</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Chung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Automatic Face and Gesture Recognition</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="94" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Eye gaze tracking under natural head movements</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="918" to="923" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Nonlinear eye gaze mapping function estimation via support vector regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">P</forename><surname>Bennett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1132" to="1135" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
