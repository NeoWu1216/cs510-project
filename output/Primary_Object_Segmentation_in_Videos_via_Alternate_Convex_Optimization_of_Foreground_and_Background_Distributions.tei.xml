<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Primary Object Segmentation in Videos via Alternate Convex Optimization of Foreground and Background Distributions</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Won-Dong</forename><surname>Jang</surname></persName>
							<email>wdjang@mcl.korea.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Korea University</orgName>
								<orgName type="institution" key="instit2">Northumbria University</orgName>
								<orgName type="institution" key="instit3">Korea University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chulwoo</forename><surname>Lee</surname></persName>
							<email>chulwoo.lee@northumbria.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Korea University</orgName>
								<orgName type="institution" key="instit2">Northumbria University</orgName>
								<orgName type="institution" key="instit3">Korea University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang-Su</forename><surname>Kim</surname></persName>
							<email>changsukim@korea.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Korea University</orgName>
								<orgName type="institution" key="instit2">Northumbria University</orgName>
								<orgName type="institution" key="instit3">Korea University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Primary Object Segmentation in Videos via Alternate Convex Optimization of Foreground and Background Distributions</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:44+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>An unsupervised video object segmentation algorithm, which discovers a primary object in a video sequence automatically, is proposed in this work. We introduce three energies in terms of foreground and background probability distributions: Markov, spatiotemporal, and antagonistic energies. Then, we minimize a hybrid of the three energies to separate a primary object from its background. However, the hybrid energy is nonconvex. Therefore, we develop the alternate convex optimization (ACO) scheme, which decomposes the nonconvex optimization into two quadratic programs. Moreover, we propose the forward-backward strategy, which performs the segmentation sequentially from the first to the last frames and then vice versa, to exploit temporal correlations. Experimental results on extensive datasets demonstrate that the proposed ACO algorithm outperforms the state-of-the-art techniques significantly.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Video object segmentation is the process to separate a primary object from the background in a video sequence. It is applicable as a preliminary to various vision applications, such as action recognition, content-based video retrieval, targeted content replacement, and video summarization. It is hence important to develop robust video object segmentation techniques. However, video object segmentation is challenging due to a variety of difficulties, e.g., cluttered background, occlusion, and non-rigid object deformation. To overcome these issues, many attempts have been made.</p><p>Video object segmentation methods can be categorized into supervised or unsupervised approaches. Supervised methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b32">33]</ref> address the problem by employing user annotations on a few selected frames. In contrast, unsupervised methods <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b36">37]</ref> identify an object automatically. Without the prior information about the object, the unsupervised segmentation is more difficult than the supervised one.</p><p>In this work, we propose a novel unsupervised algorithm for video object segmentation. To segment a primary foreground object from the background, we define three energies in terms of foreground and background probability distributions: Markov energy, spatiotemporal energy, and antagonistic energy. Then, we minimize the hybrid energy of the three energy terms to achieve the segmentation. More specifically, since the hybrid energy is nonconvex, we develop the alternate convex optimization (ACO) scheme that converts the nonconvex problem into two quadratic programs. We perform the segmentation forwardly from the first to the last frames, and then backwardly from the last to the first frames. Experimental results demonstrate that the proposed ACO algorithm outperforms the state-of-theart conventional algorithms in <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b36">37]</ref> on the SegTrack <ref type="bibr" target="#b32">[33]</ref>, SegTrack v2 <ref type="bibr" target="#b22">[23]</ref>, and VidSeg datasets. To summarize, this paper has three main contributions.</p><p>• Introduction of the hybrid energy of foreground and background distributions and its ACO to segment a primary object from the background.</p><p>• The forward-backward strategy, to transfer object information sequentially from the first frame to the last frame and vice versa, for accurate object segmentation.</p><p>• Remarkable performance achievement on the three datasets, including the proposed VidSeg dataset that consists of challenging sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Supervised methods for video object segmentation, requiring user annotations about a primary object, include non-rigid object tracking and interactive video segmentation. In non-rigid object tracking, a primary object is manually delineated at the first frame and then tracked at subsequent frames <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b32">33]</ref>. In interactive video segmentation, user annotations on a few selected frames are utilized to separate an object from its background <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b29">30]</ref>. However, the manual delineation or annotation is exhausting. Hence, in this work, we focus on unsupervised methods.  Unsupervised methods automatically extract an object from a video sequence. Brendel and Todorovic <ref type="bibr" target="#b6">[7]</ref> proposed a bottom-up video over-segmentation algorithm, which determines the temporal connection between per-frame image segmentation results. Grundmann et al. <ref type="bibr" target="#b17">[18]</ref> developed another video over-segmentation algorithm based on the graph-based optimization. Video over-segmentation is a versatile tool for various computer vision tasks, but it does not resolve the object-level segmentation problem.</p><p>To detect moving objects in a video sequence, Barnich and Droogenbroeck <ref type="bibr" target="#b4">[5]</ref> proposed a background subtraction algorithm, which uses a background model based on interframe temporal consistencies. Also, Han and Davis <ref type="bibr" target="#b18">[19]</ref> trained an SVM classifier, using multiple features, to build a background model. However, since background subtraction assumes fixed or slowly panning cameras, these algorithms <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b18">19]</ref> are applicable to limited situations only.</p><p>Shi and Malik <ref type="bibr" target="#b30">[31]</ref> constructed a graph for pixels in a video and then segmented motions using normalized cuts. By grouping long-term point trajectories, Brox and Malik <ref type="bibr" target="#b8">[9]</ref> achieved object segmentation. Ochs and Brox <ref type="bibr" target="#b26">[27]</ref> exploited sparse point trajectories to extract dense object regions. Ochs and Brox <ref type="bibr" target="#b27">[28]</ref> also performed spectral clustering on point trajectories for object segmentation.</p><p>With advances in object proposal techniques [2, 10, 13], Lee et al. <ref type="bibr" target="#b20">[21]</ref> first applied them to the video object segmentation task by ranking proposals in a video. Ma and Latecki <ref type="bibr" target="#b24">[25]</ref> identified a primary object by determining maximum weight cliques on a series of object proposals. Zhang et al. <ref type="bibr" target="#b36">[37]</ref> introduced a layered acyclic graph for object proposals and discovered an optimal path using dynamic programming. Banica et al. <ref type="bibr" target="#b3">[4]</ref> constructed salient segment chains by performing matching between object proposals. Levinshtein et al. <ref type="bibr" target="#b21">[22]</ref> applied the parametric maxflow to group spatiotemporal superpixels. Papazoglou and Ferrari <ref type="bibr" target="#b28">[29]</ref> estimated motion-based inside-outside maps to de-lineate moving objects. Li et al. <ref type="bibr" target="#b22">[23]</ref> generated multiple segment tubes by tracking many hypotheses.</p><p>Recently, Wang et al. <ref type="bibr" target="#b33">[34]</ref> proposed a saliency-driven video object segmentation algorithm using geodesic distances. Giordano et al. <ref type="bibr" target="#b13">[14]</ref> exploited a continuity of superpixels across consecutive frames to extract moving objects. Also, Taylor et al. <ref type="bibr" target="#b31">[32]</ref> inferred long-term occlusions to discover objects in a video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Algorithm</head><p>This section proposes a novel unsupervised video object segmentation algorithm, referred to as ACO. The input is a set of consecutive video frames {I (1) , ..., I (T ) }, and the output is a set of pixel-wise binary label maps, discriminating a primary foreground object from its background. <ref type="figure" target="#fig_1">Figure 1</ref> shows an overview of the proposed ACO algorithm. First, we estimate initial probability distributions of the foreground and background by performing the manifold ranking processes with the boundary priors for each frame. Second, we optimize the foreground and background distributions, by employing the ACO, and then determine the label maps. This is done frame-by-frame, and the label map of a previous frame is exploited to obtain that of a current frame. More specifically, the forward-backward strategy is adopted, which obtains the label maps from the first to the last frames and then improves their accuracies from the last to the first frames. Finally, by refining the superpixel-level segmentation results, we obtain pixel-wise object segments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Motion Estimation and Graph Construction</head><p>For each frame τ , we estimate the optical flows <ref type="bibr" target="#b23">[24]</ref> from I (τ ) to I (τ −1) and I (τ −2) , respectively. Also, we apply the SLIC algorithm <ref type="bibr" target="#b0">[1]</ref> to over-segment each frame.</p><p>Let G = (V, E) be a graph, where V = {x 1 , . . . , x N } is the set of nodes and E = {e ij } is the set of edges. The superpixels become nodes. Each edge e ij , connecting x i and x j , is assigned with weight (or affinity) w ij ,</p><formula xml:id="formula_0">w ij = exp − d 2 (xi,xj ) σ 2 if e ij ∈ E, 0 otherwise,<label>(1)</label></formula><p>where d denotes a distance between x i and x j in a feature space, and σ 2 is a scale parameter. We define the k-ring graph. In the k-ring graph, two nodes x i and x j are connected by an edge, if there is a sequence (x i = x g1 , x g2 , . . . , x gn = x j ) for n ≤ k + 1 and every pair of consecutive nodes (or superpixels) in the sequence share a boundary. As k increases, the k-ring graph connects a node to a larger number of nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Initial Probability Estimation</head><p>We generate initial probability distributions of the foreground and background, respectively. While a foreground object is likely to be near the center of a frame, boundary regions tend to belong to the background <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b35">36]</ref>. By adopting this boundary prior, we perform the manifold ranking algorithm <ref type="bibr" target="#b37">[38]</ref>. Let us briefly review the manifold ranking. First, the affinity matrix W = [w ij ] is computed, and a query vector y is defined. For instance, y i = 1 if node i is a query, and y i = 0 otherwise. Then, the affinity matrix W is symmetrically normalized by Π = D −1/2 WD −1/2 , where D is the diagonal matrix whose element d ii equals the sum of the elements in the ith row of W. Then, the ranking vector r is given by</p><formula xml:id="formula_1">r = (I − αΠ) −1 y (2)</formula><p>where α is a parameter within [0, 1]. The element r i in r represents the ranking of the ith node toward the query node, i.e. the similarity of the ith node to the query node on the graph. In this work, to determine initial foreground and background distributions at frame τ , we construct the 4-ring graph. We also compute each element w (τ ) ij of the affinity matrix W (τ ) using both the average LAB color difference and the average optical flow difference between x i and x j .</p><p>For the background distribution, we set a query vector y</p><formula xml:id="formula_2">(τ ) b</formula><p>based on the boundary prior. Specifically, we set the query amount y</p><formula xml:id="formula_3">(τ ) b,j at node j to be proportional to 1/ max i {π (τ )</formula><p>ij } if node j is at the image boundary, and 0 otherwise, as shown in <ref type="figure" target="#fig_4">Figure 2</ref></p><formula xml:id="formula_4">(b). π (τ )</formula><p>ij is an element of the normalized affinity matrix Π (τ ) . Thus, we assign a large query amount to a highly spreadable node, which is connected to many similar nodes. Then, by employing y   <ref type="figure" target="#fig_4">Figure 2</ref>(c), which is used as the initial background distribution. Notice that, if we assign query amounts uniformly to all boundary nodes, the background distribution r (τ ) b, uniform may be confined within the boundaries, as in <ref type="figure" target="#fig_4">Figure 2</ref> On the other hand, for the foreground distribution, we convert the background distribution r</p><formula xml:id="formula_5">(d). (a) Frame τ (b) y (τ ) b (c) r (τ ) b (d) r (τ ) b, uniform (e) y (τ ) f (f) r (τ ) f</formula><formula xml:id="formula_6">(τ ) b into the foreground query vector y (τ ) f via y (τ ) f,i ∝ exp (−r (τ ) b,i ).</formula><p>Then, we compute the corresponding ranking vector r (τ ) f , which is the initial foreground distribution. In <ref type="figure" target="#fig_4">Figure 2</ref>(f), we see that r</p><p>(τ ) f roughly indicates the shape of the foreground object.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Forward Pass of Video Object Segmentation</head><p>Next, we delineate a primary object in each frame, sequentially from the first to the last frames. We use the segmentation results of previous frames to process a current frame. For simplicity, let us describe the algorithm mainly in terms of the foreground distribution p</p><formula xml:id="formula_7">(τ ) f , in which p (τ )</formula><p>f,i is the probability that the foreground object is found at node i at frame τ . The background distribution p (τ ) b is handled in a symmetrical manner.</p><p>For the video object segmentation, we define an energy function, composed of three terms: Markov energy, spatiotemporal energy, and antagonistic energy. Let us describe these three energy terms subsequently.</p><p>Markov Energy: The Markov random walk process simulates movements of an agent on a graph <ref type="bibr" target="#b11">[12]</ref>. A movement is made with a higher probability, as the two nodes have more similar features. Many properties, including the stationary distribution of the agent, provide useful information for data clustering <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b25">26]</ref>.</p><p>An agent moves from node j to node i according to the transition probability</p><formula xml:id="formula_8">a ij = w ij / N k=1 w kj .<label>(3)</label></formula><p>The movements of the agent are modeled by</p><formula xml:id="formula_9">p (θ+1) = Ap (θ)<label>(4)</label></formula><p>(a) Input points where θ is a time instance and A = [a ij ] is the transition matrix. The stationary distribution p (∞) can be found when the squared distance ||Ap (∞) − p (∞) || 2 is minimized to 0. <ref type="figure" target="#fig_5">Figure 3</ref>(b) shows an example of the stationary distribution. We observe two desirable properties for clustering: First, it has high probabilities near the center of a cluster (i.e. a region of high point density) and low probabilities along the boundary of a cluster (i.e. a region of low point density). Second, nearby points have similar probabilities, thus facilitating the assignment of those points to the same cluster.</p><formula xml:id="formula_10">(b) E M = 0 (c) E M = 1.9×10 −12 (d) E M = 2.0×10 −12 (e) E M = 1.5 × 10 −4 (f) E M = 1.0 × 10 −3</formula><p>To enforce these two properties, we define the Markov energy E M as</p><formula xml:id="formula_11">E M (p (τ ) f ) = ||A (τ ) p (τ ) f − p (τ ) f || 2<label>(5)</label></formula><p>where A (τ ) is the transition matrix at frame τ , derived from the affinity matrix W (τ ) . Notice that, when the agent moves within a single cluster in <ref type="figure" target="#fig_5">Figure 3</ref>(c) or (d), the Markov energy is also small and the aforementioned two properties are also satisfied. In contrast, the uniform and random distributions in <ref type="figure">Figures</ref>  in Section 3.2 provides a rough estimate of the foreground distribution. However, the per-frame estimate is insufficient for the video object segmentation due to its lack of temporal consistency. We hence combine the initial distribution with the segmentation results of previous frames to yield spatially accurate and temporally coherent segments.</p><p>At frame τ , we first obtain the temporal foreground confidence map φ (τ ) f . To this end, we compute the propagation matrix C (τ,τ ) that transfers the foreground labels at frame τ to frame τ . An element c (τ,τ ) ij in C (τ,τ ) is 1 if at least one pixel within node i at frame τ is matched to a pixel within node j at frameτ according to the optical flow, and 0 otherwise. Then, we transfer the segmentation results of the previous two frames to the current frame τ to generate the temporal confidence map φ (τ ) f , which is given by</p><formula xml:id="formula_12">(a) Frame τ − 1 (b) l (τ −1) f (c) φ (τ ) f (d) Frame τ (e) r (τ ) f (f) s (τ ) f</formula><formula xml:id="formula_13">φ (τ ) f = C (τ,τ −1) l (τ −1) f + C (τ,τ −2) l (τ −2) f (6) where l (τ −1) f and l (τ −2) f</formula><p>denote the foreground binary label vectors at frames τ − 1 and τ − 2, respectively.</p><p>Then, we compute the spatiotemporal distribution</p><formula xml:id="formula_14">s (τ ) f = β × φ (τ ) f ⊗ r (τ ) f<label>(7)</label></formula><p>where ⊗ denotes the element-wise multiplication, and β is a constant to normalize s (τ )</p><p>f . By combining the spatial and temporal estimates, the spatiotemporal distribution s (τ ) f provides a more reliable estimate of the foreground, as shown in <ref type="figure" target="#fig_6">Figure 4</ref>(f). Therefore, we define the spatiotemporal energy E S as</p><formula xml:id="formula_15">E S (p (τ ) f ) = ||p (τ ) f − s (τ ) f || 2<label>(8)</label></formula><p>in order to enforce the foreground distribution to be similar to the spatiotemporal distribution.  Optimization for background distribution p</p><formula xml:id="formula_16">E A (p (τ ) f , p (τ ) b ) = N i=1 j∈Mi w (τ ) ij p (τ ) f,i p (τ ) b,j<label>(9)</label></formula><formula xml:id="formula_17">(τ ) b ⊲ (16) 7: until E(p (τ ) f , p (τ ) b ) stops decreasing ⊲<label>(10)</label></formula><p>Output: Foreground and background distributions p ij is the affinity between nodes i and j at frame τ . The antagonistic energy is reduced, when a highly probable foreground node is surrounded by unlikely background neighbors. Thus, for a low antagonistic energy, the foreground and the background should form their own dominant regions.</p><p>Alternate Convex Optimization: We combine the three energy terms into a hybrid energy. For notational simplicity, let us omit the superscripts for frame indices. Then, to obtain the optimal foreground and background distributions p f and p b , we minimize the hybrid energy</p><formula xml:id="formula_18">E (p f , p b ) = E M (p f ) + E M (p b ) + γ · E S (p f ) + γ · E S (p b ) + δ · E A (p f , p b )<label>(10)</label></formula><p>subject to the constraints</p><formula xml:id="formula_19">0 ≤ p f,i ≤ 1, N i=1 p f,i = 1,<label>(11)</label></formula><formula xml:id="formula_20">0 ≤ p b,i ≤ 1, N i=1 p b,i = 1.<label>(12)</label></formula><p>In <ref type="formula" target="#formula_0">(10)</ref>, nonnegative parameters γ and δ control the tradeoffs between the three energy terms.</p><formula xml:id="formula_21">Let p = [p T f , p T b ] T .</formula><p>Then, the hybrid energy in (10) can be rewritten as</p><formula xml:id="formula_22">E(p) = p T Bp − 2γ[s T f , s T b ]p + γs T f s f + γs T b s b (13) where B = (A−I) T (A−I)+γI δ 2 W δ 2 W (A−I) T (A−I)+γI .<label>(14)</label></formula><p>Notice that, without the antagonistic energy E A (p f , p b ), the non-diagonal sub-matrices in <ref type="bibr" target="#b13">(14)</ref> would be zero and B would be positive semidefinite. In such a case, the minimization of E(p) subject to the constraints in <ref type="bibr" target="#b10">(11)</ref> and <ref type="formula" target="#formula_0">(12)</ref> becomes a quadratic program <ref type="bibr" target="#b5">[6]</ref>, which can be solved easily, e.g., using Lagrange multipliers. However, the antagonistic energy makes B indefinite, and the minimization problem is a nonconvex one, which is difficult to solve. To overcome this difficulty, we develop the ACO scheme, which decomposes the nonconvex problem into two convex subproblems. First, after fixing the background distribution p b , we solve a quadratic program:</p><formula xml:id="formula_23">(a) E S + E A (b) E M + E A (c) E M + E S (d) E M + E S + E A</formula><formula xml:id="formula_24">min pf p T f (A − I) T (A − I) + γI p f − 2γs T f − δp T b W p f<label>(15)</label></formula><p>subject to the constraints in <ref type="bibr" target="#b10">(11)</ref>. Then, using the resultant p f , we update the background distribution p b by solving the other quadratic program: <ref type="bibr" target="#b15">(16)</ref> subject to the constraints in <ref type="bibr" target="#b11">(12)</ref>. We solve the two quadratic programs alternately, using the software in <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref>. When we first solve (15), the initial distribution r b is used as the background distribution p b . The alternate scheme is guaranteed to converge and yield a locally optimal solution, since each quadratic program in <ref type="bibr" target="#b14">(15)</ref> or <ref type="formula" target="#formula_0">(16)</ref> monotonically decreases the hybrid energy in (13) that is bounded below. Algorithm 1 summarizes the ACO scheme. <ref type="figure" target="#fig_9">Figure 5</ref> exemplifies how each of the three energy terms affects the resulting distributions. Specifically, we exclude one of the three terms to analyze its efficacy. In <ref type="figure" target="#fig_9">Figure 5(a)</ref>, without the Markov energy E M , the distributions do not spread according to the node affinities and the background distribution fails to cover the region near the object boundary. In <ref type="figure" target="#fig_9">Figure 5(b)</ref>, the spatiotemporal energy E S is omitted. The foreground distribution is concentrated on only a few superpixels, since the spatial and temporal correlations are not exploited. In <ref type="figure" target="#fig_9">Figure 5</ref>(c), the antagonistic energy E A is excluded. The two distributions discover the object and the background relatively well. However, without the interaction between the two distributions, the foreground distribution fails to find the diver's legs. In contrast, in <ref type="figure" target="#fig_9">Figure 5</ref>(d), the diver is accurately separated from the background, by combining all three terms. Foreground and Background Labeling: After the ACO of the foreground and background distributions, we use the maximum a posteriori (MAP) criterion to determine the binary segmentation label of each superpixel <ref type="bibr" target="#b19">[20]</ref>. The probability p</p><formula xml:id="formula_25">min pb p T b (A − I) T (A − I) + γI p b − 2γs T b − δp T f W p b</formula><p>(τ )</p><p>f,i that the foreground is found on node i at frame τ is regarded as the likelihood p(x i |ω </p><formula xml:id="formula_26">p(ω (τ ) f |x i ) = p(x i |ω (τ ) f )p(ω (τ ) f ) p(x i |ω (τ ) f )p(ω (τ ) f ) + p(x i |ω (τ ) b )p(ω (τ ) b )</formula><p>, <ref type="bibr" target="#b16">(17)</ref> which represents the probability that node i is occupied by the foreground. Then, the segmentation labels l </p><formula xml:id="formula_27">(τ ) f,i , l (τ ) b,i ) = (1, 0) if p(ω (τ ) f |x i ) &gt; p(ω (τ ) b |x i ), (0, 1) otherwise.</formula><p>(18) We estimate the prior probabilities p(ω (τ ) f ) and p(ω</p><formula xml:id="formula_28">(τ ) b )</formula><p>of the foreground and background at frame τ , by employing the distributions at the previous frame (τ − 1). Suppose that the two distributions are completely separated and that each distribution is uniformly spread in its own region. Then, the number of the nodes that each distribution occupies is equal to the inverse of its uniform probability. Inspired by this, we estimate the priors by</p><formula xml:id="formula_29">p(ω (τ ) f ) = 1 max i p (τ −1) f,i , p(ω (τ ) b ) = 1 max i p (τ −1) b,i .<label>(19)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Backward Pass of Video Object Segmentation</head><p>In the forward pass, the probability distributions in later frames are more reliable than those in earlier frames, since the segmentation results in past frames are used to improve the clustering performance for a current frame. Thus, we further carry out a backward pass, which progresses from the last to the first frames. The backward pass is the same as the forward pass, except for primary object selection. The backward pass selects a primary object in each frame among multiple connected components of foreground superpixels. We determine the priority score for each component, by summing up the foreground probabilities of the corresponding superpixels. We then declare the component with the highest priority as the primary object.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Pixel-wise Refinement</head><p>Notice that each frame is over-segmented into superpixels to reduce the number of graph nodes. Thus, we refine segmentation results at the superpixel-level into those at the pixel-level by employing the Markov random field optimization scheme in <ref type="bibr" target="#b36">[37]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results</head><p>We test the proposed ACO video object segmentation algorithm on three datasets: SegTrack <ref type="bibr" target="#b32">[33]</ref>, SegTrack v2 <ref type="bibr" target="#b22">[23]</ref>, and VidSeg. We use the same parameters in all experiments.</p><p>SegTrack <ref type="bibr" target="#b32">[33]</ref> was initially announced to evaluate nonrigid object tracking algorithms. Among the six videos, five are typically used to assess video object segmentation performance <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b36">37]</ref>, and their pixel-wise groundtruth maps are available. Since a primary object in each video maintains similar sizes over all frames, the average number of mislabelled pixels per frame is used as the performance metric in <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b36">37]</ref>. <ref type="table" target="#tab_2">Table 1</ref> compares the performance of the proposed ACO algorithm on the SegTrack dataset with those of 12 conventional algorithms: unsupervised-single <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b36">37]</ref>, unsupervised-multiple <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b27">28]</ref>, and supervised non-rigid object tracking <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b32">33]</ref>. For the performance assessment, the unsupervised-multiple algorithms should select the best segment track, among the multiple tracks, that maximally matches with the ground-truth. The proposed ACO algorithm outperforms most conventional unsupervised-single algorithms. Furthermore, ACO even surpasses most unsupervised-multiple algorithms and two supervised trackers <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b32">33]</ref>.</p><p>SegTrack v2 <ref type="bibr" target="#b22">[23]</ref> extends the SegTrack dataset by supplying eight video sequences and their ground-truth maps. We select five sequences, each of which contains a single prominent object. Also, we propose a new dataset, VidSeg. We collect eight videos from the YouTube and four movie clips. Except for the "Long Jump" sequence, we extract the ground-truth map for every fifth frame. For "Long Jump," we annotate the ground-truth labels for all frames because of its relatively short length. The VidSeg videos are chal-  <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b28">[29]</ref>, and the proposed algorithm from top to bottom. lenging due to complex object appearance, object deformation, cluttered background, and long video lengths.</p><p>The average number of mislabelled pixels per frame can be misleading, since it simply counts wrong labels regardless of an object size. Li et al. <ref type="bibr" target="#b22">[23]</ref> pointed out this issue, and used the intersection over union (IoU) score, IoU = 100 · |T ∩R| |T ∪R| where T and R are the sets of foreground pixels in a segmentation result and the corresponding groundtruth map, respectively. Since object sizes vary dramatically in SegTrack v2 and VidSeg, we measure the segmentation performance by IoU. We compute the IoU score for each frame and report the average IoU score over all frames. <ref type="table" target="#tab_3">Table 2</ref> summarizes the IoU scores on all three datasets. Two conventional algorithms <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b36">37]</ref> are compared, whose source codes are available. The proposed ACO algorithm outperforms these two algorithms. Especially, ACO is effective for long-duration videos, whose primary objects suffer from non-rigid appearance deformation and background variations. In terms of the average scores, ACO outperforms <ref type="bibr" target="#b28">[29]</ref> and <ref type="bibr" target="#b36">[37]</ref> by significant margins, about 19.0% and 14.2%, respectively. <ref type="figure" target="#fig_12">Figure 6</ref> shows segmentation results, obtained by <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b28">[29]</ref>, and the proposed ACO algorithm. ACO delineates primary objects precisely and robustly in these sequences, even though they have appearance deformation, motion blur ("Floor Exercise"), and cluttered background ("Spider Man"). Since <ref type="bibr" target="#b36">[37]</ref> relies on object proposals, it fails when the proposal scheme does not identify small or complex objects in "Spider Man." Also, on "Floor Exercise," <ref type="bibr" target="#b28">[29]</ref> provides unsuccessful results due to its strong dependency on motion boundaries. In contrast, ACO robustly identi-  fies primary objects by minimizing the hybrid energy of the Markov, spatiotemporal, and antagonistic terms.</p><formula xml:id="formula_30">E S E S + E A E M + E A E M + E S E M + E S + E</formula><p>Step Analysis: <ref type="table" target="#tab_4">Table 3</ref> lists the average IOU score of the proposed ACO algorithm, after performing each step. The forward pass improves the initial score by exploiting temporal, as well as spatial, correlations. Then, the backward pass corrects misjudged segments in the forward pass. Finally, the pixel-wise refinement further boosts the performance to the final score of 70.24.</p><p>Energy Term Analysis: We analyze the efficacy of each energy term, by testing various combinations of the three energy terms. For each combination, <ref type="table">Table 4</ref> reports the average IoU score without the pixel-wise refinement to focus on the energy terms only. Note that the energies, E S and E M + E S , are minimized by the global convex optimization, instead of the ACO. The spatiotemporal energy E S plays an essential role by exploiting motion information, as well as spatial correlations, in video sequences. Thus, the combination E M + E A provides the worst performance. However, the Markov energy E M and the antagonistic energy E A are also important, and thus E M + E S + E A outperforms both E S + E A and E M + E S significantly. <ref type="table">Table 4</ref>, together with <ref type="figure" target="#fig_9">Figure 5</ref>, indicates that the three energy terms are complementary to one another. The energy terms yield excellent segmentation performance, when they are jointly minimized.</p><p>Multiple Primary Object Segmentation: Next, we apply the proposed ACO algorithm to videos that contain multiple primary objects. The number of primary objects, k, is assumed to be known. For the initialization, we first perform the scheme in Section 3.2 to obtain initial foreground and background distributions. Then, we divide the initial foreground distribution into k distributions using the k-means clustering technique. Consequently, we use k foreground distributions (one for each object) and a single background one. The remaining steps are straightforwardly generalized from those of the single object segmentation. For example, in the ACO, we optimize each distribution, after fixing the other k distributions, repeatedly in a round-robin manner. <ref type="table">Table 5</ref> presents the IoU scores on the three sequences in SegTrack v2 <ref type="bibr" target="#b22">[23]</ref>, each of which has two primary objects. Three conventional algorithms <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b27">28]</ref> are compared, whose source codes are publicly available and applicable to the multiple object segmentation task. <ref type="bibr" target="#b20">[21]</ref> extracts a primary object by selecting a hypothesis, which is a set <ref type="table">Table 5</ref>. Multiple primary object segmentation performances (in IoU scores) of the proposed ACO algorithm and the conventional algorithms <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b27">28]</ref>. The best and the second best results are boldfaced and underlined, respectively.  of object proposals across frames. It yields hypotheses with priorities. We test <ref type="bibr" target="#b20">[21]</ref> in two ways. In the column <ref type="bibr" target="#b20">[21]</ref>-T, we compute the IoU score of the two hypotheses with the highest priorities. On the other hand, in <ref type="bibr" target="#b20">[21]</ref>-A, we report the IoU score of the best combination of two hypotheses, among all combinations. The best combination is selected using the ground truth. <ref type="bibr" target="#b8">[9]</ref> and <ref type="bibr" target="#b27">[28]</ref> are the motion segmentation algorithms to yield dense segments. To measure their primary object segmentation performances, we find the maximally matched segment for each ground-truth object using the IoU criterion. It is hence unfair to compare the proposed ACO algorithm, requiring only the number k of objects, with <ref type="bibr" target="#b20">[21]</ref>-A, <ref type="bibr" target="#b8">[9]</ref> and <ref type="bibr" target="#b27">[28]</ref> that use the ground-truth data. However, ACO significantly outperforms <ref type="bibr" target="#b20">[21]</ref>-T, <ref type="bibr" target="#b8">[9]</ref> and <ref type="bibr" target="#b27">[28]</ref>, and provides comparable performance to <ref type="bibr" target="#b20">[21]</ref>-A. <ref type="figure" target="#fig_13">Figure 7</ref> shows examples of the multiple primary object segmentation results of the proposed ACO algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>We proposed a novel unsupervised video object segmentation algorithm. We first defined a hybrid of the Markov, spatiotemporal, and antagonistic energies, and then minimized the hybrid energy to delineate a primary object. To minimize the nonconvex hybrid energy, we developed the ACO scheme, which optimized the foreground and background distributions alternately by solving quadratic programs. We also proposed the forward-backward strategy to yield temporally consistent segmentation results. Experiments showed that the proposed ACO algorithm outperforms the state-of-the-art techniques significantly.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>Overview of the proposed ACO algorithm. In the initial estimation and the alternate convex optimization, both foreground and background distributions are determined. However, only foreground distributions are shown here.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>query vector in (2), we obtain the ranking vector r</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 .</head><label>2</label><figDesc>Computation of initial probability distributions: r in (d) is the background distribution, when uniform query amounts are assigned to all boundary nodes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 .</head><label>3</label><figDesc>(a) Input points, (b) stationary distribution, (c) left cluster, (d) right cluster, (e) uniform distribution, and (f) random distribution. Note that each distribution is normalized, and high probabilities are depicted by highly saturated red colors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 .</head><label>4</label><figDesc>The</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>i denotes the set of the neighbors of node i and w (τ )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 5 .</head><label>5</label><figDesc>The resulting distributions of minimizing various energy combinations on "Cliff Diving." The upper and lower rows are the foreground and background distributions, respectively. Yellow boundaries are the outlines of the ground-truth object. The images are cropped for better visualization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>is the prior probability of the foreground at frame τ . Also, p(x i |ω(τ ) b ) and p(ω (τ ) b ) are similarly defined. Then, we compute the posterior by</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 6 .</head><label>6</label><figDesc>Comparison of video object segmentation results. Segmentation boundaries are depicted in red or green. Each sub-figure shows the results of</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 7 .</head><label>7</label><figDesc>Multiple primary object segmentation results of the proposed ACO algorithm. The boundaries of the two primary objects are depicted in yellow and green, respectively. The frames are from the "BMX" in SegTrack v2<ref type="bibr" target="#b22">[23]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>3(e) and (f) yield much higher energies. In a feature space, the left and right clusters in Figures 3(c) and (d) may correspond to the foreground and background, respectively. We can make the foreground and background distributions form such separate clusters, by minimizing the spatiotemporal energy E S and the antagonistic energy E A , in addition to the Markov energy E M . Let us describe the additional energies E S and E A subsequently.</figDesc><table>Spatiotemporal Energy: For each frame, the initial distri-

bution r 

(τ ) 
f 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>these two distributions should have high probabilities at mutually exclusive regions. In other words, they should not have high probabilities at the same region. To formulate this mutual exclusiveness between p b , we define the antagonistic energy E A as</figDesc><table>Antagonistic Energy: We segment a foreground object 
from its background by comparing the foreground and 
background distributions, p 

(τ ) 
f 

and p 

(τ ) 

b . For reliable and 
accurate segmentation, (τ ) 
f 

and p 

(τ ) 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 1 .</head><label>1</label><figDesc>The average numbers of mislabelled pixels per frame on the SegTrack dataset<ref type="bibr" target="#b32">[33]</ref>. Lower values are better. The best and the second best results are boldfaced and underlined, respectively.</figDesc><table>Unsupervised -Single 
Unsupervised -Multiple 
Supervised 
Video (Number of frames) 
ACO [34] 
[29] 
[37] 
[25] 
[14] 
[23] 
[28] 
[5] 
[21] 
[9] 
[33] 
[11] 

SegTrack 

Birdfall (30) 
144 
209 
217 
155 
189 
278 
199 
468 
606 
288 
468 
252 
454 
Cheetah (29) 
617 
796 
890 
633 
806 
824 
599 1175 11210 905 1968 1142 1217 
Girl (21) 
1195 1040 3859 1488 1598 1029 1164 5683 26409 1785 7595 1304 1755 
Monkeydog (71) 
354 
562 
284 
365 
472 
192 
322 1434 12662 521 1434 563 
683 
Parachute (51) 
200 
207 
855 
220 
221 
251 
242 1595 40251 201 1113 235 
502 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="true"><head>Table 2 .</head><label>2</label><figDesc>Comparison of IoU scores. Higher values are better. The best and the second best results are boldfaced and underlined.</figDesc><table>Video (Number of frames) 
[29] 
[37] 
ACO 

SegTrack 

Birdfall (30) 
3.88 
71.98 73.29 
Cheetah (29) 
44.95 
65.48 
64.23 
Girl (21) 
56.83 81.46 
86.75 
Monkeydog (71) 
72.62 
72.06 
76.12 
Parachute (51) 
85.61 94.47 
94.68 
Bird of Paradise (98) 
83.46 
27.02 
93.92 

SegTrack 
Frog (279) 
65.20 72.00 
81.58 

v2 
Monkey (31) 
69.28 
63.28 63.96 
Soldier (32) 
46.48 
39.57 
36.84 
Worm (243) 
73.62 
44.59 61.79 
Bike Rampage (347) 
21.82 
0.59 
46.13 
Bike Riding (631) 
34.87 73.80 
77.41 
Cliff Diving (104) 
60.28 77.82 
84.56 
Floor Exercise (114) 
15.65 
14.28 
61.84 
Long Jump (84) 
60.14 78.14 
79.85 

VidSeg 
Tennis (679) 
33.89 45.81 
56.08 
White Bird (628) 
63.49 78.86 
79.60 
Wolf (362) 
17.15 
78.30 
75.91 
Hulk (160) 
65.18 
31.14 
78.82 
Jurassic Park (118) 
50.79 
83.60 
37.97 
Silver Surfer (100) 
69.88 
0.68 
75.02 
Spider Man (118) 
32.24 37.54 
58.84 
Average 
51.24 56.02 
70.24 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 .</head><label>3</label><figDesc>Average IoU score after each proposed step.Table 4. Average IoU scores of the proposed ACO algorithm using various energy combinations.</figDesc><table>Initial 
Forward 
Backward 
Pixel-wise 
Average 
44.31 
64.80 
67.71 
70.24 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head></head><label></label><figDesc>Video (Number of frames) [9] [21]-T [21]-A [28] Ours</figDesc><table>SegTrack 
BMX (36) 
4.90 37.25 63.76 4.90 55.42 

v2 
Drifting Car (74) 59.04 35.52 50.91 21.49 57.72 
Hummingbird (29) 32.42 31.46 44.28 27.46 35.92 
Average 
32.12 34.74 52.98 17.95 49.69 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">SLIC superpixels compared to state-of-the-art superpixel methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lucchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Susstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2274" to="2282" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">What is an object? In CVPR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Alexe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="73" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Video SnapCut: robust video object cutout using localized classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Simons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graphics</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">70</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Video object segmentation by salient segment chain composition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Banica</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agape</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCVW</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="283" to="290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">ViBe: A universal background subtraction algorithm for video sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Barnich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">V</forename><surname>Droogenbroeck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1709" to="1724" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Convex Optimization. Cambridge University Press</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Vandenberghe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Video object segmentation by tracking regions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Brendel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Todorovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="833" to="840" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">The anatomy of a large-scale hypertextual web search engine. Computer Networks and ISDN Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Brin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Page</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="107" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Object segmentation by long term analysis of point trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="282" to="295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Constrained parametric min-cuts for automatic object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="3241" to="3248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Adaptive fragments-based tracking of non-rigid objects using level sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chockalingam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Pradeep</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Birchfield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">R K</forename><surname>Chung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Spectral Graph Theory. American Mathematical Society</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Category independent object proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Endres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="575" to="588" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Superpixel-based video object segmentation using perceptual organization and location prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Giordano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Murabito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Palazzo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Spampinato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Random walks for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Grady</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1768" to="1783" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Graph implementations for nonsmooth convex programs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Grant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Boyd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Recent Advances in Learning and Control</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="95" to="110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">CVX: Matlab software for disciplined convex programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Grant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Boyd</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Efficient hierarchical graph-based video segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Grundmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kwatra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Essa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="2141" to="2148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Density-based multifeature background subtraction with support vector machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1017" to="1023" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multiple random walkers and their application to image cosegmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-D</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y.</forename><surname>Sim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-S</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Key-segments for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Optimal image and video closure by superpixel grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Levinshtein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dickinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="99" to="119" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Video segmentation by tracking many figure-ground segments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Humayun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="2192" to="2199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Beyond Pixels: Exploring New Representations and Applications for Motion Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
		<respStmt>
			<orgName>Massachusetts Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Maximum weight cliques with mutex constraints for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Latecki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning segmentation by random walks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Meila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="873" to="879" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Object segmentation in video: A hierarchical variational approach for turning point trajectories into dense regions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ochs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1583" to="1590" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Higher order motion models and spectral clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ochs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="614" to="621" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Fast object segmentation in unconstrained video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Papazoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Livecut: Learningbased interactive video segmentation by evaluation of multiple propagated cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">L</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">S</forename><surname>Morse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="779" to="786" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Motion segmentation and tracking using normalized cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="1154" to="1160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Causal video object segmentation from persistence of occlusions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Karasev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4268" to="4276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Motion coherent tracking using multi-label MRF optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Flagg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nakazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Saliency-aware geodesic video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Geodesic saliency using background priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="29" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Saliency detection via graph-based manifold ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3166" to="3173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Video object segmentation through spatially accurate and temporally dense extraction of primary object regions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Javed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Ranking on data manifolds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="169" to="176" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
