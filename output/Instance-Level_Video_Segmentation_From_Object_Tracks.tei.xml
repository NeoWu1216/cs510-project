<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Instance-level video segmentation from object tracks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Seguin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">1É cole Normale Supérieure 2 Inria</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">1É cole Normale Supérieure 2 Inria</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rémi</forename><surname>Lajugie</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">1É cole Normale Supérieure 2 Inria</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">1É cole Normale Supérieure 2 Inria</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Instance-level video segmentation from object tracks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:41+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We address the problem of segmenting multiple object instances in complex videos. Our method does not require manual pixel-level annotation for training, and relies instead on readily-available object detectors or visual object tracking only. Given object bounding boxes at input, we cast video segmentation as a weakly-supervised learning problem. Our proposed objective combines (a) a discriminative clustering term for background segmentation, (b) a spectral clustering one for grouping pixels of same object instances, and (c) linear constraints enabling instance-level segmentation. We propose a convex relaxation of this problem and solve it efficiently using the Frank-Wolfe algorithm. We report results and compare our method to several baselines on a new video dataset for multi-instance person segmentation. * WILLOW project-team, Département</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Semantic object segmentation in images and videos is a challenging computer vision task <ref type="bibr" target="#b22">[24,</ref><ref type="bibr" target="#b27">29,</ref><ref type="bibr" target="#b29">31,</ref><ref type="bibr" target="#b37">39,</ref><ref type="bibr" target="#b43">45]</ref>. Common difficulties arise from frequent occlusions <ref type="bibr" target="#b41">[43]</ref> and background clutter, as well as variations in object shape and appearance. Video object segmentation also requires accurate tracking of object boundaries over time in the presence of possibly fast and non-rigid motions. An additional challenge addressed by several recent works is in segmentation of individual instances of the same object class <ref type="bibr" target="#b16">[18,</ref><ref type="bibr" target="#b17">19,</ref><ref type="bibr" target="#b43">45,</ref><ref type="bibr" target="#b42">44,</ref><ref type="bibr" target="#b49">51]</ref>. Indeed, while it may be easy to segment a herd of cows from a grass field, segmenting each cow separately is a much harder task.</p><p>Instance-level object segmentation in video is an interesting and understudied problem at the intersection of semantic and motion-based video segmentation. Solutions to this problem can benefit from class-specific object models and motion cues. Segmentation of static and/or partially occluded objects of the same class, however, pose additional challenges, difficult to solve with existing methods of <ref type="figure">Figure 1</ref>: Results of our method applied to multi-person segmentation in a sample video from our database. Given an input video together with the tracks of object bounding boxes (left), our method finds pixel-wise segmentation for each object instance across video frames (right). motion-based and semantic segmentation. Meanwhile, successful solutions to instance-level video segmentation can serve in several tasks such as video editing and dynamic scene understanding.</p><p>Given recent advances in object detection <ref type="bibr" target="#b34">[36]</ref> and visual object tracking <ref type="bibr" target="#b9">[11]</ref>, coarse object localization in the form of object bounding boxes can now be used as input for solving other problems. In particular, we address in this paper the problem of instance-level video segmentation given object tracks. We assume that prior (weak) knowledge about objects is available in the form of tracked object bounding boxes, obtained by a separate process. For instance, pretrained object detectors or visual object tracking algorithms as the ones cited above can be used.</p><p>Segmentation methods typically optimize carefully designed objective functions combining data terms and prior knowledge. Object prior knowledge in such methods is often encoded by higher-order potentials <ref type="bibr" target="#b24">[26,</ref><ref type="bibr" target="#b25">27,</ref><ref type="bibr" target="#b36">38]</ref>, which enable richer modeling but lead to hard optimization problems. Here we take an alternative approach and build on the discriminative clustering framework <ref type="bibr" target="#b1">[3,</ref><ref type="bibr" target="#b15">17]</ref>. Following previous work on co-segmentation <ref type="bibr" target="#b22">[24]</ref> and weakly-supervised classification <ref type="bibr" target="#b4">[6]</ref>, we formulate our problem as a quadratic program under linear constraints. We use object tracks as constraints to guide segmentation, but other forms of prior knowledge could easily be integrated in our method. Our final segmentation is obtained by solving a convex relaxation of our objective with the Frank-Wolfe algorithm <ref type="bibr" target="#b13">[15]</ref>.</p><p>We compare our method to the state of the art and show competitive results on a new dataset for instance-level video segmentation. In contrast to most previous methods, our approach segments multiple instances of the same object class and supports reasoning about occlusions. <ref type="figure">Figure 1</ref> illustrates results of our method on a video from our dataset.</p><p>The contributions of this paper are three-fold. (i) We propose a discriminative clustering approach for instance-level video segmentation using external guidance in the form of object bounding boxes. (ii) We introduce a new dataset for multiple person segmentation with challenging sequences, including self-occlusions, crowded scenes and varied poses.</p><p>(iii) We demonstrate the high accuracy and flexibility of our model on the task of multi-instance person segmentation in video.</p><p>The rest of the paper is organized as follows. We discuss related work in Section 2 and then present our problem formulation in Section 3. We describe the convex relaxation of our model and the optimization of the cost function with the Frank-Wolfe algorithm in Section 4. The new dataset is presented in Section 5. Finally, Section 6 presents our experimental setup and results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Segmentation of multiple objects in video has been addressed for example in <ref type="bibr" target="#b8">[10,</ref><ref type="bibr" target="#b12">14,</ref><ref type="bibr" target="#b21">23]</ref>. Typical approaches include (a) pure color-or motion-based segmentation <ref type="bibr" target="#b26">[28,</ref><ref type="bibr" target="#b32">34,</ref><ref type="bibr" target="#b41">43]</ref>, for instance using long term tracks <ref type="bibr" target="#b28">[30,</ref><ref type="bibr" target="#b30">32]</ref>, (b) tracking segmentation proposals through the entire video <ref type="bibr" target="#b8">[10,</ref><ref type="bibr" target="#b29">31]</ref> or (c) learning instance-specific object appearance models <ref type="bibr" target="#b12">[14]</ref>. Deep neural networks can be used on each frame to perform high quality semantic segmentation <ref type="bibr" target="#b50">[52]</ref> or multiinstance segmentation for a specific class and setup, for instance to segment cars in images recorded by a car-mounted camera <ref type="bibr" target="#b49">[51]</ref>.</p><p>Prior information, such as object bounding boxes or pose estimation can be used to guide the segmentation. Person and body-part detectors as well as skin color models have been used to seed the GrabCut algorithm <ref type="bibr" target="#b18">[20]</ref>, as unary potentials in CRFs <ref type="bibr" target="#b27">[29,</ref><ref type="bibr" target="#b35">37]</ref> or as higher order terms <ref type="bibr" target="#b25">[27,</ref><ref type="bibr" target="#b43">45]</ref>. Parameters of semantic segmentation models can be estimated using bounding boxes instead of exact ground-truth segmentations <ref type="bibr" target="#b31">[33]</ref>. Pose estimation has also been used as a unary term for pixel-wise segmentation in 3D movies <ref type="bibr" target="#b36">[38]</ref>, although erroneous pose estimation will cause false segmentations. The use of object detectors as weak cues for semantic video segmentation been explored in <ref type="bibr" target="#b48">[50]</ref>. Multiple forms of weak annotations, such as imagelevel tags, bounding boxes and incomplete pixel-level labels, have been combined for semantic segmentation <ref type="bibr" target="#b45">[47]</ref>. A recently proposed task, simultaneous detection and segmentation <ref type="bibr" target="#b16">[18]</ref>, closes the gap between object detection and segmentation. In our work, bounding boxes are used to constrain the set of possible segmentations instead of being involved in the energy function.</p><p>Our formulation of the segmentation problem is related to discriminative clustering <ref type="bibr" target="#b1">[3,</ref><ref type="bibr" target="#b15">17]</ref>. The idea is to jointly Video frames and superpixels t t + 1 <ref type="figure">Figure 2</ref>: Spatio-temporal graph of superpixels. For the yellow superpixel, spatial edges are shown in dark blue and temporal edges in dark green.</p><p>partition the data and learn a discriminative model for each cluster seen as a class. Discriminative clustering has recently been applied to several problems including image co-segmentation <ref type="bibr" target="#b22">[24,</ref><ref type="bibr" target="#b23">25]</ref>, object co-localization in images <ref type="bibr" target="#b40">[42]</ref>, finding actor identities in movies <ref type="bibr" target="#b4">[6,</ref><ref type="bibr" target="#b33">35]</ref> and temporal action localization <ref type="bibr" target="#b5">[7]</ref>. Each of these techniques is built upon a task-dependent set of constraints, modeling simple assumptions and encoding prior knowledge. We use a similar framework for video segmentation and define an original set of constraints that are well suited to our problem.</p><p>We build upon these works and combine a grouping model, a foreground-background model and weak priors. Our proposed method is experimentally compared to several previous approaches <ref type="bibr" target="#b16">[18,</ref><ref type="bibr" target="#b30">32,</ref><ref type="bibr" target="#b32">34,</ref><ref type="bibr" target="#b36">38,</ref><ref type="bibr" target="#b50">52]</ref> in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Problem formulation</head><p>The segmentation problem we aim to solve is to assign to every pixel a label in {0, . . . , K}. To design a suitable cost function, we follow previous work on cosegmentation <ref type="bibr" target="#b22">[24,</ref><ref type="bibr" target="#b23">25]</ref>. This implies using two complementary cost functions: the first one is a spectral clustering term <ref type="bibr" target="#b37">[39]</ref>, which enforces spatial and temporal consistencies according to some descriptors φ. The second term is a discriminative clustering cost based on the square loss <ref type="bibr" target="#b1">[3]</ref> which learns a foreground vs. background classifier. In order to include prior information, we propose several constraints which we detail in Section 3.4. The proposed constraints are linear, leading to a tractable (relaxed) optimization problem (see Section 4).</p><p>The intuition behind our approach is that constraints provide weak localization cues for each object instance. Discriminative clustering separates foreground objects from the background based on appearance features. Spectral clustering helps producing clean spatial boundaries, separating different instances of the same class and smoothing the segmentation in time for each object instance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Notations and model</head><p>We are given a video clip composed of T frames indexed by t. Our problem is to assign a label k in {0, 1, . . . , K} to each pixel in each frame, where label k = 0 corresponds to the background and all other integers in {1, . . . , K} correspond to the K object instances in the video. Since the number of pixels in a video is usually high, we propose to work with superpixels instead. Assuming that there are N superpixels in the whole video, we index them by n in {1, . . . , N }.</p><p>Let us define a label matrix y in {0, 1} N ×(K+1) . The matrix y is such that y nk is equal to one if and only if the superpixel n is of label k. This matrix sums up to one along rows, since every superpixel is assigned to a single label. In Section 3.4, we propose several constraints that will restrain the set of admissible matrices y. We denote by Y this set. The constraints can be indexed by c in {1, . . . , C}. Since some of them may not be satisfied, for every constraint c, we define a slack variable ξ c which will allow us to violate it. Let ξ be the concatenation of all the ξ c into a single vector. We denote by C(y, ξ) the set of constraints over a specific y with slack ξ.</p><p>The cost we minimize is a sum of three terms: a grouping term E G , a discriminative term E D , and a term penalizing the slack ξ:</p><formula xml:id="formula_0">min y∈Y, ξ∈R C + E G (y) + α E D (y) + β ξ 2 ,<label>(1)</label></formula><p>under linear constraints C(y, ξ), where α and β allow us to weigh the different terms. We provide a detailed description of these terms in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Grouping term</head><p>The grouping term E G is a classic spectral clustering term meant to ensure spatial and temporal consistency of the segmentation. To this end, we define a superpixel graph G = (S, E), whose nodes correspond to superpixels and edges encode spatio-temporal neighborhood information. A sample graph G is illustrated in <ref type="figure">Fig. 2</ref>. For two nodes n and n ′ from the same frame, there is an edge (n, n ′ ) in E if the two superpixels are spatial neighbours. For node n in frame t and node n ′ in frame t + 1, we add an edge (n, n ′ ) to E if n and n ′ are temporal neighbours. The exact way we define neighbourhoods is discussed in Section 6.1.</p><p>For each superpixel n, we define a set of descriptors φ i n indexed by i in {1, . . . , I}. We denote by d i the dimension of φ i n and by d the sum of all the d i . Let us denote by φ n the concatenation of all the φ i n . We then define the similarity matrix W in R N ×N which encodes the similarities between superpixels:</p><formula xml:id="formula_1">W nn ′ = I i=1 µ i exp(−λ i φ i n −φ i n ′</formula><p>2 ) if (n, n ′ ) ∈ E and 0 otherwise. µ i and λ i are weighting parameters for the i-th descriptor.</p><p>Following <ref type="bibr" target="#b37">[39]</ref>, we define the associated unnormalized Laplacian matrix L = D − W . D is the diagonal matrix composed of the row sums of W : D = Diag(W 1 N ). Using these definitions, the grouping term can be written as the following quadratic form:</p><formula xml:id="formula_2">E G (y) = 1 N Tr(y T Ly).<label>(2)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Discriminative term</head><p>E D is a standard discriminative clustering term, which aims to learn an affine classifier for separating foreground vs. background. Let M be a binary matrix in {0, 1} (K+1)×2 which maps labels to foreground and background. Let us denote by w ∈ R d×2 and b in R 2 the parametrization of this model. We also define the matrix Φ in R N ×d whose rows are the φ n . The discriminative cost is defined as follows:</p><formula xml:id="formula_3">E D (y) = min w∈R d×2 b∈R 2 1 N yM −Φw−1 N b T 2 F +κ||w|| 2 F . (3)</formula><p>The minimization w.r.t. w in <ref type="formula">(3)</ref> is a ridge regression problem, whose solution can be found in closed form, and E D is easily rewritten <ref type="bibr" target="#b22">[24]</ref> as a quadratic form in y:</p><formula xml:id="formula_4">E D (y) = 1 N Tr(M T y T AyM ),<label>(4)</label></formula><formula xml:id="formula_5">where A = 1 N Π N (I N − Φ(Φ T Π N Φ + N κI d ) −1 Φ T )Π N and Π N is the centering projection matrix I N − 1 N 1 N 1 T N .</formula><p>Note that the foreground vs. background model we learn is fit for segmenting the background from multiple instances of the same object category. If needed, we could easily learn one model per object category by adapting the M matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Constraints</head><p>As mentioned earlier, our model incorporates constraints on the y matrix. They allow us to encode simple priors as well as more complicated, instance-specific information. We can constrain the number of superpixels assigned to a given label in a spatio-temporal region using linear inequalities. We can also use strict equality constraints to fix the labels of some superpixels. We first provide a general form and then describe the different variants used in our experiments. Some of them are also illustrated in <ref type="figure">Fig. (</ref>3) for multi-instance person segmentation using head and fullbody tracks.</p><p>Object tracks. We assume that we are given a track of bounding boxes for each object in the video. We denote by B this set and index the elements B of B by k in {1, . . . , K} and t in {1, . . . , T }, such that B t k denotes the bounding box of the k-th object in frame t.</p><p>Inequality constraints. We want to impose linear inequality constraints on a set of superpixels in the video. In the following sections we will describe in details what these sets can correspond to. For now, let us denote by R a subset of the indices of superpixels, R ⊂ {1, . . . , N }. We can represent R by the indicator vector 1 R , such that the n-th entry is equal to one if the superpixel n is in R. Note that  In this setup we are provided head detections, from which we derive body boxes. We require 75% of pixels inside head detections (a) and 50% of pixels inside body boxes (b) to belong to the instance. Part (c) illustrates the background constraint (96% of this surface should be background); non-person constraints which enforce superpixels far from the person to be assigned to the corresponding label (d) ; and the superpixels which can only be background (e).</p><p>for videos, this set R can correspond to a spatio-temporal region. We use the notation e k to denote the k-th vector of the canonical basis of R K+1 . For some region R and a label k, we propose to constrain the matrix y using constraints of the following form:</p><formula xml:id="formula_6">0 ≥ σ 1 T R y e k − ρ − ξ c ,<label>(5)</label></formula><p>where σ ∈ {−1, 1} controls whether this is an at least or an at most constraint, ρ a parameter and ξ c is the slack variable allowing this constraint to be violated. The parameters R, σ, k and ρ depend on the kind of prior we want to enforce. Note that while our notations refer to superpixels and counts of superpixels, in practice we weigh the contribution of each superpixel to the constraint by its relative area in region R. Likewise, we reason in terms of pixels when computing the ρ parameters.</p><p>Equality constraints. When some supervision is available (semi-supervised setting), or when a strong cue allows us to freeze variables, we want to use equality constraints. Let us suppose that we have a set of superpixels R and a set of labels Q. We set variables for region R and labels Q to predetermined values stored inỹ: ∀r ∈ R, ∀q ∈ Q, y rq =ỹ rq .</p><p>As for the inequality constraints, the definitions of R, Q and y depend on the prior. Track constraints. Given an object bounding box B t k , we require that at least ρ B superpixels inside B t k get assigned the label k. This can be enforced by setting R, and σ appropriately in Eq. (5). We set R to the set of superpixels that lie inside B t k . Since this is an at least constraint, we set σ = −1. The amount of superpixels ρ B is set to a ratio of the total number of superpixels in B t k . In <ref type="figure" target="#fig_1">Figure 3</ref>  In complex videos picturing multiple objects, the bounding boxes, and thus the corresponding constraint regions, can heavily overlap. Without slack variables, our problem may be infeasible in such situations, and even with slack variables the constraints may still be misleading. To cope with occlusions, we propose a simple occlusion reasoning. In a given frame, for each pair of overlapping bounding boxes, pixels inside the region of overlap are marked as occluded. In turn, we reduce the strength of each such con-straint by multiplying ρ B <ref type="figure">by (1 − o)</ref> where o is the ratio of occluded pixels in the bounding box.</p><p>Area constraints. To reduce "leaking" effects in the segmentation, we constrain the area of each object segment in each frame. For object k in frame t, we impose that at most ρ area of the superpixels in frame t get assigned the label k. This can be expressed by setting R to be the set of superpixels in frame t. Since this is an at most constraint, we have σ = +1. We set ρ area to the amount of superpixels in track B t k times a constant, to take object size into account. We can also enforce a minimal amount of superpixels per label and per frame. We do so by changing σ to −1 and setting an appropriate ρ. This constraint can be used if we know the object is in the frame but lack the corresponding bounding box.</p><p>Background constraints. We request that most superpixels which are outside object bounding boxes belong to the background label. The rationale is that only a few of the superpixels outside object detections may belong to objects, as shown in <ref type="figure" target="#fig_1">Figure 3</ref> (c). Typically, in the case of multiple people segmentation, these superpixels belong to lower arms. We express this constraint by setting R to the set of superpixels that do not belong to any track in frame t. This is an at least constraint so we set σ = −1. We set ρ = ρ bg to a ratio of the cardinality of R.</p><p>Non-object constraints. In our work, we make the assumption that if a pixel is far enough from an object detection, it is reasonable to assume that it does not belong to the corresponding object. We assume that when there are no detections at all, we do not apply these constraints. For a bounding box B t k , we build R as the set of superpixels in frame t that are further away from B t k than a given distance, as shown in <ref type="figure" target="#fig_1">Figure 3 (d)</ref>. In practice, we set this minimum distance to the width of the object bounding box. R can be computed by performing a distance transform and then thresholding. We then enforce an equality constraint with Q containing only the label k andỹ filled with zeros.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Optimization</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Continuous relaxation</head><p>The quadratic problem defined in Eq. (1) is known to be NP hard when y takes binary values. Indeed, when the quadratic cost matrix has positive off diagonal entries, this is as hard as solving a max-cut problem. Classic relaxations of such problems <ref type="bibr" target="#b22">[24]</ref> imply working with equivalence matrices Y = yy T . Doing so in our case would be intractable due to the problem size and would prevent us from imposing constraints relating superpixels to labels. Instead, we propose a continuous relaxation of our problem by solving it over the convex hull Y of the initial set Y. Then, we aim at solving the minimization of a positive semi-definite quadratic form over a convex compact set defined by a large number of linear constraints. Due to the size of y (of the order of 10 6 entries) and the number of constraints it is not realistic to use a standard off-the-shelf quadratic programming solver based on interior point methods <ref type="bibr" target="#b6">[8]</ref>. Nevertheless it is possible to solve linear programs of such a size. This is why, following other approaches to discriminative clustering <ref type="bibr" target="#b5">[7]</ref>, we propose to use the Frank-Wolfe optimization algorithm <ref type="bibr" target="#b13">[15,</ref><ref type="bibr" target="#b20">22]</ref> which only relies on the minimization of linear forms over Y.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Frank-Wolfe algorithm</head><p>The Frank-Wolfe algorithm is an iterative method to optimize convex objectives over compact convex sets and suites well for our problem. Let us now briefly describe the iterations. We define our optimization variable z = (y, ξ) in Z = Y × R C + . For the sake of simplicity, we rewrite as E(z) the sum of the three terms from Eq. (1). Let us denote by z k the current point at iteration k. At iteration k, we compute the gradient ∇ z E(z k ) and minimize the following linear form: Tr(∇ z E(z k )(z − z k )). This can be easily done using a generic LP solver, and yields a corner of the polytope that we will denote z FW . We then update the current point as follows: z k+1 = z k +γ(z FW −z k ). The optimal parameter γ * leading to the best improvement in that direction can be found in closed form by doing an exact line search.</p><p>Rounding. Using the Frank-Wolfe algorithm we obtain a solution z * = (y * , ξ * ). The solution continuous solution we obtain needs to be rounded. We first freeze the slack variables of the constraints to the values ξ * . We then round y * into a binary matrix by finding the closest point to y * in Y in terms of Frobenius norm y−y * 2 F which is equivalent to min y∈Y −2Tr(y * T y).</p><p>We solve this linear program using the LP solver.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Non-convex refinement</head><p>Experimentally, we observe that the convex relaxation of our problem may lead to sub-optimal rounded solutions. Indeed, our model is attracted to a degenerate solution with all constant entries of value 1 K+1 , which has a low objective value for the discriminative term. This is a common drawback of discriminative clustering techniques, as noted by <ref type="bibr" target="#b22">[24,</ref><ref type="bibr" target="#b15">17]</ref>. In order to push our solution away from these near-constant solution, and following the approach of graduated non-convexity <ref type="bibr" target="#b3">[5,</ref><ref type="bibr" target="#b47">49]</ref>, we propose to add a concave quadratic term to our objective: Tr(y T (1 − y)), and weight it using a parameter δ. This term encourages the entries y to be close to either 0 or 1. The corresponding optimization problem is the following: min y∈Y, ξ∈R+ C E G (y) + αE D (y) + β ξ 2 + δ Tr(y T <ref type="figure">(1 − y)</ref>).</p><p>The parameter δ can be a function of the iteration count k. In practice however, choosing a scalar value is already complicated and we therefore use a piecewise constant function. We first optimize the convex relaxation of our problem with δ = 0. Then we perform Frank-Wolfe steps on the nonconvex objective with a non-zero δ which has been selected by parameter search. Although we are only guaranteed to converge to a local optimum of this non-convex function [4, Section 2.2.2], we empirically observe a drastic improvement of performance as shown in <ref type="table" target="#tab_0">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Dataset</head><p>To evaluate the performance of our method on the task of instance-level video segmentation, we have collected a dataset composed of 27 video clips, corresponding to a total of 2476 frames. The video clips are taken from the 3D feature movie "StreetDance 3D" [Giwa and Pasquini, 2010]. The proposed dataset is an improved version of the Inria 3D Movie Dataset <ref type="bibr" target="#b36">[38]</ref> adding a substantial amount of challenges, such as longer shots, self-occlusions, inter-person occlusions, and hard poses such as dancing or jumping.</p><p>Providing ground-truth annotations for evaluation in an entire video is a highly time-consuming task. As a consequence, we have only annotated a sparse subset of 235 frames out of 2476, for all 632 person instances present in these frames. We split the dataset into a set of 7 clips for adjusting hyperparameters and a set of 20 clips for evaluation. Note that there is no training step in our method, but only a validation step to find appropriate hyperparameters. Our dataset and code, including the procedure for adjusting parameters using the Bayesian framework of <ref type="bibr" target="#b39">[41]</ref>, are available on the project website [1].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experiments</head><p>In this section, we describe experimental details and evaluation procedures for the proposed method. We evaluate multi-instance person segmentation in 3D movies using head tracks and full-body bounding boxes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Implementation details</head><p>Superpixels. We extract video superpixels using <ref type="bibr" target="#b7">[9]</ref>. The superpixels are evenly distributed, fairly compact, and tracked in time. We use temporal links obtained from superpixel tracks as edges in the superpixel graph (Section 3.2). We also add edges between superpixels from consecutive frames if sufficient pixel-wise correspondence is provided Features. We first compute dense optical flow between consecutive frames using DeepFlow <ref type="bibr" target="#b44">[46]</ref>. Then, we use two different sets of features φ n for the grouping and discriminative terms. These features are computed for each superpixel based on the underlying image pixels. For the spatial edges in the similarity matrix W of the grouping term, we use: (i) a histogram of optical flow with 8 bins for orientations and one bin for no motion, and (ii) the average CIE L*a*b* color, over the superpixel. For the temporal edges of W , we use the average CIE L*a*b* color. As discriminative features in Φ, we use: (i) the same histogram of optical flow, (ii) a color histogram computed over RGB colors, with 8 bins per color channel, 512 bins in total, and (iii) the average SIFT descriptor over the superpixel, obtained by first computing dense SIFTs over the whole image, and then averaging the SIFTs which cover the superpixel.</p><p>We also optionally exploit recent advances in semantic segmentation by including features produced by a deep neural network trained for semantic segmentation for the PAS-CAL dataset <ref type="bibr" target="#b50">[52]</ref>. We take the output of the method for each pixel and pool it (either using max-pooling or meanpooling) over the superpixel, and use it as an additional discriminative feature in Φ. As this output represents a strong semantic cue, it should help our discriminative term to separate the foreground from the background.</p><p>For 3D movies, we also include median disparity over the superpixel in both spatial grouping and discriminative features. The method of Ayvaci et al. <ref type="bibr" target="#b0">[2]</ref> is used to estimate the disparity map from stereo pairs.</p><p>Person detection and tracking. We evaluate our method on ground-truth (manually annotated) head tracks as well as on tracks automatically produced by a trackingby-detection method: we use a CNN-based detector <ref type="bibr" target="#b14">[16]</ref> trained on heads in movies. The tracker associates these detections based on KLT tracks <ref type="bibr" target="#b38">[40]</ref>, interpolates missing detections and smooths the tracks in time <ref type="bibr" target="#b10">[12]</ref>. Using ground-truth or automatic tracks, we extrapolate full-body bounding boxes from the head bounding boxes using a linear transformation. Note that our full-body bounding boxes start below the head, as shown in <ref type="figure">Fig. 1</ref>. This way, the superpixels on the sides of the head are not involved in the corresponding constraints, since they do not belong to the person in most cases. Occlusion reasoning. We adapt the occlusion reasoning of Section 3.4 to stereo videos by computing a depth estimate from the median disparity inside the head box. Given two overlapping bounding boxes in the frame, we mark the pixels of the object which is behind as occluded. This procedure allows a more accurate handling of occlusions than the original reasoning, since constraint strength will only be reduced for objects which may actually be occluded.</p><p>We evaluate the proposed method on stereo videos where head (bounding boxes) tracks for multiple people are given as input to our algorithm. We use these tracks and extrapolated full-body bounding boxes, to derive two types of track constraints in our framework. We also integrate the corresponding background and non-object constraints from Section 3.4. We combine disparity, appearance and motion cues and evaluate performance on a new dataset extracted from 3D movies with challenging scenes and poses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Baselines</head><p>We compare our method to multiple baselines, spanning the whole range of methods from pure semantic segmentation to pure motion segmentation. Some of them are completely unsupervised: Multi-modal motion segm. <ref type="bibr" target="#b30">[32]</ref>, FG/BG motion segm. <ref type="bibr" target="#b32">[34]</ref>. Some other require pixel-wise supervision to train appearance models: Pose &amp; segm. <ref type="bibr" target="#b36">[38]</ref>, SDS <ref type="bibr" target="#b16">[18]</ref>, CRF as RNN <ref type="bibr" target="#b50">[52]</ref>. We used the publicly available code and models for all methods.</p><p>CRF as RNN <ref type="bibr" target="#b50">[52]</ref> 1 is the state-of-the-art semantic segmentation method. It uses an end-to-end deep network combining a standard Convolutional Neural Network with a Recurrent Neural Network to perform dense CRF inference. We adapt this method to the task of instance-level segmentation for a given semantic class by assigning each pixel labelled with the said semantic class to the instance which has the closest bounding box. In practice, for humans we assign the pixels to the person which spine (derived from the head bounding box) is the closest.</p><p>SDS <ref type="bibr" target="#b16">[18]</ref> 2 is a simultaneous detection and segmentation method. It classifies region proposals by scoring CNN features extracted from the region and the corresponding bounding box. <ref type="bibr" target="#b16">[18]</ref> is an instance-level segmentation method, and we evaluate it directly. Since <ref type="bibr" target="#b16">[18]</ref> uses its own set of detections, we use the same set of detection within our method when comparing results with SDS.</p><p>Pose &amp; segm. <ref type="bibr" target="#b36">[38]</ref> 3 is based on multi-class graph cuts, has been designed for a similar dataset, and uses the same set of features. Given person tracks, it combines pose estimates and disparity cues in an unary term after reasoning on occlusions. A binary term encodes spatio-temporal smoothness using color and motion features.</p><p>Multi-modal motion segm.</p><p>[32] 4 separates objects which exhibit different motions. It is a classic method for video segmentation. We adapt it to our problem by assigning the biggest segment (in terms of surface) to be the background segment, and inside each object bounding box we label the largest non-background segment as belonging to the instance.</p><p>FG/BG motion segm <ref type="bibr" target="#b32">[34]</ref> 5 is a pure figure-ground motion segmentation method. We adapt it to the task of instance-level segmentation using the same method as for the first baseline, by splitting the foreground segment in multiple segments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Results</head><p>We evaluate segmentation by computing per-person precision, recall, overlap (defined as the intersection over union between the ground-truth and predicted labels <ref type="bibr" target="#b11">[13,</ref><ref type="bibr" target="#b19">21]</ref>) and F 1 score (the harmonic mean between precision and recall). We report the average of these measures over people and frames. We show qualitative results of our method in Comprehensive analysis. We first analyze each component of our method in <ref type="table" target="#tab_0">Table 1</ref>. It is interesting to note that similar results are achieved when removing temporal edges from the graph (No temporal smoothness), or when processing frames one by one (Single frames). Experiments on single frames have a higher recall, while segmenting all frames at once without temporal smoothness produces higher precision, showing the influence of the discriminative term when it has access to the whole video context. Results obtained using the Grouping term only are quite good, whereas using the Discriminative term only has a lower performance since it only models foreground vs. background segmentation without any spatial or temporal consistency. Still, combining the two terms (Ours) leads to the best performance as the discriminative term helps to improve precision. Performance is pushed even further when the discriminative term contains strong semantic cues (Ours + semantic cue). The non-convex refinement from Section 4.3 used in Full method produces significantly better performance than using Convex only optimization. As discussed in <ref type="bibr" target="#b1">[3,</ref><ref type="bibr" target="#b4">6]</ref>, using No constraint leads to trivial solutions and very poor results. Last, even without disparity features (No disparity), which are strong cues, our method produces decent results. Baselines comparison. Quantitative and qualitative comparisons between our method and baselines are shown in <ref type="table" target="#tab_1">Table 2</ref> and <ref type="figure" target="#fig_6">Figure 5</ref>.</p><p>The motion segmentation baselines Multi-modal motion segm. and FB/BG motion segm. perform poorly on this challenging dataset. Both methods completely miss nonmoving and almost non-moving person by nature. Multimodal motion segm. also tends to separate the different limbs of a single person into multiple segments.</p><p>The SDS method performs fairly well. Its detection performance is better than the automatic detector we used (on some key sequences SDS detects twice more people than our detector), but it still misses a significant part of person  instances. For instance, it misses most heavily occluded persons. The other main downside is that the method mostly provides upper body segmentations (due to either the region proposals or the classifier itself which has been trained on a mix of face, upper body and full body examples), in spite of the refinement procedure which is applied at the end of their method and is meant to provide more complete segmentations.</p><p>The CRF as RNN method is the best performing baseline. It produces a clean figure-ground segmentation for a given object class. When people are separated in the image, our relabelling procedure inherently produces good instance-level segmentation results. However, when the person instances are close by or overlap, our method often outperforms the baseline. Our method, which uses only generic features (color, motion, SIFT) and ad-hoc constraints, still performs as well as this strong baseline. It successfully segments each object instance with only coarse localization cues (encoded in the constraints) and without training a pixel-level appearance model for the segmentation as does the baseline. Moreover, when using semantic features of the baseline in our discriminative term, our method outperforms the baseline.</p><p>Pose &amp; segm., which uses instance-specific pose masks, performs significantly worse than our method as it makes strong assumptions about the pose or disparity priors. For instance, it can not recover from errors from the pose estimator. In comparison, our constraints only restrict the space of possible segmentations. They can even be violated in <ref type="figure">Figure 6</ref>: Results of our method applied to two multi-instance videos from SegTrack v2 <ref type="bibr" target="#b29">[31]</ref>.</p><p>situations which do not satisfy the implicit priors they are enforcing. However, they are strong enough to successfully guide the segmentation even for complicated poses, crowded scenes and cluttered backgrounds. We provide pervideo quantitative results on the project website [1].</p><p>Other object classes. The major strength of our method is that it is mostly agnostic to the underlying object class. We provide the method with a single floating point parameter specifying which amount of each bounding box is expected to belong to the object. With this single parameter, the video input and the corresponding bounding box tracks, our method is able to properly segment the object instance from the background of the video and from the other object instances. To the best of our knowledge, there is no proper complete dataset for instance-level segmentation in videos for the moment. To show that our method can handle non-person object classes, we ran it on two videos with multiple object instances from the popular SegTrack v2 dataset <ref type="bibr" target="#b29">[31]</ref>. We show two sample frames in <ref type="figure">Figure 6</ref> and videos on the project website [1]. Quantitatively, our method achieves an average overlap of 86.8% on the drifting car sequence, compared to 79.9% by the recent state-ofthe-art method <ref type="bibr" target="#b46">[48]</ref> which combines the analysis of appearance and motion cues with a reasoning on disocclusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Discussion and future work</head><p>We have presented a flexible and effective framework for multi-instance object segmentation. We have demonstrated its experimental performance on a challenging dataset, showing that constraining the space of segmentations is a robust way to incorporate object tracks information. We plan to extend this method to multiple instances of multiple object categories. This implies having a multi-class discriminative model instead of a foreground vs. background one. More class-or instance-specific knowledge can be incorporated in our constraints. This includes weighing our constraints using noisy pixel-level information such as pose masks. Also, more complex models -including non-convex costs -could use our convex relaxation as an initialization. These refinements could lead to improved segmentation quality.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Constraints (see Section 3.4) used in our model for multi-person segmentation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(a) and (b), head tracks and object tracks are used for such constraints.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Qualitative results of our method. Note that most of the visually unpleasant artifacts are due to the use of superpixels. by optical flow.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Figure 4 and video results on the project website [1].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Multi-modal motion segm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Qualitative comparison between our method and the five baselines. Note that Pose &amp; segm. may drop detections if the pose estimator fails, and that SDS is producing both detection and segmentation, so it uses its own set of detections. See Section 6.3 for comments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><head>Table 1 :</head><label>1</label><figDesc>Comprehensive study of the influence of each component of our method on our new dataset. See Section 6.3 for comments.</figDesc><table>Method 
F 1 
Precision Recall Overlap 

Ours + semantic cue 
80.1% 81.9% 
79.6% 68.6% 
Ours 
78.3% 80.8% 
77.3% 66.0% 

No temporal smoothness 76.4% 79.2% 
75.4% 63.7% 
Single frames 
76.4% 77.9% 
76.4% 63.7% 

Grouping term only 
77.6% 79.4% 
77.2% 65.0% 
Discriminative term only 66.9% 70.7% 
64.7% 52.1% 

No constraint 
12.8% 10.4% 
40.0% 09.0% 
Convex only 
75.6% 78.0% 
74.1% 62.4% 

No disparity 
74.0% 77.5% 
72.6% 59.9% 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 2 :</head><label>2</label><figDesc>Quantitative performance comparison of our method with 5 baselines. Please note that the results from Ground truth tracks, Automatic tracks and SDS detections sections are not comparable as they use different sets of detections. See Section 6.3 for comments.</figDesc><table>Method 
F 1 
Precision Recall Overlap 

Ground truth tracks: 
Ours 
78.3% 80.8% 
77.3% 66.0% 
Ours (+ semantic cue) 
80.1% 81.9% 
79.6% 68.6% 
CRF as RNN [52] 
78.5% 83.2% 
77.7% 66.5% 
Pose &amp; segm. [38] 
68.5% 68.3% 
76.1% 55.0% 
Multi-modal motion segm. [32] 27.4% 41.0% 
30.4% 19.4% 
FB/BG motion segm. [34] 
52.2% 65.1% 
49.8% 38.8% 

Automatic tracks: 
Ours 
63.6% 61.6% 
68.6% 52.0% 
CRF as RNN [52] 
56.2% 58.2% 
54.9% 46.5% 
Pose &amp; segm. [38] 
52.7% 57.2% 
59.5% 40.8% 
Multi-modal motion segm. [32] 27.4% 40.6% 
30.4% 19.4% 
FB/BG motion segm. [34] 
48.4% 57.6% 
50.7% 34.9% 

SDS detections: 
Ours 
72.5% 68.4% 
80.8% 59.3% 
SDS [18] 
65.1% 73.5% 
62.8% 52.6% 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">http://www.robots.ox.ac.uk/˜szheng/CRFasRNN.html 2 http://www.eecs.berkeley.edu/Research/Projects/CS/vision/shape/sds/ 3 http://www.di.ens.fr/willow/research/stereoseg/ 4 http://lmb.informatik.uni-freiburg.de/resources/software.php 5 http://groups.inf.ed.ac.uk/calvin/FastVideoSegmentation/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. The authors would like to thank Jean Ponce for helpful suggestions. This work is partly supported by the MSR-INRIA laboratory and ERC grants Activia and VideoWorld.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Sparse occlusion detection with optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ayvaci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Raptis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>IJCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Diffrac: a discriminative and flexible framework for clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Nonlinear Programming. Athena Scientific</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bertsekas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Visual reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987" />
			<publisher>MIT press Cambridge</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Finding actors and actions in movies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Weakly supervised action labeling in videos under ordering constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lajugie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Convex optimization. Cambridge university press</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Vandenberghe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A video representation using temporal superpixels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Segmentation and tracking of multiple video objects. Pattern Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Colombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fusiello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Murino</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Accurate scale estimation for robust visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Häger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Hello! my name is... buffy&quot; -automatic naming of characters in tv video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">The pascal visual object classes (voc) challenge. IJCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Combining self training and active learning for video segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-F</forename><surname>Balcan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">An algorithm for quadratic programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wolfe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Naval Research Logistics Quarterly</title>
		<imprint>
			<date type="published" when="1956" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Convex relaxations of latent variable training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Schuurmans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Simultaneous detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multi-instance object segmentation with exemplars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshop</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Grabcut-based human segmentation in video sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hernández-Vela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Reyes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Escalera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The distribution of the flora in the alpine zone</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jaccard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">New Phytologist</title>
		<imprint>
			<date type="published" when="1912" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Revisiting Frank-Wolfe: Projection-free sparse convex optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaggi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Supervoxel-consistent foreground propagation in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">D</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Discriminative clustering for image co-segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Multi-class cosegmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Associative hierarchical CRFs for object class image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ladicky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">What, where and how many? combining object detectors and CRFs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ladickỳ</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sturgess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Key-segments for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Image segmentation with a bounding box prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">S</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sharp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Track to the future: Spatio-temporal video segmentation with long-range motion cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lezama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Video segmentation by tracking many figure-ground segments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Humayun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Segmentation of moving objects by long term video analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ochs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Weakly-and semi-supervised learning of a DCNN for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Fast object segmentation in unconstrained video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Papazoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Linking people with &quot;their&quot; names using coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">GrabCut&quot;: Interactive foreground extraction using iterated graph cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kolmogorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Pose estimation and segmentation of people in 3d movies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Seguin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Normalized cuts and image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Good features to track</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tomasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Practical Bayesian optimization of machine learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Co-localization in real-world images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Causal video object segmentation from persistence of occlusions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Karasev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Scene parsing with object instances and occlusion ordering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tighe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niethammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Human instance segmentation from video using detector-based conditional random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Warrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ladicky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">DeepFlow: Large displacement optical flow with deep matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learning to segment under various forms of weak supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Self-occlusions and disocclusions in causal video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sundaramoorthi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">A path following algorithm for the graph matching problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zaslavskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-P</forename><surname>Vert</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Semantic object segmentation via detection in weakly labeled video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Monocular object instance segmentation and depth ordering with cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jayasumana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
		<title level="m">Conditional random fields as recurrent neural networks. In ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
