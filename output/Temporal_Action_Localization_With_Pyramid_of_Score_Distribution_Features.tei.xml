<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Temporal Action Localization with Pyramid of Score Distribution Features</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Yuan</surname></persName>
							<email>yuanjun@nus.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingbing</forename><surname>Ni</surname></persName>
							<email>nibingbing@sjtu.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokang</forename><surname>Yang</surname></persName>
							<email>xkyang@sjtu.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashraf</forename><forename type="middle">A</forename><surname>Kassim</surname></persName>
							<email>ashraf@nus.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Temporal Action Localization with Pyramid of Score Distribution Features</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:47+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We investigate the feature design and classification architectures in temporal action localization. This application focuses on detecting and labeling actions in untrimmed videos, which brings more challenge than classifying presegmented videos. The major difficulty for action localization is the uncertainty of action occurrence and utilization of information from different scales. Two innovations are proposed to address this issue. First, we propose a Pyramid of Score Distribution Feature (PSDF) to capture the motion information at multiple resolutions centered at each detection window. This novel feature mitigates the influence of unknown action position and duration, and shows significant performance gain over previous detection approaches. Second, inter-frame consistency is further explored by incorporating PSDF into the state-of-the-art Recurrent Neural Networks, which gives additional performance gain in detecting actions in temporally untrimmed videos. We tested our action localization framework on the THUMOS'15 and MPII Cooking Activities Dataset, both of which show a large performance improvement over previous attempts.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Action recognition is an active research area with many potential applications in multimedia, home care, logistic support and security-based applications. With the technology advancement of network infrastructure, social media and video-capturing devices, the explosion of video contents has greatly lifted the demand of automatic content analysis. There are considerable amounts of work on analyzing human activities in videos; many pieces of work focus on action classification, labeling human actions with various datasets <ref type="bibr" target="#b30">[30,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b40">39,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b12">13]</ref>. It is not a easy problem since human activities often vary in pose, interaction, clothing and background. During the last decade, great progress has been made on both the dataset quality and recognition algorithms. Despite the great improvements in action classification, there are still drawbacks in this field. Most human action classification algorithms are developed on datasets of trimmed videos <ref type="bibr" target="#b26">[26,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b20">20]</ref>, where each video file is aligned with a single human action. The videos are often trimmed by hand. This setting simplifies the real-life video content analysis, however it is most of the time unrealistic, since a real video often contains multiple action instances as well as irrelevant backgrounds, i.e. the video is untrimmed. A more practical setting is to label every video frame with action classes; this can be considered as a detection task which brings in new challenges due to the uncertainty in action occurrence, where the action label, position and duration are all unknown.</p><p>This per-frame labeling process utilizes both the current frame action information and inter-frame consistency. It can be considered as an automatic trimming processes, including the detection of action start / stop time and classifying the underlying action at the same time. This task is much more demanding than outputting a label from a video file, and requires huge amount of temporally labeled video data. Due to the cost of annotation, the studies in the temporal ac-tion localization problem are still developing at a primitive stage <ref type="bibr" target="#b25">[25]</ref>, with a limited number of temporally annotated datasets.</p><p>The THUMOS Action Recognition Challenge <ref type="bibr" target="#b8">[9]</ref> is developed to address this issue. The dataset provides a large amount of untrimmed videos from real-life human activities with temporal annotations, with which it is viable to further investigate temporal action localization under the realistic setting. Yet, the problem is still difficult in practice. First, the scale of sliding window is hard to choose. Since the true duration can vary vastly, a single size detection window can hardly match the effective portion of the underlying action. Second, the multi-resolution approach to address the scaling issue often results in high dimensionality if the multi-scale features are simply concatenated; if the features are scored, a simple non-maximal suppression used in <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b25">25]</ref> is prone to noise and might not well exploit the underlying feature distribution.</p><p>To address those difficulties, we propose the Pyramid of Score Distribution Features (PSDF) for the temporal action localization task. This feature aims at capturing the multi-resolution context around anchor frames. Each anchor frame serves as the center of context pyramid, and each scale of the pyramid is described with action scores extracted with Fisher Encoded Improved Dense Trajectories (IDT) <ref type="bibr" target="#b35">[35]</ref>. This multi-resolution feature is straight forward to understand: if one scale is close to the correct action occurrence, its corresponding score should be high, together with its neighboring scales. In other words, we utilize the multi-resolution action response distribution for each anchored detection window and explore the action information from various temporal scales centered on this anchor point. This multi-resolution approach is more robust than a single scale feature representation where only the information from a fixed length window is considered. By exploiting the distribution among these scores, we can use a classifier to label the frames. We test our features on the THUMOS'15 <ref type="bibr" target="#b8">[9]</ref> Dataset and MPII Cooking Activities Dataset <ref type="bibr" target="#b25">[25]</ref>.</p><p>To further explore the temporal consistency, we combine our PSDF descriptor with the state-of-the-art Recurrent Neural Networks (RNN). RNN is widely used in sequential data analysis, with a natural Markovian structure to utilize context information. The PSDF focuses on confidence level at current, while the combination with RNN gives a better modeling of its temporal transition -both consistency over one action and switching between different actions. Sometimes the windows at current frame appear to be noisy, nonetheless the representation can be much more robust with the propagation of previous multi-resolution contexts. We use two types of RNN, the Elman-Net <ref type="bibr" target="#b6">[7]</ref> and Long Short Term Memory (LSTM) <ref type="bibr" target="#b11">[12]</ref> networks on our PSDF descriptor for detection tasks; these non-linear classi-fiers have higher precision over linear ones, with utilization of temporal feedbacks. The framework is shown in <ref type="figure" target="#fig_0">Figure 1</ref>.</p><p>The rest of this paper is organized as follows: Section 2 briefly introduces the related work on human action recognition. Section 3 describes our Pyramid of Score Distribution Features (PSDF) for temporal action localization. The further refinements on action detection with Elman-Net and LSTM networks are covered in Section 4. The experiment results are reported in Section 5. Section 6 proposes potential improvements to our framework as future studies. Section 7 concludes this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>A substantial amount of work has been developed in the area of action recognition. There are various human action datasets built for algorithm development, on sources from movies, surveillance videos as well as daily captures <ref type="bibr" target="#b25">[25]</ref>. An extensive list can be found in <ref type="bibr" target="#b1">[2]</ref>. The KTH, UCF-101, HMDB-51 and Hollywood2 <ref type="bibr" target="#b26">[26,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b20">20]</ref> datasets are frequently used in human activity classification. The Coffee and Cigarettes, High Five, MSR Action Dataset and MEXaction2 <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b42">41,</ref><ref type="bibr" target="#b31">31]</ref> are datasets with action detection tasks. Some datasets, like the MPII Cooking Activities Dataset <ref type="bibr" target="#b25">[25]</ref>, are developed for both classification and detection tasks.</p><p>Many datasets for temporal action localization are small, and with highly unbalanced action classes. This is due to the high cost of temporal annotation, and often results in overfitting in detection algorithms. Thus the studies of temporal localization are still quite primitive. The THUMOS Action Recognition Challenge <ref type="bibr" target="#b8">[9]</ref>, first held in 2013, is a significant attempt to bring in large scale dataset for both classification and detection; it has become an important platform for exploring new approaches in action studies. The THU-MOS'15 dataset contains over 430 hours of video data and 45 million frames <ref type="bibr" target="#b8">[9]</ref> for untrimmed action classification and temporal localization, based on which we develop our localization framework.</p><p>The methodologies for action recognition have been developing fast. Earlier attempts include space-time features like STIP <ref type="bibr" target="#b17">[17]</ref> and extensions on classical image descriptors <ref type="bibr" target="#b27">[27,</ref><ref type="bibr" target="#b37">37,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b41">40]</ref>. Heng Wang and Cordelia Schmid <ref type="bibr" target="#b34">[34]</ref> proposed the Dense Trajectories (DT), which is widely used in subsequent action recognition studies. This feature tracks points on dense optical flow field, and encodes the trajectories with local descriptors. They further improved their DT features (IDT) by offsetting camera motion, and use Fisher Encoding to improve the classification performance <ref type="bibr" target="#b35">[35]</ref>. Minh Hoai et al. <ref type="bibr" target="#b10">[11]</ref> proposed a score ranking method for action classification. They partitioned the video at global level with different granularity, formed Fisher Vectors <ref type="bibr" target="#b22">[22]</ref> (FV) and ranked the action scores to an ordered distribution for classification. Our PSDF is partially inspired by their work, while we use score distribution locally rather than globally; it is extracted at anchor frames at multiple scales to form a robust descriptor for action localization.</p><p>For detection tasks, the High Five <ref type="bibr" target="#b21">[21]</ref> and Coffee and Cigarettes <ref type="bibr" target="#b18">[18]</ref> datasets are earlier attempts for action localization. Both focus on spatial-temporal detection which is more challenging, however these datasets are small and the methodologies have yet to be tested on large-scale datasets. Yuan et al. <ref type="bibr" target="#b42">[41]</ref> proposed an efficient pattern searching algorithm. Marcus et al. <ref type="bibr" target="#b25">[25]</ref> proposed the MPII Cooking Activities Dataset which contains fine-grained human activities; the dataset contains both classification and temporal localization tasks. The detection is based on DT features, with integrated histogram for efficient computation. Stoian A. et al. <ref type="bibr" target="#b31">[31]</ref> developed the MEXaction2 dataset for two-class action retrieval; their baseline methodology is quite similar to <ref type="bibr" target="#b35">[35]</ref>, with Fisher Encoding on DT features. In THUMOS'14, the CUHK submission <ref type="bibr" target="#b36">[36]</ref> applied both IDT and Convolutional Neural Networks (CNN) features with a single resolution sliding window. The FV from motions and average pooled CNN features from scenes are fused to give the detection labels on video segments. The INRIA submission <ref type="bibr" target="#b9">[10]</ref> used IDT features only, but applied sliding windows at multiple resolutions; its methodology is similar to <ref type="bibr" target="#b25">[25]</ref>, where a simple non-maximal suppression is applied to separate detection windows. They also used many post-processing techniques to refine detection labels. In our work, we take a more structural approach by using classifiers on distributions, which better utilizes context information and generates more robust results.</p><p>The neural networks has achieved great success in computer vision applications. The CNN <ref type="bibr" target="#b19">[19]</ref> has been dominating in image classification <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b29">29]</ref>, and its intermediate features generated by hidden layers have been widely used in object detection, segmentation and saliency applications. Karen et al. <ref type="bibr" target="#b28">[28]</ref> proposed a two-stream CNN for action recognition in videos. Andrej et al. <ref type="bibr" target="#b12">[13]</ref> proposed various CNN fusion models for large-scale video classification. Xu et al. <ref type="bibr" target="#b40">[39]</ref> proposed the latent concept descriptors (LCD), combined with Fisher / VLAD encoding on differ-ent layers of CNN intermediates, which gives comparative performance with IDT features. This method is used by most participants in the classification task of THUMOS'15. The RNN has also been widely used in video content analysis. Pedro et al. <ref type="bibr" target="#b24">[24]</ref> proposed a recurrent structure for scene labeling. The LSTM structure was incorporated by Jeff D. et al. <ref type="bibr" target="#b5">[6]</ref> and Wonmin B. et al. <ref type="bibr" target="#b4">[5]</ref> respectively in video recognition and scene labeling. Wu. et al. <ref type="bibr" target="#b39">[38]</ref> proposed a hybrid deep network for video classification, which uses LSTM networks on top of spatial and temporal CNN features. Vivek V. et al. <ref type="bibr" target="#b33">[33]</ref> proposed a differential RNN for action recognition, emphasizing the information gain with the salient motions between successive frames. These works have more focus on classification, while our paper emphasizes more on temporal action localization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Pyramid of Score Distribution Features</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Motivation</head><p>In a typical action localization task, the dataset normally consists of untrimmed videos with multiple actions mixed with background movements. Neither the action label, position nor the duration is known and a per-frame labeling process is generally required. This is different from the trimmed action classification tasks, where only one action is contained in a file and a video-level pooling is applicable.</p><p>Deciding the sliding window size can be challenging in the localization task. A window too small can only cover a small action fragment, makes the action features noisy and causes extra computation cost; while a window too large will include lots of background movements and make the feature biased from its real distribution. The multiresolution approaches are used in <ref type="bibr" target="#b25">[25,</ref><ref type="bibr" target="#b9">10]</ref> to address the problem; they use action scores instead of raw features to avoid high dimensionality, and use the max scores to separate detection windows. Each window is classified with one action for the localization task. However, using noisy max scores is not robust enough and fails to utilize more abundant distribution information, which motivates our structural approach.</p><p>In our experiments, we find the action response are cor- <ref type="figure">Figure 3</ref>. The formation of PSDF descriptor at anchor frame t. We use unnormalized Fisher Vector (uFV) at the base resolution and compute the uFV recursively in all resolution levels. The additive uFV is re-normalized, combined with class SVM to generate score distributions. The score should be high at the closest window coresponding to the underlying action. The PSDF is a straight forward descriptor, since the score distribution contains information of action label, position and duration. related in different temporal scales. If an action is found at one scale, its neighboring scales are still representative for this instance. Specifically, the action scores should satisfy a distribution in a temporal pyramid. Based on this observation, we propose a novel Pyramid of Score Distribution Features (PSDF). A fixed-size sliding window is applied across the untrimmed video at evenly distributed anchor frames to detect potential action positions. The process is repeated at multiple temporal resolutions to cover different action durations. Each window is treated as an video segment and the action classification pipeline in <ref type="bibr" target="#b35">[35]</ref> is applied. We use the class SVM scores instead of encoded trajectory features as descriptor at the anchor frames. The PSDF is later used to train temporal action detectors, with SVM or RNN. The process is illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>.</p><p>The PSDF is an intuitive feature descriptor: it represents the confidence level of action class, position and duration. For example, if an action is found in resolution r, the confidence should be high at this correct temporal scale, together with its neighbors, as in <ref type="figure">Figure 3</ref>. We can train a linear SVM on the distribution from each anchor frame to detect the labels. Since this SVM is directly related to temporal localization, we call it Temporal-SVM to avoid confusion with the Class-SVM used to generate action scores. Alternatively, we can use the PSDF to train more complicated RNN classifiers.</p><p>The benefits of our approach lies in the following: 1. The use of multi-resolution naturally captures the information spanned in temporal scales. A single scale <ref type="bibr" target="#b36">[36]</ref> might not contain enough discriminative information, while a temporal pyramid is more comprehensive and provides significantly better performance.</p><p>2. The Temporal-SVM is a better classifier compared to the crude max score approach; the latter is more prone to noise and fails to fully utilize the distribution information.</p><p>The PSDF also allows us to apply more complex classifiers as in Section 4.</p><p>3. The PSDF descriptor can be calculated in a linear recursive way which greatly accelerates the computation, as shown in the following section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Feature Encoding in Multiple Resolutions</head><p>We score the IFV at each anchor frame and across all the temporal resolutions to form our PSDF descriptor. Due to the high cost of computation, we modify the IFV process so that the multi-scale representation can be obtained by a linear combination from a base resolution, as in <ref type="figure">Figure 3</ref>.</p><p>We use the IDT <ref type="bibr" target="#b35">[35]</ref> with Improved Fisher Encoding to get the IFV <ref type="bibr" target="#b23">[23]</ref> in a single sliding window. The IDT features are compressed by PCA, and clustered with Gaussian Mixture Models (GMM). The clusters are denoted as tuple {(π k , µ k , Σ k ) | k = 1, 2, ... K}. Given all the IDT features in a window F = (x 1 , x 2 , ... x N ), we apply Fisher Encoding <ref type="bibr" target="#b22">[22]</ref> on each of the sub-features:</p><formula xml:id="formula_0">u jk = 1 N √ π k N i=1 q ik x ji − µ jk σ jk ,<label>(1)</label></formula><formula xml:id="formula_1">v jk = 1 N √ 2π k N i=1 q ik x ji − µ jk σ jk 2 − 1 . (2)</formula><p>u jk and v jk stand for the deviation brought by all the samples at element j and mode k. q ik stands for the posterior probability of sample i at mode k:</p><formula xml:id="formula_2">q ik = exp − 1 2 (x i − µ k ) T Σ −1 k (x i − µ k ) K m=1 exp − 1 2 (x i − µ m ) T Σ −1 k (x i − µ m )<label>(3)</label></formula><p>The FV is obtained by concatenating the vector u and v. Further applying signed square-rooting and L 2 normalization on the FV elements yields the IFV. An Class-SVM can be trained to classify actions; this is the standard pipeline in a trajectory based action classification. In detection tasks, we use the scores generated by this Class-SVM (from the training set) as descriptor in a sliding window.</p><p>It is expensive to score the IFV on all the anchor frames across all the resolutions; for R resolutions and A anchor frames, this results in AR computations for IFV. Since the number of features increases with temporal scales, the overall complexity can be O(AR 2 ), which is extremely costly. To accelerate the computation, we unnormalize the FV (uFV) to make them additive by taking away the normalization factor N √ π k and N √ 2π k . We define a base resolution 0 sampled at half of resolution 1, and use the uFV computed at this level to get uFV for other resolutions. Let r stand for the resolution level, N (t) stand for the set of features at base resolution 0 after anchor frame t, we have:</p><formula xml:id="formula_3">φ t,0 u ′ jk,t,0 = i∈N (t) q ik x ji − µ jk σ jk ,<label>(4)</label></formula><formula xml:id="formula_4">ψ t,0 v ′ jk,t,0 = i∈N (t) q ik x ji − µ jk σ jk 2 − 1 , (5) φ t,1 = φ t−1,0 + φ t,0 , (6) ψ t,1 = ψ t−1,0 + ψ t,0 , (7) φ t,r = φ t,r−1 + φ t−r,0 + φ t+r−1,0 ,<label>(8)</label></formula><p>ψ t,r = ψ t,r−1 + ψ t−r,0 + ψ t+r−1,0 .</p><p>The above recursive addition only needs to compute the uFV once at the base resolution, which greatly reduces the computation cost. FV can be obtained by re-normalizing with the factor √ π k and √ 2π k ; the factor N is ignored since it gets canceled out by subsequent IFV normalization. The IFV of all sliding windows can be computed efficiently by moving down the anchor frame. Since the computation is dominated by generating uFV, this approach reduces the complexity from O(AR 2 ) to O(A).</p><p>We can get the PSDF by applying the Class-SVM on the IFV at anchor frames. A Temporal-SVM can be trained on the PSDF to directly detect actions, or as the input of more complicated network classifiers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Recurrent Neural Networks for Action Detection</head><p>While the PSDF describes the confidence of action class, position and duration at anchor frames, it is desired to explore the evolution of this feature with temporal structures. Within the same action instance, the PSDF at neighboring anchor frames should have consistency; while at transition points, the neighboring PSDF should have distinct distributions. Also, the feature becomes more robust to noise with temporal contexts. To model the temporal evolution of PSDF in action localization, we combine the feature with the state-of-the-art Recurrent Neural Networks (RNN). The multi-resolution context from PSDF propagates in the network, allowing a more comprehensive representation of temporal action information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Recurrent Structures</head><p>We use two types of RNN, the Elman-Net <ref type="bibr" target="#b6">[7]</ref> and LSTM <ref type="bibr" target="#b11">[12]</ref> networks to generate detection labels at anchor frames. The RNN structure combines the current input with previous states, propagating the context information to subsequent time steps. A typical feedback structure of RNN is shown in <ref type="figure" target="#fig_0">Figure 1</ref>. We use the PSDF as the network input, and class labels as the network output.</p><p>Elman-Net. The Elman-Net linearly combines the current input and the hidden state from the last time step. The current hidden state is generated via a non-linear activation on the combination, typically a sigmoid or tanh function. A softmax classifier as the output layer is applied to recognize the underlying data. The forward propagation of Elman-Net is as follows:</p><formula xml:id="formula_6">h t = tanh(W x x t + W h h t−1 + b h ),<label>(10)</label></formula><formula xml:id="formula_7">y t = sof tmax(W h t + b).<label>(11)</label></formula><p>LSTM. The LSTM networks are developed to address the vanishing / exploding gradients in the training process, by enforcing constant error flow in its memory cell <ref type="bibr" target="#b11">[12]</ref>. The cell has of four elements: an input gate, an output gate, a forget gate and a self-recurrent connection with a weight of one to ensure constant error flow. The gates serve as nonlinear units, modulating the interaction between the cell and current input / output, as well as its previous states. We use the setting in <ref type="bibr" target="#b0">[1]</ref> to determine the gate and cell candidate states:</p><formula xml:id="formula_8">i t = σ(W i x t + U i h t−1 + b i ),<label>(12)</label></formula><formula xml:id="formula_9">f t = σ(W f x t + U f h t−1 + b f ),<label>(13)</label></formula><formula xml:id="formula_10">o t = σ(W o x t + U o h t−1 + b o ),<label>(14)</label></formula><formula xml:id="formula_11">C t = tanh(W c x t + U c h t−1 + b c ).<label>(15)</label></formula><p>The cell state and output are determined:</p><formula xml:id="formula_12">C t = i t * C t + f t * C t−1 , (16) h t = o t * tanh(C t ). (17) y t = sof tmax(W h t + b).<label>(18)</label></formula><p>The experiment setting in our detection task is similar to Elman-Net, with PSDF arranged by time steps as input and anchor frame labels as output. Since the PSDF is already a descriptive feature, we only use one hidden layer in these networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Smoothing Regularizer</head><p>The PSDF contains action scores from C classes and R scales, together with dimension RC. The scores from different resolutions are often correlated, e.g. they should not vary vastly at neighboring scales. As a result, the corresponding network weights applied to PDSF should also present a similar internal structure. To incorporate this characteristic, we apply a smoothing loss on the network weights:</p><formula xml:id="formula_13">J(W x ) = C c=1 R−1 r=1 w x(c,r+1) − w x(c,r) 2 2<label>(19)</label></formula><p>This regularizer results in about 1% performance improvement in our detection experiments. It is combined with the L 2 regularizer to limit the effective degree of freedom and promote better generalization capabilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Practical Considerations</head><p>Unbalanced Data. The background actions often dominate the untrimmed videos in the localization task, and the number of action instances from different classes can vary vastly. These result in a highly uneven composition of labels. To address the problem, we apply different weights, typically inverse proportional to the action instances in each class, in training the Temporal-SVM and RNN. The class balancing gives improved classifier performance in the detection task.</p><p>Different Video Lengths. The RNN does not require uniform sequence duration; nonetheless padding the sequences in a batch to the same length will allow fast parallel processing. Also, it is practical to break very long sequences into smaller fragments, since the correlation with long time lags is generally small. For the simplicity of training, we break the video sequences into shorter fixed-length segments, with the tail padded with zeros.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>We report the experiment results of our framework on THUMOS'15 <ref type="bibr" target="#b8">[9]</ref> and MPII Cooking Activities Dataset <ref type="bibr" target="#b25">[25]</ref>, and compare with previous state-of-the-art attempts. Both datasets are large-scale and serve as benchmarks of many related studies. We also report the experiments with different resolution levels and classification methods to demonstrate the strength of our approach. The PSDF outperforms the benchmarks, while the combination with RNN further boosts the detection performance. Extensive evaluation is still difficult in action localization because of the lack of datasets and different evaluation standards.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">The THUMOS'15 Challenge</head><p>The THUMOS Action Recognition Challenge is the largest action recognition challenge, benchmarking the per-formance in both action classification and detection. The training set of THUMOS'15 Challenge is the UCF-101 <ref type="bibr" target="#b30">[30]</ref> dataset, containing over 13,000 trimmed videos from 101 classes. The validation, test and background sets contain approximately 2100, 5600 and 3000 untrimmed videos from YouTube. We resize the videos so that their long edges has a fixed length of 320 pixels.</p><p>In the temporal action localization task, only 20 out of the 101 classes need to be detected. The action classes as well as its temporal annotation need to be retrieved. The evaluation is based on Interpolated Average Precision (AP), and mean AP (mAP) averaged over all classes. A detection R p is considered correct if its overlap o with ground truth R gt is over a specified value:</p><formula xml:id="formula_14">o = R p ∩ R gt R p ∪ R gt<label>(20)</label></formula><p>The given overlap takes five discrete values, ranging from 0.1 to 0.5, as specified in the evaluation toolbox. We use the training set to generate PCA projection matrix with a reduction factor of 2, and 256 GMM clusters. We use the openCV implementation on IDT extraction <ref type="bibr" target="#b35">[35]</ref> and VLFeat <ref type="bibr" target="#b32">[32]</ref> library on GMM clustering. The VLFeat library for Fisher Encoding is modified so that it can compute uFV efficiently; this significantly accelerates the computation of PSDF descriptors. The background set is combined with the training set to generate one-vs-the-rest Class-SVM under the liblinear <ref type="bibr" target="#b7">[8]</ref> implementation. Thus for each given video segment, we can get 101 class scores based on IFV pooling on IDT features.</p><p>The validation set is used to generate the Temporal-SVM. To generate PSDF, we use nine resolutions from 10 to 90 frames with increment of 10 frames. The base resolution to generate uFV is chosen at 5 frames (∼0.2s), which is also the distance between anchor frames. The short period is chosen to ensure the instantaneous actions are not diluted by backgrounds. We use a full score distribution on 101 classes, so each anchor point has 101*9 = 909 action scores which are used for training the Temporal-SVM. For demonstration of the score distribution, we manually crop a video clip and show the scores from different resolution levels in <ref type="figure" target="#fig_2">Figure 4</ref>.</p><p>The detection results on the testing set of THUMOS'15 is given in <ref type="table">Table 1</ref> with PSDF and Temporal-SVM. Our results outperform the previous state-of-the-art submission on THUMOS'14, given that the size of testing set has significantly expanded from 1500 to 5600 videos. The THU-MOS'15 validation set is constructed from THUMOS'14 validation and test set, so it is possible to evaluate our framework on this set for more comparative results. Although some of the non-detection videos (not belonging to the 20 detection classes) are not included, these background videos turn out to have very limited impact on the detection  performance.</p><p>We also provide experimental results with different number of resolution levels and use of max classifier (used in non-maximal suppression) in <ref type="figure" target="#fig_3">Figure 5</ref> and <ref type="table">Table 2</ref>. The results validates our approach on multi-resolution and distribution of scores.</p><p>For non-linear Elman-Net and LSTM, we use only one hidden layer initialized with our Temporal-SVM. The input PSDF are segmented under different sequence lengths: 20, 40 and 100. A 10% overlap is applied to promote smooth transition between neighboring video segments, minimizing the effects of initialization of RNN. The Elman-Net and LSTM are written in python with Theano library <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>, with smoothness and L 2 regularization, and ADADELTA <ref type="bibr" target="#b43">[42]</ref> learning algorithm. The experiment results with Elman-Net and LSTM are shown in <ref type="table">Table 3</ref>.   <ref type="table">Table 3</ref>. Detection performance with Elman-Net and LSTM network on PSDF with different sequence lengths. The context information is better utilized with the non-linear recurrent networks, resulting in enhanced detecion performance.</p><p>We also list the detection AP of each class in <ref type="figure" target="#fig_4">Figure 6</ref>. The easy classes include CleanAndJerk, CliffDiving and GolfSwing with relatively higher mAP, while CricketShot and FrisbeeCatch are difficult classes whose mAP are much lower.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">MPII Cooking Activities Dataset</head><p>The MPII Cooking Activities Dataset <ref type="bibr" target="#b25">[25]</ref> consists of high resolution videos of fine-grained activities. It has 65 classes of cooking activities with both classification and detection tasks. We use a similar experiment setting with THUMOS'15 except that the videos are not resized; this is because the fine-grained actions can only be well captured in high resolutions.</p><p>The midpoint hit criterion is used to evaluate the detection performance: if the midpoint of the detection lies in the ground truth, the detection is considered correct. Due to the computation cost of cross validation, we only evaluate the detection task on the first subject (also re-evaluation of the baseline results). The experiment results is shown in <ref type="table">Table 4</ref>, where the utilization of RNN has greatly improved  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Computation Requirement</head><p>We use a server with Intel Core i7-5930K, 64 GB RAM, and an nVidia Geforce TITAN X graphics card for our experiments. The approximate computation requirement for THUMOS'15 Dataset and MPII Cooking Activities Dataset are shown in <ref type="table" target="#tab_3">Table 5</ref>. Unlisted processes (dictionary, Temporal-SVM, etc.) have negligible computation cost. The computation of IDT takes most of the time, especially at high resolutions; it also consumes considerable disk space. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Future Studies</head><p>Our detection framework is based on trajectory features from actions. The studies in <ref type="bibr" target="#b40">[39]</ref> use a novel feature pool-ing technique based on CNN as enhancement of trajectories, and achieve best performance in action classification in THUMOS'15. Incorporating scene features are more challenging in action localization tasks since they are more sensitive to the background, nonetheless it provides a novel study point we would like to further explore into.</p><p>We shall also study different network structures on the classification performance. The impact of different number of hidden layers, hidden units and feedback structures are to be further studied. It is also possible to learn feature descriptors at an earlier stage of processing.</p><p>The additive feature generation can be extended similarly in spatial domain. Combination with the heuristic information, like human / object detection and density of optical flows would provide prospects for more challenging spatial-temporal localizations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>We propose a novel PSDF descriptor for temporal action localization. Based on IDT, we apply Fisher Encoding on each anchor frame at multi-resolutions, followed by the Class-SVM to generate PSDF scores. The PSDF is a intuitive descriptor for action class, position and duration; and it can be calculated in an efficient linear recursive way. The descriptor is robust and representative by utilizing the distribution information.</p><p>We also apply the Elman-Net and LSTM networks to refine the localization tasks. These non-linear classifiers further exploit the consistency of context information and further boost the detection performance. We tested our framework on the THUMOS'15 and MPII Cooking Activities Dataset, and both give improved performance the previous attempts.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Our framework for temporal action localization. We encode visual features at multiple temproal resolutions, combine with class SVM scores to form our Pyramid of Score Distribution Features (PSDF). The features are further processed by Recurrent Neural Networks (RNN) to produce action detection results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Our temporal localization framework. The boxes in the first row are generated with features from training set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Confidence scores with action instances. The scores are higher at the corresponding position and duration, and smoother at larger temporal scales. Scores from different resolutions are highly correlated, making the representation more robust.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>mAP Vs. # of Resolution Levels. The detection performance improves with a larger number of temporal scales. Log scale is used for visualizing mAP under larger overlaps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>Detection AP over different action classes with overlap = 0.1. the detection performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>Table 2. Max Vs. SVM classifier on PSDF descriptor. Temporal-SVM utilizes distribution information and gives better performance.</figDesc><table>mAP 
0.1 
0.2 
0.3 
0.4 
0.5 

TM'14 

CUHK [36] 18.2 17.0 14.0 11.7 8.3 
INRIA [10] 36.6 33.6 27.0 20.8 14.4 
Ours. 
51.4 42.6 33.6 26.1 18.8 
TM'15 Ours. 
40.9 36.3 30.8 23.5 18.3 

Table 1. Detection results on THUMOS'14 and THUMOS'15 
Datasets. 

mAP 
0.1 
0.2 
0.3 
0.4 
0.5 
Max Classifier 45.6 36.8 28.6 22.1 16.7 
T-SVM (Ours.) 51.4 42.6 33.6 26.1 18.8 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>55.1 43.9 33.9 24.8 16.3</figDesc><table>mAP 
0.1 
0.2 
0.3 
0.4 
0.5 

Elman-Net 

20 
62.1 51.0 38.0 26.4 16.3 
40 
60.8 49.2 36.8 26.1 16.5 
100 59.0 47.1 36.2 25.5 16.9 

LSTM 

20 
58.8 47.5 35.9 24.9 16.0 
40 
56.9 46.2 34.4 25.7 16.4 
100 </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head></head><label></label><figDesc>Table 4. Detection results on MPII Cooking Dataset. Our experiments are based on the PSDF descriptor.</figDesc><table>Precision Recall 
Baseline [25] 
17.7 
40.3 

Ours. 

Temporal-SVM 
27.3 
42.5 
Elman-Net (20) 
34.1 
56.3 
LSTM (20) 
36.3 
59.7 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="true"><head>Table 5 .</head><label>5</label><figDesc>Computation requirement for THUMOS / MPII Datasets.</figDesc><table>Process 
Device 
Storage 
Time 
Video 
-
500G / 8G 
-
Trajectory 
cpu 
10T / 240G 21d / 10d 
Encoding 
cpu 
6T / 90G 
20h / 4h 
Class-SVM 
cpu 
-
1d / 1h 
PSDF 
gpu 
40G / 350M 12h / 2h 
Elman-Net 
gpu 
-
1d / 3h 
LSTM 
gpu 
-
2d / 6h 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Lstm networks for sentiment analysis</title>
		<ptr target="http://deeplearning.net/tutorial/lstm.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Action dataseta survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A R</forename><surname>Ahad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ishikawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SICE Annual Conference (SICE), 2011 Proceedings of</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1650" to="1655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bastien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bergeron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bouchard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Theano: new features and speed improvements. Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Theano: a CPU and GPU math expression compiler</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Breuleux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bastien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Python for Scientific Computing Conference (SciPy)</title>
		<meeting>the Python for Scientific Computing Conference (SciPy)</meeting>
		<imprint>
			<date type="published" when="2010-06" />
		</imprint>
	</monogr>
	<note>Oral Presentation</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Scene labeling with lstm recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Byeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Breuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Raue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liwicki</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Finding structure in time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Elman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive science</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="179" to="211" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">LIBLINEAR: A library for large linear classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R.-E</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1871" to="1874" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">THUMOS challenge: Action recognition with a large number of classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gorban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Idrees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<ptr target="http://www.thumos.info/" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">THUMOS challenge: Action recognition with a large number of classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gorban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Idrees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<ptr target="http://www.thumos.info/" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Improving human action recognition using score distribution and ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hoai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ACCV 2014 -12th Asian Conference on Computer Vision</title>
		<meeting><address><addrLine>Singapore, Singapore; Part V</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3" to="20" />
		</imprint>
	</monogr>
	<note>Revised Selected Papers</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 IEEE Conference on Computer Vision and Pattern Recognition, CVPR &apos;14</title>
		<meeting>the 2014 IEEE Conference on Computer Vision and Pattern Recognition, CVPR &apos;14<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1725" to="1732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">A spatio-temporal descriptor based on 3d-gradients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Klaser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marszałek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<title level="m">British Machine Vision Association</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="275" to="276" />
		</imprint>
	</monogr>
	<note>British Machine Vision Conference</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<editor>F. Pereira, C. Burges, L. Bottou, and K. Weinberger, editors</editor>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
			<date type="published" when="2012" />
			<publisher>Curran Associates, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">HMDB: a large video database for human motion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Garrote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision (ICCV)</title>
		<meeting>the International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">On space-time interest points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="107" to="123" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Retrieving actions in movies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 11th International Conference on</title>
		<imprint>
			<date type="published" when="2007-10" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
	<note>Computer Vision</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Gradientbased learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Actions in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marszałek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision &amp; Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">High five: Recognising human interactions in tv shows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Patron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marszalek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<idno type="DOI">10.5244/C.24.50</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference</title>
		<meeting>the British Machine Vision Conference</meeting>
		<imprint>
			<publisher>BMVA Press</publisher>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Fisher kernels on visual vocabularies for image categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dance</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition, 2007. CVPR&apos;07. IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Improving the fisher kernel for large-scale image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th European Conference on Computer Vision: Part IV, ECCV&apos;10</title>
		<meeting>the 11th European Conference on Computer Vision: Part IV, ECCV&apos;10<address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="143" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Recurrent convolutional neural networks for scene labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Machine Learning (ICML-14)</title>
		<editor>T. Jebara and E. P. Xing</editor>
		<meeting>the 31st International Conference on Machine Learning (ICML-14)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="82" to="90" />
		</imprint>
	</monogr>
	<note>JMLR Workshop and Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A database for fine grained activity detection of cooking activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on</title>
		<imprint>
			<date type="published" when="2012-06" />
			<biblScope unit="page" from="1194" to="1201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Recognizing human actions: A local svm approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schuldt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Pattern Recognition, 17th International Conference on (ICPR&apos;04</title>
		<meeting>the Pattern Recognition, 17th International Conference on (ICPR&apos;04<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2004" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="32" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A 3-dimensional sift descriptor and its application to action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Scovanner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th international conference on Multimedia</title>
		<meeting>the 15th international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="357" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>abs/1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">UCF101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Roshan</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno>CRCV-TR-12-01</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Action recognition and detection by combining motion and appearance features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Stoian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">M</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-P</forename><forename type="middle">J</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Crucianu</surname></persName>
		</author>
		<ptr target="http://mexculture.cnam.fr/xwiki/bin/view/Datasets/Mex+action+dataset" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">VLFeat: An open and portable library of computer vision algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fulkerson</surname></persName>
		</author>
		<ptr target="http://www.vlfeat.org/" />
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Differential recurrent neural networks for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Veeriah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-J</forename><surname>Qi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.06678</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Action Recognition by Dense Trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kläser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cheng-Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2011 -IEEE Conference on Computer Vision &amp; Pattern Recognition</title>
		<meeting><address><addrLine>Colorado Springs, United States</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011-06" />
			<biblScope unit="page" from="3169" to="3176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Action recognition with improved trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<meeting><address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Action recognition and detection by combining motion and appearance features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<ptr target="http://crcv.ucf.edu/THUMOS14/papers/CUHK&amp;SIAT.pdf" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">An efficient dense and scale-invariant spatio-temporal interest point detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Willems</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2008</title>
		<imprint>
			<biblScope unit="page" from="650" to="663" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Modeling spatial-temporal clues in a hybrid deep learning framework for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.01561</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A discriminative cnn video representation for event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1798" to="1807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Local trinary patterns for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yeffet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision, 2009 IEEE 12th International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="492" to="497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Discriminative video pattern search for efficient action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1728" to="1743" />
			<date type="published" when="2011-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">ADADELTA: an adaptive learning rate method. CoRR, abs/1212</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">5701</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
