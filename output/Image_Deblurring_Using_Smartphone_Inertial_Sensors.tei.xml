<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Image Deblurring using Smartphone Inertial Sensors</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Hu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Merced</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Merced</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Image Deblurring using Smartphone Inertial Sensors</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:41+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Removing image blur caused by camera shake is an ill-posed problem, as both the latent image and the point spread function (PSF) are unknown. A recent approach to address this problem is to record camera motion through inertial sensors, i.e., gyroscopes and accelerometers, and then reconstruct spatially-variant PSFs from these readings. While this approach has been effective for highquality inertial sensors, it has been infeasible for the inertial sensors in smartphones, which are of relatively low quality and present a number of challenging issues, including varying sensor parameters, high sensor noise, and calibration error. In this paper, we identify the issues that plague smartphone inertial sensors and propose a solution that successfully utilizes the sensor readings for image deblurring. With both the sensor data and the image itself, the proposed method is able to accurately estimate the sensor parameters online and also the spatially-variant PSFs for enhanced deblurring performance. The effectiveness of this technique is demonstrated in experiments on a popular mobile phone. With this approach, the quality of image deblurring can be appreciably raised on the most common of imaging devices.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>A tremendous number of photographs are captured each day using smartphones. Unfortunately, camera shake on these light, hand-held devices is a common occurrence that results in blurry images. To remove the motion blur, both its point spread function (PSF), i.e., blur kernel, and the latent image need to be estimated from the blurred photograph, which is an ill-posed problem. Complicating this problem further is the fact that motion blur often varies spatially across the image in practice. Although there exist many algorithms for removing spatially-invariant motion blur, the spatially-variant deblurring problem remains unsolved, and it is difficult to remove such blur without additional information about the camera motion.</p><p>Unlike priors commonly used for latent images (e.g., those derived from natural image statistics), the distribution of camera motions is difficult to model due to its large diversity and inherent randomness. To gain additional information on such motion, inertial sensors such as gyroscopes and accelerometers have recently been exploited <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b18">19]</ref>, with blur kernels estimated directly from the recorded camera motions and then used together with non-blind deconvolution techniques to recover the sharp latent image. These previous techniques assume the inertial sensor readings to be reliable enough for accurate blur kernel recovery. However, the inertial sensors in smartphones are far from ideal, as they are designed for tasks such as gaming that do not require high-level accuracy. Among the issues common to smartphone inertial sensors are a time delay between sensor and image measurements, and significant sensor noise that cannot be fully addressed through offline calibration. In addition, the rotational center of camera motion can vary with each photograph, and inaccurate accelerometer readings can accumulate into substantial errors in measured camera translation. These practical issues lead to a considerable challenge in using inertial sensors for deblurring smartphone photos.</p><p>In this paper, we present a solution that both manages these individual problems and compensates for the shortcomings in sensor data quality. Techniques are proposed for dynamically recovering the time delay between the inertial and camera sensors, calibrating the rotational center of camera motion online, and refining the blur kernels initially reconstructed from noisy sensor readings. Equally important, our method utilizes inertial sensor readings only as guidance for kernel estimation, rather than directly computing camera motion from the sensors as done in previous inertial sensor-based deblurring systems <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b18">19]</ref>. Our work jointly leverages the inertial sensor data and the captured image itself to overcome the limitations each may have in solving for spatially-variant PSFs. The experiments show that, even with smartphone inertial sensors, this approach can produce high-quality deblurring results, as exemplified for an Apple iPhone 6 in <ref type="figure" target="#fig_0">Figure 1</ref>.</p><p>This work presents a significant step towards the practical use of image deblurring. Existing deblurring algorithms have exhibited limited performance on real-world images captured with smartphones, which are by far the dominant device for digital photography. With effective use of smartphone inertial sensors, image deblurring can become a more practical and mainstream operation for millions of users.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Significant progress in blind deconvolution has been witnessed in recent years (e.g., <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b29">30]</ref>). Since blind deconvolution is an ill-posed problem, prior knowledge or additional captured information is often required to obtain effective solutions.</p><p>The most common prior knowledge comes from natural image statistics, such as a heavy-tailed image gradient distribution <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b21">22]</ref>, alpha matte of the blurry edge <ref type="bibr" target="#b13">[14]</ref>, sparsity of wavelet coefficients <ref type="bibr" target="#b1">[2]</ref>, and adaptive texture priors <ref type="bibr" target="#b4">[5]</ref>. Besides priors on image statistics, knowledge in the form of camera motion models has been used in solving more complicated spatially-variant blur. The blurry image can be represented as the integration of all the intermediate images captured by the camera along the motion trajectory <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b25">26]</ref>. The camera motion can be modeled by three dimensions of camera rotation <ref type="bibr" target="#b28">[29]</ref> or 6D camera motion with an in-plane translation and rotation <ref type="bibr" target="#b7">[8]</ref>. To speed up the optimization step, fast patch-based non-uniform deblurring methods have been developed <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref>. These forms of prior knowledge have been shown to be helpful, but they often do not adequately constrain the deblurring result by themselves.</p><p>Another line of research tackles image deblurring by leveraging auxiliary information acquired at capture time <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b26">27]</ref>, for example, blurred and noisy image pairs <ref type="bibr" target="#b30">[31]</ref>, or high-resolution blurred and lowresolution sharp image pairs <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b26">27]</ref>.</p><p>More accurate information about camera motion can be obtained using inertial sensors such as gyroscopes and accelerometers, which have been integrated into many ordinary mobile devices.Šindelář andŠroubek <ref type="bibr" target="#b23">[24]</ref> proposed a deblurring system for smartphones that synthesizes local blur kernels based on gyroscope readings. The system compares favorably to recent deblurring algorithms, but re-quires user interaction to synchronize the camera and gyroscope sensors. Joshi et al. <ref type="bibr" target="#b14">[15]</ref> built an elaborate DSLRbased camera system with gyroscope and accelerometer. The inertial sensors are assumed to have been calibrated and synchronized beforehand. A similar approach for gyroscope calibration was employed by Park and Levoy <ref type="bibr" target="#b18">[19]</ref> for the multiple image deblurring problem. Since noisy sensor readings cause drifting problems, both methods <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b18">19]</ref> correct the sensor parameters using image priors or multiple images. All of these works assume their recorded sensor data to be reliable, or only slightly relax this assumption. However, there are more challenging issues in using smartphone inertial sensors, which cannot provide high-quality sensor data for effective image deblurring. By contrast, our method does not directly estimate camera motion from the sensors as done in these prior techniques <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b14">15]</ref>. We only consider sensor readings as guidance for kernel estimation, and use image priors for further kernel refinement. Instead of offline calibration commonly used before, we utilize an online calibration to synchronize cameras and sensors, and dynamically estimate two neglected factors (i.e., rotational center and time delay).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Practical Issues in using Inertial Sensors</head><p>In this section, we first describe how to estimate camera motion and blur kernels from inertial sensor readings. This estimation, however, is complicated by a few issues that arise in practice and generally lead to inaccurate results. We then discuss and provide an analysis of these issues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Camera Motion Blur</head><p>The spatially-invariant image blurring process can be expressed as</p><formula xml:id="formula_0">B = I * k + n,<label>(1)</label></formula><p>where B is the input blurred image, I is the latent sharp image, * is the convolution operator, k is the blur kernel (point spread function), and n is the noise term. In practice, camera shake usually results in spatially-variant blur determined by the underlying 6D camera motion, including rotations and translations. Inertial sensors, such as gyroscopes and accelerometers, record this information and have become standard components in smartphones. From their measurements of angular motions and accelerations, camera rotations and translations can be derived and used to estimate spatially-variant blur.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gyroscope.</head><p>A gyroscope provides the rate of change in angular position θ R (t) with respect to time, i.e., the angular velocity</p><formula xml:id="formula_1">α(t) = dθ R (t) dt</formula><p>in three dimensions with units of deg/s. From these measurements, the angular position can be determined by integrating the angular velocity,</p><formula xml:id="formula_2">θ R (t) = t 0 α(t)dt ≈ t 0 α(t)t s (2)</formula><p>where t s is the sampling interval and we assume θ R = 0 at t = 0. Since the camera is a rigid body, each point on the camera shares the same rotation with respect to a certain</p><formula xml:id="formula_3">rotation center o = (o x , o y , o z ).</formula><p>The kernel at each location is thus dependent on the rotation center.</p><p>Accelerometer. We assume that the change of acceleration, a ′ s (t), is measured by the accelerometer in three dimensions with respect to time. With a s (t) denoting the measured acceleration, we thus have a ′ s (t) = das(t) dt . Similar to <ref type="bibr" target="#b14">[15]</ref>, we model the actual acceleration a(t) as</p><formula xml:id="formula_4">a s (t) = a(t)+g +(R θ (t)×(R θ (t)×r(t)))+(α(t)×r(t)),<label>(3)</label></formula><p>where r(t) represents the vector from the accelerometer to the center of rotation, o, and g(t) is the gravity expressed in the current coordinate system. As indicated by this model, the measured acceleration is affected by the actual acceleration, gravity g, centripetal acceleration caused by rotation, and the tangential component of angular acceleration. The position of the accelerometer relative to its initial point at t = 0 can be expressed in terms of its initial speed v 0 as</p><formula xml:id="formula_5">θ T (t) = t 0 v(t)dt = t 0 v 0 + (a s (t) − g)tdt (4) ≈ t 0 v 0 t s + 1/2 * (a s (t) − g)t 2 s .<label>(5)</label></formula><p>The estimated angular position θ R and translational position θ T are taken as the camera pose θ = (θ R , θ T ) for calculating the kernel at each location.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Practical Issues</head><p>Previous work <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b23">24]</ref> have made great progress in incorporating inertial sensor data in the deblurring process. These techniques, however, assume that the sensor readings are reliable and that the model used to generate blur kernels accounts for all the contributing factors. Here we present the practical issues involved in using the inertial sensors in smartphones. Time Delay. It is well known that the inertial sensors and camera sensor are not well synchronized in smartphones, due to differences in warm-up times. The sensors in our experiment (from an iPhone 6) support a sampling rate of 100 Hz, which is equivalent to a sampling interval of t s = 10 ms. In tests on 20 examples, we found variations in time delay from 10 ms to 120 ms, which corresponds to 1 to 12 data samples from the inertial sensors. We additionally found that estimating the correct time delay is critical to the quality of deblurring, and this variable time delay cannot be determined by offline calibration. To tackle this issue, an online calibration method is proposed in Section 4.1. An example of its effect is shown in <ref type="figure" target="#fig_1">Figure 2</ref>.</p><p>Rotation Center. It is often assumed that the rotation center lies at the optical center of the camera <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b23">24]</ref>. In reality, the center of rotation may be located at a point far away, such as in the photographer's wrist if the camera motion is caused by hand shake. This discrepancy in the rotational center can significantly affect the computation of blur kernels, as discussed in <ref type="bibr" target="#b18">[19]</ref>. Park et al. <ref type="bibr" target="#b18">[19]</ref> use multiple shots with the same exposure time to jointly estimate the rotational center, sensor drift and time delay. Their method assumes that the rotation center does not change when taking a sequence of images, which is not necessarily the case. In our tests, we found that the rotation center may even vary during capture. For a single shot with a not very long exposure, the change in rotation center is generally slight and can be neglected. But for different shots, the rotation centers are often varied and this would result in inaccurate motion estimation if the rotation center is assumed to be unchanged. Our single image approach addresses this challenging issue through an online calibration method, as described in Section 4.1. The effect of this online calibration is illustrated in <ref type="figure" target="#fig_2">Figure 3</ref>.</p><p>Noisy Sensor Data. Since inertial sensors in smartphones are not intended for high-quality deblurring, the recorded sensor data are typically noisy, especially for accelerometers. We have examined the noise on our smartphone with the sensors held stationary, i.e., with zero angular acceleration and a constant acceleration from only gravity. We found that the gyroscope data has relatively less noise, while the accelerometer data exhibits considerable noise, as sensor drifts arise from the integral over the acceleration data in <ref type="bibr" target="#b4">(5)</ref>. To approximate the drift model from noisy data, a naive idea is to fit a linear function to the resulting deviation of the stationary device. However, the noise with each captured image appears to be different and cannot be adequately recovered with a generic linear model. Instead, we propose an image-based method to refine the blur kernels estimated from sensor data in Section 4.2.</p><p>Gravity Effect. As described in Section 3.1, the contribution of gravity to the measured translation a s (t) is unknown, since it is difficult to estimate the initial orientation and velocity of the camera. If the camera is in an upright position, then gravity would mainly affect y-axis translation in the camera's local coordinate system. If the camera is oriented instead with a 90 o rotation, then the gravity would mainly affect the x-axis translation. This problem can be partially addressed by taking the mean of the translation measurements from the accelerometer <ref type="bibr" target="#b14">[15]</ref> when the camera has little translation and is in an upright position. To address this issue better, we consider an image-based method (described in Section 4.2) as well. The errors from both noisy sensor data and the gravity effect can be regarded as the noise term in our kernel estimation. Scene Depth. Scene depth is a critical element for estimating spatially-variant blur, and its effects always appear when there are camera translations. However, scene depth is usually unknown and difficult to infer from a single image. Assuming a constant depth for the whole scene would introduce errors to kernels at locations with large depth differences. In this work, we take advantage of the phase-based auto-focus built into most popular smartphones to provide sparse depth information, which can be helpful to verify and refine our kernel estimation. The details will be described in Section 4.2.</p><p>The aforementioned issues are usually neglected in previous work and cannot be well addressed solely using sensor readings. We therefore advocate using image data to help calibrate camera/sensor parameters online and to reduce errors in blur kernel estimation. The joint use of image and sensor data leads to better performance than using either one alone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Deblurring System</head><p>Based on the previous analysis, we propose a deblurring system using the data from inertial sensors in smartphones. Our test platform is an iPhone 6 with iOS 8. The device is equipped with a 8MP camera, gyroscope and accelerometer. Our data capturing app supports adjustments of exposure time and focus, and our deblurring framework consists of online calibration, kernel estimation and refinement, and non-blind image deconvolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Online Calibration</head><p>In this work, we propose an image-based online calibration approach to estimate the time delay and rotational center. Whenever available, we specifically take advantage of light streaks, a commonly existing phenomenon first exploited by Hu et al. <ref type="bibr" target="#b12">[13]</ref>. Light streaks are imaged paths of point light sources, which indicate the PSFs at the corresponding locations and thus provide reliable shape cues for estimating blur kernels. We adopt the method in <ref type="bibr" target="#b12">[13]</ref> to extract light streaks and use them as visual cues in the calibration. We denote a set of detected light streaks (flipped to match the blur kernel) as {l i } i , and we use k (i) (t s , o) to represent the corresponding kernel at location i with respect to time delay t s and rotational center o. The exact time delay is adjusted according to the kernel location i and the rolling shutter effect. We interpolate the sensor data by a factor of 4 to account for the higher sampling rate of the rolling shutter effect. The calibration process can thus be formulated as (t s ,ô) = arg min</p><formula xml:id="formula_6">ts,o i ||k (i) (t s , o) − l i || 2 .<label>(6)</label></formula><p>With the rotational center assumed to be constant during the exposure time and located in the imaging plane, i.e. o z = 0, we need only to estimate o x and o y . The parameters are estimated in a coarse-to-fine manner. If no light streaks are detected in an image, we instead make use of the power spectrum analysis presented in <ref type="bibr" target="#b6">[7]</ref> and reformulate the calibration in <ref type="formula" target="#formula_6">(6)</ref> as</p><formula xml:id="formula_7">(t s ,ô) = arg min ts,o i ||P (k (i) (t s , o)) − P (p i * L)|| 2 ,<label>(7)</label></formula><p>where P is the spectral autocorrelation, and L is the square root of the Laplacian filter (please see <ref type="bibr" target="#b6">[7]</ref> for details). Here, p i denotes a region that covers a significant number of salient edges determined from the magnitude of image gradients. Since we consider spatially-varying blur in this work, the region size of p i cannot be large, and it is set to twice the kernel support size in our implementation.</p><p>Since noise and saturation in light streaks and patches may reduce accuracy, we fine-tune the parameters by searching within a neighborhood of their values estimated from <ref type="bibr" target="#b5">(6)</ref>, and identify the solution with the lowest blur level in the results. This is done by first extracting a few nonsaturated regions {r j } j similar to {p i } i that contain salient edges but are of a larger size. For each region, we generate its kernel using the estimated parameters and apply nonblind deconvolution to obtain the deblurred results. Determining the blur level is a challenging task that has been addressed in recent algorithms <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b22">23]</ref> using measures based on image statistics and learning techniques. In our task, there exists a true pair of parameter settings with which the correct kernels can be approximately derived from the sensor readings. Thus, we propose a measure on sharpness to identify the correct setting.</p><p>According to <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b6">7]</ref>, the spectral autocorrelation of image derivatives in a sharp image is close to that of a delta function. We use this property to fine-tune the estimates for (t s , o) as</p><formula xml:id="formula_8">(t s ,ō) = arg max (ts,o)∈N (ts,ô) j (w j R(∇I j (t s , o))),<label>(8)</label></formula><p>where R represents the maximum response of the autocorrelation function, ∇I j is the deblurred patch corresponding to r j using the kernels generated by the parameter setting (t s , o), and w j are weights that give greater emphasis to patches with more texture. For deconvolution, we employ the Richardson-Lucy (RL) method due to its simplicity. Moreover, since the RL method acts as an inverse filter, the sum of the intensity values does not change much. In <ref type="figure" target="#fig_2">Figure 3</ref>, we display an example of kernels recovered using our estimated rotational center, which are seen to be more accurate than those computed using the optical center as the rotational center. In this example, we minimize the influence of translational blur by deliberately avoiding translations in our hand shake and capturing a distant scene.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Blur Kernel Refinement</head><p>The cumulative errors from sensor noise, scene depth, and calibration error can lead to inaccurately estimated kernels from sensor readings. We consider the errors in the noise term and propose to refine the blur kernels in a nonuniform deblurring framework. Similar to <ref type="bibr" target="#b10">[11]</ref>, we employ a region-based non-uniform blur model that assumes a locally uniform kernel within each region:</p><formula xml:id="formula_9">B = r k (r) * (ω (r) ⊙ I) + n,<label>(9)</label></formula><p>where r is a region index, and ω (r) is an image of weights with the same dimensions as the latent image I, such that the pixels in region r can be expressed as ω (r) ⊙ I with ⊙ denoting the pixel-wise product. Here, k (r) represents the kernel at region r and is modeled as a linear combination of kernel bases b:</p><formula xml:id="formula_10">k (r) = θ µ θ b (r) θ .<label>(10)</label></formula><p>Each kernel basis b θ is induced by the homography H θ of a sampled camera pose θ <ref type="bibr" target="#b10">[11]</ref>:</p><formula xml:id="formula_11">b θ = C(R θ + 1 d T θ [0, 0, 1])C −1<label>(11)</label></formula><p>where R θ and T θ are the rotation matrix and translation vector for pose θ. C denotes the intrinsic matrix of the camera, which needs to be calibrated beforehand. d represents scene depth. The variables µ are the coefficients of the kernel bases, which are determined by the rotational and translational camera movement.</p><p>To remove the blur and recover the latent image, we solve an optimization problem for both the latent image I and the coefficients µ:</p><formula xml:id="formula_12">arg min I,µ r θ µ θ b (r) θ * (ω (r) ⊙I)−B 2 +βϕ(I)+γϕ(µ),<label>(12)</label></formula><p>where ϕ(I) and ϕ(µ) are regularization terms for the latent image I and the coefficients µ. The two regularization terms are weighted respectively by the parameters β and γ.</p><p>The refinement is performed in a coarse-to-fine manner, with the kernels estimated from the sensor readings as initialization. For efficiency, the kernels are refined for each region r j . Using the geometric model in <ref type="bibr" target="#b11">(12)</ref>, we alternatingly estimate the coefficients µ:</p><formula xml:id="formula_13">arg min µ θ µ θ ( rj b (rj )</formula><p>θ T (rj ) )−B (rj ) 2 +γϕ(µ), <ref type="bibr" target="#b12">(13)</ref> and regions of the latent image T (rj ) = ω (r) ⊙ I:</p><formula xml:id="formula_14">arg min T (r j ) ( θ µ θ b (rj ) θ )T (rj ) −B (rj ) 2 +βϕ(T (rj ) ),<label>(14)</label></formula><p>where B (rj ) and I (rj ) represent corresponding regions in the blurry image and latent image, respectively. The regularization term ϕ(T (rj ) ) is defined as ϕ(T (rj ) ) = ∂ x T (rj ) + ∂ y T (rj ) , and has been shown to be effective in recovering smooth results in recent work <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b3">4]</ref>. We set the term ϕ(µ) so that the coefficients µ are similar to the coefficients upsampled from the coarser level µ c : ϕ(µ) = ||µ − (µ c ) ↑ || 2 , where ↑ denotes bilinear upsampling in the camera pose space. The initial coefficients at the coarsest level are obtained by downsampling the values estimated from the inertial sensors. We note that the coarse-to-fine scheme cannot have too many levels, otherwise the kernels estimated from the inertial sensors would have little effect on the optimization at the finest level. The number of levels should depend on the accuracy of the initial estimation. In this work, we use three levels with a downsampling factor of √ 2. The kernel basis is built using angular position θ R and translational position θ T from sensor readings in <ref type="bibr" target="#b10">(11)</ref>. Here, to get the depth information, we make use of the built-in phase-detection based auto-focus that is equipped in mobile devices such as the iPhone and Samsung Galaxy. With the help of this component, we are able to obtain a factor value related to depth at each focus point sampled on a grid prior to image capture. In this work, we directly extract the factor values using the iOS API, and calibrate its correspondence to exact scene depth by shooting objects at known depths.</p><p>In the experiments, we use 6 × 9 overlapping tiles discussed in <ref type="bibr" target="#b8">(9)</ref>, and depth values are sampled at the centers of the regions. Each region is associated with a depth value obtained using our data capture app. The depth at each site is then used to determine the blur kernel using (10) and (11).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Non-Blind Deconvolution</head><p>Once we estimate the coefficients µ, we synthesize the blur kernels using (10) and apply non-blind deconvolution to recover the latent sharp image. For the non-blind deconvolution, we employ the method of <ref type="bibr" target="#b27">[28]</ref>, which is able to handle outliers such as saturation which often exist in lowlight images. To better handle noise in the blurry image, the method in <ref type="bibr" target="#b32">[33]</ref> could be used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experimental Results</head><p>In this section, we show the results of our sensor-based deblurring method on downsampled images of resolution 800×600. We evaluate each step of our method and present results on real blurry images. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Calibration</head><p>To validate our proposed method in estimating the time delay t s and rotational center o, we test the optimization of (6) as well as the measure in <ref type="formula" target="#formula_8">(8)</ref>  with camera motions of data sequences collected from inertial sensors. The dataset consists of 2 sharp images, 5 sets of sensor readings and 9 selected rotation centers shifted from the optical center by 0, 50, . . . , 400 pixels, for a total of 90 blurry test images. We present the errors with respect to shift distance in <ref type="figure" target="#fig_3">Figure 4</ref>. As shown in the figure, the proposed method performs consistently well in terms of shift distance, with the estimated rotation centers slightly displaced from the correct values.</p><p>To evaluate the autocorrelation-based measure of image sharpness in <ref type="formula" target="#formula_8">(8)</ref>, we test it on four regions of a synthetic image. Blurry images are synthesized from it by applying the blurs for different camera motions taken from the inertial sensors at different time periods. <ref type="figure" target="#fig_5">Figure 5</ref>(b) depicts the normalized response curve for each region with respect to time period, where each time period is a forward shift from the previous time period by 10 ms (one sample period of the inertial sensors). Here, the x-axis represents the time delay with respect to the number of samples. The maximum responses of the curves occur at time periods near to the correct period. An alternative measure based on the average of the cumulative power spectrum was proposed in <ref type="bibr" target="#b22">[23]</ref>, where the measure is smaller for a blurry patch than for a sharp patch. We apply the measure of <ref type="bibr" target="#b22">[23]</ref> on the deblurred regions as well and present the results in <ref type="figure" target="#fig_5">Figure 5(c)</ref>. The results show that our method performs more consistently in this task, and the measure of <ref type="bibr" target="#b22">[23]</ref> produces similar values for different periods. The reason is that the kernels generated from these time periods are similar and therefore lead to similar deblurred results, which are not distinguished well using the cumulative power spectrum in comparison to our method based on the maximum spectral autocorrelation response.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Kernel Refinement</head><p>We also validate the kernel refinement step using blurry images synthesized from camera motions computed from the data sequences of the inertial sensors. Noise with the standard deviation of the sensor readings is added to simulate our inaccurate sensor inputs, which are used to compute our kernel initialization. We perform tests on 20 synthetic examples and obtain an average PSNR of 26.62 after refinement, in comparison to 25.05 before refinement. <ref type="figure" target="#fig_4">Figure 6</ref> shows an example of our kernel refinement. With the refinement step, more accurate blur kernels are estimated, leading to better deblurring results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Deblurring Results</head><p>Here, we qualitatively compare our method to state-ofthe-art general image deblurring techniques <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b6">7]</ref>, the method based on light streaks <ref type="bibr" target="#b12">[13]</ref>, and a non-uniform deblurring method <ref type="bibr" target="#b27">[28]</ref> on some real-world examples. As shown in <ref type="figure" target="#fig_6">Figure 7</ref>, previous single-image deblurring methods that do not utilize inertial sensor data perform poorly on these images. For images with insufficient salient edges for kernel estimation, they are likely to converge to a delta kernel. We also compare to results without the rotational center estimation and kernel refinement step, which is similar to previous sensor-based methods <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b23">24]</ref>. In <ref type="figure" target="#fig_6">Figure 7</ref>(a)-(f), our method obtains results of higher quality than other methods, though our results exhibit some ringing artifacts. This is because the varying depth values of the foosball table degrade kernel estimation. The method in <ref type="bibr" target="#b12">[13]</ref> fails in situations where light streaks cannot be well extracted. In <ref type="figure" target="#fig_6">Figure 7</ref>(g)-(l), the results from our method without the kernel refinement step contain some blurs, while those after kernel refinement are sharper, which also demonstrates the effectiveness of the refinement process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Discussion</head><p>In this work, we examine the problems that arise in image deblurring with smartphone inertial sensors and propose a single-image deblurring method to address these practical issues. A central element of our method is an image-based technique for calibrating the sensors online and refining the inaccurate spatially-variant blur kernels.</p><p>There are a few limitations of the proposed method. One is that the method is unable to handle cases when the blur kernels initially estimated from the sensor readings differ substantially from the true kernels. This may occur when the scene depth is shallow, e.g. less than 0.5 meter, and the camera undergoes significant translational motion. Since our kernel refinement method is initialized by the kernels estimated from the sensor readings, poor initial kernels may bias the final results. Another limitation is the amount of computation. Since our method employs a refinement step involving a joint optimization of the non-uniform blur model, the computational load is heavier than that of simply applying non-blind deconvolution with the kernels directly reconstructed from the sensor. A possible solution is to employ cloud processing by uploading the blurred image and sensor readings. How to make the computation efficient on mobile devices is an interesting direction for future work. Moreover, we only obtain depth values at sparsely sampled points and correspond them to tiled regions. To obtain dense depth estimation, a multiple camera system, e.g., the Amazon Fire phone, could instead be used with stereo algorithms.</p><p>(a) Input image (b) Cho and Lee <ref type="bibr" target="#b3">[4]</ref> (c) Xu et al. <ref type="bibr" target="#b29">[30]</ref> (d) Hu et al. <ref type="bibr" target="#b12">[13]</ref> (e) Whyte et al. <ref type="bibr" target="#b27">[28]</ref> (f) Our method (g) Input image (h) Krishnan et al. <ref type="bibr" target="#b15">[16]</ref> (i) Goldstein and Fattal <ref type="bibr" target="#b6">[7]</ref> (j) Hu et al. <ref type="bibr" target="#b12">[13]</ref> (k) Ours without kernel refinement (l) Our method </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Example of smartphone image deblurring. (a) Blurry input image. (b) Estimated PSFs with the help of inertial sensors. (c) Recovered latent image. (d) A patch from the blurry image in (a). (e) The corresponding patch from the recovered latent image in (c).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Problem of sensor synchronization. (a) Input blurry image. (b) Light streak patches from blurry image. (c) Generated PSFs using inertial sensor data before synchronization. (d) Generated PSFs after synchronization by our method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Effect of rotational center estimation. (a) Blurry input image. (b) Patches from the blurry image. (c) PSFs computed using the optical center as the rotational center. (d) PSFs generated using the rotational center estimated by our method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Rotation center estimation with respect to shift distance on synthetic examples. x-axis: distance between the true rotation center and the optical center; y-axis: average distance between estimated rotation centers and the optical center.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>on synthetic examples. For evaluating the rotational center estimation, we synthesize blurry images with selected rotation centers, and Examples of PSF refinement with inaccurate sensor readings. (c)(e) Zoom-in views are shown at the bottom-left corners.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Measure from (8) for identifying the correct parameters. (a) Input blurry image and selected regions. (b) Responses from our method on different regions. (c) Average of the cumulative power spectrum [23] on different regions. The black lines in the figures indicate the correct time periods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .</head><label>7</label><figDesc>Comparisons on real examples. The results are best viewed on a high-resolution digital display.</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Motion-based motion deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ben-Ezra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="689" to="698" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Blind motion deblurring from a single image using sparse approximation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Robust dual motion deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-K</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Quan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Fast motion deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM SIGGRAPH Asia</title>
		<meeting>ACM SIGGRAPH Asia</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A content-aware image prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">B</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Removing camera shake from a single photograph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hertzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">T</forename><surname>Roweis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM SIGGRAPH</title>
		<meeting>ACM SIGGRAPH</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Blur-kernel estimation from spectral irregularities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Goldstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision</title>
		<meeting>European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Single image deblurring using motion density functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Curless</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision</title>
		<meeting>European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deblurring by example using dense correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hacohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision</title>
		<meeting>IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Spacevariant single-image blind deconvolution for removing camera shake</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harmeling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hirsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fast removal of non-uniform camera shake</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hirsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Schuler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harmeling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision</title>
		<meeting>IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Psf estimation via gradient domain correlation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="386" to="392" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deblurring lowlight images with light streaks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Single image motion deblurring using transparency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Image deblurring using inertial measurement sensors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">B</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM SIGGRAPH</title>
		<meeting>ACM SIGGRAPH</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Blind deconvolution using a normalized sparsity measure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Image and depth from a conventional camera with a coded aperture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A no-reference metric for evaluating the quality of motion deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Finkelstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rusinkiewicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Gyro-based multi-image deconvolution for removing handshake blur</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Levoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Total variation blind deconvolution: The devil is in the details</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Perrone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Coded exposure photography: motion deblurring using fluttered shutter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Raskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tumblin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM SIGGRAPH</title>
		<meeting>ACM SIGGRAPH</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">High-quality motion deblurring from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM SIG-GRAPH</title>
		<meeting>ACM SIG-GRAPH</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Discriminative blur detection features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Image deblurring in smartphone devices using built-in inertial measurement sensors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Šindelář</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Šroubek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Electronic Imaging</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="11003" to="011003" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Edge-based blur kernel estimation using patch priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computational Photography</title>
		<meeting>IEEE International Conference on Computational Photography</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Richardson-Lucy deblurring for scenes under projective motion path</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1603" to="1618" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Correction of spatially varying image and video motion blur using a hybrid camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1012" to="1028" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deblurring shaken and partially saturated images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Whyte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal Computer Vision</title>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="185" to="201" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Non-uniform deblurring for shaken images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Whyte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Unnatural l0 sparse representation for natural image deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Image deblurring with blurred/noisy image pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Multi-observation blind deconvolution with an adaptive sparse prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1628" to="1643" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Handling noise in single image deblurring using directional filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Paris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
