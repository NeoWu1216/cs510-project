<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Situation Recognition: Visual Semantic Role Labeling for Image Understanding</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Yatskar</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science &amp; Engineering</orgName>
								<orgName type="institution">University of Washington</orgName>
								<address>
									<settlement>Seattle</settlement>
									<region>WA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science &amp; Engineering</orgName>
								<orgName type="institution">University of Washington</orgName>
								<address>
									<settlement>Seattle</settlement>
									<region>WA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science &amp; Engineering</orgName>
								<orgName type="institution">University of Washington</orgName>
								<address>
									<settlement>Seattle</settlement>
									<region>WA</region>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Allen Institute for Artificial Intelligence (AI2)</orgName>
								<address>
									<settlement>Seattle</settlement>
									<region>WA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Situation Recognition: Visual Semantic Role Labeling for Image Understanding</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:46+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>When we look at an image, we instantly and effortlessly recognize not only what is happening (e.g., clipping) but who and what is involved (e.g., a person, shears, a sheep, wool) and how these entities relate to each other, i.e. the roles that they play (e.g., the person does the clipping, the shears are the clipping tool, and the wool is being clipped from the sheep). In this paper, we argue for explicitly encoding such semantic roles, a key missing ingredient in current paradigms of recognition, in image understanding. We introduce situation recognition, a problem that involves predicting activities along with actors, objects, substances, and locations and how these pieces fit together (semantic roles). For example, the leftmost table in <ref type="figure">Figure 1</ref> shows one such representation: a situation where a man (agent) is clipping (activity) wool (item) from a sheep (source) using shears (tool) in a field (place).</p><p>Situation recognition generalizes activity recognition and human-object interaction, using the assignment of roles to define how actors, objects, substances, and locations participate in activities. For example, <ref type="figure">Figure 1</ref> has image pairs that depict the same overall activity but look very different when the participating entities change for the different roles. Previous work has presented models for some aspects of a complete situation, including activity scene models <ref type="bibr" target="#b34">[35]</ref> and models of very specific activities paired with a few prototypical objects, such as playing a musical in-strument <ref type="bibr" target="#b47">[48]</ref>. However, our formulation provides a more complete representation of the different roles that each of the participants can play, and allows us to scale to hundreds of different activities. In essence, we are building representations that support the understanding not just of "What is happening?" but also "Who is doing it?" (the agent role), "What are they doing it to?" (patient), "What are they doing it with?" (tool), "Where did it start?" (source), and so on, as appropriate for each activity.</p><p>It is difficult to know a priori what roles entities can play in each activity. However, we can draw inspiration from the way verbs are used in the English language by building on FrameNet <ref type="bibr" target="#b13">[14]</ref>, a linguist-authored verb lexicon. FrameNet pairs every verb with a frame, which specifies a set of semantic roles. Semantic roles categorize how objects can participate in the activity described by a verb. For example, the two rightmost images of <ref type="figure">Figure 1</ref> show frames for spraying, which includes semantic roles such as agent and destination. Such frames have been used to build semantic parsers that match verbs to their arguments in English sentences, for example see <ref type="bibr" target="#b2">[3]</ref>. However, here we instead use them to define the space of possible situations, much like how WordNet <ref type="bibr" target="#b12">[13]</ref> was used to define Im-ageNet <ref type="bibr" target="#b40">[41]</ref> object classes. For each frame, the verb defines an activity label, and the semantic roles specify how Word-Net entities participate in the activity. For example, <ref type="figure">Figure 1</ref> shows situations where the FrameNet verb spraying has a semantic role tool that is filled with WordNet synsets such as spray can or hose.</p><p>To demonstrate the generality of the situation recognition task, we introduce SituNet, a collection of over 125,000 images depicting 200,000 distinct situations. Each situation includes one of 500 possible activities and values for up to 6 activity-specific roles (3.5 on average and 1,700 unique roles in total with 190 types). The images were gathered from Google image search with query expansion techniques and labeled with complete situations on Amazon Mechanical Turk. The annotators specified one of 80,000 possible WordNet sysnets for each role, providing over 11,000 unique values for this image collection. In addition to being large scale, this data is also high quality. For example, even though the space of possible values is very large, 2 out of 3 annotators provided the same synset for over 75% of roles. Sections 4 and 5 provide the full details of the data collection and statistics.</p><p>To support future work on the SituNet data, we provide results for a baseline model -a Conditional Random Field (CRF) which includes CNN <ref type="bibr" target="#b42">[43]</ref> features (fine tuned by backpropagating the CRF error). This approach significantly outperforms a 5000-way classifier that predicts one of the 10 most frequent situations per verb. The CRF achieves 32.3% top-1 and 58.9% top-5 accuracy for activity prediction and predicts entire situations correctly 14.2% of the time. When compared to independent models trained on the same activity-centric data, the approach improves top-1 accuracy for object recognition by 8.6% and top-1 activity recognition by 1.2%, demonstrating that the model benefits significantly from the context that is provided by jointly predicting the full situation. In aggregate, these results suggest that situation recognition with the SituNet dataset has the potential to become a strong benchmark for the study of relatively complete understanding of situations in images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Formal Task Definition</head><p>In situation recognition, we assume discrete sets of verbs V , nouns N , and frames F . Each frame f ∈ F is paired with a discrete set of semantic roles E f . For example, <ref type="figure">Figure 1</ref> shows six different situations, representing the verbs clipping, jumping, and spraying. While some semantic roles, e.g. agent, are shared across all three frames, others (e.g., tool) only appear for some. Additionally, each semantic role e ∈ E f is paired with a noun value n e ∈ N ∪ {∅}, where ∅ indicates the value is either not known or does not apply. For example, in the first image in <ref type="figure">Figure 1</ref>, the semantic role item takes the value wool. In this paper, the verb set V and frame set F are derived from FrameNet, while the noun set N is drawn from Word-Net. We refer to the set of pairs of semantic roles and their values as a realized frame, R f = {(e, n e ) : e ∈ E f }. In the third image of <ref type="figure">Figure 1</ref>, R f = {(agent,boy), (source,cliff), (obstacle,∅), (destination,water), (place,lake) }. Finally, a realized frame is valid if and only if each value e ∈ E f is assigned exactly one noun n e . Now, given an image, our task is to predict a situation, S = (v, R f ), specified by a verb v ∈ V and a valid realized frame R f . For example, in the last image of <ref type="figure">Figure 1</ref>, the predicted situations is S = (spraying, {(agent,fireman), (source,hose), (substance,water), (destination,fire), (place,outside)}).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Related Work</head><p>Activity recognition in still images has been widely studied <ref type="bibr" target="#b20">[21]</ref>, and it is generally accepted that objects and scenes are important for recognition <ref type="bibr" target="#b30">[31]</ref>. These intuitions are often built directly into datasets by framing activity recognition as a discrete classification problem, with a small set of multi-word category labels that combine a verb with a scene or object <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b48">49]</ref>. Although recent work has scaled the number of classes <ref type="bibr" target="#b29">[30]</ref>, they are still hand selected and it can be difficult to know what should be included in the set. For example, while "cutting-vegetables" is a category in Stanford-40, many others possibilities, like "cutting-grass" or the more generic "cutting," are missing (similar examples can be found in all current activity recognition datasets). In contrast, our task formulation uses lin-guistic resources to define a very large and significantly more comprehensive space of possible situations.</p><p>Many methods have been proposed for modeling semantic context in activity recognition <ref type="bibr" target="#b5">[6]</ref>. Our approach is most closely related to work that models object cooccurrence <ref type="bibr" target="#b37">[38]</ref> and uses graphical models to combine many sources of contextual information <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b11">12]</ref>. Actions have been a particularly fruitful source of context <ref type="bibr" target="#b34">[35]</ref>, especially when combined with pose to create human-object interactions <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b46">47]</ref>. However, we present the first approach to define how multiple objects participate in a single activity, allowing us to systematically recover activityspecific facts such as "Who is doing it?" (the agent role), "What are they doing it to?" (patient), etc.</p><p>There is also significant related work in the intersection of language and vision. WordNet <ref type="bibr" target="#b12">[13]</ref> is used to define Im-ageNet <ref type="bibr" target="#b4">[5]</ref> classes, much like how we use FrameNet <ref type="bibr" target="#b13">[14]</ref> to define our situation space. Recent work has also explored other areas of cross pollination, including video recognition <ref type="bibr" target="#b19">[20]</ref>, cross modal mappings <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b15">16]</ref>, coreference <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b27">28]</ref>, and affordances <ref type="bibr" target="#b51">[52]</ref>. In particular, sentence generation is closely related and has received significant attention <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b31">32]</ref>. Our situations are inspired by semantic role labeling models <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b26">27]</ref>, which are designed to provide a type of shallow semantics for verbs; in essence, our frames correspond to simple declarative sentences. However, we sidestep the evaluation challenges that come with generating sentences <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b6">7]</ref>, while also providing visual evidence for verbs that should aid captioning. At least partially motivated by the same concerns, there are recent efforts to formulate Visual Question Answering (VQA) tasks <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b8">9]</ref>, where the system must answer questions like "What is the person using to cut the grass?" In a pilot study on a VQA dataset <ref type="bibr" target="#b0">[1]</ref>, we found that up to 20% of questions ask about a semantic role, suggesting that situation recognition could be beneficial.</p><p>Finally, situation recognition is related to two parallel efforts to define visual semantic role labeling tasks. Both provide instance-level information with bounding regions for objects <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b39">40]</ref>. We instead focus on classification, annotate an order of magnitude more images and are the first to consider more than two semantic roles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Dataset Collection</head><p>We introduce SituNet, a dataset of images labeled with situations. Our annotation approach is scalable, the image labeling is done on Mechanical Turk and covers over 500 verbs with 125,000 images, and is relatively affordable, annotation cost approximately $80 per verb.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Filtering and Labeling FrameNet</head><p>FrameNet is a rich resource that pairs verbs with frames and semantic roles. It is designed to cover, as much as pos-sible, all English verbs and all roles they can take, not just those that can be visually recognized in an image. For example, it would include verbs such as attempt with roles such as goal that take other verbs as arguments. To define our recognition task, we manually filtered FrameNet to find verbs and roles that could be reliably recognized in images, and provide English labels for use in the crowdsourcing interface. This was done by a small set of trusted annotators.</p><p>Finding Visual Verbs and Roles We gathered 9683 candidate verbs and asked annotators to determine if they could be reliably recognized in images, and, if so, to provide a support image. <ref type="bibr" target="#b0">1</ref> Verbs that were not recognizable generally fell into one of a few classes, including: (a) abstract, such as "presuming," (b) representational, such as "thinking," where we could find a supporting image evocative of the verb but did not depict it literally happening (c) technical, including "blanching," where crowd workers were unlikely to know the word's meaning, or otherwise just (d) hard, including "insufflating," where the annotator does not know the word or what it would look like. Annotators were first calibrated to confirm they understood these categories and confusing cases were publicly discussed. In total, 1053 verbs (10.9%) were marked as visually recognizable. To find visual roles, annotators were shown visual verbs and their example images and asked to select the subset of visually recognizable semantic roles, a generally easier task.</p><p>Labeling Verbs and Roles To support later crowd sourcing, the annotators also provided simple English descriptions of the visual verbs and roles. They wrote a single sentence that summarizes all of the roles for each verb. For example, for the verb clipping in <ref type="figure">Figure 1</ref>, the sentence would be "An AGENT clips an ITEM from a SOURCE using a TOOL in a PLACE." This sentence was shown to crowd workers to define the roles that each verb supports.</p><p>Example Creation Finally, to help crowd workers understand how to produce situation annotations, a few example image labels were produced for each verb. Five computer science undergraduates read definitions for all 1053 candidate verbs and retrieved three images that correspond to each verb from Google Image Search. If the annotators were unable to find such images, the verb was removed. Overall, 580 verbs passed this filtering stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Image Annotation</head><p>The final image annotations were gathered on Amazon Mechanical Turk in a two-stage process, that involved first filtering automatically collected images and then filling in the role values for target frames. Verbs with a low rate, e.g. "flossing" (0.7%) have specific meaning as compared to verbs such as "putting" (15.5%) or "biting" (7.7%).</p><p>Candidate Image Filtering Candidate images were retrieved by searching for phrases related to a target activity in Google Image Search. Phrases were mined from a subset of Google Syntactic N-Grams <ref type="bibr" target="#b18">[19]</ref> that focuses on verb-argument structure. The phrases we extracted contain the target verb and include all descendants of the verb in a syntactic parse. We selected 450 such phrases, picking the most frequent 150 that contain "n-subj," ,"d-obj", or "p-obj" dependencies. For example,"cutting" would have the p-obj "scissors." Using dependencies guarantees that the queried words occur in different syntactic positions relative to the target verb. We retrieved 200 full-color medium-sized images that pass safe search and consider all returned images as candidates. Workers were instructed to select images that contain the desired activity and (1) are not modified or computer generated and (2) contain at least some part of the main entity doing the action in the image.</p><p>Value Filling Selected images were next presented for value filling. Workers were shown a definition of the target verb, a sentence summarizing the semantic roles associated with verb and example images of realized frames for that verb. They were asked to chose a category from an auto-complete drop-down menu, that also presents synset definitions, to fill slots; to select the most specific WordNet synset, and if more than one could apply, select the most relevant. For groups, they were asked to either find a word that refers to the group (for example, "people," "couple") or simply use the singular ("person"). They were required to annotate at least one value per image and not to fill in values that could not be reasonably inferred from the image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Diversity and Coverage</head><p>The goal of SituNet is to include as many verbs as possible and have samples for all unique combinations of semantic roles and values. This is challenging because situations are structured and there can be a combinatorial number of possible realized frames. We adopted a dynamic strategy to increase diversity while not wasting money on verbs where we have already seen most combinations. First, candidate images from Google Image Search were presented for filtering by uniformly drawing images from query phrases, thus maximizing the diversity of types of images. 200 images were annotated in this way with full structures, providing a lower bound on the number of images per verb in SituNet. Then, we dynamically decided whether to continue to collect more annotations.</p><p>The rate at which unseen combinations occur can be approximated by splitting the data into a train and test set and computing how often a value appears in a semantic role in the test set but never appeared in train set. We refer to this as the out of vocabulary (OOV) rate of a verb, and compute it by averaging 1000 random splits of the data. <ref type="figure" target="#fig_0">Figure 2</ref> visualizes the current OOV rate for a sample of verbs currently in SituNet. If during the collection process the OOV rate of verb was greater than 5%, we continued to collect images, up to a maximum of 400 images. While for some verbs this significantly improved the OOV rate, other verbs will always have a high rate. For example, despite collecting 400 images of the verb "making" and "putting," both have an OOV rate of 15%. This is a fundamental challenge in situation recognition. On the other hand, "baptizing" has an OOV of zero with just 200 image samples. The final global OOV rate in SituNet is 3.5%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Cost</head><p>During the collection process, every verb had a hard constraint of costing no more than $120 and was discontinued when it exceeded this amount. The largest contributor to the cost of collecting SituNet was the true positive rate of candidates retrieved from Google Image Search. <ref type="figure" target="#fig_1">Figure 3</ref> shows the true positive rates for a sample of verbs currently in SituNet. Over 25% of verbs were cost prohibitive to collect directly from Google Image Search results. In cases when we were able to collect at least 50 images but ex- ceeded a cost threshold before collecting 200 images, we made a second effort. A new set of queries for Google Image Search was constructed by pairing the verb with a noun that occurred in an annotated frame. The returned images were used to reseed the filter phase of our annotation. This second round allowed us to reduce the percentage of failed verbs to 13%. Overall, failed verbs contributed $7 to the cost of annotating each verb. <ref type="table">Table 1</ref> provides summary statistics about SituNet, collected as described in the last section. In this section, we summarize the overall annotator agreement and highlight several interesting aspects of the data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Dataset Statistics</head><p>Agreement Quality control at scale is challenging. We used an automatic algorithm that discards annotations from workers that it estimates to be unreliable. The details are described in the supplementary material.</p><p>All images were annotated by three crowd workers. We measure agreement by comparing the values that workers annotated for semantic roles. We say that two semantic role annotations on a single image agree when they indicate the same WordNet synset (or ∅). Furthermore, we compute a relaxed version of this criterion, allowing two annotations to match if the synsets are within 1, 2 or 3 links in the WordNet hierarchy. As a point of reference, the following synsets are all 3 links away from each other: "musical instrument" and "trumpet," "child" and "little girl," and "girl" and "person." <ref type="table">Table 2</ref> summarizes agreement in SituNet.</p><p>While the agreement numbers are very high, especially considering Turkers can select one of 80,000 values for each semantic role, there are systematic sources of ambiguity. Place, a semantic role present in all frames, is highly am-  . The number of semantic roles a noun can participate in, on a log-scale. 62% of nouns in SituNet appear with more than one semantic role. The most frequent noun, "man" appears in 44.6% of the roles. On the right are the different roles the nouns "car" and "elephant" paricipate in. Some roles can define particular viewpoints, such the role "place" being assigned "car" commonly indicates the interior view of the car.</p><p>biguous because it can be identified in three ways: a close interacting object, (e.g., reading at a "desk"), an overall location type (e.g., reading in an "office") or a coarse identifier (e.g., reading "inside"). <ref type="table">Table 2</ref> demonstrates that place is indeed a major contributor to disagreement, accounting for over 25% cases where workers failed to produce a majority. This type of disagreement provides a number of alternative correct answers. Other sources of disagreement are described in the supplementary material. <ref type="figure" target="#fig_2">Figure 4</ref> shows a uniform sample of nouns and the number of semantic roles they participate in. As expected there is a large variance; for example,"man" can take up to 798 roles while "basin" only takes 1 role. We also compute the inverse of these statistics: the number of nouns that a role can take, as shown in <ref type="figure" target="#fig_3">Figure 5</ref>. <ref type="figure">Figure 6</ref> shows the number of entities a sample of verbs can take. As expected, less structured verbs like "putting" have 653 entities and heavily structured verbs like "flossing" only take 42 nouns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Entity-Role Relations</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Entity-Verb Relations</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Structured Prediction of Frames</head><p>Our CRF for predicting a situation, S = (v, R f ), given an image i, decomposes over the verb v and semantic rolevalue pairs (e, n e ) in the realized frame R f = {(e, n e ) : e ∈ E f }. The CRF parameters θ can be trained directly from our situation-labeled data. The full distribution, with poten-  </p><p>Computing the normalization is efficient: we can enumerate all valid verb-semantic role pairs and then for all pairs sum all possible semantic role values. Each potential in the CRF is log linear:</p><formula xml:id="formula_1">ψ v (v, i; θ) = e φv(v,i)θ<label>(2)</label></formula><p>ψ e (v, e, n e , i; θ) = e φe(v,e,ne,i)θ</p><p>where φ e and φ v encode scores from the output of a CNN. To learn this model, we assume that for an image i in dataset D there can, in general, be a set A i of possible ground truth situations. We optimize the log-likelihood of observing at least one situation S ∈ A i :</p><formula xml:id="formula_3">i∈D log 1 − S∈Ai (1 − p(S|i; θ))<label>(4)</label></formula><p>CRF Features In Equation 2 and 3 we introduce two feature functions that are implemented by adapting a neural network pretrained on the ImageNet Challenge <ref type="bibr" target="#b40">[41]</ref>. We use VGG Large Network <ref type="bibr" target="#b42">[43]</ref> in Caffe <ref type="bibr" target="#b24">[25]</ref> with the final layers reduced to dimensionality 1024. The output of VGG  <ref type="figure">Figure 6</ref>. On the left, the number of nouns that appear with a sample of verbs (not all labeled). Some verbs (e.g. putting or scooping) require the ability to predict hundreds of noun values, while others (e.g. flossing) can only happen in a few canonical ways. On average, 199 nouns occur with a verb. On the right are example nouns for "scooping" and "feeding" and the roles they play.</p><p>is used as the input to a fully connected layer which predicts potential values in our CRF, similar to neural networks used for semantic role labeling in sentences <ref type="bibr" target="#b14">[15]</ref>. At training time, we optimize Equation 4 with stochastic gradient ascent using a batch size of 192. We fine tune all layers of VGG for 30 epochs and reduce the initial learning rate of 1e-5 by a factor of ten for every ten epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Experiments</head><p>We present the first results for situation prediction in SituNet and also compare performance to baselines that independently recognize activities and objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.">Situation Recognition</head><p>Metrics We measure accuracy for different components of predicted situations. Because the evaluation data has situations provided by multiple annotators, we consider verb predictions (verb) and semantic role-value pair predictions (value) correct if they match any of the annotations. A realized frame is correct if it either strictly matches all semantic role-value pairs provided by a single annotation (value-full) or if each pair matches at least one (possibly different) annotation (value-any). We also report accuracy with ground truth verbs.</p><p>Systems In addition to the CRF model described in Section 6, we also present a simple discrete classification baseline. The classifier selects one of the 10 most frequent realized frames for each verb seen in the training data, pro- ducing a 5040-class problem. For training, each realized frame is assigned as a positive example to the classifier output with the fewest number of differences. The classifier uses the same VGG features and fine tuning procedure as the CRF but with an initial learning rate of 1e-3. <ref type="table">Table 3</ref> summarizes our experiments on the SituNet development set. We also ran these experiments once on the SituNet test which confirms our development results. Overall, the CRF outperforms the discrete classifier by large margins. Verb accuracy is 32.5% and rises to 59% in the top-5. We can isolate the performance of assigning values to semantic roles by considering prediction accuracy given ground truth verbs. The discrete classifier is significantly worse in this context at value and full prediction because it cannot assign new combinations of entities to roles at test time. <ref type="figure" target="#fig_4">Figure 7</ref> shows a random selection of predictions from the CRF model on the development images where it predicted the correct verb. Over two thirds of the cases are correct or have only one incorrect role assignment. Furthermore, many of the errors are actually somewhat plausible. For example, the pole vaulter in the bottom right image is going over a horizontal pole. Other cases show similar reasonable errors, including confusing a cow with a horse, in the image second from the top and right.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Quantitative Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Qualitative Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.">Activity and Object Recognition</head><p>Metrics We evaluate activity and object recognition using top-1 and top-5 accuracy. For activity recognition, we treat the situation activity label as the gold standard. For object recognition, we assume any synset value annotated in a labeled frame is a gold standard object in the image.</p><p>Systems For activity recognition, we adapt our situation CRF by maximizing the potential in Equation 1 and predicting the corresponding verb. As a baseline, we train a discrete classifier for all verbs in SituNet, using VGG features and an identical fine tuning setup as the CRF but with an initial learning rate of 1e-3. For object recognition, we use our CRF to compute probability of observing any synset in the dataset by marginalizing Equation unique synset associated with an image and train the classifier on this expanded dataset, using identical training setup as the CRF but with an initial learning rate of 1e-3. <ref type="table">Table 4</ref> summarizes our experiments on the SituNet development set. We also ran these experiments once on the SituNet test data, which confirms our development results. Our situation CRF significantly outperforms predicting either activities or objects in isolation, by 1.2% and by 8.6% at top-1, respectively. Overall, the results are encouraging; the context provided by situations is helping significantly, and improved models that more accurately reason about how objects interact with activities have significant potential to improve all three recognition tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Quantitative Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Conclusion</head><p>We introduced the problem of situation recognition and described the construction of SituNet, a large new situation recognition data set. Key to the formulation was the use of semantic roles to represent how objects, actors, and other entities participate in different activities. The situation recognition task is challenging but provides strong context for recognizing activities and objects. Future work involves developing more accurate models and using them in applications, including image captioning and visual QA.  On the right is random output from our CRF model when it correctly predicted the activity. Incorrect semantic role values are highlighted in red, whereas correct ones are green.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SKIDDING</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>A word cloud of verbs in SituNet where larger words have a larger rate of unseen value-role combinations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>A word cloud of verbs in SituNet where larger words have a larger true positive rate for images retrieved from Google Image Search. Verbs with low rates, i.e. "fanning" (1%), were cost prohibitive to annotate. For all verbs, the average rate was 6.6%.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4</head><label>4</label><figDesc>Figure 4. The number of semantic roles a noun can participate in, on a log-scale. 62% of nouns in SituNet appear with more than one semantic role. The most frequent noun, "man" appears in 44.6% of the roles. On the right are the different roles the nouns "car" and "elephant" paricipate in. Some roles can define particular viewpoints, such the role "place" being assigned "car" commonly indicates the interior view of the car.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>On the left, the number of nouns that can participate in a sample of semantic roles (not all labeled). On average 64.7 nouns appear per role. Some roles, such as the "tool" of "surfing" take very few values, indicating the majority of the information about the situation is indicated by the verb. On the right are examples of nouns that fill the "target" of "brushing" (the thing being brushed) and the "item" of carrying (the thing being carried), showing significant visual variation when the values are changed. tials for verbs ψ v and semantic roles ψ e takes the form: p(S|i; θ) ∝ ψ v (v, i; θ) (e,ne)∈R f ψ e (v, e, n e , i; θ)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 .</head><label>7</label><figDesc>Example realized situations from SituNet. Below each image is a table where the first row is the activity, the left column is semantic roles, and the right column is values for those roles. On the left outlined in gold are examples of gold standard annotated data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head></head><label></label><figDesc>1 over verbs and predicting the synset with the maximum marginal probability. As a baseline, we train an discrete classifier for all noun synsets in SituNet. We create psuedo-examples for everyTable 4. Object and activity recognition results in SituNet.</figDesc><table>activity 
object 
top-1 
top-5 
top-1 top-5 

dev 

Activity 
30.6 
57.4 
-
-
Object 
-
-
64.9 
94.1 
Situation 32.25 
58.6 
72.9 
95.0 

test 

Activity 
31.1 
57.7 
-
-
Object 
-
-
64.1 
94.2 
Situation 
32.3 
58.9 
72.7 
94.8 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We extended the nearly 5,000 verbs in FrameNet to include additional verbs from PropBank<ref type="bibr" target="#b26">[27]</ref>, a closely related verb lexicon, that were mapped to FrameNet as part of the SemLink project<ref type="bibr" target="#b36">[37]</ref>.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements This research was supported in part by the NSF (IIS-1252835, IIS-1338054), DARPA under the CwC program through the ARO (W911NF-15-1-0543), ONR (N00014-13-1-0720), two Allen Distinguished Investigator Awards, the Allen Institute for AI, and an AWS in Education Grant award. We thank Vicente Ordonez for critical feedback on the draft and Jayant Krishnamurthy for help with the quality control algorithms. We also appreciate the undergraduate and turk annotators, and in particular Diana Wang for her tireless effort. Finally, Eunsol Choi, Ricardo Martin, Nicholas Fitzgerald, Chloe Kiddon, Yannis Konstas and the reviewers provided feedback that greatly improved the work.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1505.00468</idno>
		<title level="m">Visual question answering</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Learning a recurrent visual representation for image caption generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.5654</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Semi-Supervised and Latent-Variable Models of Natural Language Semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Das</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CMU</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Recognizing human actions in still images: a study of bag-of-features and part-based representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Delaitre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Construction and Analysis of a Large Scale Image Ontology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Vision Sciences Society</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">An empirical study of context in object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Comparing automatic evaluation measures for image description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Elliott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Linking people with &quot;their&quot; names using coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Building a large-scale multimodal knowledge base for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1507.05670</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The pascal visual object classes challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2th PASCAL Challenge Workshop</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.4952</idno>
		<title level="m">From captions to visual concepts and back</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Every picture tells a story: Generating sentences from images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV 2010</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fellbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wordnet</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>Wiley Online Library</publisher>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Background to framenet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Fillmore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of lexicography</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Semantic role labelling with neural network factors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Fitzgerald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Devise: A deep visual-semantic embedding model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Frome</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Context based object categorization: A critical survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Galleguillos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">3</biblScope>
		</imprint>
		<respStmt>
			<orgName>CVIU</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Are you talking to a machine? dataset and methods for multilingual image question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">E</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.05612</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A dataset of syntactic-ngrams over time from a very large corpus of english books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">*SEM</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Youtube2text: Recognizing and describing arbitrary activities using semantic hierarchies and zero-shot recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A survey on still image based human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Beyond nouns: Exploiting prepositions and comparative adjectives for learning visual classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.04474</idno>
		<title level="m">Visual semantic role labeling</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Framing image description as a ranking task: Data, models and evaluation metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hodosh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JAIR</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<title level="m">Convolutional architecture for fast feature embedding</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Deep visual-semantic alignments for generating image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.2306</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">From treebank to propbank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kingsbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Palmer</surname></persName>
		</author>
		<editor>LREC. Citeseer</editor>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">What are you talking about? text-to-image coreference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Is this a wampimuk</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lazaridou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Trento universal human object interaction dataset. V&amp;L Net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-T</forename><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">What, where and who? classifying events by scene and object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y.</forename><surname>Lin</surname></persName>
		</author>
		<idno>ECCV. 2014. 3</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Action recognition from a distributed representation of pose and appearance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.1090</idno>
		<title level="m">Explain images with multimodal recurrent neural networks</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Actions in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marszalek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Im2text: Describing images using 1 million captioned photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Semlink: Linking propbank, verbnet and framenet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Palmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">GLC</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="9" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Image question answering: A visual semantic embedding model and a new dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ren</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.02074</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Describing common human visual actions in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ronchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Grounded models of semantic representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Silberer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition. CoRR, abs/1409.1556</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Ucf101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">2012</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.5726</idno>
		<title level="m">Consensus-based image description evaluation</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Show and tell: A neural image caption generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.4555</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Modeling mutual context of object and human pose in human-object interaction activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yao</surname></persName>
		</author>
		<idno>CVPR. 3</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Grouplet: A structured image representation for recognizing human and object interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Human action recognition by learning bases of action attributes and parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">See no evil, say no evil: Description generation from densely labeled images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yatskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">*SEM</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Visual madlibs: Fill in the blank image generation and question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">E</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.00278</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Reasoning about object affordances in a knowledge base representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<idno>ECCV. 2014. 3</idno>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
