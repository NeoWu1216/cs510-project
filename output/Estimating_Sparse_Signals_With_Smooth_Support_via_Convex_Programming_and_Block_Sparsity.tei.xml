<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Estimating Sparse Signals with Smooth Support via Convex Programming and Block Sparsity</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sohil</forename><surname>Shah</surname></persName>
							<email>sohilas@umd.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Goldstein</surname></persName>
							<email>tomg@cs.umd.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Studer</surname></persName>
							<email>studer@cornell.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Cornell University</orgName>
								<address>
									<settlement>Ithaca</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Estimating Sparse Signals with Smooth Support via Convex Programming and Block Sparsity</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:40+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Conventional algorithms for sparse signal recovery and sparse representation rely on ℓ 1 -norm regularized variational methods. However, when applied to the reconstruction of sparse images, i.e., images where only a few pixels are non-zero, simple ℓ 1 -norm-based methods ignore potential correlations in the support between adjacent pixels. In a number of applications, one is interested in images that are not only sparse, but also have a support with smooth (or contiguous) boundaries. Existing algorithms that take into account such a support structure mostly rely on nonconvex methods and-as a consequence-do not scale well to high-dimensional problems and/or do not converge to global optima. In this paper, we explore the use of new block ℓ 1 -norm regularizers, which enforce image sparsity while simultaneously promoting smooth support structure. By exploiting the convexity of our regularizers, we develop new computationally-efficient recovery algorithms that guarantee global optimality. We demonstrate the efficacy of our regularizers on a variety of imaging tasks including compressive image recovery, image restoration, and robust PCA.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>A large number of existing models used in sparse signal processing and machine learning rely on ℓ 1 -norm regularization in order to recover sparse signals or to identify sparse features for classification tasks. Sparse ℓ 1 -norm regularization is also prominently used in the image-processing and computer vision domain, where it is used for segmentation, tracking, and background subtraction tasks. In computer vision and image processing, we are often interested in regions that are not only sparse, but also spatially smooth, i.e., regions with contiguous support structure. In such situations, it is desirable to have regularizers that promote the selection of large, contiguous regions rather than merely sparse (and potentially isolated) pixels. In contrast, simple ℓ 1 -norm regularization adopts an unstructured approach that induces sparsity wherein each variable is treated independently, disregarding correlation among neighboring variables. For example, smooth support structure is relevant to compressive background subtraction <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b8">9]</ref> which detects contiguous regions of movement against a stationary background.</p><p>For imaging applications, ℓ 1 -norm regularization may result in regions with spurious active (or isolated) pixels or non-smooth boundaries in the support set. This issue is addressed by the image-segmentation literature, where spatially correlated priors (such as total variation or normalized cuts) are used to enforce smooth support boundaries <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b11">12]</ref>. An important hallmark of existing image-segmentation methods is that they are able to enforce spatially contiguous support. However, the concept of correlated support has yet to be ported to more complex reconstruction tasks, including (but not limited to) robust PCA and compressive background subtraction. The development of such structured sparsity models has been an active research topic <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b19">20]</ref>, with new models and applications still emerging <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b21">22]</ref>. In this paper, we develop a class of convex priors based on overlapping block/group sparsity, which are able to enforce sparsity of the support set and promote spatial smoothness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Relevant Previous Work</head><p>Existing work on spatially-smooth support-set regularization can be divided into two main categories: (i) non-convex models that rely on graphs and trees, and (ii) convex models that rely on group-sparsity inducing norms. Cevher et al. <ref type="bibr" target="#b8">[9]</ref> promote sparsity using Markov random fields (MRFs) in combination with compressive-sensing signal recovery, which is referred to as lattice matching pursuit (LaMP). LaMP recovers structured sparse signals using fewer noisy measurements than methods that ignore spatially correlated support sets. Baraniuk et al. <ref type="bibr" target="#b1">[2]</ref> prove theoretical guarantees on robust recovery of structured sparse signals using a non-convex algorithm; their approach has been validated using wavelet-tree-based hierarchical group structure, as well as signals with non-overlapping blocks in the support set. Huang et al. <ref type="bibr" target="#b18">[19]</ref> developed a theory of greedy approximation methods for general non-convex structured sparse models. All these methods, however, are limited in that they are either non-convex, computationally expensive, or do not allow for overlapping (or not aligned) group structure. Jenatton et al. <ref type="bibr" target="#b20">[21]</ref> showed the possibility of coming up with a problem-specific optimal group-sparsity-inducing norm using prior knowledge of the underlying structure. While they consider a convex relaxation of the structured sparsity problem, it remains unclear how their proposed active-set algorithm for least squares regression can be generalized to a broader range of applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.">Contributions</head><p>Our work is inspired by the ℓ 1 /ℓ 2 -norm spatial coherence priors used in <ref type="bibr" target="#b20">[21]</ref>, as well as group sparsity priors used in statistics (e.g., group lasso) <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b39">40]</ref>. Our main contributions can be summarized as follows: (i) We propose new regularizers for imaging and computer vision applications including compressive image recovery, sparse &amp; low rank decomposition, and a block-sparse generalization of total variation. (ii) We develop computationally efficient global minimization algorithms that are suitable for overlapping pixel-cliques. Existing methods for group sparsity use the alternating direction method of multipliers (ADMM), and have excessive memory requirements for large clique sizes. We therefore discuss a new approach using fast convolution algorithms to perform gradient descent with low memory requirements and a complexity that is independent of the clique size. (iii) We propose the use of our regularizers within greedy pursuit methods for compressive reconstruction. (iv) We demonstrate that our algorithms can be used to suppress artifacts and enhance the quality of sparse recovery methods when applied to a variety of imaging applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3.">Notation</head><p>For any column vector x ∈ R n , we define its ℓ α -norm with α ≥ 1 as x α = ( n i=1 |x i | α ) 1/α . For x ∈ R n , the vector x c consists only of the entries associated to the index set c. The support set (i.e., the set of indices of non-zero entries) of a vector or vectorized image x is denoted by supp(x). For a matrix A ∈ R M ×N with rank r = min{M, N } and singular values σ i , the nuclear norm is defined by</p><formula xml:id="formula_0">A * = r i=1 σ i . We use A 1 = ij |A ij | to denote the element-wise ℓ 1 -norm for A.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Problem Formulation</head><p>Consider the measurement model y = Φx 0 + z 0 , where y ∈ R M is the observed signal, x 0 ∈ R N is the original sparse signal we wish to recover, z 0 ∈ R M is a non-sparse component of the signal (comprising both the background image and potential noise), Φ ∈ R M ×N is the linear operator that models the signal acquisition process. Based on this model, we study signal recovery by solving convex optimization problems of the following general form:</p><formula xml:id="formula_1">{ẑ,x} = arg min z∈R M ,x∈R N D(x, z | y, Φ) + J(x).<label>(1)</label></formula><p>Here, D : R M × R M → R is a convex data-consistency term, and J : R M → R is a regularizer that enforces both sparsity and support smoothness on the vector x. The proposed regularizer is a hybrid ℓ 1 /ℓ 2 -norm penalty of the form</p><formula xml:id="formula_2">J(x) = c∈C x c 2 ,<label>(2)</label></formula><p>where C is a set of cliques over the graph G defined over the pixels of x. This regularizer (2) is a natural generalization of the group (or block) sparsity model that has been explored in the literature for a variety of purposes including statistics and radar <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24]</ref>. We focus on the case where the collection of sparse cliques consist of regularlyspaced groups of adjacent pixels. For example, consider two types of cliques shown in <ref type="figure" target="#fig_0">Figure 1</ref>(a) and 1(b). Notice the (a) 2-clique and (b) 4-clique wherein all nodes are connected to each other. These cliques can be translated over the entire image graph to generate various overlapping clique geometries as shown in (c) and (d), respectively. In (c), eight overlapping cliques, each of size two, overlap at a central point. In the image processing literature this is referred to as an 8-connected neighborhood <ref type="bibr" target="#b10">[11]</ref>. In contrast, <ref type="figure" target="#fig_0">Figure 1</ref>(d) uses a higher-order connectivity model, which is obtained using four rectangular cliques of size four (each shown in a different color). Overlapping group-sparsity models of the form depicted in <ref type="figure" target="#fig_0">Figure 1</ref>(d) effectively enforce spatial coherence of the recovered support. When such an overlapping group-sparsity model is used, all pixels in a clique tend to be either zero or non-zero at the same time (see, e.g., <ref type="bibr" target="#b0">[1]</ref>). Since each pixel shares multiple overlapping cliques with its neighbors, this regularizer suppresses "rogue" (or isolated) pixels from entering the support without their neighbors and hence, promotes smooth (contiguous) support boundaries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Applications</head><p>The proposed regularizer <ref type="bibr" target="#b1">(2)</ref> can be used as a building block for various applications in computer vision, image processing, and compressive sensing. In what follows, we will focus on the following three imaging applications:</p><p>1) Compressive sensing signal recovery: Consider a signal x ∈ R N that is K-sparse, i.e., only K ≪ N entries of x are non-zero. In the CS literature, the signal is acquired via M &lt; N linear projections y = Φx. The K-sparse signal x can then be recovered if, for example, the matrix Φ satisfies the 2K-RIP or similar conditions <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b7">8]</ref>. The underlying recovery problem is usually formulated as follows:</p><formula xml:id="formula_3">x ⋆ = arg min x∈R N y − Φx 2 2 subject to x 0 = K. (3)</formula><p>When the sparse signals are images, simple sparse recovery may not exploit the entire image structure; this is particularly true for background-subtracted surveillance video. Background subtraction is used in applications where one is interested only in inferring foreground objects and activities.</p><p>Background subtraction is easily achieved in the compressive domain by computing the difference between adjacent image data or by subtracting a long term signal mean (or median). Background-subtracted frames are generally more sparse than frames containing background information, and can thus be reconstructed from far fewer measurements M . We propose to extend the problem in (3) by adding a regularizer of the form <ref type="bibr" target="#b1">(2)</ref> to promote correlation in the support set of the foreground objects. The optimization problem defined in <ref type="formula">(3)</ref> is non-convex and is commonly solved using greedy algorithms <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b8">9]</ref>. We will show that the use of our prior (2) leads to faster signal recovery with a small number of measurements compared to existing methods.</p><p>2) Total-variation denoising: Total variation (TV) denoising restores a noisy image y (e.g., vectorized image) by finding an image that lies close to y in an ℓ 2 -norm sense, while simultaneously having small total variation; this can be accomplished by solving</p><formula xml:id="formula_4">x ⋆ = arg min x∈R N 1 2 x − y 2 + λ ∇ d x 1 ,<label>(4)</label></formula><p>where ∇ d : R N → R 2N is a discrete gradient operator that acts on an N -pixel image, and produces a stacked horizontal and vertical gradient vector containing all first-order differences between adjacent pixels. TV-based image processing assumes that images have a piecewise constant representation, i.e., the gradient is sparse and locally contiguous <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b15">16]</ref>. Numerous generalizations of TV exist, including the recently proposed vectorial TV for color images <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b30">31]</ref>. Such regularizers are of the form of (4) merely by changing the definition of the discrete gradient operator. We propose to extend total variation by penalizing the gradient of cliques in order to enforce a greater degree of spatial coherence. In particular, we consider</p><formula xml:id="formula_5">x ⋆ = arg min x∈R N 1 2 x − y 2 + J(∇ d x),<label>(5)</label></formula><p>where J(·) denotes the regularizer <ref type="bibr" target="#b1">(2)</ref>. Furthermore, we explore formulations where the discrete gradient operator is given by the decorrelated color TV operator described in <ref type="bibr" target="#b30">[31]</ref>. With our approach, we also show the application of proposed structured sparsity prior on 3-D blocks. Note that <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b26">27]</ref> explores the use of 1-D and 2-D overlapping group sparsity for TV image denoising, but using a majorizationminimization algorithm combined with ADMM.</p><p>3) Robust PCA (RPCA): Suppose Y = [y 1 , . . . , y L ] is a matrix of L measurement vectors, and Y is the sum of a low rank matrix Z and a sparse matrix X. For this case, Candès et al. show that exact recovery of these components is possible using the following formulation <ref type="bibr" target="#b6">[7]</ref>:</p><formula xml:id="formula_6">{Ẑ,X} = arg min Z,X∈R N ×L Z * + λ X 1 subject to Y = Z + X.<label>(6)</label></formula><p>The nuclear-norm in (6) promotes a low rank solution for Z; the ℓ 1 -norm penalty on promotes sparsity in X. For this reason the solution to <ref type="formula" target="#formula_6">(6)</ref> is sometimes referred to as a sparseplus-low-rank decomposition. A well-known application of RPCA is background subtraction in videos with a stationary background. For such datasets, the shared background in the frames {y i } can be represented using a low-rank subspace. The moving foreground objects often have sparse support, and thus are absorbed into the sparse term X.</p><p>We propose to replace the ℓ 1 -norm regularization prior on X in <ref type="formula" target="#formula_6">(6)</ref> with the proposed regularizer in <ref type="bibr" target="#b1">(2)</ref>; this enables us to promote spatial smoothness in the support set of the foreground objects. Here, we build on the work of <ref type="bibr" target="#b14">[15]</ref>, where structured sparsity with non-overlapping blocks is used in RPCA for foreground detection, and <ref type="bibr" target="#b37">[38]</ref>, where a hybrid of ALM and network flow methods <ref type="bibr" target="#b27">[28]</ref> are used to solve ℓ 1 /ℓ ∞ regularized RPCA problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Optimization Algorithms</head><p>We now develop efficient numerical methods for solving problems involving the regularizer (2). A common approach to enforce group sparsity in the statistics literature is consensus ADMM <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b3">4]</ref>, which we will briefly discuss in Section 2.2.1. For image processing and vision applications, where the datasets as well as the cliques tend to be large, the high memory requirements of ADMM render this approach unattractive. As a consequence, we propose an alternative method that uses fast convolution algorithms to perform gradient descent that exhibits low memory requirements and requires low computational complexity. In particular, our approach is capable of handling large-scale problems, such as those in video applications, which are out of the scope of memory-hungry ADMM algorithms.</p><p>We note that numerical methods for overlapping group sparsity have been studied in the context of statistical regression <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b3">4]</ref>, but for different purposes. Yuan et al. <ref type="bibr" target="#b38">[39]</ref> solves the regression variable selection problem using an accelerated gradient descent approach, whereas Deng <ref type="bibr" target="#b12">[13]</ref> and Boyd <ref type="bibr" target="#b3">[4]</ref> use consensus ADMM, which does not scale to high-dimensional problems. Compared to these methods, our approach provides significant speedups (see Section 3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Proximal Minimization and ADMM</head><p>The simplest instance of the problem (1) is the proximal operator for the penalty term J in <ref type="formula" target="#formula_2">(2)</ref>, defined as follows:</p><formula xml:id="formula_7">prox J (v, λ) = arg min x x − v 2 + λJ(x).<label>(7)</label></formula><p>Proximal minimization is a key sub-step in a large number of numerical methods. For example, the ADMM for TV minimization <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b15">16]</ref> requires the computation of the proximal operator of the ℓ 1 -norm. For such methods, the regularizer <ref type="formula" target="#formula_2">(2)</ref> is easily incorporated into the numerical procedure by replacing this proximal minimization with <ref type="formula" target="#formula_7">(7)</ref>.</p><p>In the simplest case where the cliques in C are small and no other regularizers are needed, the proximal minimization <ref type="formula" target="#formula_7">(7)</ref> can be computed using ADMM <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b15">16]</ref>. Similar approaches have been used for other applications of overlapping group sparsity <ref type="bibr" target="#b12">[13]</ref>. It is key to realize that the regularization term in <ref type="formula" target="#formula_7">(7)</ref> can be reformulated as follows:</p><formula xml:id="formula_8">x = arg min x∈R N x − v 2 2 + λ s i=1 c∈Ci x c 2 .<label>(8)</label></formula><p>Here, C 1 , . . . , C s are clique subsets for which the cliques in C i are disjoint. For example, consider the case where the set of cliques contains all 2 × 2 image patches as shown in <ref type="figure" target="#fig_0">Figure 1(d)</ref>. For such a scenario, we need four subsets of disjoint cliques to represent every possible patch. The reformulated problem for the example graph will be of the form (8) with s = 4. In general, if cliques are formed by translating an l × l patch, l 2 subsets of cliques are required so that every subset contains only disjoint cliques.</p><p>To apply ADMM to this problem, we need to introduce s auxiliary variables z 1 , . . . , z s each representing a copy of the original pixel values. The resulting problem is</p><formula xml:id="formula_9">{x,ẑ i ∀i} = arg min x,{z i } s i=1 x − v 2 2 + λ s i=1 c∈Ci z i c 2 subject to z i = x, ∀i.<label>(9)</label></formula><p>This is an example of a consensus optimization problem, which can be solved using ADMM (see <ref type="bibr" target="#b12">[13]</ref> for more details). An important property of this ADMM reformulation is that each vector z i can be updated in closed form-an immediate result of the disjoint clique decomposition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Forward-Backward Splitting (FBS) with Fast Fourier Transforms</head><p>The above discussed ADMM approach has several drawbacks. First, it is difficult to incorporate more regularizers (in addition to the support regularizer J) without the introduction of an excessive amount of additional auxiliary variables. Furthermore, the method becomes inefficient and memory intensive for large clique sizes and large data-sets (as it is the case for multiple images). For instance in RPCA, if the cliques are generated by l × l patches, l 2 variables {z i } are required, each having the same dimensionality as original image data-set N L. Additionally, the dual variables for each equality constraints in (9) will require another l 2 N L storage entries. As a consequence, for large values of l, the memory requirements of ADMM become prohibitive. We propose a new forward-backward splitting algorithm that exploits fast convolution operators and prevents the excessive memory overhead of ADMM-based methods. To this end, we propose to "smoothen" the objective via hyperbolic regularization of the ℓ 2 -norm as</p><formula xml:id="formula_10">x c 2 ≈ x c 2,ǫ = x 2 1 + · · · + x 2 n + ǫ 2<label>(10)</label></formula><p>for some small ǫ &gt; 0. For the sake of clarity, we describe the forward-backward splitting approach in the specific case of robust PCA. Note, however, that other regularizers are possible with only minor modifications. Using the proposed support prior (6), we write</p><formula xml:id="formula_11">{Ẑ,X} = arg min Z,X Z * +λJ ǫ (X)+ µ 2 Y−Z−X 2 F (11) where J ǫ (X) = L t=1 c∈C X t,c 2,ǫ<label>(12)</label></formula><p>is the smoothed support regularizer, and X t,c refers to the clique c drawn from column t of X. We note that this formulation differs from that in Liu et al. <ref type="bibr" target="#b25">[26]</ref>, where the structured sparsity is induced across columns of X rather than blocks, and is solved using conventional ADMM. The forward-backward splitting (or proximal gradient) method is a general framework for minimizing objective functions with two terms <ref type="bibr" target="#b16">[17]</ref>. For the problem (11), the method alternates between gradient descent steps that only act on the smooth terms in <ref type="bibr" target="#b10">(11)</ref>, and a backward/proximal step that only acts on the nuclear norm term. The gradient of the (smoothed) proximal regularizer in (11) is given columnwise (i.e., image-wise) by</p><formula xml:id="formula_12">∇J ǫ (X t ) = c∈C X t,c X t,c −1 2,ǫ .<label>(13)</label></formula><p>The gradient formula (13) requires the computation of the sum (12) for every clique c, and then, a summation over the reciprocals of these sums; this is potentially expensive if done in a naïve way. Fortunately, every block sum can be computed simultaneously by squaring all of the entries in X, and then convolving the result with a block filter. The result of this convolution contains the value of X t,c 2 2,ǫ for all cliques c. Each entry in the result is then raised to the − 1 /2 power, and convolved again with a block filter to compute the entries in the gradient <ref type="bibr" target="#b12">(13)</ref>. Both of these Algorithm 1 Forward-backward proximal minimization Input: Y, µ &gt; 0, λ, C i , α &gt; 0 Initialize: X (0) = 0, Z (0) = 0 Output: X (n) , Z (n) 1: while not converged do 2:</p><p>Step 1: Forward gradient descent on X,</p><p>3:</p><formula xml:id="formula_13">X (n) k = X (n−1) k − αλ∇J ǫ (X) +αµ(Y k − Z (n−1) k − X (n−1) k ) 4:</formula><p>Step 2: Forward gradient descent on Z, 5:</p><formula xml:id="formula_14">Z (n) k = Z (n−1) k + αµ(Y k − Z (n−1) k − X (n−1) k ) 6:</formula><p>Step 3: Backward gradient descent on Z, <ref type="bibr">7:</ref> Z (n) = prox * (Z (n) , α) 8: end while two convolution operations can be computed quickly using fast Fourier transforms (FFTs), so that the computational complexity becomes independent of clique size.</p><p>Algorithm 1 shows the pseudocode for solving <ref type="bibr" target="#b10">(11)</ref>. In Steps 1 and 2, the values of X and Z are updated using gradient descent on (11), ignoring the nuclear norm regularizer.</p><p>Step 3 accounts for the nuclear-norm term using its proximal mapping, which is given by</p><formula xml:id="formula_15">prox * (Q, δ) = U(sign(S) • max{|S| − δ, 0})V T ,</formula><p>where Q = USV T is a singular value decomposition of Q, |S| denotes element-wise absolute value, and • denotes element-wise multiplication.</p><p>The forward-backward splitting (FBS) procedure in Algorithm 1 is known to converge for sufficiently small stepsizes α <ref type="bibr" target="#b2">[3]</ref>. Practical implementations of FBS 1 include adaptive stepsize selection <ref type="bibr" target="#b35">[36]</ref>, backtracking line search, or acceleration <ref type="bibr" target="#b2">[3]</ref>. We use the FASTA solver from <ref type="bibr" target="#b16">[17]</ref>, which combines such acceleration techniques.</p><p>We note that FBS 1 only requires a total of 4N L storage entries for X, Y, Z and gradient ∇J ǫ (X). However, in order to solve RPCA formulation using ADMM we require 2l 2 N L storage entries for auxiliary variables (as discussed before) and 4N L storage entries for the variables X, Y, Z and dual variable of Y = X + Z, leading to total of (2l 2 + 4)N L storage entries. Since the memory usage and runtime of FBS is independent of the clique size, the advantage of FBS over ADMM is much greater for larger cliques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.3">Matching Pursuit Algorithm</head><p>For compressive-sensing problems involving large random matrices, matching pursuit algorithms (such as CoSaMP <ref type="bibr" target="#b29">[30]</ref>) are an important class of sparse recovery methods. When signals have structured support, modelbased matching pursuit routines have been proposed that require non-convex minimizations over Markov random fields <ref type="bibr" target="#b8">[9]</ref>. In this section, we propose a model-based matching pursuit algorithm that achieves structured compressive Algorithm 2 CoLaMP -Convex Lattice Matching Pursuit Input: y, Φ, K, λ, ǫ Initialize: x (0) = 0, s (0) = 0, r (0) = y Output: x (n) 1: while n ≤ max iterations and r (n) 2 &gt; ǫ do 2:</p><p>Step 1: Form temporary target signal <ref type="bibr">3:</ref> v (n) ← Φ T r (n−1) + x (n−1)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4:</head><p>Step 2: Refine signal support using convex prior 5:</p><formula xml:id="formula_16">x (n) r = arg min x x − v (n) 2 2 + λJ(x), 6: s ← supp(x (n) r ) 7:</formula><p>Step 3: Estimate target signal <ref type="bibr" target="#b7">8</ref>:</p><formula xml:id="formula_17">Solve Φ T s Φ s x s = Φ T s y, with Φ s = Φ(:, s) 9:</formula><p>Set all but largest K entries in x s to zero, <ref type="bibr">10:</ref> x (n) (s) = x s (s)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>11:</head><p>Step 4: Calculate data residual <ref type="bibr">12:</ref> r (n) ← y − Φx (n)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>13:</head><p>n ← n + 1 14: end while signal recovery using convex sub-steps for which global minimizers are efficiently computable.</p><p>The proposed method, Convex Lattice Matching Pursuit (CoLaMP), is a greedy algorithm that attempts to solvê</p><formula xml:id="formula_18">x = arg min x Φx − y 2 2 + λJ(x) subject to x 0 ≤ K.<label>(14)</label></formula><p>The complete method is listed in Algorithm 2. In Step 1, CoLaMP proceeds like other matching pursuit algorithms; the unknown signal is estimated by multiplying the residual by the adjoint of the measurement operator. In Step 2, this estimate is refined by solving a support regularized problem of the form <ref type="formula" target="#formula_7">(7)</ref>. We solve this problem either via ADMM or the FBS method in Algorithm 1). In Step 3, a least-squares (LS) problem is solved to identify the signal that best matches the observed data, assuming the correct support was identified in Step 2. This LS problem is solved by a conjugate gradient method. Finally, in Step 4, the residual (the discrepancy between Φx and the data vector y) is calculated. The algorithm is terminated if the residual becomes sufficiently small or a maximum number of iterations is reached. CoLaMP has several desirable properties. First, the support set regularization (Step 2) helps to prevent signal support from growing quickly, and thus minimizes the cost of the least-squares problem in Step 3. Secondly, the use of a convex prior guarantees that a global minimum is obtained for every subproblem in Step 2, regardless of the considered clique structure. This is in stark contrast to other modelbased recovery algorithms, such as LaMP 1 , and model-based CoSaMP <ref type="bibr" target="#b1">[2]</ref>, which requires the solution to non-convex optimization problems to enforce structured support set models. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Numerical Experiments</head><p>We now apply the proposed regularizer to a range of datasets to demonstrate its efficacy for various applications. Unless stated otherwise, we showcase our algorithms using overlapping cliques of size 2 × 2 as shown in <ref type="figure" target="#fig_0">Figure 1(b)</ref>. Note that the numerical algorithms need not be restricted to those discussed above as different schemes (such as primaldual decomposition) are needed for different situations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Compressive Image Recovery</head><p>We first consider the recovery of background-subtracted images from compressive measurements. We use the "walk-ing2" surveillance video data <ref type="bibr" target="#b36">[37]</ref> with frames of dimension 288 × 384. Test data is generated by choosing two frames from a video sequence and computing the pixel-wise difference between their intensities. We compare the output of our proposed CoLaMP algorithm to that of other state-of-the-art recovery algorithms, such as overlapping group lasso <ref type="bibr" target="#b23">[24]</ref>, fixed-point continuation (FPC) <ref type="bibr" target="#b17">[18]</ref> and CoSaMP <ref type="bibr" target="#b29">[30]</ref>. Note that CoSaMP defines the support set using the 2K largest components of the error signal. The group lasso algorithm is equivalent to minimizing the objective in (14) using variational method. Unlike the CoLaMP algorithm, this method does not consider prescribed signal sparsity K. An example recovery using M = 3K measurements is shown in <ref type="figure" target="#fig_1">Figure 2</ref>. The sparsity level K is chosen such that the recovered images account for 97% of the compressive signal energy. The average K across datasets is 2800 and we fix λ = 2. Note that the spatially clustered pixels are recovered almost perfectly. Further, we randomly generated 50 such test images from the above dataset and compared the performance of the CoLaMP, group lasso, and FPC algorithms under varying numbers of measurements from 1K to 5K. The performance is measured in terms of the magnitude of reconstruction error normalized by the original image magnitude. Results are shown in <ref type="figure">Figure 4</ref> (left). We clearly see that the proposed smooth sparsity prior significantly improves the reconstruction quality over FPC. Furthermore, our algorithm is 7× faster than the group lasso algorithm. For M/K = 3, the average runtime is 215s for CoLaMP and 1510s for the group lasso algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Robust Signal Recovery</head><p>We next showcase the suitability of CoLaMP for signal recovery from noisy compressive measurements. We consider a 100 × 100 Shepp-Logan phantom image with a support size of K = 2636. A Gaussian random measurement matrix was used to sample M = 2K measurements, and the measurements were corrupted with additive white Gaussian noise. The signal-to-noise ratio of the resulting measurements is 10 dB. <ref type="figure">Figure 3</ref> shows the original and recovered images for various recovery algorithms. We also show the output from the first few iterates of the CoLaMP algorithm. The support of the target signal is almost exactly recovered within four iterations of CoSaMP and stabilizes by the end of 10 iterations. <ref type="figure">Figure 3</ref> also shows the recovery times of various algorithms running on the same laptop computer. CoLaMP is approximately 40× faster than the CoSaMP algorithm and it is at least 2× faster than FPC.</p><p>To enable a fair comparison, we also show the output obtained with CoLaMP using the 8-connected pixel clique in <ref type="figure" target="#fig_0">Figure 1</ref>(c), as well as the output of the group lasso algorithm <ref type="bibr" target="#b39">[40]</ref>, where each clique is of size 2 × 2. All these algorithms and our proposed method are implemented using ADMM. Not surprisingly, while all these algorithms beat CoLaMP in terms of runtime, their recovered signals do not match CoLaMP in terms of perceived closeness to target signal as shown in <ref type="figure">Figure 3</ref>. The CoLaMP results are regularized by λ 0 = 16. We then used an increasing value of λ n = 1.02 n λ 0 where n is the iteration number. In practice, we obtain better results if λ increases over time as it will heavily penalize sparse, blocky noise. For all other algorithms, we used the implementations provided by the authors.</p><p>For detailed quantitative comparisons, we repeat the above experiment using 100 Gaussian random measurement   <ref type="figure">Figure 4</ref> (center) illustrates that CoSaMP outperforms FPC at all SNRs even with 1.5K fewer measurements. Group lasso performs best at low SNR while its performance flattens out starting at 10 dB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Color Image Denoising</head><p>We now consider a variant of the denoising problem <ref type="bibr" target="#b4">(5)</ref> where the image gradient is defined over color images using the decorrelated vectorized TV (D-VTV) proposed in <ref type="bibr" target="#b30">[31]</ref> x = arg min</p><formula xml:id="formula_19">x∈R 3N c∈C λ ∇ d x ℓ c 2 + ∇ d x ch c 2 subject to x − y 2 ≤ κm.<label>(15)</label></formula><p>Here, ∇ d x ℓ ∈ R 2N and ∇ d x ch ∈ R 4N represent the stacked gradients of luminance and chrominance channels of the input color image, the constant m depends on the noise level, and κ is a fidelity parameter. To solve this problem numerically, we use the primal-dual algorithm described in <ref type="bibr" target="#b30">[31]</ref>, but we replace the shrinkage operator with the proximal operator (7) to adapt our clique-based regularizer. Following a protocol similar to D-VTV <ref type="bibr" target="#b30">[31]</ref>, we conduct experiments using 300 images from the Berkeley Segmentation Database <ref type="bibr" target="#b28">[29]</ref>. Noisy images with average PSNR 20 dB are obtained by adding white Gaussian noise. The resulting denoised output of our method (Block D-VTV) is compared to D-VTV in <ref type="figure" target="#fig_3">Figure 5</ref>. The zoomed-in version reveals that our method exhibits less uneven color artifacts and less pronounced staircasing artifacts than the D-VTV results. A quantitative comparison measured using average PSNR gain (in dB) is drawn in <ref type="figure">Figure 4</ref> (right) for various values of κ. Our method outperforms D-VTV by 0.25 dB. Also note that our method, Block D-VTV, obtains relatively better PSNR gain than the state-of-the-art D-VTV method at smaller values of κ. This observed gain is significant because smaller κ values lead to a tighter fidelity constraint and thus a smaller solution space around the noisy input. In such situations, Block D-VTV helps to improve image quality by leveraging input from neighboring pixels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Video Decomposition</head><p>We finally consider the robust PCA (RPCA) problem for structured sparsity of size 10 × 10 as formulated in (11) and using Algorithm 2. We consider the same airport surveillance video data <ref type="bibr" target="#b24">[25]</ref> as in <ref type="bibr" target="#b6">[7]</ref> with frames of dimension 144 × 176. For a clique formed from l × l patches, we observed that λ = 1/(l √ n 1 ) works best for our experiments as opposed to λ = 1/ √ n 1 used in <ref type="bibr" target="#b6">[7]</ref>. This is because each element of the matrix X is shared by l 2 sparsity inducing terms. The resulting low rank components (background) and foreground components of three such example video frames are shown in <ref type="figure" target="#fig_4">Figure 6</ref>. For all the approaches, the low rank components are nearly identical. We observe that the rank of the low-rank component remains the same. As highlighted with the green box, the noisy sparse edges appearing in the original RPCA disappear from the foreground component using our proposed method. We also display the foreground  component obtained using smaller overlapping cliques of size 3 × 3, but solved using ADMM as opposed to forwardbackward splitting (Algorithm 1). We found that for clique size of 10 × 10 the ADMM method becomes intractable because it requires approximately 50× more memory than the proposed forward-backward splitting method with fast convolutions (i.e., 204N L vs. 4N L).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusions</head><p>We have proposed a novel structured support regularizer for convex sparse recovery. Our regularizer can be applied to a variety of problems, including sparse-and-low-rank decomposition and denoising. For compressive signal recovery using large unstructured matrices, our convex regularizer can be used to improve the recovery quality of existing matchingpursuit algorithms. Compared to existing algorithms for this task, our proposed approach enjoys the capability of fast signal reconstruction from fewer measurements while ex-hibiting superior robustness against spurious artifacts and noise. For color image denoising, the restored images reveal more homogeneous color effects. For robust PCA, we achieve improved foreground-background separation with far fewer artifacts. We envision many more applications that could benefit of the proposed regularizer, including deblurring and inpainting. More sophisticated directions include using support regularization for structured dictionary learning <ref type="bibr" target="#b40">[41]</ref> and multitask classification.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Illustration of cliques and overlapping cliques.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Compressed sensing recovery results for background subtracted images using M = 3K.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>Robust recovery results for the phantom image from a noisy compressed signal. Quantitative Comparison: (left) Recovery performance of compressed sensing on background subtracted images; (center) Robust compressed sensing recovery error at various SNR; (right) Average denoising gain in PSNR (dB) for various values of κ matrices and record the average reconstruction error with SNR varying from 5 dB to 20 dB. For each algorithm, M is fixed to the minimal measurement number required to give close to perfect recovery in the presence of noise. For CoLaMP and overlapping group lasso, we set M = 2K, whereas for FPC and non-overlapping group lasso we set M = 3.5K.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Restoration of noisy images using Block D-VTV and existing D-VTV (best viewed in color).Original FramesLow rank component -background Original Robust PCA Robust PCA with block sparsity of 3x3 With block sparsity of 10x10</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Sparse-and-low-rank decomposition using original robust PCA and proposed approach.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">It is possible to restrict LaMP to planar Ising models, in which case a global optimum is computable<ref type="bibr" target="#b8">[9]</ref>.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The work of S. Shah and T. Goldstein   </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Convex optimization with sparsity-inducing norms. Optimization for Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jenatton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Obozinski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="19" to="53" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Model-based compressive sensing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">G</forename><surname>Baraniuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Cevher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Duarte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hegde</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A fast iterative shrinkagethresholding algorithm for linear inverse problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Teboulle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Imaging Sciences</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="183" to="202" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Distributed optimization and statistical learning via the alternating direction method of multipliers. Foundations and Trends® in Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Peleato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Eckstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Fast approximate energy minimization via graph cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Boykov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Veksler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zabih</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<publisher>PAMI</publisher>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="1222" to="1239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Fast dual minimization of the vectorial total variation norm and applications to color image processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">F</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inverse Problems and Imaging</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="455" to="484" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Robust principal component analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Candès</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the ACM (JACM)</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Robust uncertainty principles: Exact signal reconstruction from highly incomplete frequency information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Candès</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="489" to="509" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Sparse signal recovery using markov random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Cevher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Duarte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hegde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Baraniuk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Compressive sensing for background subtraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Cevher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Duarte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">G</forename><surname>Baraniuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV 2008</title>
		<imprint>
			<biblScope unit="page" from="155" to="168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Subband weighting with pixel connectivity for 3-d wavelet coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-L</forename><surname>Hwang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="52" to="62" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Mean shift: A robust approach toward feature space analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Comaniciu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Meer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<publisher>PAMI</publisher>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="603" to="619" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Group sparse optimization by alternating direction method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SPIE Optical Engineering+ Applications</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Efficient graphbased image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Huttenlocher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="167" to="181" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Block-sparse rpca for consistent foreground detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-F</forename><surname>Cheong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2012</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="690" to="703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The split Bregman method for l1-regularized problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Goldstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Imaging Sciences</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">A field guide to forward-backward splitting with a FASTA implementation. CoRR, abs/1411</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Goldstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Studer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">G</forename><surname>Baraniuk</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">3406</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A fixed-point continuation method for l1-regularized minimization with applications to compressed sensing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">T</forename><surname>Hale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CAAM TR07-07</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
		<respStmt>
			<orgName>Rice University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning with structured sparsity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3371" to="3412" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Group lasso with overlap and graph lasso</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jacob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Obozinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-P</forename><surname>Vert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Structured variable selection with sparsity-inducing norms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jenatton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Audibert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="2777" to="2824" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Structured sparse principal component analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jenatton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Obozinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<idno type="arXiv">arXiv:0909.1440</idno>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Spatio-temporal event classification using time-series kernel based structured sparsity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Jeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lőrincz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Szabó</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV 2014</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="135" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Multipath exploitation in through-the-wall radar imaging using sparse reconstruction. Aerospace and Electronic Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Leigsnering</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zoubir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Statistical modeling of complex backgrounds for foreground object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I.-H</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1459" to="1472" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Robust recovery of subspace structures by low-rank representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="171" to="184" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Image restoration using total variation with overlapping group sparsity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">W</forename><surname>Selesnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-G</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-Y.</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">295</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="232" to="246" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Network flow algorithms for structured sparsity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jenatton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">R</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">R</forename><surname>Obozinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="416" to="423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Cosamp: Iterative signal recovery from incomplete and inaccurate samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Needell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Tropp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied and Computational Harmonic Analysis</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Decorrelated vectorial total variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ono</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Yamada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2014</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Nonlinear total variation based noise removal algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">I</forename><surname>Rudin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Fatemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physica D: Nonlinear Phenomena</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Total variation denoising with overlapping group sparsity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">W</forename><surname>Selesnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-Y.</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="5696" to="5700" />
		</imprint>
	</monogr>
	<note>2013 IEEE International Conference on</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Normalized cuts and image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="888" to="905" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Signal recovery from random measurements via orthogonal matching pursuit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Tropp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Gilbert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="4655" to="4666" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Sparse reconstruction by separable approximation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">D</forename><surname>Nowak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Figueiredo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2479" to="2493" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Online object tracking: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Foreground detection using low rank and structured sparsity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Multimedia and Expo (ICME), 2014 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Efficient methods for overlapping group lasso</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="352" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Model selection and estimation in regression with grouped variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series B (Statistical Methodology)</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning structured low-rank representations for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="676" to="683" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
