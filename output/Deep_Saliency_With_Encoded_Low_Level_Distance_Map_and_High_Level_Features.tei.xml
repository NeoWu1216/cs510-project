<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Saliency with Encoded Low level Distance Map and High Level Features</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gayoung</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">SenseTime Group Limited</orgName>
								<orgName type="institution">KAIST</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wing</forename><surname>Tai</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">SenseTime Group Limited</orgName>
								<orgName type="institution">KAIST</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junmo</forename><surname>Kim</surname></persName>
							<email>junmo.kim@kaist.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="laboratory">SenseTime Group Limited</orgName>
								<orgName type="institution">KAIST</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaist</forename></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">SenseTime Group Limited</orgName>
								<orgName type="institution">KAIST</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Saliency with Encoded Low level Distance Map and High Level Features</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:39+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent advances in saliency detection have utilized deep learning to obtain high level features to detect salient regions in a scene. These advances have demonstrated superior results over previous works that utilize hand-crafted low level features for saliency detection. In this paper, we demonstrate that hand-crafted features can provide complementary information to enhance performance of saliency detection that utilizes only high level features. Our method utilizes both high level and low level features for saliency detection under a unified deep learning framework. The high level features are extracted using the VGG-net, and the low level features are compared with other parts of an image to form a low level distance map. The low level distance map is then encoded using a convolutional neural network(CNN) with multiple 1 × 1 convolutional and ReLU layers. We concatenate the encoded low level distance map and the high level features, and connect them to a fully connected neural network classifier to evaluate the saliency of a query region. Our experiments show that our method can further improve the performance of stateof-the-art deep learning-based saliency detection methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Saliency detection aims to detect distinctive regions in an image that draw human attention. This topic has received a great deal of attention in computer vision and cognitive science because of its wide range of applications such as content-aware image cropping <ref type="bibr" target="#b21">[22]</ref> and resizing <ref type="bibr" target="#b2">[3]</ref>, video summarization <ref type="bibr" target="#b23">[24]</ref>, object detection <ref type="bibr" target="#b19">[20]</ref>, and person re-identification <ref type="bibr" target="#b30">[31]</ref>. Various papers such as DRFI <ref type="bibr" target="#b12">[13]</ref>, GMR <ref type="bibr" target="#b29">[30]</ref>, DSR <ref type="bibr" target="#b16">[17]</ref>, RBD <ref type="bibr" target="#b31">[32]</ref>, HDCT <ref type="bibr" target="#b14">[15]</ref>, HS <ref type="bibr" target="#b28">[29]</ref> and GC <ref type="bibr" target="#b6">[7]</ref> utilize low level features such as color, texture and location information to investigate characteristics of salient regions including objectness, boundary convexity, spatial distribution, and global contrast. The recent success of deep learning in object recognition and classification <ref type="bibr" target="#b22">[23]</ref> brought to a revolution in computer vision. Inspired by the (a) (b) (c) (d) (e) (f) <ref type="figure">Figure 1</ref>: (a) Input images, (b) Ground truth masks, (c) Fuzzy saliency masks from VGG16 features (HF setting, described in Section 3.3), (d-f) Results of (d) MDF <ref type="bibr" target="#b15">[16]</ref>, (e) MCDL <ref type="bibr" target="#b20">[21]</ref>, and (f) our method.</p><p>human visual system, deep learning builds hierarchical layers of visual representation to extract the high level features of an image. Using extracted high level features, several recent works <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b20">21]</ref> have demonstrated state-of-the-art performance in saliency detection that significantly outperform previous works that utilized only low level features.</p><p>As discussed in <ref type="bibr" target="#b8">[9]</ref>, while high level features are good to evaluate objectness in an image, they are relatively weak in for determining precise localization. This is because multiple levels of convolutional and pooling layers "blur" the object boundaries, and high level features from the output of the last layer are too coarse spatially for the saliency detection task. This problem is illustrated in <ref type="figure">Figure 1</ref>(c). To generate a precise saliency mask, previous studies utilized various methods including object proposal <ref type="bibr" target="#b26">[27]</ref> and superpixel classification <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b20">21]</ref>. Yet, it was still very hard to differentiate salient regions from their adjacent non-salient regions because their feature distances were not directly encoded.</p><p>In this paper, we introduce the encoded low level distance map (ELD-map), which directly encodes the feature distance between each pair of superpixels in an image. Our ELD-map encodes feature distance for various low level features including colors, color distributions, Gabor filter responses, and locations. Our ELD-map is unique in that it uses deep learning as an auto-encoder to encode these low level feature distances by multiple convolutional layers with 1 × 1 kernels. The encoded feature distance map has strong Reused for every query region in testing time <ref type="figure">Figure 2</ref>: Overall pipeline of our method. We compute the ELD-map from the initial feature distance map for each query region and concatenate the high level feature from the output of the conv5 3 layer of the VGG16 model. discriminative power to evaluate similarities between different parts of an image with precise boundaries among superpixels. We concatenate our ELD-map and the output of the last convolutional layer from the VGG-net (VGG16) <ref type="bibr" target="#b24">[25]</ref> to form a new feature vector which is a composite of both high level and low level information. Using our new feature vector, we can precisely estimate saliency of superpixels. Without any post-processing, this method generates an accurate saliency map with precise boundaries. In summary, our paper offers the following contributions:</p><p>• We introduce the ELD-map which shows that low level features can play complementary roles to assist high level features with the precise detection of salient regions.</p><p>• Compared with previous works that utilized either high level or low level features, but not both, our work demonstrates consistent improvements across different benchmark datasets.</p><p>• Because high level features can be reused for different query regions in an image, our method runs fast. The testing time in the ECSSD dataset <ref type="bibr" target="#b28">[29]</ref> takes only around 0.5 seconds per an image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>In this section, representative works in salient region detection are reviewed. We refer readers to <ref type="bibr" target="#b3">[4]</ref> and <ref type="bibr" target="#b4">[5]</ref> for a survey and a benchmark comparison of the state-of-the-art salient region detection algorithms.</p><p>Recent trends in salient region detection utilize learningbased approaches, which were first introduced by Liu et al. <ref type="bibr" target="#b18">[19]</ref>. Liu et al. were also the first group to released a benchmark dataset (MSRA10K) with ground truth evaluation. Following this work, several representative benchmarks with ground truth evaluation were released. These benchmarks include ECSSD <ref type="bibr" target="#b28">[29]</ref>, Judd <ref type="bibr" target="#b13">[14]</ref>, THUR15K <ref type="bibr" target="#b5">[6]</ref>, DUTOMRON <ref type="bibr" target="#b29">[30]</ref>, PASCAL-S <ref type="bibr" target="#b17">[18]</ref>, and FT <ref type="bibr" target="#b0">[1]</ref>. They cover rich variety of images containing different scenes and subjects. In addition, each one exhibits different characteristics. For example, the ground truth of the MSRA10K dataset are binary mask images which were manually segmented by human, while the ground truth of the FT <ref type="bibr" target="#b0">[1]</ref> dataset were determined by human fixation.</p><p>Discriminative Regional Feature Integration(DRFI) <ref type="bibr" target="#b12">[13]</ref>, Robust Background Detection(RBD) <ref type="bibr" target="#b31">[32]</ref>, Dense and Sparse Reconstruction(DSR) <ref type="bibr" target="#b16">[17]</ref>, Markov Chain(MC) <ref type="bibr" target="#b11">[12]</ref>, High Dimensional Color Transform(HDCT) <ref type="bibr" target="#b14">[15]</ref>, and Hierarchical Saliency(HS) <ref type="bibr" target="#b28">[29]</ref> are the top 6 models for salient region detection reported in the benchmark paper <ref type="bibr" target="#b4">[5]</ref>. These algorithms consider various heuristic priors such as the global contrast prior <ref type="bibr" target="#b28">[29]</ref> and the boundary prior <ref type="bibr" target="#b12">[13]</ref> and often generate high-dimensional features to increase discriminative power <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b12">13]</ref> to distinguish salient regions from non-salient regions. These methods are all based on hand-crafted low level features without deep learning.</p><p>Deep learning has emerged in the field of saliency detection last year. Several methods that utilize deep learnings for saliency detection were simultaneously proposed. This includes Multiscale Deep Feature(MDF) <ref type="bibr" target="#b15">[16]</ref>, Multi-Context Deep Learning(MCDL) <ref type="bibr" target="#b20">[21]</ref>, and Local Estimation and Global Search(LEGS) <ref type="bibr" target="#b26">[27]</ref>. They utilized high level features from the deep convolutional neural network (CNN) and demonstrated superior results over previous works that utilized only low level features. MDF and MCDL utilize superpixel algorithms, and query each region individually to assign saliency to superpixels. For each query region, MDF generates three input images that cover different scopes of an input image, and MCDL uses sliding windows with deep CNN to compute the deep features of the center superpixel. LEGS first generates an initial rough saliency mask from deep CNN and refines the saliency map using an object proposal algorithm.</p><p>Compared to the aforementioned methods, our work uti-  <ref type="figure">Figure 3</ref>: Visualization of the construction process for the initial low level feature distance map. Each grid cell, which represents uniformly divided area of an image, is described by the features of the superpixel that occupies the largest area of the grid cell. Using the features, we construct an N × N × K feature distance map. The computed features and distances are summarized in <ref type="table" target="#tab_3">Table 1 and Table 2</ref> lizes high level and low level features simultaneously. The high level features evaluate the objectness in an image with coarse spatial location and the low level features evaluate similarities between the different superpixels in an image. Our high level and low level features are combined and evaluated by a multi-level fully connected neural network classifier, that seamlessly considers both high level and low level features to assign saliency to query superpixels. Experiments demonstrate that our method significantly outperforms previous methods that utilize either low level features or high level features, but not both.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Algorithms</head><p>The overall pipeline of our method is illustrated in <ref type="figure">Figure 2</ref>. First, the process for construction of the ELD-map is described. Then, we describe how the high level features were extracted and integrated with the ELD-map for salient region classification. At the end of this section, we report the results of our self evaluations to analyze the effects of the ELD-map and the high level features in our saliency detection framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Construction of the ELD-map</head><p>Our algorithm utilizes a superpixel-based approach for saliency detection. To segment an image into superpixels, the SLIC <ref type="bibr" target="#b1">[2]</ref> algorithm is used. The major benefits of using the SLIC algorithm for superpixel segmentation are that the segmented superpixels are roughly regular and that it provides control on the number of superpixels.</p><p>After superpixel segmentation, the initial hand-crafted low level features of each superpixel are calculated, and the superpixel representation is converted into a regular grid representation as illustrated in <ref type="figure">Figure 3</ref>. To be more specific, we assign superpixels to grid cells according to their occupying area in each cell. This regular grid representation is efficient for CNN architecture because we can convert images with different resolutions and aspect ratios into a fixed size distance map without resizing and cropping.</p><p>In our implementation, the size of the regular grid was set to 23 × 23. We index the superpixels as S =    <ref type="table">Table 1</ref>. Our hand-crafted features are all low level features related to colors (average colors in RGB, LAB, and HSV spaces, and their local color histograms), textures (Gabor filter responses <ref type="bibr" target="#b27">[28]</ref> averaged over pixels in each region), and locations (center location of a superpixel). We normalize the pixel coordinates so that the range of coordinates was within [0, 1] and include the maximum over 24 values for the Gabor filter response in each region. Each grid cell descriptor is equal to the descriptor of the superpixel which occupies the largest area inside that grid cell, i.e., f (c ij ) := f (r * c ), where r * c = arg max rc #pixels(r c ∩ c ij ). Similar to MCDL <ref type="bibr" target="#b20">[21]</ref> and MDF <ref type="bibr" target="#b15">[16]</ref>, we query the saliency score of each region individually. For each query region, we compute a low level feature distance map that modelled the feature distances between the queried superpixel f (r q ) and grid cells f (c ij ) in the regular grid. For the mean color value and Gabor response, we simply compute the differences within them where negative values are allowed, and use the Chi-square (χ 2 ) distance for color histograms between r * c and r q . We attach the average colors of f (c ij ) at the end of the distance measurements as a reference point, and find that this improved the performance. <ref type="table" target="#tab_3">Table 2</ref> summarizes the computed feature distances of the initial feature distance map where the number of the initial features (K) is 54. After computing the distances, the size of the initial feature distance map becomes 23 × 23 × 54.</p><formula xml:id="formula_0">Distance map features #f (·) Feature Index f (c ij ) − f (r q ) 1-36 1-36 χ 2 distance(f (c ij ), f (r q )) 37-110 37-45 f (c ij ) 1-9 46-54</formula><p>The initial feature distance map is then encoded to a compact but accurate feature distance map using the multiple 1 × 1 convolutional and ReLU layers, as illustrated in <ref type="figure">Figure 2</ref>. The multiple 1 × 1 convolutional and ReLU layers work as a fully connected layer across channels to find the best nonlinear combination of feature distances that better describe the similarities and dissimilarities between a query superpixel and the other regions of an image. Because the dimension of the initial map is reduced, we call this distance map as an encoded low level distance map (ELD-map). In our implementation, the size of the ELDmap was 23 × 23 × 3. In the self-evaluation experiment in <ref type="table" target="#tab_5">Table 3</ref>, we find that encoding the low level feature distance map with the deep CNN with 1 × 1 kernel enhances the performance of our method. The effects of the encoding will be discussed in Section 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Integration with High Level Features</head><p>We extract the high level features using the VGG16 model pretrained by the ImageNet Dataset <ref type="bibr" target="#b22">[23]</ref>. The VGG16 <ref type="bibr" target="#b24">[25]</ref> won the ImageNet2014 CLS-LOC task. We used the VGG16 model distributed by Caffe Model Zoo <ref type="bibr" target="#b10">[11]</ref> without finetuning. We resize the input images to 224 × 224 to fit to the fixed input size of the VGG16 model and extract a "conv5 3" feature map, which is generated after passing the last convolutional layer. The extracted features has 512 channels and 14 × 14 resolution. To fit the features to our GPU memory, we attach an additional convolutional layer with a 1 × 1 kernel for feature selection and dimensionality reduction as in GoogleNet <ref type="bibr" target="#b25">[26]</ref>.</p><p>For each input image, we process it with the pre-trained deep CNN only once and reuse the extracted high level feature map for all queried regions. Therefore, our computational cost is small even when we use a very deep and powerful model such as the VGG16 model. Although other parts of our algorithm, including generating the ELD-map and applying fully-connected layers, should be repeated each time, the cost from these parts is much smaller than running the VGG16 model.</p><p>Before applying the fully-connected layers to classify the queried region, we concatenate the ELD-map and "conv5 3" feature map after flattening each map. Afterwards, two fully-connected layers with 1024 nodes generate a saliency score for the queried region using the concatenated features. We use the cross entropy loss for softmax classifier to evaluate the outputs:</p><formula xml:id="formula_1">L = − 1 j=0 1 (y=j) log( e zj e z0 + e z1 )<label>(1)</label></formula><p>where 0 and 1 denote non-salient and salient region labels respectively, and z 0 and z 1 are the score of each label of training data. Since the ELD-map features and the high  <ref type="table" target="#tab_5">Table 3</ref>.</p><p>(a) ECSSD (b) PASCAL-S <ref type="figure">Figure 5</ref>: Precision-Recall graphs of the controlled experiments described in <ref type="table" target="#tab_5">Table 3</ref> (a) ECSSD (b) PASCAL-S <ref type="figure">Figure 6</ref>: Precision-Recall graphs of the controlled experiments to show the effect of the statistical features.</p><p>level features are fixed in length, their spatial correlation can be learnt from training data automatically in the fully connected layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Analysis of the Effects of the Encoded Low level Distance map</head><p>Although theoretically neural networks can model any   complex function <ref type="bibr" target="#b9">[10]</ref>, practically they may suffer from limited training data and limited computational resources. For instance, overfitting and underfitting frequently occur because of a small dataset and the complexity of desired features. It is also common for CNN to generate feature maps with much lower resolution than original input images. By providing strongly relevant information, the encoded low level distance map(ELD-map) complements the features from deep CNN and guides the classifier to learn properly. ELD-map has two main advantages: (1) it can easily generate the fine-grained dense saliency mask, and (2) it provides additional low level feature distances, which can be hard to learn for CNN, such as Chi-square distance between histograms.</p><formula xml:id="formula_2">(a) (b) (c) (d) (e) (f) (g) (h) (i) (j)</formula><p>We performed multiple controlled experiments to demonstrate the effects of the ELD-map in our algorithm. We conducted the experiments using four different settings: The ELD-HF setting uses both the ELD-map and the high level feature map from the VGG16 model. The LD-HF setting utilizes both the low level feature distances and the high level feature map, but does not encode the low level distances with the 1 × 1 convolutional network. The ELD setting uses only ELD-map without high level features from the VGG16 model. The HF setting uses the high level feature map from VGG16 model and the location distance between the query region and other regions to notify which region is queried. We ran all models until the training data loss converged.</p><p>The results of the controlled experiments are shown in <ref type="figure" target="#fig_1">Figure 4</ref>. The model using only the high level feature map from the deep CNN detected the approximate location of the salient objects but was unable to capture detailed location because the high level feature maps had lower resolution than the resolution of the original input images. On the other hand, the model with only the low level features failed to distinguish the salient object in the first row. With both the low level feature distances and the high level feature map, the models could correctly capture salient objects and their exact boundaries. Also, we found that the ELD-map often helps to find salient objects that are difficult to detect using only CNN as shown in the second row. We speculate that the ELD-map can provide additional information which is hard to be accurately modeled by the convolutional layers. Some of the hand-crafted features of our method are statistical features, e.g. histogram, and we use χ 2 distance to measure the distance between histograms that would be difficult to learn by CNN. To demonstrate the effects of the statistical features, we re-train our network with the histogram features removed from our network. The comparisons are shown in <ref type="figure">Fig. 6</ref>. Clearly, the histogram features improve the performance of our work. Similarly, for features in other color space, e.g. LAB and HSV, it may require more layers  <ref type="table">Table 4</ref>: The F-measure scores of salient region detection algorithms on five popular datasets. The best score is marked in bold.</p><p>to model such transformation, but we can easily adopt them from hand-crafted features. <ref type="table" target="#tab_5">Table 3</ref> summarizes the controlled experiments for the self-evaluation of our method. It also shows the quantitative comparisons in terms of f-measure on the ECSSD and the PASCAL-S datasets. The corresponding quantitative comparisons in terms of the Precision-Recall graphs are presented in <ref type="figure">Figure 5</ref>. The model utilizing both ELD-map and high level features exhibits the best performance. By comparing ELD-HF and LD-HF settings, we found that it is useful to apply 1 × 1 kernels among the low level features. <ref type="figure" target="#fig_2">Figure 7</ref> shows the initial hand-crafted distance features and ELD-map. For the ELD-map, which is originally the 23 × 23 size grid, we visualized each superpixel using the feature value of the closest grid cell according to the location of the center pixel. Each hand-crafted feature has its own weakness but it captures different aspects of similarities or dissimilarities between superpixels. Our 1 × 1 kernels work as fully-connected layers among low level feature distances and generate a powerful feature distance map by combining all of the original feature distances nonlinearly. This nonlinear mapping is data-driven which is directly learnt from training data automatically. We can see the strong discriminative power of feature distances in ELD-map. While the third channel (j) is related to the position of the query region, the other two channels (h-i) seem to indicate the differences of appearance such as color and texture. Therefore, the ELD-map helps to group regions that belong to the same object, because regions which have the similar color and texture have similar values in the two channels of the ELD-map regardless of their position.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiment and Discussion</head><p>We evaluated the performance of our algorithm using various datasets. The MSRA10K <ref type="bibr" target="#b18">[19]</ref> is a dataset with 10,000 images which includes the ASD dataset <ref type="bibr" target="#b0">[1]</ref>. Most images in this dataset contains single object. The PASCAL-S <ref type="bibr" target="#b17">[18]</ref> is generated from the PASCAL VOC dataset <ref type="bibr" target="#b7">[8]</ref>   We trained our model using 9,000 images from the MSRA10K dataset after excluding the same images in ASD dataset. We did not use validation set and trained the model until its training data loss converges. From each image, we use about 30 salient superpixels and 70 non-salient superpixels; around 0.9 million input data are generated. The layers of VGG16 model are fixed by setting the learning rate equal to zero. For other layers, we initialize the weights by the "xavier" (caffe parameter), and we set the base learning rate equal to 0.001. We use stochastic gradient descent method with momentum 0.9 and decrease running rate 90% when training loss does not decrease. Training our model takes 3 hours for 100,000 iterations with mini-batch size 128.</p><p>Our results were compared with MCDL <ref type="bibr" target="#b20">[21]</ref>, MDF <ref type="bibr" target="#b15">[16]</ref>, LEGS <ref type="bibr" target="#b26">[27]</ref>, DRFI <ref type="bibr" target="#b12">[13]</ref>, DSR <ref type="bibr" target="#b16">[17]</ref>, GMR <ref type="bibr" target="#b29">[30]</ref>, HDCT <ref type="bibr" target="#b14">[15]</ref>, and HS <ref type="bibr" target="#b28">[29]</ref>, which are the state-of-the-art algorithms. DRFI, DSR, GMR, HDCT and HS use low level features and MCDL, MDF and LEGS utilize deep CNN for high level context. We obtained the result images from the project site of each algorithm or the benchmark evaluation <ref type="bibr" target="#b4">[5]</ref>. The results which were not provided were generated from the authors' source codes published in the web. The comparisons on Precision-Recall(PR) graph and Mean Absolute Error(MAE) graph are presented in <ref type="figure" target="#fig_3">Figure 8</ref>. Maximum F-measure scores and MAE values are also described in <ref type="table" target="#tab_8">Table 4 and Table 5</ref>. We used the evaluation codes used in the benchmark paper <ref type="bibr" target="#b4">[5]</ref>. The PR graph and f-measure score tend to be more informative than ROC curve because salient pixels are usually less than nonsalient <ref type="bibr" target="#b4">[5]</ref>. Following the criteria by Achanta et. al. <ref type="bibr" target="#b0">[1]</ref>, we moved the threshold from 0 to 255 to generate binary masks(M ). Using the ground truth(G), the precision and recall is calculated as follows: We also reported the F-Measure score which is a balanced measurement between precision and recall as follows:</p><formula xml:id="formula_3">P recision = |M G| |M | , Recall = |M G| |G|<label>(2)</label></formula><formula xml:id="formula_4">F β = (1 + β 2 )P recision × Recall β 2 × P recision + Recall<label>(3)</label></formula><p>where β 2 is typically set to 0.3. We visualized f-measure score for the different thresholds and reported the maximum f-measure score which well describes the overall detection performance <ref type="bibr" target="#b4">[5]</ref>. In our algorithm, making binary masks using the high threshold around 240 generated good f-measure score.</p><p>The overlapping-based evaluations give higher score to methods which assign high saliency score to salient pixel correctly. However, the evaluation on non-salient regions can be unfair especially for the methods which successfully detect non-salient regions, but missed the detection of salient regions <ref type="bibr" target="#b4">[5]</ref>. Therefore, we also calculated the mean absolute error(MAE) for fair comparisons as suggested by <ref type="bibr" target="#b4">[5]</ref>. The MAE evaluates the detection accuracy as follow:</p><formula xml:id="formula_5">M AE = 1 W × H W x=1 H y=1 |S(x, y) − G(x, y)| (4)</formula><p>where W and H are width and height of an image, S is <ref type="figure">Figure 10</ref>: Failure cases of our algorithm. (a) Input images, (b) Ground truths, Results of (c) our method, (d) MCDL <ref type="bibr" target="#b20">[21]</ref>, (e) MDF <ref type="bibr" target="#b15">[16]</ref>.</p><formula xml:id="formula_6">(a) (b) (c) (d) (e)</formula><p>the estimated saliency map and G is the ground truth binary mask.</p><p>In <ref type="figure" target="#fig_3">Figure 8</ref>, the PR-graph indicates our algorithm achieves the better performance than the previous works including MDF and MCDL which also utilize CNN models. Our algorithm shows the lowest MAE and the highest maximum F-measure score on most of the datasets. Visual comparisons of various methods are shown in <ref type="figure">Figure 9</ref>.  <ref type="bibr" target="#b20">[21]</ref> (e) MDF <ref type="bibr" target="#b15">[16]</ref> (f) LEGS <ref type="bibr" target="#b26">[27]</ref> (g) DRFI <ref type="bibr" target="#b12">[13]</ref>, (h) DSR <ref type="bibr" target="#b16">[17]</ref>, (i) GMR <ref type="bibr" target="#b29">[30]</ref>, (j) HDCT <ref type="bibr" target="#b14">[15]</ref> (k) HS <ref type="bibr" target="#b28">[29]</ref>. From the top to the bottom, row 1-2 are the images with a low-contrast salient object, row 3-4 are with complicated background, row 5-6 are with multiple salient objects and row 7 is with a salient object touching the image boundaries.</p><p>We visualize the results from various difficult cases including low-contrast objects (row 1-3), complicate backgrounds (row 4-6), small salient objects (row 7-8), multiple salient objects(row 9-10) and touching boundary examples (row <ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref>. Our algorithm shows especially good performance on images with low-contrast salient objects and complicated backgrounds, and also works well on other difficult scenes.</p><p>In <ref type="figure">Figure 10</ref>, we reported some failure cases. The first and the second results contain correct salient objects but also highlight non-salient regions. The third and fourth examples have the extremely difficult scenes with a small, low-contrast and boundary touching the salient object. Because these kinds of data are not provided much by the training data, MSRA10K, we may further improve the performance with richer training data. For these difficult scenes, MCDL <ref type="bibr" target="#b20">[21]</ref> and MDF <ref type="bibr" target="#b15">[16]</ref> also fail to find the salient objects precisely.</p><p>The running time of our algorithm was measured from the ECSSD dataset, where tested images were of size 400 × 300. We used a server machine with intel i7 CPU, 8GB RAM and GTX Titan-Black for testing. Our model, developed by C++ and based on Caffe <ref type="bibr" target="#b10">[11]</ref> library, took around 0.5 seconds per image. The training of our deep CNN took around 3 hours under the same environment. The short training time and testing time is also an advantage of our method. This is due to the sharing of our high level features which only need to be computed once for a whole image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we have introduced a new method to integrate the low-level and the high-level features for saliency detection. The Encoded Low-level Distance map (ELDmap) has stronger discriminative power than the original low-level feature distances to measure similarities or dissimilarities among superpixels. When concatenated with the high-level features from the deep CNN model (VGG16), our method shows the state-of-the-art performance in terms of both visual qualities and quantitative comparisons. As a future work, we are planning to explore more various CNN architectures to further improve the performance of our work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>{r 1 , ..., r M }, and the grid cells of the regular grid as G = {c 11 , c 12 , ..., c N N }, N = 23. We denote the computed feature descriptor of each superpixel region as f (r c ). The collected features for each superpixel are summarized in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Visual comparisons of results in our selfevaluation experiments. (a) Input images, (b) Ground truth masks, (c-f) the results of our algorithm (c) using both ELDmap and high level features (ELD-HF) (d) using both nonencoded low level distance map and high level features (LD-HF) (e) using only encoded low level distance map (ELD) (f) using only high level features (HF). Details of each experiment are described in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 7 :</head><label>7</label><figDesc>Comparisons of the discriminative power of different features and our ELD-map feature space. (a) Input images, the query superpixels are highlighted. (b)-(g) are the distance maps of the different features between the query superpixel and other superpixels in an image. (b)-(c) Distance maps of average color of (b) R-channel (RGB color space), and (c) L-channel (LAB color space). (d) Differences of the first Gabor filter responses. (e) Differences of the maximum gabor filter responses. (f)-(g) Chi-square distance maps of (f) L-channel histogram (LAB color space), and (g) H-channel histogram (HSV color space). (h)-(j) our Encoded Low level Distance map (ELD-map).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 8 :</head><label>8</label><figDesc>From top to bottom, Precision-Recall (PR) graph, F-measure score with different thresholds and Mean Absolute Error (MAE) of various algorithms on five popular datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>(a) Inputs (b) GT (c) Ours (d) MCDL (e) MDF (f) LEGS (g) DRFI (h) DSR (i) GMR (j) HDCT (k) HS Figure 9: Visual comparisons of our results and the state-of-the-art methods on difficult scenes. (a) original image, (b) ground truth, (c) Ours (d) MCDL</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head></head><label></label><figDesc>Table 1: The list of extracted features of a superpixel.</figDesc><table>Features of a superpixel (f (r c )) Feature Index 
Average RGB value 
1-3 
Average LAB value 
4-6 
Average HSV value 
7-9 
Gabor filter response 
10-33 
Maximum Gabor response 
34 
Center location 
35-36 
RGB color histogram 
37-61 
LAB color histogram 
62-86 
HSV color histogram 
87-110 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc>The list of feature distances used for computing the initial low level feature distance map. f (r q ) is the extracted features of a query superpixel, r q , and f (c ij ) is the extracted features of a grid cell c ij , where f (c ij ) := f (r * c ). Details are described in Section 3.1.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 3 :</head><label>3</label><figDesc>The detail of settings of the controlled experiments. Using both ELD-map and high level features from VGG16 shows the best performance.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>The Mean Absolute Error(MAE) of salient re-
gion detection algorithms on five popular datasets. The best 
score is marked in bold. 

truth segmentation. It also contains images with complex 
structures. The DUT-OMRON [30] has 5,168 high qual-
ity images and the THUR15K [6] contains 6,232 images of 
specific classes. 
</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Frequency-tuned salient region detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hemami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Estrada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Susstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Slic superpixels compared to state-of-the-art superpixel methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lucchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Susstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell. (TPAMI)</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2274" to="2282" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Seam carving for content-aware image resizing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Avidan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shamir</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">26</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Salient object detection: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Salient object detection: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Salientshape: group saliency in image collections. The Visual Computer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-M</forename><surname>Hu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Global contrast based salient region detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-M</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell. (TPAMI)</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="569" to="582" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The pascal visual object classes challenge: A retrospective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="98" to="136" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Hypercolumns for object segmentation and fine-grained localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multilayer feedforward networks are universal approximators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hornik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stinchcombe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>White</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural networks</title>
		<imprint>
			<date type="published" when="1989" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="359" to="366" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Saliency detection via dense and sparse reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Int&apos;l Conf. on Computer Vision (ICCV)</title>
		<meeting>of Int&apos;l Conf. on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Salient object detection: A discriminative regional feature integration approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">Z Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning to predict where humans look</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Judd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ehinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Int&apos;l Conf. on Computer Vision (ICCV)</title>
		<meeting>of Int&apos;l Conf. on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Salient region detection via high-dimensional color transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Visual saliency based on multiscale deep features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Saliency detection via dense and sparse reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Int&apos;l Conf. on Computer Vision (ICCV)</title>
		<meeting>of Int&apos;l Conf. on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The secrets of salient object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning to detect a salient object</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N.-N</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Switchable deep network for pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="899" to="906" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Saliency detection by multi-context deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">O R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Autocollage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bordeaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hamadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="847" to="852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2015-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Summarizing visual data using bidirectional similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Simakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Caspi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition. CoRR, abs/1409.1556</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep networks for saliency detection via local estimation and global search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="3183" to="3192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Efficient gabor filter design for texture segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">P</forename><surname>Weldon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">E</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">F</forename><surname>Dunn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2005" to="2015" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Hierarchical saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Saliency detection via graph-based manifold ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Unsupervised salience learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3586" to="3593" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Saliency optimization from robust background detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
