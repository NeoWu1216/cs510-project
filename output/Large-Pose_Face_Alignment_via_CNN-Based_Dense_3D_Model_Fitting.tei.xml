<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Large-pose Face Alignment via CNN-based Dense 3D Model Fitting</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amin</forename><surname>Jourabloo</surname></persName>
							<email>jourablo@msu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Michigan State University</orgName>
								<address>
									<postCode>48824</postCode>
									<settlement>East Lansing</settlement>
									<region>MI</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Michigan State University</orgName>
								<address>
									<postCode>48824</postCode>
									<settlement>East Lansing</settlement>
									<region>MI</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Large-pose Face Alignment via CNN-based Dense 3D Model Fitting</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:42+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Large-pose face alignment is a very challenging problem in computer vision, which is used as a prerequisite for many important vision tasks, e.g, face recognition and 3D face reconstruction. Recently, there have been a few attempts to solve this problem, but still more research is needed to achieve highly accurate results. In this paper, we propose a face alignment method for large-pose face images, by combining the powerful cascaded CNN regressor method and 3DMM. We formulate the face alignment as a 3DMM fitting problem, where the camera projection matrix and 3D shape parameters are estimated by a cascade of CNN-based regressors. The dense 3D shape allows us to design pose-invariant appearance features for effective CNN learning. Extensive experiments are conducted on the challenging databases (AFLW and AFW), with comparison to the state of the art.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Face alignment is the process of aligning a face image and detecting specific fiducial points, such as eye corners, nose tip, etc. Improving the face alignment accuracy is beneficial for many computer vision tasks related to facial analysis, because it is used as a prerequisite for these tasks, e.g., face recognition <ref type="bibr" target="#b27">[28]</ref>, 3D face reconstruction <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref> and face de-identification <ref type="bibr" target="#b9">[10]</ref>.</p><p>Given its importance, face alignment has been an active research topic since 1990s <ref type="bibr" target="#b28">[29]</ref>, with the well-known Active Shape Model <ref type="bibr" target="#b4">[5]</ref> and Active Appearance Model (AAM) <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b12">13]</ref>. Recently, face alignment works are very popular in top vision venues, as demonstrated by the progress in Constrained Local Model based approaches <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b21">22]</ref>, AAM-based approaches <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14]</ref> and regressionbased approaches <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b32">33]</ref>. Despite the fruitful prior work and continuous progress of face alignment (e.g., the latest impressive iBUG results <ref type="bibr" target="#b25">[26]</ref>), face alignment for largepose faces is still very challenging and there is only a few published work in this direction, as summarized in <ref type="table" target="#tab_0">Table 1</ref>. <ref type="figure">Figure 1</ref>. The proposed method estimates landmarks for large-pose faces by fitting a dense 3D shape. From left to right: initial landmarks, fitted 3D dense shape, estimated landmarks with visibility. The green/red/yellow dots in the right column show the visible/invisible/cheek landmarks, respectively. Therefore, this is a clear research gap that needs to be addressed, which is exactly the focus of this work.</p><p>To tackle large-pose face alignment, our technical approach is driven by the inherent challenges associated with this problem. First of all, faces have different numbers of visible landmarks under pose variation, and the spatial distribution of the landmarks is highly pose dependent. This presents challenges for existing face alignment approaches since most are based on 2D shape models, which inherently have difficulty in modeling the 3D out-of-plane deformation. In contrast, given the fact that a face image is a projection of a 3D face, we propose to use a dense 3D Morphable Model (3DMM) and the projection matrix as the representation of a 2D face image. Therefore, face alignment amounts to estimating this representation, i.e., performing the 3DMM fitting to a face image with arbitrary poses.</p><p>Second, the typical analysis-by-synthesis-based optimization approach for 3DMM fitting is inefficient and also assumes the 2D landmarks are provided either manually or with a separate face alignment method, which conflicts with the goal of our work. This motivates us to employ the powerful cascaded regressor approach to learn the mapping between a 2D face image and its representation. Since the representation is composed of 3D parameters, the mapping is likely to be more complicated than the cascaded regressor in 2D face alignment <ref type="bibr" target="#b3">[4]</ref>. As a result, we propose to use Convolutional Neural Networks (CNN) as the regressor in the cascaded framework, to learn the mapping. While prior work on CNN for face alignment estimate no more than 6 2D landmarks per image, our cascaded CNN can estimate a substantially larger number (34) of 2D and 3D landmarks. Further, using landmark marching <ref type="bibr" target="#b35">[36]</ref>, our algorithm can adaptively adjust the 3D landmarks during the fitting, so that the cheek landmarks can contribute to the fitting. Third, conventional 2D face alignment approaches are often driven by the local feature patch around each estimated 2D landmark. Even at the ground truth landmark, such as the outer eye corner, it is hard to make sure that the local patches from faces at various poses cover the exactly the same part of facial skin anatomically, which poses additional challenge for the learning algorithm to associate a unified pattern with the ground truth landmark. Fortunately, in our work, we can use the dense 3D face model as an oracle to build enhanced feature correspondence across various poses and expressions. Therefore, we propose two novel pose-invariant local features, as the input layer for CNN learning. We also utilize person-specific surface normals to estimate the visibility of each landmark.</p><p>These algorithm designs collectively lead to the proposed large-pose face alignment algorithm. We conduct extensive experiments to demonstrate its capability in aligning faces across poses, in comparison with the state of the art.</p><p>We summarize the main contributions of this work as: ⋄ Large-pose face alignment by fitting a dense 3DMM. ⋄ The cascaded CNN-based 3D face model fitting algorithm that is applicable to all poses, with integrated landmark marching.</p><p>⋄ Dense 3D face-enabled pose-invariant local features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Prior Work</head><p>We review papers in three areas related to the proposed method: large-pose face alignment, face alignment via deep learning, and 3D face model fitting to a single image.</p><p>Large-pose face alignment The methods of <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b6">7]</ref> combines face detection, pose estimation and face alignment. By using a 3D shape model with optimized mixture of parts, <ref type="bibr" target="#b30">[31]</ref> can be applied to faces with a large range of poses. In <ref type="bibr" target="#b29">[30]</ref>, a face alignment method based on cascade regressors is proposed to handle invisible landmarks. Each stage is composed of two regressors for estimating the probability of landmark visibility and the location of landmarks. This method is applied to profile view faces of FERET database <ref type="bibr" target="#b17">[18]</ref>. As a 2D landmark-based approach, it cannot estimate 3D face poses. Occlusion-invariant face alignment, such as RCPR <ref type="bibr" target="#b0">[1]</ref>, may also be applied to handle large poses since non-frontal faces are one type of occlusions. <ref type="bibr" target="#b24">[25]</ref> is a very recent work that performs 3D landmark estimation via regressors. However, it only tests on synthesized face images up to ∼50 • yaw. The most relevant prior work is <ref type="bibr" target="#b8">[9]</ref>, which aligns faces of arbitrary poses with the assistant of a sparse 3D point distribution model. The model parameter and projection matrix are estimated by the cascade of linear or non-linear regressors. We extend <ref type="bibr" target="#b8">[9]</ref> in a number of aspects, including fitting a dense 3D morphable model, employing the powerful CNN as the regressor, using 3D-enabled features, and estimating cheek landmarks. <ref type="table" target="#tab_0">Table 1</ref> compares the large-pose face alignment methods.</p><p>Face alignment via deep learning With the continuous success of deep learning in vision, researchers start to apply deep learning to face alignment. Sun et al. <ref type="bibr" target="#b23">[24]</ref> proposed a three-stage face alignment algorithm with CNN. At the first stage, three CNNs are applied to different face parts to estimate positions of different landmarks, whose averages are regarded as the first stage results. At the next two stages, by using local patches with different sizes around each landmark, the landmark positions are refined. Similar face alignment algorithms based on multi-stage CNNs are further developed by Zhou et al. <ref type="bibr" target="#b34">[35]</ref> and CFAN <ref type="bibr" target="#b31">[32]</ref>. TCDCN <ref type="bibr" target="#b33">[34]</ref> uses one-stage CNN to estimates positions of five landmarks given a face image. The commonality among all these prior works is that they only estimate 2D landmark locations and the number of landmarks is limited to 6. In comparison, our proposed method employs CNN to estimate 3D landmarks, as part of the 3D surface reconstruction. As a result, the number of estimated landmarks is bounded by the number of 3D vertexes, although the evaluation is conducted for 34 landmarks. 3D face model fitting <ref type="table">Table 2</ref> shows the comparison of most recent 3D face model fitting methods to a single image. Almost all prior works assume that the 2D landmarks of the input face image is either manually labeled or estimated via a face alignment method. The authors in <ref type="bibr" target="#b18">[19]</ref> aim to make sure that the location of 2D contour landmarks is consistent with 3D face shape. In <ref type="bibr" target="#b37">[38]</ref>, a 3D face model fitting method based on the similarity of frontal view face images is proposed. In contrast, our proposed method is the first approach to integrate 2D landmark estimation as part of the 3D face model fitting for large poses. Furthermore, all prior 3D face model fitting works process face images with up to 60 • yaw while our method can handle all view angles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Unconstrained 3D Face Alignment</head><p>The core of our proposed 3D face alignment method is the ability to fit a dense 3D Morphable Model to a 2D face image with arbitrary poses. The unknown parameters of fitting, the 3D shape parameters and the projection matrix parameters, are sequentially estimated through a cascade of CNN-based regressors. By employing the dense 3D shape model, we enjoy the benefits of being able to estimate the locations of cheek landmarks, to use person-specific 3D surface normals, and extract pose-invariant local feature representation. <ref type="figure" target="#fig_0">Figure 2</ref> shows the overall process of the proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">3D Morphable Model</head><p>To represent a dense 3D shape of an individual's face, we use 3D Morphable Model (3DMM),</p><formula xml:id="formula_0">A = A 0 + N id ∑ i=1 p i id A i id + Nexp ∑ i=1 p i exp A i exp ,<label>(1)</label></formula><p>where A is the 3D shape matrix, A 0 is the mean shape, A i id is the ith identity basis, A i exp is the ith expression basis, p i id is the ith identity coefficient, and p i exp is the ith expression coefficient. The collection of both coefficients is denoted as the shape parameter of a 3D face, p = (p ⊺ id , p ⊺ exp ) ⊺ . We use the Basel 3D face model as the identity bases <ref type="bibr" target="#b15">[16]</ref> and the face wearhouse as the expression bases <ref type="bibr" target="#b2">[3]</ref>. The 3D shape A, along with A 0 , A i id , and A i exp , is a 3 × Q matrix which contains x, y and z coordinates of Q vertexes on the 3D face surface,</p><formula xml:id="formula_1">A =   x 1 x 2 · · · x Q y 1 y 2 · · · y Q z 1 z 2 · · · z Q   .<label>(2)</label></formula><p>Any 3D face model will be projected onto a 2D image where the face shape may be represented as a sparse set of N landmarks, on the facial fiducial points. We denote x and y coordinates of these 2D landmarks as a matrix U,</p><formula xml:id="formula_2">U = ( u 1 u 2 · · · u N v 1 v 2 · · · v N ) .<label>(3)</label></formula><p>The relationship between the 3D shape A and 2D landmarks U can be described by using the weak perspective projection, i.e.,</p><formula xml:id="formula_3">U = sRA(:, d) + t,<label>(4)</label></formula><p>where s is a scale parameter, R is the first two rows of a 3×3 rotation matrix controlled by three rotation angles α, β, and γ, t is a translation parameter composed of t x and t y , d is a N -dim index vector indicating the indexes of semantically meaningful 3D vertexes that correspond to 2D landmarks. By collecting all parameters related to this projection, we form a projection vector m = (s, α, β, γ, t x , t y ) ⊺ .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1: Landmark marching g(A, m).</head><p>Data: Estimated 3D face A and projection parameter m</p><formula xml:id="formula_4">Result: Index vector d / * Rotate A by the estimated α, β * / 1Â = R(α, β, 0)A 2 if 0 • &lt; β &lt; 70 • then 3 foreach i = 1, · · · , 4 do 4 V cheek (i) = arg max id (Â(1, Path cheek (i))) 5 if −70 • &lt; β &lt; 0 • then 6 foreach i = 5, · · · , 8 do 7 V cheek (i) = arg min id (Â(1, Path cheek (i))) 8 Update 8 elements of d with V cheek .</formula><p>At this point, we can represent any 2D face shape as the projection of a 3D face shape. In other words, the projection parameter m and shape parameter p can uniquely represent a 2D face shape. Therefore, the face alignment problem amounts to estimating m and p, given a face image.</p><p>Cheek landmarks correspondence The projection relationship in Eqn. 4 is correct for frontal-view faces, given a constant index vector d. However, as soon as a face turns to the side view, the original 3D landmarks on the cheek become invisible on the 2D image. Yet most 2D face alignment algorithms still detect 2D landmarks on the contour of the cheek, termed "cheek landmarks". Therefore, in order to still maintain the correspondences as Eqn. 4, it is best to estimate the 3D vertexes that match with these cheek landmarks. A few prior works have proposed various approaches to handle this <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b1">2]</ref>. We leverage the landmark marching method proposed in <ref type="bibr" target="#b35">[36]</ref>.</p><p>Specifically, we define a set of paths each storing the indexes of vertexes that are not only the most closest ones to the original 3D cheek landmarks, but also on the contour of the 3D face as it turns. Given a non-frontal 3D face A, we rotate A by using the α and β angles (pitch and yaw angles), and search for a vertex in each defined path which has the maximum (minimum) x coordinate, i.e., the boundary vertex on the right (left) cheek. These searched vertexes will be the new 3D landmarks that correspond to the 2D cheek landmarks. We will then update relevant elements of d to make sure these vertexes are selected in the projection of Eqn. 4. This landmark marching process is summarized in Algorithm 1 as a function d ← g(A, m). Note that when the face is almost of profile view (|β| &gt; 70 • ), we do not apply landmark marching since the marched landmarks would overlap with the existing 2D landmarks on the middle of nose and mouth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Data Augmentation</head><p>Given that the projection parameter m and shape parameter p are the representation of a face image, we should  have a collection of face images with ground truth m and p so that the learning algorithm can be applied. However, for most existing face alignment databases, only 2D landmark locations and sometimes the visibilities of landmarks are manually labeled, with no associated 3D information such as m and p. In order to make the learning possible, we propose a data augmentation process for 2D face images, with the goal of estimating its m and p representation.</p><p>Specifically, given the labeled visible 2D landmarks U and the landmark visibilities V, we use the following objective function to estimate m and p,</p><formula xml:id="formula_5">J(m, p) = ||(sRA(:, g(A, m)) + t − U) ⊙ V|| 2 F ,<label>(5)</label></formula><p>which basically minimizes the difference between the projection of 3D landmarks and the 2D labeled landmarks. Note that although the landmark marching g(:, :) can make cheek landmarks "visible" for non-profile views, the visibility V is useful to avoid invisible landmarks such as outer eye corners and half of the face at the profile view being part of the optimization.</p><p>To minimize this objective function, we alternate the minimization w.r.t. m and p at each iteration. We initialize the 3D shape parameter p = 0 and estimate m first. At each iteration, the g(A, m) is a constant computed using the currently estimated m and p.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Cascaded CNN Coupled-Regressor</head><p>Given a set of N d training face images and their augmented (a.k.a. ground truth in this context) m and p representation, we are interested in learning a mapping function that is able to predict m and p from the appearance of a face image. Clearly this is a complicated non-linear mapping function. Given the success of CNN in vision tasks such as pose estimation <ref type="bibr" target="#b16">[17]</ref>, face detection <ref type="bibr" target="#b11">[12]</ref>, and face alignment <ref type="bibr" target="#b33">[34]</ref>, we decide to marry the CNN with the cascade regressor framework by learning a series of CNN-based regressors to alternate the estimation of m and p. To the best of our knowledge, this is the first time CNN is used in 3D face alignment, with the estimation of over 10 landmarks.</p><p>In addition to the ground truth m and p, we also assume each training image has the initial values of these two parameters, denoted as m 0 and p 0 . Thus, at the stage k of the cascaded CNN, we can learn a CNN to estimate the desired update of the projection parameter,</p><formula xml:id="formula_6">Θ k m = arg min Θ k m N d ∑ i=1 ||∆m k i − CNN k m (I i , U i , v k−1 i ; Θ k m )|| 2 ,<label>(6)</label></formula><p>where the true projection update is the difference between the current projection parameter and the ground truth, i.e., Similarly another CNN regressor can be learned to estimate the updates of the shape parameter,</p><formula xml:id="formula_7">∆m k i = m i − m k−1 i , U i is current estimated 2D land- marks,</formula><formula xml:id="formula_8">Θ k p = arg min Θ k p N d ∑ i=1 ||∆p k i − CNN k p (I i , U i , v k i ; Θ k p )|| 2 . (7)</formula><p>Note that U i will be re-computed via Eqn. 4, based on the updated m k i and d k i by CNN m . We use a six-stage cascaded CNN, including CNN 1 m , CNN 2 m , CNN 3 p , CNN 4 m , CNN 5 p , and CNN 6 m . At the first stage, the input layer of CNN 1 m is the entire face region cropped by the initial bounding box, with the goal of roughly estimating the pose of the face. The input for the second to sixth stages is a 114 × 114 image that contains an array of 19 × 19 pose-invariant feature patches, extracted from the current estimated 2D landmarks U i . In our implementation, since we have N = 34 landmarks, the last two patches of 114 × 114 image are filled with zero. Similarly, for invisible 2D landmarks, their corresponding patches will be filled with zeros as well. These concatenated feature patches encode sufficient information about the local appearance around the current 2D landmarks, which drives the CNN to optimize the parameters Θ k m or Θ k p . This method can be extended to use a larger number of landmarks and hence a more accurate dense 3D model can be estimated.</p><p>Note that since landmark marching is used, the estimated 2D landmarks U i include the projection of marched 3D landmarks, i.e., 2D cheek landmarks. As a result, the appearance features around these cheek landmarks are part of the input to CNN as well. This is in sharp contrast to <ref type="bibr" target="#b8">[9]</ref> where no cheek landmarks participate the regressor learning. Effectively, these additional cheek landmarks serve as constraints to affect how the facial silhouettes at various poses should look like, which is basically the shape of the 3D face surface.</p><p>We used rectified linear unit (ReLU) <ref type="bibr" target="#b5">[6]</ref> as the activation function which enables CNN to achieve the best performance without unsupervised pre-training. We use the same CNN architecture <ref type="figure" target="#fig_1">(Fig. 3)</ref> for all six stages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Visibility and 2D Appearance Features</head><p>One notable advantage of employing a dense 3D shape model is that more advanced 2D features, which might be only possible because of the 3D model, can be extracted and contribute to the cascaded CNN learning. In this work, these 2D features refer to the 2D landmark visibility and the appearance patch around each 2D landmark.</p><p>In order to compute the visibility of each 2D landmark, we leverage the basic idea of examining whether the 3D surface normal of the corresponding 3D landmark is pointing to the camera or not, under the current camera projection matrix <ref type="bibr" target="#b8">[9]</ref>. Instead of using the average 3D surface normal for all humans, we extend it by using person-specific 3D surface normal. Specifically, given the current estimated 3D shape A, we compute the 3D surface normals for a set of sparse vertexes around the 3D landmark of interest, and the average of these 3D normals is denoted as ⃗ N. <ref type="figure" target="#fig_3">Figure 4</ref> shows the advantage of using the average 3D surface normal. Given ⃗ N, we compute v = ⃗ N ⊺ · (R 1 × R 2 ), where R 1 and R 2 are the first two rows of R. If v is positive, the 2D landmark is considered as visible and its 2D appearance feature will be part of the input for CNN. Otherwise, it is invisible and the corresponding feature will be zero for CNN. Note that this method does not estimate occlusion due to other objects such as hairs.</p><p>In addition to visibility estimation, a 3D shape model can also contribute in generating advanced appearance features as the input layer for CNN. Specifically, we aim to extract a pose-invariant appearance patch around each estimated 2D landmark, and the array of these patches will form the input layer. We now describe two proposed approaches to extract an appearance feature, i.e., a 19 × 19 patch, for the nth 2D landmark. Piecewise affine-warped feature (PAWF): Feature correspondence is always very important for any visual learning, as evident by the importance of eye-based rectification to face recognition <ref type="bibr" target="#b22">[23]</ref>. Yet, due to the fact that a 2D face is a projection of 3D surface with an arbitrary view angle, it is hard to make sure that a local patch extracted from this 2D image corresponds to the patch from another 2D image, even both patches are centered at the ground truth locations of the same nth 2D landmark. Here, "correspond" means that the patches cover the exactly same local region of faces anatomically. However, with a dense 3D shape model in <ref type="figure">Figure 5</ref>. Examples of extracting PAWF feature. When one of the four neighborhood points (red point in the bottom-right) is invisible, it connects to the 2D landmark, extends the same distance further, and generate a new neighborhood point. This helps to include the background context around the nose. hand, we may extract local patches across different subjects and poses with anatomical correspondence.</p><p>In the offline learning stage, we first search for T vertexes on the mean 3D shape A 0 that are the most closest to the nth landmark. Second, we rotate the T vertexes such that the 3D surface normal of the nth landmark points toward the camera. Third, among the T vertexes we find four "neighborhood vertexes", which have the minimum and maximum x and y coordinates, and denote the four vertex IDs as a 4-dim vector d correspond to the same face vertexes anatomically. Therefore, we warp the imagery content within these neighborhood points to a 19 × 19 patch by using the piecewise affine transformation.</p><p>This novel feature representation can be well extracted in most cases, except for cases such as the nose tip at the profile view, where one of the two scenarios could happen. One is the projection of the nth landmark is outside the region specified by the neighborhood points. The other is that one of neighborhood points is invisible. When these happen, we change the location of the invisible point by using its relative distance to the projected landmark location, as shown in <ref type="figure">Fig. 5</ref>. Direct 3D projected feature (D3PF): Both D3PF and PAWF start with the T vertexes surrounding the nth 3D landmark. Instead of finding four neighborhood vertexes as in PAWF, D3PF put a 19 × 19 grid covering the T vertexes, and store the vertexes of the grid points in d  <ref type="figure" target="#fig_6">Fig. 6</ref>. We also estimate  <ref type="figure">Figure 7</ref>. (a) AFLW original (yellow) and added landmarks (green), (b) Comparison of mean NME of each landmark for RCPR (blue) and proposed method (green). The radius of circles is determined by the mean NME multipled with the face box size. the visibility of these 3D vertexes via their surface normals, and zero will be placed in the patch for invisible vertexes. For D3PF, every pixel in the patch will be corresponding to the same pixel in the patches of other images, while for PAWF, this is true only for the four neighborhood points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results</head><p>Databases Given that this work focus on large-pose face alignment, we choose two publicly available face datasets with labeled landmarks and a large range of poses.</p><p>AFLW database <ref type="bibr" target="#b10">[11]</ref> is a large face dataset of 25K face images. Each image is manually labeled with up to 21 landmarks, with a visibility label for each landmark. In <ref type="bibr" target="#b8">[9]</ref>, a subset of AFLW is selected to have a balanced distribution of yaw angles, including 3, 901 images for training and 1, 299 images for testing. We use the same subset and manually label 13 additional landmarks for all 5, 200 images. The definition of original landmarks and added landmarks is shown in <ref type="figure">Fig. 7(a)</ref>. Using ground truth landmarks of each image, we find the tightest bounding box, expand it by 10% of its size, and add 10% noise to the top-left corner, width and height of the bounding box. These randomly generated bounding boxes mimic the imprecise face detection window and will be used for both training and testing.</p><p>AFW dataset <ref type="bibr" target="#b36">[37]</ref> contains 468 faces in 205 images. Each face image is manually labeled with up to 6 landmarks and has a visibility label for each landmark. For each face image a detected bounding box is provided. Given the small number of images, we only use this dataset for testing.</p><p>We use the N id = 199 bases of Basel Face Model <ref type="bibr" target="#b15">[16]</ref> for representing identity variation and the N exp = 29 bases of face wearhouse <ref type="bibr" target="#b2">[3]</ref> for representing expression variation. In total, there are 228 bases representing 3D face shapes  <ref type="bibr" target="#b8">[9]</ref>. The eye-to-eye distance is not used in NME since it is not well defined in large poses such as profile. For AFW dataset, we use the Mean Average Pixel Error (MAPE) <ref type="bibr" target="#b30">[31]</ref>. Feature extraction methods To show the advantages of the proposed features, <ref type="table" target="#tab_2">Table 3</ref> compares the accuracy of the proposed method on AFLW, with various feature presentation (i.e., the input layer for CNN 2 to CNN 6 ). The "Extracted Patch" refers to extracting a constant size <ref type="bibr">(19 × 19)</ref> patch from a face image normalized using the bounding box, which serves as a baseline feature. For the feature "+Cheek Landmarks", additional up to four 19 × 19 patches of the contour landmarks, which are invisible for nonfrontal faces, will be replaced with patches of the cheek landmarks, and used in the input layer of CNN learning. The PAWF feature can achieve a higher accuracy than the D3PF. By comparing Column 1 and 3 of <ref type="table" target="#tab_2">Table 3</ref>, it shows that extracting features from cheek landmarks are very effective in acting as additional visual cues for the cascaded CNN regressors. The combination of using the cheek landmarks and extracting PAWE feature achieves the highest accuracy, which will be used in the remaining experiments. CNN is known for requiring a large training set, while the AFLW training set is certainly small from CNN's perspective. However, our CNN-based regressor is still able to learn and align well on unseen images. We attribute this fact to the effective appearance features proposed in this work, i.e., we hypothesize that the good feature correspondence reduces CNN's demand for massive training data. Experiments on AFLW dataset We compare the proposed   method with the two most related methods for aligning faces with arbitrary poses. We use the source code of RCPR for performing training and testing. Similarly for PIFA, we use the source code to train on the AFLW train set with 13 more landmarks. The accuracy of the three methods are shown in <ref type="table" target="#tab_3">Table 4</ref>. The proposed method can achieve better results than the two baselines. The error comparison for each landmark is shown in <ref type="figure">Fig. 7</ref>(b). As expected, the contour landmarks have higher errors and the proposed method has lower errors than RCPR across all of the landmarks. By using the ground truth landmark locations of the test images, we divide the test set images to six subsets according to the estimated yaw angle of each image. <ref type="figure">Fig. 8</ref> compares the proposed method with RCPR. The proposed method can achieve better results across different poses, and more importantly, is more robust or has less variation across poses. For the detailed comparison on the NME distribution, the cumulative errors distribution (CED) diagrams of various methods are shown in <ref type="figure" target="#fig_7">Fig. 9</ref>. The improvement seems to be over all NME values, and is especially larger around lower NMEs (≤ 8%). Expriments on AFW dataset The AFW dataset contains faces of all pose ranges with 6 landmarks. We report the MAPE for five methods in <ref type="table" target="#tab_4">Table 5</ref>. For PIFA, CDM and TSPM, we use the reported errors in their papers. Again we see the consistent improvement of our proposed method (with two feature types) over the baseline methods. Qualitative results Some examples of alignment results  for the proposed method on AFLW and AFW datasets are shown in <ref type="figure" target="#fig_8">Fig. 10</ref>. The result of the proposed method at each stage is shown in <ref type="figure" target="#fig_9">Fig. 11</ref>. Note the changes of the landmark position and visibility (the top-right patch) over stages.</p><p>Time complexity The speeds of PAWF and D3PF methods are 0.6 and 0.26 FPS respectively, with unoptimized Matlab implementation. We believe this can be substantially improved with C coding and parallel feature extraction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>We proposed a method to fit a 3D dense shape to a face image with large poses by combining cascade CNN regressors and the 3D Morphable Model (3DMM). We proposed two types of pose invariant features for boosting the accuracy of face alignment. Also, we estimate the location of landmarks on the cheek, which also drives the 3D face model fitting. Finally, we achieve the state-of-the-art performance on two challenging face databases with larger poses.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>The overall process of the proposed method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Architecture of CNN used in each stage of the proposed method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>landmark visibility at stage k − 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>The person-specific 3D surface normal as the average of normals around a 3D landmark (black arrow). Notice the relatively noisy surface normal of the 3D "left eye corner" landmark (blue arrow).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>the CNN learning, for the nth landmark of ith image, we project the four neighborhood vertexes onto the ith image and obtain four neighborhood points, U + t, based on the current estimated projection parameter m. Across all 2D face images, U (n) i</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>. Similar to PAWF, we can now project the set of 3D vertexes A(:, d(n) d ) to the 2D image and extract a 19 × 19 patch via bilinear-interpolation, as shown in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 .</head><label>6</label><figDesc>Example</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 .</head><label>9</label><figDesc>The comparison of CED for different methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 10 .</head><label>10</label><figDesc>The results of the proposed method on AFLW and AFW. The green/red/yellow dots show the visible/invisible/cheek landmarks, respectively. First row: initial landmarks for AFLW, Second: estimated 3D dense shapes, Third: estimated landmarks, Forth and Fifth: estimated landmarks for AFLW, Sixth: estimated landmarks for AFW.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 11 .</head><label>11</label><figDesc>The result of the proposed method across stages, with the extracted features (1st row) and alignment results (2nd row).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 .</head><label>1</label><figDesc>The comparison of large-pose face alignment methods.Table 2. The comparison of most recent 3D face model fitting methods.</figDesc><table>Method 
Dense 3D 
Visibility 
Database 
Pose 
Training 
Testing 
Landmarks Estimation 
model fitting 
range 
face # 
face # 
# 
errors 
RCPR [1] 
No 
Yes 
COFW 
frontal w. occlu. 
1, 345 
507 
19 
8.5 
TSPM [37] 
No 
No 
AFW 
all poses 
2, 118 
468 
6 
11.1 
CDM [31] 
No 
No 
AFW 
all poses 
1, 300 
468 
6 
9.1 
TCDCN [34] 
No 
No 
AFLW, AFW 
[−60 • , 60 • ] 
10, 000 3, 000; ∼313 
5 
8.0; 8.2 
PIFA [9] 
No 
Yes 
AFLW, AFW 
all poses 
3, 901 
1, 299; 468 
21, 6 
6.5; 8.6 
Proposed method 
Yes 
Yes 
AFLW, AFW 
all poses 
3, 901 
1, 299; 468 
34, 6 
4.7; 7.4 

Method 
Integrated 
# of 2D 
Testing database 
Pose 
3D bases 
Method 
2D landmark landmarks 
range 
BMVC 2015 [19] 
No 
68 
Basel 
[−30 • , 30 • ] 
Basel bases 
Adaptive contour fitting 
FG 2015 [8] 
No 
77 to 1024 BU-4DFE; BP-4DS; videos [−60 • , 60 • ] Bases from BU-4DFE &amp; BP-4DS Cascaded regressor; EM 
FG 2015 [38] 
Yes 
-
FRGC 
Frontal 
Basel bases 
Cascaded regressor 
Proposed method 
Yes 
-
AFW; AFLW 
All poses 
Basel bases 
3D cascaded regressor 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 .</head><label>3</label><figDesc>NME (%) of the proposed method with different features. for the other stages. According to our empirical evaluation, six stages of CNN are sufficient for convergence of fitting process. Evaluation metrics We use two conventional metrics for measuring the error of up to 34 landmarks. For AFLW dataset, we use the mean error of visible landmarks normalized by the bounding box size (NME)</figDesc><table>PAWF + Cheek D3PF + Cheek PAWF 
Extracted 
Landmarks 
Landmarks 
Patch 
4.72 
5.02 
5.19 
5.51 

with 53, 215 vertexes. 
Baseline selection We select the most recent large-pose 
face alignment methods for comparing with the proposed 
method, according to Table 1. We compare the proposed 
method with PIFA [9] and RCPR [1] on AFLW, and with 
PIFA [9], CDM [31] and TSPM [37] on AFW. 
Parameter setting For the proposed method, the learning 
rate of CNN is constant at 0.0001 during training. We use 
ten epochs for training each CNN. For RCPR, we use the 
parameters reported in its paper, with 100 iterations and 15 
boosted regressors. For PIFA, we use 200 iterations and 
5 boosted regressors. For PAWF and D3PF, at the second 
stage T is 5, 000, and 3, 000 </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 4 .</head><label>4</label><figDesc>The NME (%) of three methods on AFLW.Ð®±°±»¼ Ó»¬¸±¼ ÎÝÐÎFigure 8. Comparison of NME for each pose.</figDesc><table>Proposed method PIFA RCPR 
4.72 
8.04 
6.26 

ð 
îð 
ìð 
ëð 
éð 
çð 
ð 

ï 

î 

í 

ì 

ë 

Ç¿© 

ÒÓÛ øû÷ 

ð 
ë 
ïð 
ïë 
îð 
îë 
íð 
ð 

îð 

ìð 

êð 

èð 

ïðð 

ÒÓÛ øû÷ 
Ì»¬ ×³¿¹» øû÷ 

ÐßÉÚ õ Ý¸»»µ Ô¿²¼³¿®µ 
ÐßÉÚ 
ÎÝÐÎ 
Ð×Úß 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 5 .</head><label>5</label><figDesc>The MAPE of four methods on AFW.</figDesc><table>Proposed method Proposed method PIFA CDM TSPM 
(PAWF) 
(D3PF) 
7.43 
7.83 
8.61 
9.13 
11.09 

</table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Robust face landmark estimation under occlusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">P</forename><surname>Burgos-Artizzu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Displaced dynamic expression regression for real-time facial tracking and animation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">43</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Facewarehouse: a 3D facial expression database for visual computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Vis. Comput. Graphics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="413" to="425" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Face alignment by explicit shape regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="177" to="190" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Active shape models: Evaluation of a multi-resolution method for improving image search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cootes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lanitis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="327" to="336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep sparse rectifier neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Artificial Intelligence and Statistics (AISTATS)</title>
		<meeting>Artificial Intelligence and Statistics (AISTATS)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="315" to="323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Regressive tree structured model for facial landmark localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-S</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3855" to="3861" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Dense 3D face alignment from 2d videos in real-time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Jeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FG</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Pose-invariant 3D face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jourabloo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Attribute preserved face de-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jourabloo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICB</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Annotated facial landmarks in the wild: A large-scale, realworld database for facial landmark localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Köstinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCVW</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2144" to="2151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A convolutional neural network cascade for face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5325" to="5334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Discriminative face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1941" to="1954" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Video-based face model fitting using adaptive active appearance model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Image Vision Computing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1162" to="1172" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Active appearance models revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="135" to="164" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A 3D face model for pose and illumination invariant face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Paysan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Knothe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Amberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Romdhani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AVSS</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="296" to="301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep convolutional neural networks for efficient pose estimation in gesture videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="538" to="552" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The feret evaluation methodology for face-recognition algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rizvi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Rauss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1090" to="1104" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Adaptive contour fitting for pose-invariant 3D face shape reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Monari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schuchert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Beyerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Unconstrained 3D face reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Adaptive 3D face reconstruction from unconstrained photo collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Face alignment through subspace constrained mean-shifts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Saragih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Curse of mis-alignment in face recognition: problem and a novel misalignment learning solution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FG</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="314" to="320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep convolutional network cascade for facial point detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3476" to="3483" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Regressing a 3D face shape from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tulyakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Project-out cascaded regression with an application to face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3659" to="3667" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Facial point detection using boosted regression and graph models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Valstar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Binefa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="2729" to="2736" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Toward a practical face recognition system: Robust alignment and illumination by sparse representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ganesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mobahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="372" to="386" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Facial feature point detection: A comprehensive survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.1037</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Robust facial landmark detection under significant head poses and occlusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Posefree facial landmark fitting via optimized part mixtures and cascaded deformable shape model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1944" to="1951" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Coarse-to-fine autoencoder networks (CFAN) for real-time face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Conditional density learning via regression with application to deformable shape segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Comaniciu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mcmillan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Facial landmark detection by deep multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="94" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Extensive facial landmark localization with coarse-to-fine convolutional network cascade</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCVW</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="386" to="391" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">High-fidelity pose and expression normalization for face recognition in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="787" to="796" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Face detection, pose estimation, and landmark localization in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2879" to="2886" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Discriminative 3D morphable model fitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FG</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
