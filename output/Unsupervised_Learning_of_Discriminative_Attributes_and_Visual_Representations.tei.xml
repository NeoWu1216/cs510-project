<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unsupervised Learning of Discriminative Attributes and Visual Representations</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Huang</surname></persName>
							<email>chuang@ie.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">SenseTime Group Limited</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
							<email>ccloy@ie.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Shenzhen Institutes of Advanced Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
							<email>xtang@ie.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Shenzhen Institutes of Advanced Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Unsupervised Learning of Discriminative Attributes and Visual Representations</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:48+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Attributes offer useful mid-level features to interpret visual data. While most attribute learning methods are supervised by costly human-generated labels, we introduce a simple yet powerful unsupervised approach to learn and predict visual attributes directly from data. Given a large unlabeled image collection as input, we train deep Convolutional Neural Networks (CNNs) to output a set of discriminative, binary attributes often with semantic meanings. Specifically, we first train a CNN coupled with unsupervised discriminative clustering, and then use the cluster membership as a soft supervision to discover shared attributes from the clusters while maximizing their separability. The learned attributes are shown to be capable of encoding rich imagery properties from both natural images and contour patches. The visual representations learned in this way are also transferrable to other tasks such as object detection. We show other convincing results on the related tasks of image retrieval and classification, and contour detection.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Attributes <ref type="bibr" target="#b15">[16]</ref> offer important mid-level cues for many visual tasks like image retrieval. Shared attributes can also generalize across categories to define the unseen object from a new category <ref type="bibr" target="#b27">[28]</ref>. Most supervised attribute learning methods <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b47">48]</ref> require large amounts of human labeling (e.g., "big", "furry"), which is expensive to scale up to rapidly growing data. Alternatives <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b37">38]</ref> leverage texts on the web that are narrow or biased in scope <ref type="bibr" target="#b34">[35]</ref>.</p><p>To discover attributes from numerous potentially uninteresting images is much like finding needles in a haystack. It is more challenging to find those ideal attributes that are shared across certain categories and meanwhile can distinguish them from others. The above supervised methods reduce such a large searching space by directly using humangenerated labels or semantic text. Besides costing substantial human effort, the major drawback of these methods is  <ref type="figure">Figure 1</ref>. 2D feature space of our unsupervisedly learned attributes for natural images on CIFAR-10 <ref type="bibr" target="#b25">[26]</ref> and binary contour patches on BSDS500 <ref type="bibr" target="#b1">[2]</ref>. The colored lines delineate the approximate separation borderlines of the binary attributes, which are discriminative and easily interpreted semantically in both cases. In the first case, it is obvious that many attributes are shared across categories, and they together can help distinguish the categories of interest. that they cannot guarantee the manually defined attributes are sufficiently predictable or discriminative in the feature space. Recent works <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b36">37]</ref> address this drawback by mining attributes from image features to reduce intercategory confusions. Unfortunately, they are still hampered by the expense of human annotation of category labels.</p><p>In this paper, we propose an unsupervised approach to learn and predict attributes that are representative and discriminative without using any attribute or category labels. Under this scenario, a critical question arises: which attributes should be learned? We follow the hashing idea to generate meaningful attributes 1 in the form of binary codes, and train a CNN to simultaneously learn the deep features and hashing functions in an unsupervised manner. We start with pre-training CNN coupled with a modified clustering method <ref type="bibr" target="#b43">[44]</ref> to find representative visual concepts. This converts our unsupervised problem into a supervised one: we treat the visual concept clusters as surrogate/artificial classes, and the goal is to learning discriminative and sharable attributes from these concept clusters while maximizing their separability. Considering the clusters are probably noisy, we use a triplet ranking loss to fine-tune our CNN for attribute prediction, treating the cluster membership as a soft supervision instead of forcing the same attributes for all cluster members.</p><p>Our method is applied to the natural images as well as local contour patches, producing attributes that are discriminative and easily interpretable semantically (see <ref type="figure">Figure 1</ref>). We demonstrate the clear advantages of our approach over related hashing and attribute methods that lack an intermediate discriminative model or rely on clustering techniques only. We further show that the learned visual features offer state-of-the-art performance when used as unsupervised pre-training for tasks like detection on PASCAL VOC 2007 <ref type="bibr" target="#b14">[15]</ref>. On datasets CIFAR-10 <ref type="bibr" target="#b25">[26]</ref>, STL-10 <ref type="bibr" target="#b7">[8]</ref> and Caltech-101 <ref type="bibr" target="#b16">[17]</ref>, our approach enables fast and more accurate natural image classification and retrieval than other works. On dataset BSDS500 <ref type="bibr" target="#b1">[2]</ref>, our learned contour attributes lead to state-of-the-art results on contour detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>This work has two distinctive features: 1) deep learning discriminative attributes from data, 2) being unsupervised. A review of the related literature is provided below. Attribute learning: Most attribute learning methods are supervised <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b47">48]</ref>. They either require human-1 Strictly speaking, our learned attributes cannot be referred to as "attributes" by the conventional notion. Instead of being manually defined to name explicit object properties, our attributes are discovered from data. They are in the form of binary codes to describe and separate some representative data modes (clusters). In this respect, they are more conceptually related to the data driven attribute hypothesis <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b36">37]</ref> or latent attributes <ref type="bibr" target="#b17">[18]</ref>. Nevertheless our attributes are still found to highly correlate with semantic meanings, see Figures 1 and 4. In comparison to the latent attributes <ref type="bibr" target="#b17">[18]</ref> that are verified to capture the latent correlation between classes, our attributes are more appealing and only depend on clustering in an unsupervised manner. labeled attributes that are cumbersome to obtain, or use web texts <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b37">38]</ref> that are biased and noisy by nature. Their common drawback is that these manually-defined attributes may not be predictable or discriminative in a feature space. In <ref type="bibr" target="#b34">[35]</ref>, nameable and discriminative attributes are discovered from visual features, but it involves human-in-the-loop. Recent advances <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b41">42]</ref> show considerable promise on generating discriminative attributes under minimal supervision (require single attribute label per-image and rough relative scores per-attribute respectively), but can only be deemed as weakly-supervised and thus do not apply to our unsupervised setting. On the other hand, <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b36">37]</ref> try to learn attributes in an unsupervised way, but are still supervised in that they do so on the class basis. Our motivation for producing "class"-discriminating attributes is related; however, our solution is quite different as we automatically generate "classes" from visual concept clusters as well as their discriminating attribute codes. Unsupervised learning: Our first component of unsupervised clustering is related to a line of works on discriminative patch mining (e.g., <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b43">44]</ref>) for finding representative patch clusters. Li et al. <ref type="bibr" target="#b28">[29]</ref> further seek integration with CNN features which is desirable. But they need category supervision, which does not conform to our problem setting. Our method alternates between a modified clustering <ref type="bibr" target="#b43">[44]</ref> and CNN training, which results in both robust visual concept clusters and deep feature representations.</p><p>Many unsupervised studies <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b52">53]</ref> focus on learning generic visual features by modeling the data distribution (e.g., via sparse coding <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>). But Dosovitskiy et al. <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b12">13]</ref> indicate that a discriminative objective is superior and propose to randomly generate a set of image classes to be discriminated in the feature space. Clustering and data augmentation for each class further give rise to the feature robustness. More recently, self-supervised learning methods learn features by predicting within-image context <ref type="bibr" target="#b11">[12]</ref> and further solving jigsaw puzzles <ref type="bibr" target="#b33">[34]</ref>, or by ranking patches from video tracks <ref type="bibr" target="#b46">[47]</ref>. We propose here a novel paradigm for unsupervised feature learning: by predicting discriminative and sharable attributes from some typical data clusters, meaningful feature representations emerge. Learning to hash: The attributes learned by our approach are represented as binary hash codes. Among the family of unsupervised hashing methods, locality sensitive hashing (LSH) <ref type="bibr" target="#b19">[20]</ref>, iterative quantization (ITQ) <ref type="bibr" target="#b22">[23]</ref> and spectral hashing (SH) <ref type="bibr" target="#b48">[49]</ref> are best known, where the hash codes are learned to preserve some notion of similarity in the original feature space which is unstable. Supervised <ref type="bibr" target="#b39">[40]</ref> or semisupervised <ref type="bibr" target="#b45">[46]</ref> hashing methods solve this issue by directly using class labels to define similarity. The codes are usually made balanced and pairwise uncorrelated to improve hashing efficiency. Deep hashing methods <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b51">52]</ref> come with the benefits of simultaneously learning the fea-tures and hashing functions. We share the same merits, but differ in two aspects: 1) Our method is label-free unlike the supervised ones <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b51">52]</ref>. Note in <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b51">52]</ref> a triplet loss is used as in our work, but their loss needs the class label supervision; 2) Contrary to the unsupervised study <ref type="bibr" target="#b13">[14]</ref> that only focuses on minimizing the quantization error, ours additionally ensures code discrimination. Concretely, we introduce a clustering step to act as a soft discriminative constraint to learn the hash codes and features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Unsupervised Deep Learning of Visual Attributes and Representations</head><p>Our goal is to learn visual attributes and features directly from unlabeled data. We do not hand-design attributes with a list of descriptive words, but expect them to emerge from salient properties of data. We believe that we can at least first find some representative visual concepts, then we summarize their attributes based on the concept intersections. Ideally, the learned attributes are shared across some of these visual concepts and also keeping them apart from others. But how to exploit the sea of overwhelmingly uninteresting data to find interesting visual concepts? And it is not even clear which attributes should be learned, or whether the already learned ones are discriminative.</p><p>To attack the above two problems, we propose a twostage method as shown in <ref type="figure" target="#fig_1">Figure 2</ref>. The first stage performs alternating unsupervised discriminative clustering and representation learning with CNN to discover representative visual concepts. The concepts can correspond to frequent image primitives or objects (segments), see <ref type="figure" target="#fig_2">Figure 3</ref>. Then they are passed to the next stage as a soft guidance. Specifically, we perform discriminative hashing under a triplet ranking loss to predict binary attributes, while fine-tuning the feature representations. Such learned attributes are of a lower level granularity than visual concepts but at a higher level than low-level features such as HOG <ref type="bibr" target="#b8">[9]</ref>, see <ref type="figure" target="#fig_3">Figure 4</ref>. We will take the natural images from CIFAR-10 and binary contour patches from BSDS500 for example to expand the details in the following.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Unsupervised Representation Learning from Visual Concept Clusters</head><p>Good-quality visual concepts are important bases to estimate attributes from. They need to meet two requirements: to be representative in the visual world, and discriminative against the rest of the world. To find such a subset from large quantities of unlabeled noisy data, we apply the unsupervised discriminative clustering method <ref type="bibr" target="#b43">[44]</ref>. It alternates between clustering the "discovery dataset" D and training a discriminative SVM classifier for each cluster against the "natural world dataset" N for refinement. Cross-validation at each step is used to prevent overfitting. The found clusters are likely to correspond to our desired visual concepts. Then we train a CNN to learn robust feature representations through classification of these clusters with a softmax loss. Considering the initial clusters might not have been very good to begin with, we alternate between the clustering process and CNN feature learning to ensure robustness.</p><p>In practice, we set D as one halved subset from our training data, i.e., 50 thousand color images from CIFAR-10 or one million binary contour patches from BSDS500, and N as another half or random web data. Inspired by <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b12">13]</ref>, we offer two modifications to the above clustering: 1) We merge similar clusters to extract more diverse attributes. The merging starts with training SVM classifiers again but between the clusters in D this time. Using the firing score as a similarity metric, we greedily combine the most overlapping pair of clusters (threshold 0.4) until no changes can be made, ending up with only hundreds of clusters for both contour patches and CIFAR-10 images. Such cluster sizes are reasonable in view of the intrinsic complexity of the respective problem: CIFAR-10 dataset contains moderate visual variations in 10 categories, and today's common practices <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b42">43]</ref> group image edges into 100-150 categories. 2) We augment each cluster 20 times with different image transformations, aiming to make the learned features invariant to such transformations. This brings further improvement as confirmed in our empirical results.</p><p>Note we initialize the clustering of color images and binary contours with HOG <ref type="bibr" target="#b8">[9]</ref> and DAISY <ref type="bibr" target="#b49">[50]</ref> features respectively. The alternating process of clustering and CNN feature learning usually converges within 3 rounds. Figure 3 compares our CNN clustering results with those by k-means+initialized features. The visual concepts discovered by our approach are representative, meaningful and diverse. They are more consistent and convey richer information than k-means.   where samples transition from one side to the other. We show on each side the 8 most confident samples for 5 bits (rows) out of 32. We find most attributes easily human-nameable. We can name the 5 rows of CIFAR-10 images as "animal", "long legs", "aero", "big foreground", "aligned background", the 5 rows of contour patches as "simple", "curved", "double-lined", "junctional", "polyline". It is interesting to find alternative ways to describe some attributes, e.g., the last row of CIFAR-10 images also contrasts vehicle with animals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Weakly-Supervised Attribute Learning</head><p>Next, we learn attributes from the visual concepts that are probably overcomplete (e.g., horizontal and single-lined contours, dogs on the green grass in <ref type="figure" target="#fig_2">Figure 3</ref>). We aim to summarize from them compactly shared attributes while retaining discrimination. This is, however, an extremely hard optimization problem due to the large searching space. In particular, we can have extremely discriminative codes (e.g., assigning unique code to each visual concept) which are hard to predict from images. Or we can have the most predictable codes but with little information thus poor discrimination. The authors of <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b36">37]</ref> strike a compromise by balancing between attribute discrimination and predictability. Similarly, we propose to learn attributes from our visual concept clusters in a weakly-supervised manner. In Figure 2, Stage 2, we append a hashing function (h : R d → {−1, 1} K ) layer after the feature layer of our pre-trained CNN to map the feature representations f (x) ∈ R d into K-bit binary codes b ∈ {−1, 1} K as the learned attributes. Then a triplet loss is used to constrain the relative similarity between code pairs from the same and different clusters, treating the cluster membership as only a soft supervisory signal instead of forcing the same attributes for all cluster members.</p><p>This approach ensures the learned attributes are both predictable and discriminative with built-in margins. It is more favorable than the supervised and non-deep learning-based methods <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b36">37]</ref> by a joint learning of deep features and hashing functions, while not requiring class supervision. Formally, we define our hashing function as:</p><formula xml:id="formula_0">h(x; W ) = sign(W T f (x)),<label>(1)</label></formula><p>where W ∈ R d×K denotes the hashing weights. The hashing bias is zero as we zero-mean all the visual concept features f (X). The weights and bias of feature mapping f (·) are omitted here for brevity. Then our objective function is:</p><formula xml:id="formula_1">min i ε i + αtr[W T f (X)f (X) T W ] +β W W T − I 2 2 + γ W 2 2 , s.t. : max 0, ρ + H(b i , b + i )) − H(b i , b − i )) ≤ ε i , ∀i, b i = h(x i ; W ), and ε i ≥ 0,<label>(2)</label></formula><p>where ε i is a slack variable, ρ = K/2 is a preset margin between Hamming distances H(·, ·) of within-cluster code pair {b i , b + i } and between-cluster code pair {b i , b − i }, and α, β, γ are regularization parameters. The second and third terms aim to enforce the desirable properties of hashingbalanced partition and independence of the code bits respectively, as is commonly done in <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b48">49]</ref>. For the ease of optimization, they are relaxed to the data variance maximization and pairwise bit independence constraints following <ref type="bibr" target="#b45">[46]</ref>. The fourth term penalizes large weights.</p><p>The optimization of Eq. 2 is still intractable due to the discontinuity of the hashing function in Eq. 1. To this end, we further relax h(x; W ) to:</p><formula xml:id="formula_2">h(x; W ) = 2σ(W T f (x)) − 1,<label>(3)</label></formula><p>where σ(x) = 1/(1 + exp(−x)) is the logistic function. The Hamming distance thus becomes:</p><formula xml:id="formula_3">H(b i , b j ) = (K − b T i b j )/2.<label>(4)</label></formula><p>The final objective function is optimized by stochastic gradient descent. Via the back-propagation algorithm we fine-tune our CNN to update both the feature representations f (·) and hashing functions h(·). In practice, we initialize W as the projection computed by <ref type="bibr" target="#b36">[37]</ref> on low-level features. During CNN tuning, data x i is randomly sampled from all the clusters, x + i is randomly drawn from the same cluster, while x − i is chosen as the between-cluster one that most violates the triplet ranking order (loss is maximum) in one random mini-batch. We still need to decide K, the number of attribute codes we wish to learn. However, there is no clue to predict a priori which codes will be useful, so we leave it to experimental evaluation. We show that the learned attribute codes via our approach highly correlate with a variety of semantic concepts, although we do not explicitly enforce the learning to be semantic. <ref type="figure" target="#fig_3">Figure 4</ref> show examples of attribute codes learned from images and contours, e.g., "animals" at various backgrounds, "simple" contours full of various single-lined ones etc. This validates that we are indeed capable of extracting meaningful attributes shared across visual clusters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Applications</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Object Detection Pre-training</head><p>We first evaluate the quality of our feature representations under the proposed unsupervised learning paradigm. We choose to fine-tune our learned features for another task of object detection on PASCAL VOC 2007 to examine the feature transferability, treating our unsupervised attribute learning as a pre-training step. To acquire a deeper understanding of image contents and learn more effective feature representations, we train our unsupervised model this time on the large-scale ImageNet 2012 training set <ref type="bibr" target="#b9">[10]</ref>. It contains 1.3 million images (labels discarded), much more than that in CIFAR-10, and has larger image diversity. But it is more challenging to perform unsupervised learning on the high-resolution ImageNet images than on the 32 × 32 pixel CIFAR-10 images and 45 × 45 BSDS500 contour patches, since the pixel variety grows exponentially with spatial resolution. Then following <ref type="bibr" target="#b11">[12]</ref>, we sidestep this challenge by learning from sampled patches at resolution 96 × 96.</p><p>We use the AlexNet-style architecture as in <ref type="bibr" target="#b11">[12]</ref> for unsupervised pre-training, but adjust it to the Fast R-CNN framework <ref type="bibr" target="#b20">[21]</ref> for fast detection on the input image of size 227×227. For pre-training on ImageNet, we particulary initialize our first clustering step with the more elaborated features (SIFT+color Fisher Vectors) in <ref type="bibr" target="#b35">[36]</ref> in this harder case, and form 30 thousand concept clusters. For fine-tuning on the VOC training set, we copy our pre-trained weights up to the conv5 layer and initialize the fully connected layers with Gaussian random weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Image Retrieval and Classification</head><p>Image retrieval on CIFAR-10 dataset: <ref type="figure" target="#fig_3">Figures 1 and 4</ref> already show that our attributes can describe meaningful image properties learned from the CIFAR-10 dataset. Although they do not have explicit names, many of them strongly correspond to semantics and even convey highlevel category information. This makes them suitable to be used as binary hash codes for fast and effective image retrieval. Image classification on STL-10 and Caltech-101 dataset: To classify the high-resolution images from STL-10 (96 × 96) and Caltech-101 (roughly 300 × 200), we follow <ref type="bibr" target="#b12">[13]</ref> to unsupervisedly train CNN on the 32 × 32 patches extracted from STL-10 images. For an arbitrarily sized test image, we compute all the convolutional responses and apply the pooling method to the feature maps of each layer: 4-quadrant max-pooling for STL-10, and 3-layer spatial pyramid for Caltech-101. Finally, one-vs-all SVM classifiers are trained on the pooled features. Here we use our learned features again to validate their semantic quality for classification, which enables a fair comparison with <ref type="bibr" target="#b12">[13]</ref> as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Contour Detection</head><p>The attributes of image contours are little explored.</p><p>Here we argue that such mid-level attributes other than the commonly-used low-level features (e.g., DAISY <ref type="bibr" target="#b49">[50]</ref>) are very valuable to detect image contours or even their enclosed objects. We show the value of our unsupervised attribute learning method in the task of contour detection. We are aware of only two related works <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b42">43]</ref> that unsupervisedly learn "sketch tokens" by k-means for contour detection. While their results are satisfying, "sketch tokens" can be at most seen as the instances of a single-attribute about contour shape.</p><p>Recall that we previously have only learned unsupervised contour attributes from the binary contour patcheshand drawn sketches of natural images. One question remains unaddressed: how to predict for an input color image the contours and associated attributes simultaneously? We propose an efficient framework as shown in <ref type="figure">Figure 5</ref> to achieve this goal. We first train an initial CNN model to distinguish between the one million 45 × 45 color patches corresponding to contours and one million non-contour 45×45 patches from BSDS500 dataset. This binary task teaches CNN to learn preliminary features and to classify whether the central point of a given color patch is an edge or not, without taking care of the patch attributes. Such simplification usually produces contour predictions with high recall but many false positives. Then we leverage the contour attributes to refine the pre-trained CNN in a multi-label prediction task. We employ a target of K +1 labels, all normalized to {0, 1} and consisting of one binary edge label and the K-bit attributes previously learned from the corresponding binary edge patch. The attribute codes for non-edge patches are treated as missing labels, of which the gradients are not back-propagated to update CNN weights.  <ref type="figure">Figure 5</ref>. Two-step framework for predicting contours and contour attributes from natural image patches: binary edge classification followed by multi-label prediction. The CNNs have shared architectures and weights. <ref type="figure">Figure 6</ref>. Patch-wise edge map prediction. We use the thresholded K = 16-bit attributes (we only show 3 in red dots) as hash codes to retrieve the contour patches (red box) from training data.</p><p>Given a converged network, the final prediction is made by directly passing the entire image through the convolutional and pooling layers, and generating K + 1-length vectors via the fully connected layers at different spatial locations. This is more efficient than predicting for individual patches due to the shared feature computation. As a result, we obtain one edge map and K attribute maps which are of the same size with input but not thresholded (i.e., intensity maps). Note that such an edge map is pixel-wise and may be noisy and inconsistent in local regions. Therefore, we also make a robust patch-wise prediction using the thresholded attribute vector (K-length) for each patch as hash codes, and find its Hamming nearest neighbor from training data. We simply transfer the neighboring contour patches to the output and average all the overlapping patches as in <ref type="bibr" target="#b18">[19]</ref>, followed by non-maximum suppression. <ref type="figure">Figure 6</ref> illustrates an example of such patch-wise prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results</head><p>Datasets and evaluation metrics: For object detection, we follow the standard Fast R-CNN evaluation pipeline <ref type="bibr" target="#b20">[21]</ref> on the PASCAL VOC 2007 <ref type="bibr" target="#b14">[15]</ref> test set, which consists of 4952 images with 20 object categories. No bounding box regression is used. The average precision (AP) and mAP measures are reported.</p><p>For image retrieval, we use the CIFAR-10 dataset <ref type="bibr" target="#b25">[26]</ref> that contains 60000 color images in 10 classes. We follow the standard protocol of using 50000 training and 10000 testing images to unsupervisedly learn visual attributes. During the retrieval phase, we randomly sample 1000 images (100 per class) as the query, and use the remaining images as the gallery set as in <ref type="bibr" target="#b13">[14]</ref>. The evaluation metrics are mAP and precision at N samples. The average of 10 experimental results is reported. For image classification, the 100 thousand unlabeled images of STL-10 <ref type="bibr" target="#b7">[8]</ref> are used for our unsupervised attribute learning. Later, on STL-10 the SVM is trained on 10 predefined folds of 500 training images per class, an we report the mean and standard deviation of classification results on the 800 test images per class. On Caltech-101 <ref type="bibr" target="#b16">[17]</ref> we follow <ref type="bibr" target="#b12">[13]</ref> to select 30 random samples per class for training and no more than 50 samples per class for testing (images are resized to 150 × 150 pixels), and repeat the procedure 10 times to report the mean and standard deviation again.</p><p>For contour detection, we use the BSDS500 dataset <ref type="bibr" target="#b1">[2]</ref> with 200 training, 100 validation and 200 testing images. We evaluate by: fixed contour threshold (ODS), per-image best threshold (OIS), and average precision (AP) <ref type="bibr" target="#b1">[2]</ref>. Implementation details: Throughout this paper, we set α and β in Eq. 2 to 1, γ to 0.0005, and we use linear SVM classifiers with C = 0.1. For object detection pre-training, we adopt the AlexNet-style architecture <ref type="bibr" target="#b11">[12]</ref>. The unsupervised network training converges in about 200K iterations, 3.5 days on a K40 GPU. We start with an initial learning rate of 0.001, and reduce it by a factor of 10 at every 50K iterations. We use the momentum of 0.9, and mini-batches of 256 resized images. For image retrieval and classification, we use the architectures of <ref type="bibr" target="#b13">[14]</ref> (3 layers) and <ref type="bibr" target="#b12">[13]</ref> (4 layers: 92c5-256c5-512c5-1024f), respectively. We generally follow their training details. For contour detection, We use the 6-layer CNN architecture of <ref type="bibr" target="#b42">[43]</ref>. The momentum is set to 0.9, and mini-batch size to 64 patches. The initial learning rate is 0.01, and decreased by a factor of 10 every 40K iterations until the maximum iteration 150K is reached.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Object Detection Pre-training</head><p>Quality evaluation of unsupervised pre-training: Before transferring our pre-trained features to object detection, we first directly evaluate the quality of our unsupervised pretraining on the ImageNet dataset, which is a perfect testbed due to its large size and rich data diversity. We consider evaluating our discovered visual clusters and attributes, the two key outputs from our unsupervised training. <ref type="figure" target="#fig_2">Figure 3</ref> provides some qualitative clustering results on relatively simple datasets. Now we quantify the clustering effects on ImageNet given the formed 30 thousand clusters and 1 thousand ground truth classes (not used during clustering). Specifically, the cluster purity is calculated as to correspond to the percentage of cluster members with the <ref type="table">Table 1</ref>. Detection results (%) with Fast R-CNN on the test set of PASCAL VOC 2007. We directly add our results to those reported in <ref type="bibr" target="#b33">[34]</ref>. The results of <ref type="bibr" target="#b11">[12]</ref> and <ref type="bibr" target="#b46">[47]</ref> read different to their original evaluations because they use the framework of Fast R-CNN here instead of R-CNN <ref type="bibr" target="#b21">[22]</ref>. The R-CNN results of <ref type="bibr" target="#b11">[12]</ref> are included as a reference. Our features pre-trained by unsupervised attribute learning (K = 32, 64 bits) produce closer results to that of the recent unsupervised features <ref type="bibr" target="#b33">[34]</ref> and supervised features pre-trained on ImageNet with labels. ITQ <ref type="bibr" target="#b22">[23]</ref> LSH <ref type="bibr" target="#b19">[20]</ref> PCAH <ref type="bibr" target="#b45">[46]</ref> SH <ref type="bibr" target="#b48">[49]</ref>  same class label. Our CNN clustering attains the value of 0.34, much higher than 0.16 by k-means clustering with hand-crafted features <ref type="bibr" target="#b35">[36]</ref>. This validates the proposed clustering indeed discovers some level of categorical semantics, although the purity value is not as high as that on e.g., the less diverse CIFAR-10 dataset. This also demonstrates the scalability of our unsupervised paradigm to large-scale data. For our discovered unsupervised attributes, it is not easy to either subjectively name them or compare them to the available human labels (the ImageNet Attribute subset <ref type="bibr" target="#b38">[39]</ref> has ambiguous labels in itself) by consensus. We leave their quality evaluation as future work. Feature transferability to detection: <ref type="table">Table 1</ref> reports the results of transferring pre-trained features to the VOC 07 detection task. Our unsupervised features yield higher performance (49.3% mAP, K = 64) than the other unsupervised ones <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b46">47]</ref>, and are comparable to the recent unsupervised features <ref type="bibr" target="#b33">[34]</ref> and supervised one with ImageNet labels. Thus we argue that learning unsupervised visual attributes helps understand image contents and objects, and the features learned this way are semantically meaningful. <ref type="table">Table 3</ref>. Classification accuracies (%) of unsupervised (top cell) and supervised (bottom cell) methods on STL-10 and Caltech-101.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>STL-10 Caltech-101 Multi-way local pooling <ref type="bibr" target="#b5">[6]</ref> -77.3 ± 0.6 Slowness on videos <ref type="bibr" target="#b52">[53]</ref> 61.0 74.6 HMP <ref type="bibr" target="#b3">[4]</ref> 64.5 ± 1 -Multipath HMP <ref type="bibr" target="#b4">[5]</ref> -82.5 ± 0.5 View-Invariant k-means <ref type="bibr" target="#b23">[24]</ref> 63.7 -Exemplar-CNN <ref type="bibr" target="#b12">[13]</ref> 75 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Image Retrieval</head><p>This experiment shows the effectiveness of the attributes learned by our approach when they are used as binary hash codes in a retrieval task. We compare with five related unsupervised hashing methods in <ref type="table">Table 2</ref>. As shown, our method significantly outperforms traditional SH, PCAH, LSH and ITQ methods due to the joint learning of hashing functions and features. We also consistently improve the mAP at different code bits over Deep Hashing (DH) since we adopt a discriminative paradigm rather than a reconstructive one. <ref type="figure" target="#fig_4">Figure 7</ref> visualizes our clear advantages in terms of both retrieval accuracy and consistency. Since our approach can discover the latent properties of both image foreground and background in an unsupervised manner, it is better at interpreting image scenes using multi-source information. Usually we generate K = 64 attribute codes which takes about 10ms on GPU. The code retrieval time is negligible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Image Classification</head><p>For the STL-10 and Caltech-101 images, we learn visual attributes from 32 × 32 patches and use the resulting features to classify. <ref type="table">Table 3</ref> studies whether such patchlevel learning will give rise to class discrimination at image level. We observe that our features trained with rich attributes lead to higher classification accuracies (peak at K = 64) on both datasets than all the other unsupervised features, and even out-compete the supervised one <ref type="bibr" target="#b44">[45]</ref> by 6.7% on STL-10. This suggests that resolving attribute confusions indeed leads to class-discriminating features, even in the patch space. Moreover, the larger gains here than <ref type="table">Table 4</ref>. Contour detection results on BSDS500. The top and bottom cells list the pixel-wise and patch-wise methods, respectively.</p><p>Method ODS OIS AP Sketch Token <ref type="bibr" target="#b29">[30]</ref> 0.73 0.75 0.78 Ours (pixel) 0.75 0.77 0.79 N 4 -Fields <ref type="bibr" target="#b18">[19]</ref> 0.75 0.77 0.78 DeepContour <ref type="bibr" target="#b42">[43]</ref> 0  <ref type="bibr" target="#b29">[30]</ref>, our pixel-wise method, DeepContour <ref type="bibr" target="#b42">[43]</ref>, and our patch-wise method (K = 16).</p><p>those on the simple CIFAR-10 dataset show our potential to learn from more complex and diverse image data in the real world. <ref type="table">Table 4</ref> compares our pixel-wise and patch-wise contour detection methods with related methods. Since our contour pixels are jointly learned and output with rich contour attributes, our pixel-wise prediction tends to focus more on distinctive meaningful edges. The gains over k-meansbased "shape-only" learner <ref type="bibr" target="#b29">[30]</ref> validate this point, and their visual differences are shown in <ref type="figure">Figure 8</ref>. Our patchwise version largely eliminates the pixel noise by averaging nearest neighbor patches retrieved using the attribute codes, with better results than the similar methods of deep neighbor searching <ref type="bibr" target="#b18">[19]</ref> and clustering-based deep classification <ref type="bibr" target="#b42">[43]</ref>. This proves our learned attributes can provide more faithful mid-level cues about image contours (see the detailed structures in our patch-wise edge map in <ref type="figure">Figure 8</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Contour Detection</head><p>We observe saturated performance when using K = 16 attribute bits for retrieving contour patch neighbors. Note that neighbor retrieval in Hamming space is costless. We only need to enumerate the binary attribute codes with no searching at all. So to produce the final edge map, nearly all the time is spent on generating the required K + 1 maps themselves. The whole procedure takes about 60ms for a 321 × 481 image on a GPU. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Ablation Tests</head><p>We finally study the key components of our approachunsupervised feature learning from visual clusters and weakly-supervised hash learning to highlight their unique strengths. At each time we replace one component with an alternative method and report the result in <ref type="table" target="#tab_4">Table 5</ref>.</p><p>By comparing with the performance of k-means+CNN feature learning, we can find the benefit of using our modified unsupervised discriminative clustering method to guide the feature learning. We observe a further performance drop when replacing the alternating CNN feature learning with only HOG features. In this case, less meaningful visual clusters and features can be learned from data.</p><p>For our hashing component, we first indicate the importance of learning discriminative hashing functions by comparing with the unsupervised Deep Hashing (DH) <ref type="bibr" target="#b13">[14]</ref>. DH only hashes to reconstruct the latent features thus yielding much less discriminative results. In contrast, our weaklysupervised method samples hash code triplets to softly enforce the cluster discrimination. Our method outperforms the similar triplet hashing methods <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b30">31]</ref>, mainly because they lack the hash code constraints (in Eq. 2) and define triplets by the noisy raw feature similarity respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>This paper proposes an unsupervised approach to deep learning discriminative attributes from unlabeled data. The input of our system can be natural images (patches) or binary contour patches. The output is a compact list of attributes in the form of binary codes. This is achieved by a novel two-stage pipeline consisting of unsupervised discriminative clustering and weakly-supervised hashing, where the visual clusters, hashing functions and feature representations are jointly learned. The resulting visual attributes and features are found to capture strong semantic meanings, and are transferrable to several important vision tasks. The transferability to more tasks will be studied in the future.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Our two-stage pipeline for learning attributes as binary hash codes. All CNNs have shared architectures and weights.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Our discovered visual concepts. Notice their diversity and visual consistency compared to those by k-means+low-level features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Unsupervised discovery of attribute codes. Each code bit corresponds to a hyperplane (dashed line) of one attribute predictor,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 .</head><label>7</label><figDesc>Top 8 images retrieved by different hashing methods in comparison to ours (K = 64) on the CIFAR-10 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>81 Figure 8 .</head><label>818</label><figDesc>patch, K = 16 bits) 0.77 0.78 0.82 Ours (patch, K = 32 bits) 0.77 0.78 0.Contour detection examples on BSDS500: (from left to right) original image, results of Sketch Token</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>. . .</head><label>.</label><figDesc></figDesc><table>CNN 
Two-way 
softmax loss 
Feature 
layer 
Edge or non-
edge point? 

CNN 
Feature 
layer 

K-bit binary 
attribute codes 

1-bit binary edge label 

of 
Cross 
entropy 
loss 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="true"><head>Table 5 .</head><label>5</label><figDesc>Ablation tests of image retrieval (mAP %) on CIFAR-10 and classification (accuracy %) on STL-10 and Caltech-101. All baselines work on K = 64 and we compare with the alternative clustering (middle cell) and hashing (bottom cell) techniques.</figDesc><table>Algorithm 
CIFAR-10 
STL-10 
Caltech-101 
Our original method 
17.21 
76.8 ± 0.3 
89.4 ± 0.5 
k-means+CNN 
17.16 
76.2 ± 0.3 
88.6 ± 0.5 
k-means+HOG 
16.94 
75.5 ± 0.3 
87.3 ± 0.6 
Triplet hashing [27] 
17.17 
76.7 ± 0.4 
88.8 ± 0.6 
UTH [31] 
17.02 
74.9 ± 0.4 
86.7 ± 0.6 
DH [14] 
17.05 
75.6 ± 0.3 
87.9± 0.6 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Discriminative unsupervised feature learning with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Contour detection and hierarchical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="898" to="916" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Automatic attribute discovery and characterization from noisy web data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning for RGB-D based object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISER</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multipath sparse coding using hierarchical matching pursuit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Ask the locals: Multi-way local pooling for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-L</forename><surname>Boureau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Visual recognition with humans in the loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Babenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">An analysis of singlelayer networks in unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Mid-level visual element discovery as discriminative mode seeking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Unsupervised visual representation learning by context prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Discriminative unsupervised feature learning with exemplar convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.6909v2</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep hashing for compact binary codes learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">Erin</forename><surname>Liong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Moulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (VOC) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Describing objects by their attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Endres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>CVPRW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning multimodal latent attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="316" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">N4-fields: Neural network nearest neighbor fields for image transforms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">S</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Similarity search in high dimensions via hashing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gionis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Indyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Motwani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VLDB</title>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Iterative quantization: A procrustean approach to learning binary codes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Direct modeling of complex invariances for visual object features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Y</forename><surname>Hui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kaiming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xiangyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shaoqing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Simultaneous feature learning and hash coding with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning to detect unseen object classes by between-class attribute transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lampert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nickisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harmeling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Mid-level deep pattern mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Sketch tokens: A learned mid-level representation for contour and object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Veillard. Tiny descriptors for image retrieval with unsupervised triplet hashing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Morère</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Petta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Chandrasekhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1511.03055v1</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Recognizing human actions by attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kuipers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Unsupervised learning of discriminative relative visual attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ikizler-Cinbis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EC-CVW</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Unsupervised learning of visual representations by solving jigsaw puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Favaro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.09246</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Interactively building a discriminative vocabulary of nameable attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Compressed fisher vectors for large scale visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sanchez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCVW</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Attribute discovery via predictable discriminative binary codes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">What helps where -and why? semantic relatedness for knowledge transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Szarvas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gurevych</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Attribute learning in largescale datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCVW</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Semantic hashing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Approx. Reasoning</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="969" to="978" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deep carving: Discovering visual attributes by carving deep neural nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">K</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Semantic transform: Weakly supervised semantic inference for relating visual attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lasenby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Deepcontour: A deep convolutional feature learned by positivesharing loss for contour detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Unsupervised discovery of mid-level discriminative patches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Multi-task bayesian optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Semi-supervised hashing for scalable image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations using videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A discriminative latent model of object classes and attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<title level="m">Spectral hashing</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note>NIPS</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Picking the best DAISY</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Winder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Supervised hashing for image retrieval via image representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Deep semantic ranking based hashing for multi-label image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Deep learning of invariant features via simulated fixations in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
