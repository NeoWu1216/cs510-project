<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Relaxation-Based Preprocessing Techniques for Markov Random Field Inference</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Wang</surname></persName>
							<email>chenwang@cs.cornell.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Cornell University</orgName>
								<orgName type="institution" key="instit2">Cornell University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramin</forename><surname>Zabih</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Cornell University</orgName>
								<orgName type="institution" key="instit2">Cornell University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Relaxation-Based Preprocessing Techniques for Markov Random Field Inference</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:45+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Markov Random Fields (MRFs) are a widely used graphical model, but the inference problem is NP-hard. For first-order MRFs with binary labels, Dead End Elimination (DEE) <ref type="bibr" target="#b6">[7]</ref> and QPBO <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b13">14]</ref> can find the optimal labeling for some variables; the much harder case of larger label sets has been addressed by Kovtun [16,<ref type="bibr" target="#b16">17]</ref> and related methods <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25]</ref>, which impose substantial computational overhead. We describe an efficient algorithm to correctly label a subset of the variables for arbitrary MRFs, with particularly good performance on binary MRFs. We propose a sufficient condition to check if a partial labeling is optimal, which is a generalization of DEE's purely local test. We give a hierarchy of relaxations that provide larger optimal partial labelings at the cost of additional computation. Empirical studies were conducted on several benchmarks, using expansion moves <ref type="bibr" target="#b3">[4]</ref> for inference. Our algorithm runs in a few seconds, and improves the speed of MRF inference with expansion moves by a factor of 1.5 to 12.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>We address the inference problem for pairwise Markov Random Fields (MRFs) defined over n variables x = (x 1 , . . . , x n ), where each x i is labeled from a discrete label set L i . There is an energy function E(x) that we wish to minimize given a set of parameters θ; θ characterizes the unary costs θ i : L i → R + and the pairwise costs θ ij : L i × L j → R + . The energy function is</p><formula xml:id="formula_0">E(x) = i∈V θ i (x i ) + (i,j)∈E θ ij (x i , x j )<label>(1)</label></formula><p>where G = (V, E) is the graph representation of the MRF.</p><p>The MRF inference problem is to find x * = arg min x E(x), which is equivalent to finding the MAP estimate. This is widely used in applications such as image segmentation, stereo, etc <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b25">26]</ref>. Unfortunately the MRF</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Handles |L| &gt; 2? Bottleneck Our method Yes None DEE <ref type="bibr" target="#b6">[7]</ref> Sometimes None QPBO <ref type="bibr" target="#b1">[2]</ref> No max-flow Kovtun <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref> Yes max-flow MQPBO <ref type="bibr" target="#b11">[12]</ref> Yes max-flow Swoboda <ref type="bibr" target="#b24">[25]</ref> Yes LP, MRF inference Shekhovtsov <ref type="bibr" target="#b22">[23]</ref> Yes LP Shekhovtsov <ref type="bibr" target="#b23">[24]</ref> Yes LP, MRF inference <ref type="table">Table 1</ref>. Partial optimality algorithms. The bottleneck column indicates any subroutine with complexity significantly greater than linear time.</p><p>inference problem is NP-hard even when |L| = 2 (i.e. binary labels) <ref type="bibr" target="#b14">[15]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Optimal partial labelings</head><p>A popular approach to the inference problem is to try to find the optimal labeling for a subset of the variables <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b27">28]</ref>. A partial labeling that holds in every global minimizer is said to be persistent <ref type="bibr" target="#b1">[2]</ref>. An optimal labeling for a subset of the variables can be used to reduce the difficulty of the inference problem, or can be the basis for a variety of heuristics such as QPBO-I <ref type="bibr" target="#b20">[21]</ref>.</p><p>Techniques like QPBO <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b13">14]</ref> find an optimal partial labeling by seeking an even stronger condition, namely a partial labeling which will not increase the energy if it is applied to any complete labeling. QPBO in particular is widely used in computer vision since it often finds the correct label for the vast majority of the variables.</p><p>Algorithms for finding optimal partial labelings are summarized in <ref type="figure">Figure 1</ref> and discussed in Section 2. Except for Dead End Elimination (DEE) <ref type="bibr" target="#b6">[7]</ref> they all impose significant computational costs, using max flow, linear programming or both. Our technique generalizes DEE, and has significantly better performance experimentally.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.">Contributions and outline</head><p>In this paper, we address the problem of finding an optimal partial labeling as efficiently as possible. We propose a condition to guarantee that a labeling is part of every global minimizer, and represent this condition as a system of linear inequalities. We establish a hierarchy of relaxations of the original system and derive a family of tractable sufficient conditions. We then propose an efficient algorithm to find a set of variables satisfying the corresponding conditions. Using the loosest condition in the relaxation hierarchy, we can find a globally optimal subset of variables by running a small number of iterations, each of which takes O(|V | + |E|) time; this is very efficient compared to the O(|V | 2 |E|) running time of max-flow. <ref type="bibr" target="#b0">1</ref> The hierarchy of relaxations allow us to trade off between the running time and the number of variables optimally labeled. Our methods perform particularly well on binary MRFs. We conduct experiments on a variety of vision applications and obtain promising experimental results. In particular, when integrated into expansion moves <ref type="bibr" target="#b3">[4]</ref> as the MRF inference algorithm our technique labels a large number of variables with minimal overhead, thus producing a substantial speedup.</p><p>We review the literature in Section 2. The proposed algorithm is given in Section 3. Experimental results are illustrated in Section 4. Additional experimental results and all the detailed proofs are provided in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Empirical studies of MRF inference approaches can be found in <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b25">26]</ref>. Since the problem is NP-hard in general these techniques find approximate solutions. Rapidly determining the optimal labels for a subset of the variables would obviously be of great utility. Existing methods are summarized in <ref type="table">Table 1</ref>.</p><p>Optimal partial labelings are commonly used in conjunction with graph cuts, a technique that achieves strong performance on both binary and multilabel MRF inference <ref type="bibr" target="#b25">[26]</ref>. Graph cuts handle binary MRFs by reduction to min-cut, which is then solved via max-flow (see <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b7">8]</ref> for reviews). The most widely used graph cut methods for multi-label MRF inference are move-making techniques, which generate a new proposal at each iteration and reduce the multilabel problem into a series of binary subproblems (should each pixel stick with the old label or switch to the new label in the proposal) and then solved by max-flow/min-cut. Popular algorithms in this family include expansion moves <ref type="bibr" target="#b3">[4]</ref> and their generalization to fusion moves <ref type="bibr" target="#b18">[19]</ref>.</p><p>QPBO is a generalization of the binary graph cut reduction that uses max-flow to find an optimal partial labeling <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b20">21]</ref>. The graph where max-flow is run is twice the size of the original MRF, with 2|V | nodes and 2|E| + 2|V | edges. When the energy function is submodular 2 , then the partial labeling is complete (i.e., it labels every pixel and is a global minimizer). However, the computational expense of running max-flow is non-trivial, and our goal is to find substantially faster techniques. Note that the only methods with significantly better running time than max-flow are DEE and our technique.</p><p>Kovtun <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref> proposed an approach to handle multilabel MRFs by constructing a series of binary auxiliary problems and solve each of them via graph cuts. MQPBO <ref type="bibr" target="#b11">[12]</ref> and generalized roof duality <ref type="bibr" target="#b27">[28]</ref> generalized QPBO to multi-label cases. The computational costs for these methods are all at least as large as max-flow.</p><p>Recently, Swoboda et. al. <ref type="bibr" target="#b24">[25]</ref> use standard MRF inference algorithms to iteratively update the persistent variable set. Shekhovtsov <ref type="bibr" target="#b22">[23]</ref> formalized the problem to maximize the number of optimally labeled variables as an LP. They also proposed to combine these two approaches together which can take advantage of both of them <ref type="bibr" target="#b23">[24]</ref>. The number of variables labeled by these approaches are significantly more than Kovtun's approach and MQPBO. However, the running time of these approaches is significantly longer, since these approaches involve solving complex programming (either via standard MRF inference solver or LP solver) iteratively.</p><p>Dead End Elimination (DEE) <ref type="bibr" target="#b6">[7]</ref> is the only existing method with cheaper computational costs than max-flow. It checks a local sufficient condition which only involves a single vertex and its adjacent edges. We will show in Section 3.1 that this condition is a special case of the loosest condition of our approach, hence our approach will always label at least as many variables as DEE, with the same running time complexity. Experimental results confirm our approach can label substantially more variables than DEE.</p><p>An intuitive comparison of our technique with DEE is provided in <ref type="figure">Figure 1</ref>. The most striking difference is that DEE considers one individual variable at a time and potentially rules out one of its labels; for binary problems, this allows it to determine the globally optimal label for that variable. Our method can determine whether a particular label is optimal for a set of variables, and is not restricted to binary labels. Note that when an entire set of variables fails our sufficient condition for optimality, we shrink the set, as shown in the middle figure. The crucial step in our method is the second from the last one, highlighted with a gray background, where a group of 6 variables is given their optimal labels all at once. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Hierarchical Relaxation of Persistency</head><p>Notation and preliminaries Recall that we have n variables x 1 , . . . , x n in (1) and each variable x i takes its value from a discrete label set L i . We will use x to represent the vector (x 1 , . . . , x n ) and x S to represent a subvector of x with indices in S, where S ⊆ V . We will refer to x and x S as a full labeling and partial labeling (w.r.t. S) respectively. Define L S = Π i∈S L i as the label space of x S , which contains the special case where L V is the label space of x. Given two partial labelings x A and x B where A and B are disjoint, the partial labeling x A ⊕ x B defined on A ∪ B as the composition of partial labelings x A and x B . <ref type="bibr" target="#b2">3</ref> We will view overwriting a full labeling y with a partial labeling x S as a special case of label composition, i.e., x S ⊕ y V \S .</p><p>The following definitions come from the literature on pseudo-boolean optimization (see, e.g. <ref type="bibr" target="#b1">[2]</ref>).</p><formula xml:id="formula_1">Definition 1. A partial labeling x S is persistent if x S = x * S , ∀x * ∈ arg min x E(x).<label>(2)</label></formula><formula xml:id="formula_2">Definition 2. A partial labeling x S is an autarky if E(x S ⊕ z V \S ) &lt; E(y S ⊕ z V \S ), ∀z V \S ∈ L V \S and ∀y S ∈ L S such that y S = x S . (3)</formula><p>Persistency is the property that we seek, since it means that a partial labeling assigns the optimal value to its variables. Autarky is a stronger condition, which states that overwriting an arbitrary labeling with this partial labeling will reduce the energy. Autarky clearly implies persistency, and is tractable since it can be computed without knowing the global minimizer(s) x * . We will therefore use autarky as a sufficient condition for persistency. Also note that for a set S, if there exists a persistent partial labeling x S , it must be unique. So we may also say the set S is persistent without explicitly referring to its labeling.</p><p>We study two questions: 1) given the energy function E(x) and partial labeling x S , determine if x S is persistent; 2) given the energy function E(x), find a persistent partial labeling x S as large as possible, where the size of partial labeling is defined by |S|. We will refer to these as the persistency decision problem and persistency construction problem respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Persistency decision problem</head><p>Since there are exponentially many inequalities in <ref type="formula">(3)</ref>, it's computationally intractable to examine them one by one. Moreover, the persistency decision problem is NPcomplete <ref type="bibr" target="#b1">[2]</ref> so that we cannot expect to check it exactly. To handle it, we will establish a hierarchical relaxation of the autarky inequality system <ref type="formula">(3)</ref>, which gives us a family of sufficient conditions to check persistency.</p><p>Define</p><formula xml:id="formula_3">∆E(y S ← x S | z V \S ) := E(y S ⊕ z V \S ) − E(x S ⊕ z V \S )</formula><p>to be the energy change when we substitute y S for x S . Let the index set A := {i ∈ S | y i = x i } be the indices of variables we actually changed from x S to y S . Then we know that ∆E(</p><formula xml:id="formula_4">y A ← x A | x S\A , z V \S ) = ∆E(y S ← x S | z V \S ) and the autarky property in (3) is equivalent to min z V \S ∈L V \S min yi =xi,i∈A ∆E(y A ← x A | x S\A , z V \S ) &gt; 0 for all A ⊆ S, A = ∅.</formula><p>We expand the definition of energy using (1) and cancel out the unchanged terms. Then a further relaxation could be taken by pushing the min operators into the summation:</p><formula xml:id="formula_5">min z V \S ∈L V \S min yi =xi,i∈A ∆E(y A ← x A | x S\A , z V \S ) ≥ i∈A min yi =xi θ i (y i ) − θ i (x i ) + ij∈(A,S\A) min yi =xi θ ij (y i , x j ) − θ ij (x i , x j ) + ij∈(A,V \S) min yi =xi,zj ∈Lj θ ij (y i , z j ) − θ ij (x i , z j ) + ij∈(A,A) min yi =xi,yj =xj θ ij (y i , y j ) − θ ij (x i , x j ).<label>(4)</label></formula><formula xml:id="formula_6">Here ij ∈ (A, B) is short for {(i, j) ∈ E | i ∈ A, j ∈ B}.</formula><p>In order to simplify the notation in (4), we define a short- <ref type="figure">Figure 2</ref>. The terms in our relaxation <ref type="bibr" target="#b3">(4)</ref>. To prove the persistency of the partial labeling xS, we show that every subset A ⊆ S meets certain conditions. All the variables in A change from their values in xS, the variables in S\A stay the same, and the ones in V \S are arbitrary.</p><p>hand for each term inside the summation as follows:</p><formula xml:id="formula_7">δ i (x i ) := min yi =xi θ i (y i ) − θ i (x i ) δ i ij (x i , x j ) := min yi =xi (θ ij (y i , x j ) − θ ij (x i , x j )) δ i ij (x i , .) := min yi =xi,zj ∈Lj (θ ij (y i , z j ) − θ ij (x i , z j )) δ ij ij (x i , x j ) := min yi =xi,yj =xj (θ ij (y i , y j ) − θ ij (x i , x j ))<label>(5)</label></formula><p>The relationship between A, S and V and the notation we introduced in <ref type="formula" target="#formula_7">(5)</ref> is illustrated in <ref type="figure">Figure 2</ref>. Note that all the unary terms and pairwise terms inside V \A are unchanged. So δ i (x i ), which is the minimum energy change (possibly negative) in unary cost θ i inside A, and</p><formula xml:id="formula_8">δ ij ij (x i , x j ), δ i ij (x i , x j ), δ i ij (x i , .)</formula><p>, which are the minimum energy change in pairwise cost θ ij inside A and crossing the boundary of A respectively, are the only terms we need to focus on. <ref type="bibr" target="#b3">4</ref> Now we can summarize our analysis above to obtain our first relaxation of the sufficient conditions.</p><formula xml:id="formula_9">Theorem 3. The partial labeling x S is persistent if: i∈A δ i (x i ) + ij∈(A,S\A) δ i ij (x i , x j ) + ij∈(A,V \S) δ i ij (x i , .)+ ij∈(A,A) δ ij ij (x i , x j ) &gt; 0, ∀A ⊆ S, A = ∅.<label>(6)</label></formula><p>Note that there are still O(2 |S| ) inequalities in <ref type="bibr" target="#b5">(6)</ref>. To further relax it, we will focus on testing the persistency of an independent local minimum (ILM) labeling defined as follows.</p><p>Definition 4 (Independent local minimum labeling). A partial labeling x S is called an independent local minimum if it minimizes each pairwise term θ ij such that i, j ∈ S. <ref type="bibr" target="#b3">4</ref> In general, our notation δ B</p><p>A (x A ) represents the smallest energy change for one term θ A with initial labeling x A , minimizing over all different values of all the variables in B. Sometimes we need to consider variables not in B to have arbitrary values, which we represent with a period. For the unary term, the superscript in δ i i (x i ) is redundant, hence omitted.</p><p>Recall that, as shown in <ref type="figure">Figure 1</ref>, our method considers an input labeling and then shrinks it, while DEE considers a single pixel at a time. Our input must be an ILM labeling, and the larger the better. Fortunately we will show in Section 3.3 that we can easily construct large ILM labelings for the vast majority of MRFs used in vision, and can also guarantee an ILM labeling of at least size 2 for an arbitrary MRF. So even in the worst case we retain our advantage over DEE.</p><p>The direct benefit from the ILM labeling assumption is we have δ i ij (x i , x j ) ≥ 0 and δ ij ij (x i , x j ) ≥ 0 by definition. Now we can obtain a hierarchy of relaxations to (6) as follows.</p><p>Theorem 5 (k-condition for S). The ILM partial labeling x S containing at least k variables is persistent if ∀B ⊆ S, |B| = k ≥ 1, the following inequalities hold:</p><formula xml:id="formula_10">i∈C δ i (x i ) + ij∈(C,B\C) δ i ij (x i , x j ) + ij∈(C,V \S) δ i ij (x i , .) &gt; 0, ∀C ⊆ B, C = ∅<label>(7)</label></formula><p>Proof.</p><p>Here is a sketch to show <ref type="formula" target="#formula_10">(7)</ref> is a sufficient condition to derive <ref type="bibr" target="#b5">(6)</ref>. More details can be found in the supplementary material.</p><p>For any A ⊆ S, 1 ≤ |A| ≤ k, we can pick arbitrary B ⊇ A, |B| = k and let C := A, then we have i∈A δ i ( <ref type="formula" target="#formula_10">(7)</ref>. Combining with the fact that 1) (A, B\A) ⊆ (A, S\A) and δ i ij (x i , x j ) ≥ 0, 2) δ ij ij (x i , x j ) ≥ 0, we get the desired inequality in <ref type="bibr" target="#b5">(6)</ref>.</p><formula xml:id="formula_11">x i ) + ij∈(A,B\A) δ i ij (x i , x j )+ ij∈(A,V \S) δ i ij (x i , .) &gt; 0 from</formula><p>For any A ⊆ S, |A| &gt; k, we know i∈B δ i ( <ref type="formula" target="#formula_10">(7)</ref> (by choosing C := B). Now we pick all B ⊆ A, |B| = k and sum up the previous inequality, we will have i∈A δ i (</p><formula xml:id="formula_12">x i ) + ij∈(B,V \B) δ i ij (x i , .) &gt; 0 from</formula><formula xml:id="formula_13">x i ) + ij∈(A,V \A) δ i ij (x i , .) &gt; 0.</formula><p>Combining with the fact that δ ij ij (x i , x j ) ≥ 0 and δ i ij (x i , x j ) ≥ 0, we have the desired inequality in (6).</p><p>Our k-condition in <ref type="formula" target="#formula_10">(7)</ref> is a hierarchy of relaxations of (6) for different k's. There are |S| k (2 k − 1) inequalities in the k-condition for S, hence it's computationally efficient to check when k is small. Meanwhile, the larger k is, the more tightly <ref type="formula" target="#formula_10">(7)</ref> approximates <ref type="bibr" target="#b5">(6)</ref>. We thus obtain a tradeoff between the complexity and accuracy of the relaxation by varying k. Now we will claim the sufficient condition to check persistency in DEE is a special case of our 1-condition (i.e., k = 1). Note that our 1-condition says the constant partial labeling x S is persistent if</p><formula xml:id="formula_14">δ i (x i ) + j∈S,(i,j)∈E δ i ij (x i , x j ) + j ∈S,(i,j)∈E δ i ij (x i , .) &gt; 0, ∀i ∈ S,<label>(8)</label></formula><p>while the Goldstein condition used in DEE says 5 that variable x i is persistent if</p><formula xml:id="formula_15">δ i (x i ) + (i,j)∈E δ i ij (x i , .) &gt; 0.<label>(9)</label></formula><p>This is a special case of our 1-condition when S = {i}. Thus, our 1-condition generalized DEE's Goldstein condition from a single variable to an ILM partial labeling.</p><p>Approximating the k-condition Recall our k-condition consists of |S| k (2 k − 1) inequalities, so checking them one by one will become computationally intractable soon with the growth of k. Therefore, we propose an approximate way to check the k-condition that is very efficient in practice, based on the following lemma. Lemma 6. The ILM partial labeling x S is persistent if we can partition S into disjoint subsets S = t S t and each S t satisfies the corresponding |S t |-condition.</p><p>Proof. We will also provide a sketch to show this is a sufficient condition of (6) here and defer the details to the supplementary material.</p><p>For any non-empty A ⊆ S, we can define A t := A ∩ S t . Then we know that A = t A t and all A t are disjoint. Our goal is to show i∈A δ i (</p><formula xml:id="formula_16">x i ) + ij∈(A,S\A) δ i ij (x i , x j ) + ij∈(A,A) δ ij ij (x i , x j ) + ij∈(A,V \S) δ i ij (x i , .</formula><p>) is positive. By dropping non-negative terms and rearranging things, we can prove</p><formula xml:id="formula_17">i∈A δ i (x i ) + ij∈(A,S\A) δ i ij (x i , x j ) + ij∈(A,A) δ ij ij (x i , x j ) + ij∈(A,V \S) δ i ij (x i , .) ≥ t ( i∈At δ i (x i ) + ij∈(At,St\At) δ i ij (x i , x j ) + ij∈(At,V \St) δ i</formula><p>ij (x i , .)) and we know the RHS is positive due to each S t satisfying the |S t |-condition.</p><p>In practice, we can approximately test the k-condition for k &gt; 1 by doing an incremental breadth-first search style greedy partition of S = t S t such that S t are all disjoint, |S t | ≤ k, and S t satisfies the |S t |-condition, which is described in Algorithm 1. The idea is for each single variable i not satisfying the 1-condition, we will search for a subset B containing i that can satisfy the 2-condition, 3-condition, etc. The first found B will be added to our partition. Finally, we add all the left-over single variables satisfying the 1-condition into our partition and claim the remaining variables (i.e., U at the end of Algorithm 1) cannot be proved to be persistent. Note this approximation is still a sound condition guaranteed by Lemma 6, i.e., when U = ∅ at the end, we know that x S is persistent.</p><formula xml:id="formula_18">Input: ILM partial labeling x S U ← S; t ← 0; for i ∈ U s.t. {i} fails 1-condition test do for k ′ ← 2 to k do Search for B ⊆ U s.t. i ∈ B, |B| = k ′ , B satisfies k ′ -condition; if find such a B then t ← t + 1; S t ← B; U ← U \B; break; end end end for i ∈ U s.t. {i} satisfies 1-condition do t ← t + 1; S t ← {i}; U ← U \{i}; end if U = ∅ then return x S is persistent; else</formula><p>return U as the cause that x S fails the test; end Algorithm 1: Approximate k-condition Test</p><formula xml:id="formula_19">Input: Energy function E(x) W ← ∅; x W ← ∅;</formula><p>repeat Construct a set of ILM partial labeling X described in Section 3.3; for x S ∈ X do repeat Test x S using k-condition; if x S fails the test then Find x i causing violation; S ← S\{i}; end until x S passes the test or S = ∅;</p><formula xml:id="formula_20">x W ← x W ⊕ x S ; W ← W ∪ S; L i ← {(x S ) i }, ∀i ∈ S;</formula><p>end until converge or after τ iterations; return x W as the persistent partial labeling of E(x);</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 2: Persistency Construction</head><p>Theoretical connection to <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25]</ref> The autarky property in (3) is a special case of the improving mapping described in <ref type="bibr" target="#b22">[23]</ref>. Our sufficient conditions in (6) is a special case of the partial optimality criterions described in <ref type="bibr" target="#b24">[25]</ref>. However, checking the sufficient conditions in <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25]</ref> require a general MRF inference solver as a subroutine, which is computational expensive. We proposed a set of computational tractable sufficient conditions and approximation algorithm in <ref type="formula" target="#formula_10">(7)</ref>, <ref type="bibr" target="#b7">(8)</ref>. Therefore, while the sufficient conditions in <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25]</ref> are tighter, our conditions can be checked more efficiently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Persistency construction problem</head><p>In Section 3.1, we described a hierarchy of sufficient conditions and their sound approximations to check persistency of an ILM partial labeling. Now, we will use these conditions as a subroutine to construct a persistent partial labeling for a given energy E(x).</p><p>The method is shown in Algorithm 2. Assume we can find a set of ILM partial labelings X as candidates (we defer a discussion of how to do this until Section 3.3). For each x S ∈ X , we adopt a shrinking scheme. We will apply the k-condition test 6 or its approximation to check the persistency of x S . The test will either proves x S is persistent or reports some B's violating <ref type="formula" target="#formula_10">(7)</ref>; we will shrink S by remov-</p><formula xml:id="formula_21">ing i ∈ B with the minimum δ i (x i ) + (i,j)∈E δ i ij (x i , .</formula><p>) value for each violated B. If we apply our approximation to the k-condition, we will remove the remaining variables in U from S. We repeat this procedure until x S satisfies the kcondition. Now we can composite all the persistent partial labelings we found together. It's easy to see the composition of persistent partial labelings is still persistent by definition, which proves that our algorithm is sound.</p><p>Finally, similar to the iterative idea in DEE, after determining x S to be persistent, we can update E(x) by fixing x S without changing the minimizer. This in turn can potentially find additional persistent variables. We iteratively run the procedure described above until it converges or reaches the pre-defined stopping parameter τ .</p><p>Let P DEE be the set of persistent variables DEE found, and P PR the set of persistent variables our algorithm found at convergence. Our algorithm always finds at least as many persistent variables.</p><p>Theorem 7. P DEE ⊆ P PR for binary MRFs.</p><p>Proof. We prove this by contradiction. Suppose ∃x i ∈ P DEE , x i ∈ P PR , then x i satisfies the 1-condition at convergence of PR. This contradicts our assumption that PR has converged, since we should have added x i into P PR . A detailed proof is provided in the supplementary material.</p><p>Example We note that DEE is based on such a strong local condition that it may fail even in extremely simple cases. Consider a binary Potts MRF with two variables x i , x j such that θ i (0) = θ j (0) = 0, θ i (1) = θ j (1) = a ≥ 0, θ ij (0, 0) = θ ij (1, 1) = 0, θ ij (0, 1) = θ ij (1, 0) = b &gt; a. DEE cannot determine any of the variables to be persistent while our approach will easily find that x i = x j = 0 is a persistent partial labeling. Our experiments demonstrate that our approach indeed finds significantly more persistent variables than DEE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">ILM labeling construction</head><p>In this section, we will show how to construct a candidate set X of ILM partial labelings. Ideally, we want to start Algorithm 2 with as large a partial labeling as possible. We can show that for a wide family of energy functions used in typical vision problem, we can efficiently find the maximum ILM partial labeling, and even for an arbitrary MRF we can guarantee an ILM partial labeling of at least size 2. We consider three special cases that are widely used in vision: weakly associative energies, binary submodular, and binary non-submodular. Finally, we discuss the case of an arbitrary multi-label MRF.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 8 (Weakly associative energy). E(x) is called weakly associative if all of its pairwise costs satisfy</head><formula xml:id="formula_22">θ ij (x i , x j ) ≥ 0 and θ ij (x i , x j ) = 0 when x i = x j .</formula><p>Weakly associative: It's easy to see that any constant labeling (i.e., all the variables take the same value) is ILM. So for each label α, we can let S := {i | α ∈ L i } and x S := α then put it into X . Binary submodular: We use the reparameterization scheme introduced in <ref type="bibr" target="#b13">[14]</ref> to equivalently transform the energy function into a weakly associative one. Therefore, we can put the maximum constant partial labeling with label 0 and 1 into our X . Binary non-submodular: Again, we use the reparameterization scheme introduced in <ref type="bibr" target="#b13">[14]</ref>. Now, all the submodular terms will be transformed to be weakly associative. Meanwhile, all the supmodular terms will be transformed as θ ij (0, 0) = θ ij (1, 1) = 0 and θ ij (0, 1) = θ ij (1, 0) = c &lt; 0 with (0, 1) and (1, 0) as the local minimizer. Therefore, in the ILM partial labeling, we want x i and x j to take the same value when θ ij is submodular and to take different values otherwise. We use a greedy approach 7 to find a large enough ILM partial labeling. After we find the first ILM partial labeling, we can add it into X and iteratively run the greedy algorithm on the remaining variables in V \S to find more ILM partial labelings to be added into X . Arbitary multilabel: The Potts model and truncated L p prior, the two most widely used multilabel pairwise terms in vision, are weakly associative. Therefore, we can just construct the maximum constant labeling for each label and add them into X . For an arbitrary multi-label energy, it's hard to find the maximum ILM partial labeling. However, we can still use the greedy algorithm we used in the binary non-submodular case as a good heuristics in practice. This will return multiple ILM partial labelings with size at least 2, which is still better than checking persistency on a single variable as DEE does.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>Approaches We will focus on three partial optimality based preprocessing techniques in the experiment section, namely DEE <ref type="bibr" target="#b19">[20]</ref>, Kovtun's approach <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref> and our approach. They will be referred as DEE, KOVTUN and PR (Persistency Relaxation) respectively. We will use PR-k to refer our approach using k-conditions and PR-k-APX when we use its approximation. We experimented with MQPBO <ref type="bibr" target="#b11">[12]</ref>, but it was too slow to be competitive. LPbased approaches <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25]</ref> are also not considered due to their computational overhead, which is documented in <ref type="bibr" target="#b23">[24]</ref>. We will apply α-expansion for MRF inference <ref type="bibr" target="#b3">[4]</ref>, using the max-flow algorithm of <ref type="bibr" target="#b2">[3]</ref>. We will refer to αexpansion algorithm without any preprocessing technique as α-EXP, which is the baseline against which we compare all other approaches.</p><p>The α-expansion algorithm, like most move-making techniques, reduces the multi-label MRF inference problem to a series of binary MRF inference problems. Therefore, we can either (1) apply partial optimality based preprocessing techniques to the multi-label MRF directly and use α-EXP to infer the remaining variables, or <ref type="bibr" target="#b1">(2)</ref> in each iteration of α-EXP, apply the preprocessing technique to the induced binary MRF. <ref type="bibr" target="#b7">8</ref> We will refer to approach (1) as mDEE and mPR, and to (2) as iDEE and iPR. As a small optimization, for iDEE and iPR we only determined which variables do not switch to the new label, except for on the first iteration through the label set. Note that KOVTUN can only be used in approach (1) since it degenerates to QPBO for the induced binary problem, which is equivalent to the max-flow problem α-EXP needs to solve in each iteration. Dataset We conducted experiments on a variety of computer vision benchmarks for MRF inference, including brain-MRI <ref type="bibr" target="#b5">[6]</ref>, color segmentation <ref type="bibr" target="#b17">[18]</ref>, inpainting <ref type="bibr" target="#b4">[5]</ref>, Middlebury MRF dataset (including stereo, image inpainting and photomontage tasks) <ref type="bibr" target="#b25">[26]</ref> and scene decomposition <ref type="bibr" target="#b8">[9]</ref>. All these datasets are wrapped in OpenGM2 <ref type="bibr" target="#b10">[11]</ref> and are available online. <ref type="table" target="#tab_1">Table 2</ref> briefly summarizes the scale of each dataset. Note that the first three datasets use the Potts model, so the binary subproblem is submodular, while the last two have non-submodular (non-weakly associative) subproblems. Experimental Environment All the experiments were executed on a single machine with dual 3GHz Intel i7 Core and 16GB 1600 MHz DDR3 memory. Measurement We will evaluate the different approaches in two respects. First, we report the improvement in overall running time for the preprocessing and the inference on the <ref type="bibr" target="#b7">8</ref> We can also combine these two approaches, i.e., applying preprecessing for both the multi-label MRF and each induced binary MRF. Our experiments shows that it will only provide marginal improvement over only applying preprocessing to each induced binary MRF, so we don't report the results in this paper. remaining undetermined variables. We use α-EXP as the baseline and report the speedup for other methods compared to α-EXP. The reported numbers are averaged over all instances for each dataset. Second, we report the size of the partial optimal labeling found by the various preprocessing methods. The reported numbers are first averaged for each iteration of α-EXP in each instance, then averaged over all instances in one dataset. Experimental results We summarize the experimental results on several benchmarks in <ref type="figure" target="#fig_0">Figure 3</ref>; the raw numbers behind this figure are provided in the supplementary material. Besides the baseline technique α-EXP we also show results from iDEE and KOVTUN. Our approaches obtain a 1.5x-12x speedup compared to α-EXP and label significantly more variables than all other methods. For some specific instances, the speedup can be up to 40x; our speedup numbers, of course, include the cost of pre-processing. Note that KOVTUN was too slow on the Middlebury dataset to be competitive, running at least 5x slower than the baseline algorithm α-EXP. This suggests that when we have a large label set, it's very hard to compute partial optimality on the multi-label MRF directly, which is a major limitation of KOVTUN. We can also see PR-based methods find significantly more persistent variables than all the baseline methods. Per instance analysis indicates that our methods are superior on almost all instances. <ref type="figure" target="#fig_0">Figure 3</ref> also illustrate the power of the relaxation hierarchy we proposed. For example, on Color-Seg-n4 dataset, iPR-1 finds 14% more partial persistent variables than iDEE, iPR-2-APX finds additional 10% more than iPR-1. The gap between iPR-3-APX and iPR-2-APX are less significant, but still exists. It indicates that PR-based approaches significantly outperform the baseline DEE. The further we utilize the hierarchy, the more variables we can label, although the marginal gain is diminishing.</p><p>We also have run experiments with solving the multilabel problem directly, described in the supplemental material. In the multi-label setting our mPR-based methods also outperforms mDEE. However, solving the induced binary problem via expansion moves generally seems to be a better approach.</p><p>Our algorithms have one parameter, the maximum number of iterations τ . <ref type="figure" target="#fig_2">Figure 4</ref> and 5 illustrate running DEE   and PR-based methods with different τ 's on the Color-Seg-n4 dataset. Similar results are achieved on other datasets, which we report in the supplementary material. We can see the persistent var ratio converges very quickly with the growth of τ . In general, the overall running time decreases firstly and then increases due to it's a tradeoff between the speed and the quality of the preprocessing step. The proposed approach is not very sensitive to the choice of this stopping parameter, low total running time can be achieved in a very broad range. PR-based approaches significantly outperform DEE and other baseline methods no matter which τ we choose. <ref type="figure" target="#fig_0">Figure 3</ref> was computed with τ = 5, but other choices produce similar results.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 .</head><label>3</label><figDesc>Performance of various methods in terms of speedup and percentage of persistent variables. Higher numbers indicate better performance. Our three methods are at right, with numbers on the chart in bold. KOVTUN was too slow on the Middlebury-MRF dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Persistent variables ratio vs. stopping parameter τ .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Overall running time v.s. stopping parameter τ .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>Methods that optimally label a subset of the variables can obviously be used to accelerate MRF inference algorithms such as expansion moves. For example, Radhakrishnan and Su<ref type="bibr" target="#b19">[20]</ref> used DEE while Alahari et. al.<ref type="bibr" target="#b0">[1]</ref> applied Kovtun's approach.</figDesc><table>… 

Working set initialized Working set shrunk Working set converged 

Working set initialized 
Update a pixel 
Update a pixel 

… 

Figure 1. Our method (top row) versus DEE (bottom row), running 
on a binary-valued MRF with 16 variables. Optimally labeled vari-
ables are shown in red, while the working set is light blue. Green 
variables fail the sufficient test to be optimally labeled. The key 
step of each algorithm is highlighted with a grey background. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 2 .</head><label>2</label><figDesc>Dataset Description</figDesc><table>Dataset 
|V | 
|L| 
# Instances 
Brain MRI 
785,540-1,413,972 
5 
8 
Color Seg 
76,800-86,400 
3-12 
9 
Inpainting 
14400 
4 
2 
Middlebury 
21,838-514,080 
5-256 
7 
Scene Decomp 
150-208 
8 
715 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">To be precise, O(|V | + |E|) is the running time of our inner subroutine, which finds a globally optimal subset of variables that increases on each iteration. In practice this needs to be run a very limited number of times, as shown in the experimental results that run 5 iterations.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">For every pairwise cost, we have θ ij (0, 0) + θ ij (1, 1) ≤ θ ij (0, 1) + θ ij (1, 0).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Formally, y = x A ⊕ x B for disjoint A, B means that y i = (x A ) i when i ∈ A and y i = (x B ) i when i ∈ B.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">The original Goldstein condition is to claim one label cannot be persistent, which is equivalent to say its opposite is persistent for the binary case. For the multi-label case, we need to check |L i | − 1 labels cannot be persistent, so that the remaining one is persistent.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">To be completely precise, for the corner case of a tiny labeling with |S| &lt; k, we would test the |S|-condition instead of the k-condition.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">Starting from x S = x 1 ∈ L 1 , then for i = 2, 3, . . . , n, when we can find x i ∈ L i such that compositing x i into x S is still ILM, then do so.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements This research was supported by NSF grants IIS-1161860 and IIS-1447473. We thank Endre Boros and the anonymous reviewers for helpful comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Dynamic hybrid algorithms for MAP inference in discrete MRFs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Pseudo-boolean optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Boros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">L</forename><surname>Hammer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Discrete Applied Mathematics</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">An experimental comparison of min-cut/max-flow algorithms for energy minimization in vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Boykov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kolmogorov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1124" to="1137" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Fast approximate energy minimization via graph cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Boykov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Veksler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zabih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1222" to="1239" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A convex approach to minimal partitions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chambolle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Imaging Sciences</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1113" to="1158" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Brainweb: Online interface to a 3D MRI simulated brain database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename><surname>Cocosco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kollokian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K S</forename><surname>Kwan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Pike</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Evans</surname></persName>
		</author>
		<editor>NeuroImage</editor>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The dead-end elimination theorem and its use in protein side-chain positioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Desmet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Maeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hazes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lasters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">356</biblScope>
			<biblScope unit="issue">6369</biblScope>
			<biblScope unit="page" from="539" to="542" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Dynamic programming and graph algorithms in computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zabih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="721" to="740" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Decomposing a scene into geometric and semantically consistent regions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fulton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Generalized roof duality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Strandmark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Discrete applied mathematics</title>
		<imprint>
			<biblScope unit="volume">160</biblScope>
			<biblScope unit="issue">16</biblScope>
			<biblScope unit="page" from="2419" to="2434" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">A comparative study of modern inference techniques for discrete energy minimization problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Kappes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Hamprecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schnörr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kroeger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">X</forename><surname>Kausler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lellmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Savchynskyy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">On partial optimality in multi-label MRFs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shekhovtsov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kolmogorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="480" to="487" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Generalized roof duality and bisubmodular functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kolmogorov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1144" to="1152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Minimizing nonsubmodular functions with graph cuts-a review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kolmogorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<idno>MSR-TR-2006-100. 1</idno>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note>Earlier version appears as technical report</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">What energy functions can be minimized via graph cuts?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kolmogorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zabih</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>TPAMI</publisher>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="147" to="159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Partial optimal labeling search for a NPhard subclass of (max,+) problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kovtun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Image segmentation based on sufficient conditions of optimality in NP-complete classes of structural labelling problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kovtun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IRTC ITS National Academy of Sciences</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
	<note type="report_type">Ukrainian. PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Continuous multiclass labeling approaches and algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lellmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schnörr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal on Imaging Sciences</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1049" to="1096" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Fusion moves for Markov Random Field optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1392" to="1405" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Dead-end elimination as a heuristic for min-cut image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Radhakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Optimizing binary MRFs via extended roof duality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kolmogorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Szummer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Global MAP-optimality by shrinking the combinatorial search area with convex relaxation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Savchynskyy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Kappes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Swoboda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schnörr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1950" to="1958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Maximum persistency in energy minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shekhovtsov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1162" to="1169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Maximum persistency via iterative relaxed inference with graphical models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shekhovtsov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Swoboda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Savchynskyy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Partial optimality by pruning for MAPinference with general graphical models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Swoboda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Savchynskyy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Kappes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schnorr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">A comparative study of energy minimization methods for Markov Random Fields. TPAMI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zabih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Scharstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Veksler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kolmogorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tappen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="1068" to="1080" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">On the abstract properties of linear dependence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Whitney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Journal of Mathematics</title>
		<imprint>
			<biblScope unit="page" from="509" to="533" />
			<date type="published" when="1935" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Generalized roof duality for multi-label optimization: optimal lower bounds and persistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Windheuser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ishikawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="400" to="413" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
