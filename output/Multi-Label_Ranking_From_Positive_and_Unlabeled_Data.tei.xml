<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-label Ranking from Positive and Unlabeled Data</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atsushi</forename><surname>Kanehira</surname></persName>
							<email>kanehira@mi.t.u-tokyo.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="institution">The University of Tokyo</orgName>
								<address>
									<addrLine>7-3-1 Hongo Bunkyo-ku</addrLine>
									<settlement>Tokyo</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsuya</forename><surname>Harada</surname></persName>
							<email>harada@mi.t.u-tokyo.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="institution">The University of Tokyo</orgName>
								<address>
									<addrLine>7-3-1 Hongo Bunkyo-ku</addrLine>
									<settlement>Tokyo</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Multi-label Ranking from Positive and Unlabeled Data</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:43+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we specifically examine the training of a multi-label classifier from data with incompletely assigned labels. This problem is fundamentally important in many multi-label applications because it is almost impossible for human annotators to assign a complete set of labels, although their judgments are reliable. In other words, a multilabel dataset usually has properties by which (1) assigned labels are definitely positive and (2) some labels are absent but are still considered positive. Such a setting has been studied as a positive and unlabeled (PU) classification problem in a binary setting. We treat incomplete label assignment problems as a multi-label PU ranking, which is an extension of classical binary PU problems to the wellstudied rank-based multi-label classification. We derive the conditions that should be satisfied to cancel the negative effects of label incompleteness. Our experimentally obtained results demonstrate the effectiveness of these conditions.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Multi-label classification treats a problem that allows samples to take more than one label. Although the simplest solution for multi-label classification is training an independent classifier per class, a trained model is well known to have low classification performance when there is a correlation between classes <ref type="bibr" target="#b6">[7]</ref>. For this reason, a multi-label learning method, which incorporates label dependency, is needed. In recent years, many studies have specifically addressed multi-label learning <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b5">[6]</ref>. Furthermore, there are widely diverse applications in many domains including computer vision <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b17">[18]</ref>.</p><p>To collect a dataset for multi-label classification, researchers generally use crowdsourcing. An alternative is to collect data in a semi-automatic way as in <ref type="bibr" target="#b25">[26]</ref>. In most cases, the obtained labels will be incomplete but reliable because it is almost impossible to assign a full set of labels to describe images completely in the real world. For instance, let us consider the case in which human annotators attach labels to the leftmost image in <ref type="figure" target="#fig_0">Fig. 1</ref>. They might identify "dog" and "chair," and then assign them because they are the main components. However, besides these, "sofa," "carpet," and "box" can also be used. In addition, numerous other possible correct answers exist such as scene, breed of dog, and attribute.</p><p>As in the example presented above, the obtained dataset has properties by which (1) assigned labels are definitely positive and (2) absent labels are not necessarily negative. Because conventional multi-label learning models ignore this incompleteness and because they regard unlabeled objects as negative, their performance will be affected as in the case of the right-hand side of <ref type="figure" target="#fig_0">Fig. 1</ref>. Therefore, an incomplete label assignment problem is fundamentally important and critical in multi-label learning, which should be solved.</p><p>Our goal in this paper is to propose a method that enables us to train a classifier consistently from data with incompletely assigned labels. We deal with the setting as follows:</p><p>1. Assigned labels are definitely positive.</p><p>2. Absent labels are not necessarily negative.</p><p>3. Lastly, samples are allowed to take more than one label. Settings <ref type="bibr" target="#b0">(1)</ref> and <ref type="bibr" target="#b1">(2)</ref> have been studied as positive and unlabeled (PU) classification problems in a binary case <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b21">[22]</ref>. Moreover, setting (3) is a multi-label classification setting. In this work, we deal with a multi-label PU ranking problem to treat the setting, which includes all of (1), <ref type="bibr" target="#b1">(2)</ref>, and (3); then, by extending an analysis for binary PU classification <ref type="bibr" target="#b9">[10]</ref> to a multi-label problem, we derive the conditions under which the loss function should be satisfied to have consistency even if assigned labels are incomplete. The main contributions of this work are as follows:</p><p>1. We derive the conditions that should be satisfied to cancel the negative effects of label incompleteness in multi-label PU ranking.</p><p>2. We demonstrate the effectiveness of these conditions using experiments on several multi-label datasets.</p><p>In Sec.1, we describe the goals and contributions of this work. We then discuss related works in Sec. 2. In Sec. 3, we describe the settings of multi-label rankings. In Sec. 4, we explain an extension of the analyses for binary PU to multi-label problems and describe the conditions under which the loss function is satisfied. Several experiments on synthetic datasets and image annotation datasets are explained in Sec. 5 to investigate the efficacy of the derived conditions. Experimental results are discussed in Sec. 6. Then, we conclude our work in Sec. 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Multi-label ranking</head><p>Multi-label classification problems have been studied in recent years. One of the most common approaches is based on label ranking. Label ranking is aimed at ranking all positive classes higher than negative ones by minimizing rank loss. Rank loss, originally proposed by <ref type="bibr" target="#b12">[13]</ref>, has been studied well <ref type="bibr" target="#b15">[16]</ref>. Actually, <ref type="bibr" target="#b8">[9]</ref> relaxed the constraint condition to allow the application of the algorithm to large-scale data. In the computer vision domain, many algorithms have been proposed based on label ranking. <ref type="bibr" target="#b26">[27]</ref> proposed a learning model, WSABIE, that embeds image and word features to a common space by optimizing the weighted rank loss. <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b0">[1]</ref> used rank loss for the object recognition task. In addition, <ref type="bibr" target="#b16">[17]</ref> trained a deep convolutional neural network for a multi-label task by replacing the softmax loss with the weighted rank loss proposed by <ref type="bibr" target="#b26">[27]</ref>. However, all of these works assume that all labels are assigned completely and do not deal directly with label incompleteness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">PU Classification</head><p>The problem of training classifiers from positive samples and unlabeled samples is called PU classification. Some studies have addressed this problem <ref type="bibr" target="#b21">[22]</ref>. Actually, <ref type="bibr" target="#b14">[15]</ref> constructed a probabilistic model only from observable samples and estimated a proper classifier using it. In addition, <ref type="bibr" target="#b9">[10]</ref> analyzed a binary PU classification problem and revealed that PU classification can be cast as a cost-sensitive learning, which changes the weight of the penalty per class. Using a symmetric non-convex function as a surrogate loss makes it possible to learn consistently. However, these studies emphasized only binary classification problems and did not assume that samples take more than one label.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Learning from incompletely labeled data</head><p>Some works have attempted to address label incompleteness in multi-label learning as label deficits. <ref type="bibr" target="#b2">[3]</ref> tried to eliminate the influence of label deficits in the optimization process by adding a regularization term to rank loss, which forces the difference between scores for positive and negative labels to be group sparse. Then, <ref type="bibr" target="#b19">[20]</ref> extended <ref type="bibr" target="#b14">[15]</ref> to a multi-label setting by considering the label dependency. <ref type="bibr" target="#b27">[28]</ref> dealt with weak labels in a multiple-instance, multilabel learning setting. Subsequently, <ref type="bibr" target="#b22">[23]</ref> used a conditional restricted Boltzmann machine to denoise the label deficit. However, these studies did not mention that the condition loss function should be satisfied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Multi-label ranking</head><p>In this section, we describe the setting of the multi-label ranking problem. Let X be a sample space and Y={0,1} m be the possible set of labels, where m denotes the number of classes. y i denotes the status of the sample in terms of the i-th class: if y i =1, then the i-th class is positive for a given sample, and if y i =0, then it is negative. A dataset having N samples S={(x 1 ,y 1 ),(x 1 ,y 1 ),...,(x N ,y N )} is generated from an unknown distribution on X⇥Y. A score function is defined as f (x)=(f 1 (x),f 2 (x),...,f m (x)) : X− ! R m . In an empirical risk minimization framework, algorithms are used to minimize the expectations of the loss function over the (sample, label) space. In other words, the following f ⇤ =argmin L(f ) is computed:</p><formula xml:id="formula_0">L(f )=E xy [L(f (x),y)].<label>(1)</label></formula><p>L(f (x),y) indicates the loss function that takes a label and a score for the sample as input. Although some loss functions including 0-1 subset loss and Hamming loss are proposed, we treat the rank loss, which is commonly used in multilabel learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Rank loss</head><p>Rank loss imposes a penalty on a classifier when a pair of labels is incorrectly ranked. It can be defined as follows:</p><formula xml:id="formula_1">L rank (f (x),y)= X {i,j:yi=1,yj =0} [[f i &lt;f j ]]+ 1 2 [[f i =f j ]],<label>(2)</label></formula><p>where i,j are the indices of the class. The i-th element of f (x), which means the score for the i-th class, is denoted by f i , omitting the dependency for x. [[·]] is the indicator function that takes a value of 1 when the conditions inside the brackets are met; otherwise, it is 0. From <ref type="formula" target="#formula_0">(1)</ref>, the loss function that should be minimized is</p><formula xml:id="formula_2">L rank = E xy [L rank (f (x),y)] = X y2Y P (y)E x|y [L rank (f (x),y)] = X y2Y P (y) X {i,j:yi=1,yj =0} E x|y  [[f i &lt;f j ]]+ 1 2 [[f i =f j ]] . (3)</formula><p>By swapping two summations, we can rewrite <ref type="formula">(3)</ref> as</p><formula xml:id="formula_3">L rank = X {i,j:yi=1,yj =0} P (y i =1,y j =0)E x|yi=1,yj =0  [[f i &lt;f j ]]+ 1 2 [[f i =f j ]] .<label>(4)</label></formula><p>Here, we define the mis-rank rate as</p><formula xml:id="formula_4">R(i,j)=E x|y i =1,y j =0  [[f i &lt;f j ]]+ 1 2 [[f i =f j ]] = P (f i &lt;f j | y i =1,y j =0)+ 1 2 P (f i =f j | y i =1,y j =0).<label>(5)</label></formula><p>The mis-rank rate R(i,j) is the probability that the computed score of a sample for the positive label is smaller than that for the negative label. Using this, we express the rank loss L rank as</p><formula xml:id="formula_5">L rank = X {i,j:yi=1,yj =0} P (y i =1,y j =0)R(i,j) = X 1i&lt;jm P (y i =1,y j =0)R(i,j)+P (y i =0,y j =1)R(j,i).<label>(6)</label></formula><p>We can consider rank loss L rank as the expectation of the mis-rank rate R(i,j) over all possible pairs of labels. Our interest is to minimize it in the PU setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Multi-label PU ranking</head><p>As described above, multi-label PU ranking is a problem of training a label-ranking-based multi-label classifier from a dataset, which has properties in which (1) labels assigned to samples are definitely positive and (2) absent labels are not necessarily negative.</p><p>In this section, we extend the analysis for binary PU classification <ref type="bibr" target="#b9">[10]</ref> to a multi-label setting and derive the following: 1. A multi-label PU ranking problem can be cast as a cost-sensitive learning using positive and unlabeled data.</p><p>2. Applying a surrogate loss for optimization with incompletely labeled data leads to error from a correct one, which can be cancelled by selecting a symmetric surrogate loss function such as a ramp loss or a sigmoid loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Appropriately weighted cost</head><p>In this section, we explain how a multi-label PU ranking problem can be cast as a cost-sensitive learning, which means that we should weight the loss function appropriately. Cost-sensitive learning <ref type="bibr" target="#b13">[14]</ref> is a type of learning method that incorporates the mis-classification cost. Costsensitive "ranking" using rank loss is given naturally as</p><formula xml:id="formula_6">L rank = X 1i&lt;jm c ij P (y i =1,y j =0)R(i,j)+c ji P (y i =0,y j =1)R(j,i).<label>(7)</label></formula><p>where c ij is the weight of the penalty for mis-ranking for the pair y i =1,y j =0. We aim at minimizing (6) with incompletely labeled samples. In our PU setting, the mis-rank rate R(i,j) in (6) cannot be estimated directly from the data because there are no negative labels. For this reason, we introduce a pseudo-mis-rank rate R X (i,j), which can be estimated from the data, and then consider the optimization of (6) via this quantity. Here, we define the pseudo-mis-rank rate R X (i,j) as</p><formula xml:id="formula_7">R X (i,j) =P (f i &lt;f j | s i =1,s j =0)+ 1 2 P (f i =f j | s i =1,s j =0). (8)</formula><p>Therein, s i 2{0,1} shows whether the label of the i-th class is assigned or not. s i =1 and s i =0 respectively mean that the i-th class is labeled and not labeled. The pseudo-misrank rate R X (i,j) represents the possibility that the ranking score of a sample for an assigned label is smaller than that for an absent label. We can estimate this quantity from incompletely labeled data. If we ignore label incompleteness, then the expected loss as actually minimized is not <ref type="formula" target="#formula_5">(6)</ref> but</p><formula xml:id="formula_8">L rank = X 1i&lt;jm P (s i =1,s j =0)R X (i,j)+P (s i =0,s j =1)R X (j,i).<label>(9)</label></formula><p>Similarly to <ref type="bibr" target="#b9">[10]</ref>, we assume that unlabeled data are generated from a marginal distribution. This is called the "casecontrolled" setting in PU classification <ref type="bibr" target="#b24">[25]</ref>. This condition is expressed as P (y i =1|s i =0)=P (y i =1). Furthermore, we make the assumption that the deficit of a positive label is not biased on the sample space. This condition is represented as P (x|s i =1)=P (x|y i =1). By these assumptions, the pseudo-mis-rank rate R X (i,j) can be written using R(i,j) as follows (see Appendix A):</p><formula xml:id="formula_9">R X (i,j)=(1−π ij )R(i,j)+π ij R -X (i,j),<label>(10)</label></formula><p>where</p><formula xml:id="formula_10">R -X (i,j) =P (f i &lt;f j | y i =1,y j =1)+ 1 2 P (f i =f j | y i =1,y j =1),<label>(11)</label></formula><p>and</p><formula xml:id="formula_11">π ij =P (y j =1 | y i =1).<label>(12)</label></formula><p>R -X (i,j) denotes the penalty imposed on mistakes attributable to the existence of positive labels contained in absent labels. To be more specific, in the case of learning from samples with incompletely assigned labels, rank loss imposes a penalty not for (positive, negative) but for (assigned, not assigned) label pairs. However, these pairs contain (positive, positive) pairs, which should not be included in the penalty. Here, we represent this excessively imposed penalty and its ratio as R -X (i,j) and π ij , respectively. Therefore, (10) can be interpreted as a decomposition into two losses: the loss that should be given and the loss that should not be given. Transforming <ref type="formula" target="#formula_0">(10)</ref>, we obtain</p><formula xml:id="formula_12">R(i,j)= 1 1−π ij (R X (i,j)−π ij R -X (i,j)).<label>(13)</label></formula><p>By substituting this into (6) and using the relation</p><formula xml:id="formula_13">R -X (i,j)+R -X (j,i) =P (f i &gt;f j or f i =f j or f i &lt;f j | y i =1,y j =1) =1,<label>(14)</label></formula><p>we can obtain the following equation (see Appendix B):</p><formula xml:id="formula_14">L rank = X P (y i =1)R X (i,j)+P (y j =1)R X (j,i)−P (y i =1,y j =1) = X c ij P (s i =1,s j =0)R X (i,j)+c ji P (s i =0,s j =1)R X (j,i) −P (y i =1,y j =1).<label>(15)</label></formula><p>Here,</p><formula xml:id="formula_15">c ij = P (y i =1) P (s i =1,s j =0)</formula><p>.</p><p>Compared with (9), the multi-label PU ranking problem can be cast as a cost-sensitive learning. In fact, minimizing the rank loss function weighted by c ij with incompletely labeled data implies that the rank loss with completely labeled data, which should be originally minimized, can be minimized. P (s i =1,s j =0) is the ratio of the samples in a dataset that meets the condition s i =1,s j =0. This can be estimated from the training dataset. In addition, P (y i =1) can be estimated using the methods proposed in <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b1">[2]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Symmetric surrogate loss</head><p>In this section, we derive the condition for surrogate loss used in the optimization process. Mis-rank rate R can be written as the expectation of the 0-1 function over the sample space as described below:</p><formula xml:id="formula_17">R(i,j)=E x|yi=1,yj =0 [l 0-1 (f i −f j )],l 0-1 (x)= 8 &gt; &lt; &gt; : 1(ifx&lt;0) 1 2 (if x=0) 0 (otherwise) Similarly, R X (i,j)=E x|si=1,sj =0 [l 0-1 (f i −f j )].</formula><p>Because direct optimization of this expectation including the 0-1 loss is intractable, the surrogate loss function l 0 (x) is generally used instead of the 0-1 loss. For example, the pseudo-mis-rank rate is written as follows by applying the hinge loss function l 0 H , which is used in many popular models (e.g., support vector machines), as the surrogate loss.</p><formula xml:id="formula_18">R X (i,j)⇡E x|si=1,sj =0 [l 0 H (f i −f j )],l 0 H (x)= ( 1−x (if x&lt;1) 0 (otherwise)</formula><p>When fully labeled data are used, by applying surrogate loss to rank loss (6), we obtain</p><formula xml:id="formula_19">L 0 rank = X P (y i =1,y j =0)E x|yi=1,yj =0 [l 0 (f i −f j )] +P (y i =0,y j =1)E x|yi=0,yj =1 [l 0 (f j −f i )].<label>(17)</label></formula><p>By replacing R X in <ref type="bibr" target="#b14">(15)</ref> with the expectation of surrogate loss,</p><formula xml:id="formula_20">L 00 rank = X P (y i =1)E x|si=1,sj =0 [l 0 (f i −f j )] +P (y j =1)E x|si=0,sj =1 [l 0 (f j −f i )]−P (y i =1,y j =1) = X P (y i =1,y j =0)E x|yi=1,yj =0 [l 0 (f i −f j )] +P (y i =0,y j =1)E x|yi=0,yj =1 [l 0 (f j −f i )] −P (y i =1,y j =1) 1−E x|yi=1,yj =1 [l 0 (f i −f j )+l 0 (f j −f i )]<label>(18)</label></formula><p>is obtained. From the two equations <ref type="formula" target="#formula_0">(17)</ref> and <ref type="formula" target="#formula_0">(18)</ref>, we observe the relation L 00 rank =L 0 rank +(Error). It means that utilizing surrogate loss in the optimization process with incompletely labeled data causes an error. If we select the surrogate loss function l 0 (·) to meet l 0 (f i −f j )+l 0 (f j −f i )=1, then this error is cancelled and L 00 rank =L 0 rank is obtained. Convex functions such as hinge loss and exponential loss ( <ref type="figure" target="#fig_1">Fig. 2(b)</ref>) do not meet this condition. Symmetric nonconvex functions such as ramp loss and sigmoid loss ( <ref type="figure" target="#fig_1">Fig.  2(a)</ref>) satisfy it.</p><p>For example, let us consider a case in which data are completely labeled and separable. In other words, min L rank =0. In this case,</p><formula xml:id="formula_21">argmin L 0 rank-hinge =argmin L 0 rank-ramp .</formula><p>Completely identical hyperplanes are obtained for both the ramp loss and the hinge loss. Considering the PU setting, from the discussion presented above, we obtain argmin L 00 rank-ramp =argmin L 0 rank-ramp .</p><p>On the other hand, argmin L 00 rank-hinge 6 =argmin L 0</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>rank-hinge</head><p>This relation indicates that the obtained boundary differs from the optimal one when using hinge loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiment</head><p>To investigate the efficacy of the conditions stated in the previous section, we conducted experiments on three datasets: synthetic dataset, MSCOCO <ref type="bibr" target="#b23">[24]</ref>, and NUS-WIDE <ref type="bibr" target="#b7">[8]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Setting</head><p>Classifiers were trained from data containing a deficit on positive labels at a rate from 0% to 80%. The accuracy was evaluated on fully labeled data. The label deficit is given from the following steps:</p><p>1. Determine the total number of deficit labels N noise by multiplying the deficit rate by the total number of positive labels. </p><formula xml:id="formula_22">N noise = P c N c noise . 3. Choose N c</formula><p>noise samples labeled as c at random and remove their label.</p><p>We used the mean average precision in terms of samples as an evaluation criterion. Through all the experiments, hinge loss and ramp loss were used in the non-symmetric and symmetric surrogate loss function, respectively. Score functions were linear, and their weights were updated using stochastic gradient descent. Specifically for the score function f (x)=W T x and loss function 1 N P N n=1 l(f (x n ),y n ), models were updated as</p><formula xml:id="formula_23">W (t+1) =W (t) −η (τ ) ∂l(f (x n ),y n ) ∂W ,</formula><p>where N denotes the number of training samples, and η (τ ) and τ denote the learning rate and the number of iterations, respectively. In this experiment, we reduced the learning rate as η (τ ) =η (0) / p τ .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Synthetic dataset</head><p>A synthetic dataset was generated as in the following procedure, which was performed for each sample:</p><p>1. The number of relevant labels n is sampled from a Poisson distribution. 2. For n times, relevant class c is sampled from a multinomial distribution. 3. The number of feature samplings k is sampled from a Poisson distribution. 4. For k times, feature x is sampled from a multinomial distribution parametrized per class and their summation is used as a feature of the sample.</p><p>Ten thousand samples were generated and then divided into 8,000 for training and 2,000 for testing. The parameters of the multinomial distributions were sampled from a uniform distribution. In addition, L2 normalization was applied to all samples.</p><p>Experiment A. First, to show the influence of the derived condition on classification accuracy, we evaluated four methods, corresponding to whether condition 1 (use of appropriately weighted loss function) and condition 2 (use of <ref type="figure">Figure 3</ref>. Results of Experiment A on a synthetic dataset. Each figure from the left corresponds to a different mean number of labels <ref type="bibr" target="#b1">(2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr">32)</ref>. The method that meets both conditions has robustness to the label deficit. symmetric loss function) are met or not <ref type="table" target="#tab_0">(Table 1</ref>). In this experiment, the prior P (y i =1) in (16) was given from fully labeled training samples. We fixed the number of classes to 40 and the dimension of a feature to 100. The mean number of labels for each sample was chosen from <ref type="bibr" target="#b1">(2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr">32)</ref>. The result is presented in <ref type="figure">Fig. 3</ref>. In all settings, Method 3, which satisfies both conditions, could learn most robustly. As the number of labels increased, the difference in accuracy between Method 3 and the other methods became large.</p><p>Experiment B. Then, we evaluated the accuracy of Method 3 when combined with prior estimation, which is necessary for application to a real problem. To this end, we compared three methods: Method 1 (baseline), Method 3 with a prior given from a fully labeled dataset (denoted as optimal in result), and Method 3 with an estimated prior (denoted as estimated). To estimate class prior P (y i =1) for class i, we applied <ref type="bibr" target="#b10">[11]</ref> based on distribution matching <ref type="bibr" target="#b11">[12]</ref> for every class. Because we cannot know negative samples, this algorithm is designed to estimate prior θ=p(y=1) by partially matching marginal distribution p(x) and class conditional distribution p(x|y=1) weighted by prior probability. To this end, the algorithm attempts to minimize the Pearson (PE) divergence in terms of θ. Formally, we obtain</p><formula xml:id="formula_24">θ ⇤ =argmin PE =argmin 1 2 Z ✓ θp(x|y=1) p(x) −1 ◆ 2 p(x)dx.<label>(19)</label></formula><p>Instead of a direct minimization of the PE divergence, the tightest lower bound is minimized. For more details, please see <ref type="bibr" target="#b10">[11]</ref>. We changed the synthetic dataset from three points of view: (1) the number of classes, (2) the number of labels, and (3) the dimension of the features. Each setting is described as follows:</p><p>• Choose the number of labels from <ref type="bibr" target="#b1">(2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b7">8)</ref>, while the number of classes and the dimension of the features are fixed respectively to 80 and 100. The results of each setting correspond respectively to <ref type="figure" target="#fig_3">Fig. 4, Fig. 5, and Fig. 6</ref>. As shown in <ref type="figure" target="#fig_3">Fig. 4</ref>, when few labels were assigned for a fixed number of classes (2 labels out of 80 classes), and when the label deficit rate was low, the performance of the method with the estimated prior was high, sometimes even better than that with the optimal prior, and it worsened as the deficit rate increased. The result of the experiment using samples with more labels (4 labels out of 80 classes) showed that, although the performance was low when the label deficit rate was low, it was comparable to the method with the optimal prior as the deficit increased. If many labels were attached (8 labels out of 80 classes), the prior estimation failed and high accuracy could not be achieved. A similar tendency was observed from an experiment in which the number of classes varied <ref type="figure" target="#fig_4">(Fig. 5)</ref>. These  results implied that the rate of (the number of labels)/(the number of classes) in the data is a key to the success of our method combined with prior estimation. From the results of the experiment of changing the dimension of the features shown in <ref type="figure" target="#fig_5">Fig. 6</ref>, it is apparent that the accuracy of the estimated method approached the optimal one as the dimensions increased.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Image Annotation Dataset</head><p>We conducted experiments on the image annotation datasets MSCOCO <ref type="bibr" target="#b23">[24]</ref> and NUS-WIDE <ref type="bibr" target="#b7">[8]</ref>. The settings of each experiment are given as follows:</p><p>MSCOCO: This dataset contains segmentation information and captions in addition to object labels. In this experiment, we used only images and object tags attached to them. We eliminated duplicated tags to a single image. We used 82,783 samples for training and 40,504 samples for testing. The number of classes was 80. The average number of labels was 2.95 per sample.</p><p>NUS-WIDE : This dataset includes Flickr images with 5,018 types of tags. We used 81 classes predetermined in the dataset and 161,789 samples for training and 107,859 samples for testing. The average number of labels was 1.76 per sample.</p><p>As the image feature, we used the activation of the 7th layer of AlexNet <ref type="bibr" target="#b20">[21]</ref> pre-trained with the ILSVRC2012 dataset. The dimensions of the visual features were 4,096. We compared the same methods as those used in the previous experiment: Method 1 (baseline), Method 3 (opti-mal), and Method 3 (estimated). In these experiments, we estimated the prior using a naive method, which uses the ratio of a labeled sample per class in the training dataset. The experimental results for MSCOCO and NUS-WIDE are presented in <ref type="figure" target="#fig_6">Fig. 7</ref> and <ref type="figure" target="#fig_7">Fig. 8</ref>, respectively. In both datasets, Method 3 with the estimated prior was able to improve slightly the accuracy in every deficit rate even though the prior estimation was naive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Discussion</head><p>The results of Experiment A on the synthetic dataset showed that, as both (A) the label deficit rate and (B) the number of labels assigned to the samples increased, the difference in accuracy between the methods with condition 1 (Method 1 and Method 3) and those without it (baseline and Method 2) increased. This result can be explained using the following analysis. As provided in <ref type="bibr" target="#b8">(9)</ref>, letL rank denote the loss function when we do not change the weight per class. The error from true loss L rank −L rank can be decomposed into the following: (a) A term proportional to the probability that samples are unlabeled even if they are positive. (b) A term proportional to the probability that both labels within a pair of labels are assigned (see Appendix C). (a) and (b) respectively correspond to (A) and (B).</p><p>Furthermore, if condition 1 is met, the difference in accuracy between the method with condition 2 and that without it becomes large when samples have many labels. That result derives from the error caused by surrogate loss in the PU setting data, where L 00 rank −L 0 rank (provided in <ref type="formula" target="#formula_0">(17)</ref> and <ref type="formula" target="#formula_0">(18)</ref>) is proportional to the probability that both labels   are positive from every pair.</p><p>The reason for the low performance of Method 1, even when the label deficit rate was low, can be attributed to the fact that an error occurs when the "case-controlled" condition is not satisfied, which we assumed to derive the conditions. In contrast to Method 1, Method 3 still showed high performance because the error was low when symmetric loss was used. Particularly, when a label's deficit rate is 0%, or the labels are assigned completely, this error can be cancelled using symmetric loss (see Appendix D).</p><p>In many situations with Experiment B on the synthetic dataset, the accuracy near a 0% deficit rate was low because the prior estimation method we used did not assume that the labels were complete. Our method works when the label deficit rate is greater than about 20%, which is likely in realistic data. To treat both complete and incomplete labels using the same method, it is necessary to invent more flexible prior estimation methods, which is a subject for our future work. We conjecture that the reason for the prior estimation failing when numerous labels were attached is that positive samples and negative samples for one class tend to share other class's label, which causes low separability in feature space. For a task of classifying (dog, cat, human), if we collect images in which they exist together with high probability, the sample labeled as (cat, human) and (dog, cat, human) might have similar features. The rate of such a sample in a dataset increases as the number of labels increases for a fixed number of classes. It is extremely difficult to estimate the prior from such low-separability data. The per-formance of a method with prior estimation that sometimes overcomes the optimal one can be thought to estimate the distribution from which data are generated more accurately than taking a ratio of the samples in a dataset without a label deficit.</p><p>In the experiments on image annotation datasets, we estimated the class prior in a naive manner because the computational costs of existing algorithms are high. Despite its simplicity, our method slightly improved the accuracy, but an efficient estimation algorithm is needed for more improvement when applied to large-scale data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>In this paper, we specifically examined the training of a multi-label classifier from incompletely labeled data, which is essential for multi-label training. Regarding this problem as a multi-label PU classification problem, we extended the binary PU classification to label-ranking-based multi-label learning. By analyzing this problem, we derived two conditions for training classifiers consistently even if only parts of the relevant labels are obtained: (1) use of appropriately weighted loss function and (2) use of symmetric surrogate loss. We conducted experiments on several datasets and also demonstrated the efficacy of these conditions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Acknowledgment</head><p>This work was supported by CREST, JST. The authors would like to thank Prof. Masashi Sugiyama (The University of Tokyo) for the useful discussions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Multi-label dataset tends to have partially labeled samples. Absent labels are regarded as negative, and it affects classification performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>(a) Ramp loss (red dashed line) and sigmoid loss (blue solid line) meet the described condition, while (b) exponential loss (red dashed line) and hinge loss (blue solid line) do not.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>•</head><label></label><figDesc>Choose the number of classes from (40, 80, 160), while the number of labels and the dimension of the features are fixed respectively to 4 and 100. • Choose the dimension of the features from (50, 100, 150), while the number of classes and the number of labels are fixed respectively to 80 and 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Results of Experiment B. Each figure from the left corresponds to a different mean number of labels<ref type="bibr" target="#b1">(2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b7">8)</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Results of Experiment B. Each figure from the left corresponds to a different number of classes (40, 80, 160).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Results of Experiment B. Each figure from the left corresponds to a different number of features (50, 100, 150).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .</head><label>7</label><figDesc>Experimental result for the MSCOCO dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 .</head><label>8</label><figDesc>Experimental result for the NUS-WIDE dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><head>Table 1 .</head><label>1</label><figDesc>Methods used in the experiments. Each method corresponds to whether condition 1 (use of weighted loss function) and condition 2 (use of symmetric loss function) are met or not.</figDesc><table>Baseline Method 1 Method 2 

Method 3 
(proposed) 
Condition 1 
(weighted loss) 
X 
X 
Condition 2 
(symmetric loss) 
X 
X 

2. Determine the number of deficit samples N c 
noise for 
class c from a multinomial distribution to meet 
</table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Labelembedding for attribute-based classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Semi-supervised novelty detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Blanchard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="2973" to="3009" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multi-label learning with incomplete class assignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Bucak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Efficient multi-label ranking for multi-class learning: application to object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Bucak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">K</forename><surname>Mallapragada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Supervised learning of semantic classes for image annotation and retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Carneiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Moreno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>PAMI</publisher>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="394" to="410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Feature-aware label space dimension reduction for multi-label classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-N</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-T</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Bayes optimal multilabel classification via probabilistic classifier chains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hüllermeier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Dembczynski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Nus-wide: a real-world web image database from national university of singapore</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIVR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Online passive-aggressive algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Dekel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Keshet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shalev-Shwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="551" to="585" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Analysis of learning from positive and unlabeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>Du Plessis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="703" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Class prior estimation from positive and unlabeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>Du Plessis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEICE TRANSACTIONS on Information and Systems</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1358" to="1362" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Semi-supervised learning of class balance under class-prior change by distribution matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>Du Plessis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="110" to="119" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A kernel method for multilabelled classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Elisseeff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">The foundations of cost-sensitive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Elkan</surname></persName>
		</author>
		<editor>IJ-CAI</editor>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning classifiers from only positive and unlabeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Elkan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Noto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">On the consistency of multi-label learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">199</biblScope>
			<biblScope unit="page" from="22" to="44" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.4894</idno>
		<title level="m">Deep convolutional ranking for multilabel image annotation</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Tagprop: Discriminative metric learning in nearest neighbor models for image auto-annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Guillaumin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multilabel classification using bayesian compressed sensing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Viswanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Large-scale multi-label learning with incomplete label assignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIAM</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning with positive and unlabeled examples using weighted logistic regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Conditional restricted boltzmann machines for multi-label learning with incomplete labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning from corrupted binary labels via class-probability estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">V</forename><surname>Rooyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Williamson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Labeling images with a computer game</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Von Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dabbish</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CHI</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Wsabie: Scaling up to large vocabulary image annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Multi-instance multilabel learning with weak label</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
