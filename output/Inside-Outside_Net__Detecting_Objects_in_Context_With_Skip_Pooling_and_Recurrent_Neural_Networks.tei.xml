<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Inside-Outside Net: Detecting Objects in Context with Skip Pooling and Recurrent Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Bell</surname></persName>
							<email>sbell@cs.cornell.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Cornell University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
							<email>zitnick@fb.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Microsoft Research *</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kavita</forename><surname>Bala</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Cornell University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Microsoft Research *</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Inside-Outside Net: Detecting Objects in Context with Skip Pooling and Recurrent Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:41+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>It is well known that contextual and multi-scale representations are important for accurate visual recognition. In this paper we present the Inside-Outside Net (ION), an object detector that exploits information both inside and outside the region of interest. Contextual information outside the region of interest is integrated using spatial recurrent neural networks. Inside, we use skip pooling to extract information at multiple scales and levels of abstraction. Through extensive experiments we evaluate the design space and provide readers with an overview of what tricks of the trade are important. ION improves state-of-the-art on PASCAL VOC 2012 object detection from 73.9% to 77.9% mAP. On the new and more challenging MS COCO dataset, we improve state-of-the-art from 19.7% to 33.1% mAP. In the 2015 MS COCO Detection Challenge, our ION model won "Best Student Entry" and finished 3 rd place overall. As intuition suggests, our detection results provide strong evidence that context and multi-scale representations improve small object detection.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Reliably detecting an object requires a variety of information, including the object's fine-grained details and the context surrounding it. Current state-of-the-art detection approaches <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b31">32]</ref> only use information near an object's region of interest (ROI). This places constraints on the type and accuracy of objects that may be detected.</p><p>We explore expanding the approach of <ref type="bibr" target="#b8">[9]</ref> to include two additional sources of information. The first uses a multiscale representation <ref type="bibr" target="#b19">[20]</ref> that captures fine-grained details by pooling from multiple lower-level convolutional layers in a ConvNet <ref type="bibr" target="#b36">[37]</ref>. These skip-layers <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b12">13]</ref> span multiple spatial resolutions and levels of feature abstraction. The information gained is especially important for small objects, which require the higher spatial resolution provided by lower-level layers.</p><p>Our second addition is the use of contextual informa- * R. <ref type="bibr">Girshick</ref>   . In a single pass, we extract VGG16 <ref type="bibr" target="#b36">[37]</ref> features and evaluate 2000 proposed regions of interest (ROI). For each proposal, we extract a fixed-size descriptor from several layers using ROI pooling <ref type="bibr" target="#b8">[9]</ref>. Each descriptor is L2-normalized, concatenated, scaled, and dimension-reduced (1x1 convolution) to produce a fixed-length feature descriptor for each proposal of size 512x7x7. Two fully-connected (fc) layers process each descriptor and produce two outputs: a one-of-K class prediction ("softmax"), and an adjustment to the bounding box ("bbox"). tion. It is well known in the study of human and computer vision that context plays an important role in visual recognition <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b5">6]</ref>. To gather contextual information we explore the use of spatial Recurrent Neural Networks (RNNs). These RNNs pass spatially varying contextual information both horizontally and vertically across an image. The use of at least two RNN layers ensures information may be propagated across the entire image. We compare our approach to other common methods for adding contextual information, including global average pooling and additional convolutional layers. Global average pooling provides information about the entire image, similar to the features used for scene or image classification <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b21">22]</ref>.</p><p>Following previous approaches <ref type="bibr" target="#b9">[10]</ref>, we use object proposal detectors <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b42">43]</ref> to identify ROIs in an image. Each ROI is then classified as containing one or none of the objects of interest. Using dynamic pooling <ref type="bibr" target="#b13">[14]</ref> we can efficiently evaluate thousands of different candidate ROIs with a single forwards pass of the network. For each candidate ROI, the multi-scale and context information is concatenated into a single layer and fed through several fully connected layers for classification.</p><p>We demonstrate that both sources of additional infor-mation, context and multi-scale, are complementary in nature. This matches our intuition that context features look broadly across the image, while multi-scale features capture more fine-grained details. We show large improvements on the PASCAL VOC <ref type="bibr" target="#b6">[7]</ref> and Microsoft COCO <ref type="bibr" target="#b23">[24]</ref> object detection datasets and provide a thorough evaluation of the gains across different object types. We find that they are most significant for object types that have been historically difficult. For example, we show improved accuracy for potted plants which are often small and amongst clutter. In general, we find that our approach is more adept at detecting small objects than previous state-of-the-art methods. For heavily occluded objects like chairs, gains are found when using contextual information. While the technical methods employed (spatial RNNs <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b41">42]</ref>, skip-layer connections <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b12">13]</ref>) have precedents in the literature, we demonstrate that their well-executed combination has an unexpectedly positive impact on the detector's accuracy. As always, the devil is in the details <ref type="bibr" target="#b3">[4]</ref> and thus our paper aims to provide a thorough exploration of design choices and their outcomes.</p><p>Contributions. We make the following contributions: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Prior work</head><p>ConvNet object detectors. ConvNets with a small number of hidden layers have been used for object detection for the last two decades (e.g., from <ref type="bibr" target="#b40">[41]</ref> to <ref type="bibr" target="#b35">[36]</ref>). Until recently, they were successful in restricted domains such as face detection. Recently, deeper ConvNets have led to radical improvements in the detection of more general object categories. This shift came about when the successful application of deep ConvNets to image classification <ref type="bibr" target="#b21">[22]</ref> was transferred to object detection in the R-CNN system of Girshick et al. <ref type="bibr" target="#b9">[10]</ref> and the OverFeat system of Sermanet et al. <ref type="bibr" target="#b34">[35]</ref>. Our work builds on the rapidly evolving R-CNN ("region-based convolutional neural network") line of work.</p><p>Our experiments are conducted with Fast R-CNN <ref type="bibr" target="#b8">[9]</ref>, which is an end-to-end trainable refinement of He et al.'s SPPnet <ref type="bibr" target="#b13">[14]</ref>. We discuss the relationship of our approach to other methods later in the paper in the context of our model description and experimental results.</p><p>Spatial RNNs. Recurrent Neural Networks (RNNs) exist in various extended forms, including bidirectional RNNs <ref type="bibr" target="#b33">[34]</ref> that process sequences left-to-right and rightto-left in parallel. Beyond simple sequences, RNNs exist in full multi-dimensional variants, such as those introduced by Graves and Schmidhuber <ref type="bibr" target="#b10">[11]</ref> for handwriting recognition, or as hierarchical recurrent pyramids (the Neural Abstraction Pyramid <ref type="bibr" target="#b1">[2]</ref>). As a lower-complexity alternative, <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b41">42]</ref> explore running an RNN spatially (or laterally) over a feature map in place of convolutions. In this paper, we employ spatial RNNs as a mechanism for computing contextual features for use in object detection.</p><p>Skip-layer connections. Skip-layer connections are a classic neural network idea wherein activations from a lower layer are routed directly to a higher layer while bypassing intermediate layers. The specifics of the wiring and combination method differ between models and applications. Our usage of skip connections is most closely related to those used by Sermanet et al. <ref type="bibr" target="#b35">[36]</ref> (termed "multi-stage features") for pedestrian detection. Different from <ref type="bibr" target="#b35">[36]</ref>, we find it essential to L2 normalize activations from different layers prior to combining them. The need for activation normalization when combining features across layers was recently noted by <ref type="bibr">Liu et al. (ParseNet [25]</ref>) in a model for semantic segmentation that makes use of global image context features. Skip connections have also been popular in recent models for semantic segmentation, such as the "fully convolutional networks" in <ref type="bibr" target="#b25">[26]</ref>, and for object instance segmentation, such as the "hypercolumn features" in <ref type="bibr" target="#b12">[13]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Architecture: Inside-Outside Net (ION)</head><p>In this section we describe ION <ref type="figure" target="#fig_0">(Figure 1</ref>), a detector with an improved descriptor both inside and outside the ROI. We begin with a brief overview of the entire architecture, followed by specific details. To detect objects, a single deep ConvNet processes an image, and the convolutional feature maps from each layer are stored in memory. At the top of the network, a 2x stacked 4-directional IRNN ( <ref type="figure">Figure 2</ref>, explained later) computes context features that describe the image both globally and locally. The context features have the same dimensions as "conv5." This is done once per image. In addition, we have thousands of proposal regions (ROIs) that might contain objects. For each ROI, we extract a fixed-length feature descriptor from several layers ("conv3", "conv4", "conv5", and "context features"). The descriptors are L2-normalized, concatenated, re-scaled, and dimension-reduced (1x1 convolution) to produce a fixed-length feature descriptor for each proposal of size 512x7x7. Two fully-connected (FC) layers process each descriptor and produce two outputs: a one-of-K object class prediction ("softmax"), and an adjustment to the   <ref type="figure">Figure 2</ref>. Four-directional IRNN architecture. We use "IRNN" units <ref type="bibr" target="#b22">[23]</ref> which are RNNs with ReLU recurrent transitions, initialized to the identity. All transitions to/from the hidden state are computed with 1x1 convolutions, which allows us to compute the recurrence more efficiently (Eq. 1). When computing the context features, the spatial resolution remains the same throughout (same as conv5). The semantic segmentation regularizer has a 16x higher resolution; it is optional and gives a small improvement of around +1 mAP point.</p><p>proposal region's bounding box ("bbox"). The rest of this section explains the details of ION and motivates why we chose this particular architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Pooling from multiple layers</head><p>Recent successful detectors such as Fast R-CNN, Faster R-CNN <ref type="bibr" target="#b31">[32]</ref>, and SPPnet, all pool from the last convolutional layer ("conv5 3") in VGG16 <ref type="bibr" target="#b36">[37]</ref>. In order to extend this to multiple layers, we must consider issues of dimensionality and amplitude.</p><p>Since we know that pre-training on ImageNet is important to achieve state-of-the-art performance <ref type="bibr" target="#b0">[1]</ref>, and we would like to use the previously trained VGG16 network <ref type="bibr" target="#b36">[37]</ref>, it is important to preserve the existing layer shapes. Therefore, if we want to pool out of more layers, the final feature must also be shape 512x7x7 so that it is the correct shape to feed into the first fully-connected layer (fc6). In addition to matching the original shape, we must also match the original activation amplitudes, so that we can feed our feature into fc6.</p><p>To match the required 512x7x7 shape, we concatenate each pooled feature along the channel axis and reduce the dimension with a 1x1 convolution. To match the original amplitudes, we L2 normalize each pooled ROI and re-scale back up by an empirically determined scale. Our experiments use a "scale layer" with a learnable per-channel scale initialized to 1000 (measured on the training set). We later show in Section 5.2 that a fixed scale works just as well.</p><p>As a final note, as more features are concatenated together, we need to correspondingly decrease the initial weight magnitudes of the 1x1 convolution, so we use "Xavier" initialization <ref type="bibr" target="#b37">[38]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Context features with IRNNs</head><p>Our architecture for computing context features in ION is shown in more detail in <ref type="figure">Figure 2</ref>. On top of the last convo-lutional layer (conv5), we place RNNs that move laterally across the image. Traditionally, an RNN moves left-to-right along a sequence, consuming an input at every step, updating its hidden state, and producing an output. We extend this to two dimensions by placing an RNN along each row and along each column of the image. We have four RNNs in total that move in the cardinal directions: right, left, down, up. The RNNs sit on top of conv5 and produce an output with the same shape as conv5.</p><p>There are many possible forms of recurrent neural networks that we could use: gated recurrent units (GRU) <ref type="bibr" target="#b4">[5]</ref>, long short-term memory (LSTM) <ref type="bibr" target="#b14">[15]</ref>, and plain tanh recurrent neural networks. In this paper, we explore RNNs composed of rectified linear units (ReLU). Le et al. <ref type="bibr" target="#b22">[23]</ref> recently showed that these networks are easy to train and are good at modeling long-range dependencies, if the recurrent weight matrix is initialized to the identity matrix. This means that at initialization, gradients are propagated backwards with full strength. Le et al. <ref type="bibr" target="#b22">[23]</ref> call a ReLU RNN initialized this way an "IRNN," and show that it performs almost as well as an LSTM for a real-world language modeling task, and better than an LSTM for a toy memory problem. We adopt this architecture because it is very simple to implement and parallelize, and is much faster than LSTMs or GRUs to compute.</p><p>For our problem, we have four independent IRNNs that move in four directions. To implement the IRNNs as efficiently as possible, we split the internal IRNN computations into separate logical layers. Viewed this way, we can see that the input-to-hidden transition is a 1x1 convolution, and that it can be shared across different directions. Sharing this transition allows us to remove 6 conv layers in total with a negligible effect on accuracy (−0.1 mAP). The bias can be shared in the same way, and merged into the 1x1 conv layer. The IRNN layer now only needs to apply the recurrent matrix and apply the nonlinearity at each step. The output from the IRNN is computed by concatenating the hidden state from the four directions at each spatial location. This is the update for an IRNN that moves to the right; similar equations exist for the other directions:</p><formula xml:id="formula_0">h right i,j ← max W right hh h right i,j−1 + h right i,j , 0 .<label>(1)</label></formula><p>Notice that the input is not explicitly shown in the equation, and there is no input-to-hidden transition. This is because it was computed as part of the 1x1 input-to-hidden convolution, and then copied in-place to each hidden layer.</p><p>For each direction, we can compute all of the independent rows/columns in parallel, stepping all IRNNs together with a single matrix multiply. On a GPU, this results in large speedups compared to computing the RNN cells one by one. We also explore using semantic segmentation labels to regularize the IRNN output. When using these labels, we add the deconvolution and crop layer as implemented by Long et al. <ref type="bibr" target="#b25">[26]</ref>. The deconvolution upsamples by 16x with a 32x32 kernel, and we add an extra softmax loss layer with a weight of 1. This is evaluated in Section 5.3.</p><p>Variants and simplifications. We explore several further simplifications.</p><p>1. We fixed the hidden transition matrix to the identity W right hh = I, which allows us to entirely remove it:</p><formula xml:id="formula_1">h right i,j ← max h right i,j−1 + h right i,j , 0 .<label>(2)</label></formula><p>This is like an accumulator, but with ReLU after each step. In Section 5.5 we show that removing the recurrent matrix has a surprisingly small impact. 2. To prevent overfitting, we include dropout layers (p = 0.25) after each concat layer in all experiments. We later found that in fact the model is underfitting and there is no need for dropout anywhere in the network. 3. Finally, we trained a separate bias b 0 for the first step in the RNN in each direction. However, since it tends to remain near zero after training, this component is not really necessary.</p><p>Interpretation. After the first 4-directional IRNN (out of the two IRNNs), we obtain a feature map that summarizes nearby objects at every position in the image. As illustrated in <ref type="figure" target="#fig_2">Figure 3</ref>, we can see that the first IRNN creates a summary of the features to the left/right/top/bottom of every cell. The subsequent 1x1 convolution then mixes this information together as a dimension reduction. After the second 4-directional IRNN, every cell on the output depends on every cell of the input. In this way, our context features are both global and local. The features vary by spatial position, and each cell is a global summary of the image with respect to that specific spatial location.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results</head><p>We train and evaluate our dataset on three major datasets: PASCAL VOC2007 <ref type="bibr" target="#b6">[7]</ref>, VOC2012, and on MS COCO <ref type="bibr" target="#b23">[24]</ref>. We demonstrate state-of-the-art results on all three datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental setup</head><p>All of our experiments use Fast R-CNN <ref type="bibr" target="#b8">[9]</ref> built on the Caffe <ref type="bibr" target="#b17">[18]</ref> framework, and the VGG16 architecture <ref type="bibr" target="#b36">[37]</ref>, all of which are available online. As is common practice, we use the publicly available weights pre-trained on ILSVRC2012 <ref type="bibr" target="#b32">[33]</ref> downloaded from the Caffe Model Zoo. <ref type="bibr" target="#b0">1</ref> We make some changes to Fast R-CNN, which give a small improvement over the baseline. We use 4 images per mini-batch, implemented as 4 forward/backward passes of single image mini-batches, with gradient accumulation. We sample 128 ROIs per image leading to 512 ROIs per model update. We measure the L2 norm of the parameter gradient vector and rescale it if its norm is above 20 (80 when accumulating over 4 images).</p><p>To accelerate training, we use a two-stage schedule. As noted by Girshick <ref type="bibr" target="#b8">[9]</ref>, it is not necessary to fine-tune all layers, and nearly the same performance can be achieved by fine-tuning starting from conv3 1. With this in mind, we first train for 40k iterations with conv1 1 through conv5 3 frozen, and then another 100k iterations with only conv1 1 through conv2 2 frozen. All other layers are fine-tuned. When training for COCO, we use 80k and 320k iterations respectively. We found that shorter training schedules are not enough to fully converge.</p><p>We also use a different learning rate (LR) schedule. The LR exponentially decays from 5 · 10 −3 to 10 −4 in the first stage, and from 10 −3 to 10 −5 in the second stage. To reduce the effect of random variation, we fix the random seed so that all variants see the same images in the same order. For PASCAL VOC we use the same pre-computed selective search boxes from Fast R-CNN, and for COCO we use the boxes precomputed by Hosang et al. <ref type="bibr" target="#b16">[17]</ref>. Finally, we modified the test thresholds in Fast R-CNN so that we keep only boxes with a softmax score above 0.05, and keep at most 100 boxes per image.  <ref type="table">Table 3</ref>. Detection results on COCO 2015 test-dev. Legend: R: include 2x stacked 4-dir IRNN (context features), W: two rounds of bounding box regression and weighted voting <ref type="bibr" target="#b7">[8]</ref>, D: remove all dropout, +S: train with segmentation labels, SS: SelectiveSearch <ref type="bibr" target="#b39">[40]</ref>, MCG: MCG proposals <ref type="bibr" target="#b29">[30]</ref>, RPN: region proposal net <ref type="bibr" target="#b31">[32]</ref>. † Our submission to the 2015 MS COCO Detection Competition, and post-competition improvements, described in the Supplemental. *These scores are higher than <ref type="bibr" target="#b8">[9]</ref> due to our improved hyperparameters.</p><p>When re-running the baseline Fast R-CNN using the above settings, we see a +0.8 mAP improvement over the original settings on VOC 2007 test. We compare against the baseline using our improved settings where possible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">PASCAL VOC 2007</head><p>As shown in <ref type="table" target="#tab_1">Table 1</ref>, we evaluate our detector (ION) on PASCAL VOC 2007, training on the VOC 2007 trainval dataset merged with the 2012 trainval dataset, a common practice. Applying our method described above, we obtain a mAP of 76.5%. We then make some simple modifications, as described below, to achieve a higher score of 79.2%. <ref type="bibr" target="#b7">[8]</ref> introduces a bounding box regression scheme to improve results on VOC, where bounding boxes are evaluated twice: (1) the initial proposal boxes are evaluated and regressed to improved locations and then (2) the improved locations are passed again through the network. All boxes are accumulated together, and non-max supression is applied. Finally, a weighted vote is computed for each kept box (over all boxes, including those suppressed), where boxes that overlap a kept box by at least 0.5 IoU contribute to the average. For our method, we use the softmax scores as the weights. When adding this scheme to our method, our mAP rises from 76.5% to 78.5%. Finally, we  right-most bar: our best model that achieves 79.2% mAP on VOC 2007 test. Our detector has a particularly large improvement for small objects. See Hoiem <ref type="bibr" target="#b15">[16]</ref> for details on these metrics. observed that our models are underfitting and we remove dropout from all layers to get a further gain up to 79.2%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MR-CNN</head><p>For a direct comparison to MR-CNN, we also trained our method on both SelectiveSearch <ref type="bibr" target="#b39">[40]</ref> and EdgeBoxes <ref type="bibr" target="#b42">[43]</ref> (SS+EB) but not segmentation labels. With these settings, we achieve 79.4% mAP with a runtime of 2s/image, 2 while MR-CNN achieves 78.2% mAP at 30s/image. See <ref type="table" target="#tab_1">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">PASCAL VOC 2012</head><p>We also evaluate on the slightly more challenging VOC 2012 dataset, submitting to the public evaluation server. <ref type="bibr" target="#b2">3</ref> In <ref type="table">Table 2</ref>, we show the top methods on the public leaderboard as of the time of submission. Our detector obtains a mAP of 76.4%, which is several points higher than the next best submission, and is the most accurate for most categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">MS COCO</head><p>Microsoft has recently released the Common Objects in Context dataset, which contains 80k training images ("2014 train") and 40k validation images ("2014 val"). There is an associated MS COCO challenge with a new evaluation metric, that averages mAP over different IoU thresholds, from 0.5 to 0.95 (written as "0.5:0.95"). This places a significantly larger emphasis on localization compared to the PASCAL VOC metric which only requires IoU of 0.5.</p><p>We are only aware of one baseline performance number for this dataset, as published in the Fast R-CNN paper, which cites a mAP of 19.7% on the 2015 test-dev set <ref type="bibr" target="#b8">[9]</ref>.</p><p>We trained our own Fast R-CNN model on "2014 train" using our longer training schedule and obtained a higher mAP of 20.5% mAP on the same set, which we use as a baseline. As shown in <ref type="table">Table 3</ref>, when trained on the same images with the same schedule, our method obtains a large improvement over the baseline with a mAP of 24.9%.</p><p>Applying box voting <ref type="bibr" target="#b7">[8]</ref> gives us a further improvement (+1.6 mAP) on the original PASCAL VOC metric (IoU 0.5), but interestingly decreases performance (-0.3 mAP) on the new COCO metric (IOU 0.5:0.95). We note that this effect is entirely due to the change in metric, which weights localization errors differently. As described in the Supplemental, we fixed this for our competition submission by raising the box voting IoU threshold from 0.5 to ∼ 0.85.</p><p>We submitted ION to the 2015 MS COCO Detection Challenge and won the Best Student Entry with 3 rd place overall. Using only a single model (no ensembling), our submission achieved 31.0% on test-competition score and 31.2% on test-dev score <ref type="table">(Table 3)</ref>. After the competition, we further improved our test-dev score to 33.1% by adding left-right flipping and adjusting training parameters. See the Supplemental for details on our challenge submission.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Improvement for small objects</head><p>In general, small objects are challenging for detectors: there are fewer pixels on the object, they are harder to localize, and there can be many more of them per image. Small objects are even more challenging for proposal methods. For all experiments, we are using selective search <ref type="bibr" target="#b39">[40]</ref> for object proposals, which performs very poorly on small objects in COCO with an average recall under 10% <ref type="bibr" target="#b28">[29]</ref>.</p><p>We find that our detector shows a large relative improvement in this category. For COCO, if we look at small 4 objects, average precision and average recall improve from 4.1% to 7.0% and from 7.3% to 10.7% respectively. We highlight that this is even higher than the baseline proposal method, which is only possible because we perform bounding box regression to predict improved box locations. Similarly, we show a size breakdown for VOC2007 test in <ref type="figure" target="#fig_3">Figure 4</ref> using Hoiem's toolkit for diagnosing errors <ref type="bibr" target="#b15">[16]</ref>, and see similarly large improvements on this dataset as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Design evaluation</head><p>In this section, we explore changes to our architecture and justify our design choices with experiments on PAS- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Pool from which layers?</head><p>As described in Section 3.1, our detector pools regions of interest (ROI) from multiple layers and combines the result. A straightforward approach would be to concatenate the ROI from each layer and reduce the dimensionality using a 1x1 convolution. As shown in <ref type="table">Table 4</ref> (left column), this does not work. In VGG16, the convolutional features at different layers can have very different amplitudes, so that naively combining them leads to unstable learning. While it is possible in theory to learn a model with inputs of very different amplitude, this is ill-conditioned and does not work well in practice. It is necessary to normalize the amplitude such that the features being pooled from all layers have similar magnitude. Our method's normalization scheme fixes this problem, as shown in <ref type="table">Table 4</ref> (right column).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">How should we normalize feature amplitude?</head><p>When performing L2 normalization, there are a few choices to be made: do you sum over channels and perform one normalization per spatial location (as in ParseNet <ref type="bibr" target="#b24">[25]</ref>), or should you sum over all entries in each pooled ROI and normalize it as a single blob. Further, when re-scaling the features back to an fc6-compatible magnitude, should you use a fixed scale or should you learn a scale per channel? The reason why you might want to learn a scale per channel is that you get more sharing than you would if you relied on the 1x1 convolution to model the scale. We evaluate this in <ref type="table" target="#tab_6">Table 5</ref>, and find that all of these approaches perform about the same, and the distinction doesn't matter for this problem. The important aspect is whether amplitude is taken into account; the different schemes we explored in are all roughly equivalent in performance.</p><p>To determine the initial scale, we measure the mean scale of features pooled from conv5 on the training set, and use that as the fixed scale. Using Fast R-CNN, we measured the mean norm to be approximately 1000 when summing over all entries, and 130 when summing across channels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">How much does segmentation loss help?</head><p>Although our target task is object detection, many datasets also have semantic segmentation labels, where the object class of every pixel is labeled. Many images in PAS-CAL VOC and every image in COCO has these labels. This is valuable information that can be incorporated into a training algorithm to improve performance.</p><p>As shown in <ref type="figure">Figure 2</ref>, when adding stacked IRNNs it is possible to have them also predict a semantic segmentation output-a multitask setup. In <ref type="table">Table 6</ref>, we see that these extra labels consistently provide about a +1 point boost in mAP for object detection. This is because we are training the network with more bits of supervision, so even though we are adding extra labels that we do not care about during inference, the features inside the network are trained to contain more information than they would have otherwise if only trained on object detection. Since this is an extra layer used only for training, we can drop the layer at test time and get a +1 mAP point boost with no change in runtime.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">How should we incorporate context?</head><p>While RNNs are a powerful mechanism of incorporating context, they are not the only method. For example, one could simply add more convolutional layers on top of conv5 and then pool out of the top convolutional layer. As shown in <ref type="figure" target="#fig_4">Figure 5</ref>, stacked 3x3 convolutions add two cells worth of context, and stacked 5x5 convolutions add 4 cells. Alternatively, one could use a global average and unpool (tile or repeat spatially) back to the original shape as in ParseNet <ref type="bibr" target="#b24">[25]</ref>.</p><p>We compared these approaches on VOC 2007 test, shown in <ref type="table">Table 7</ref>. The 2x stacked 4-dir IRNN layers have fewer parameters than the alternatives, and perform better on the test set (both with and without segmentation labels). Therefore, we use this architecture to compute "context features" for all other experiments. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Context method</head><p>Seg. mAP  <ref type="table">Table 7</ref>. Comparing approaches to adding context. All rows also pool out of conv3, conv4, and conv5. Metric: detection mAP on VOC07 test. Seg: if checked, the top layer received extra supervision from semantic segmentation labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Which IRNN architecture?</head><p>When designing the IRNN for incorporating context, there are a few basic decisions to be made, namely how many layers and how many hidden units per layer. In addition, we explore the idea of entirely removing the recurrent transition (equivalent to replacing it with the identity matrix), so that the IRNN consist of repeated steps of: accumulate, ReLU, accumulate, etc. Note that this is not the same as an integral/area image, since each step has ReLU.</p><p>As shown in <ref type="table">Table 8</ref>, the number of hidden units does not have a strong effect on the performance <ref type="table">(Table 8)</ref>, so we choose 512 as the baseline size for all other experiments. In the Supplemental, we include additional experiments showing that 2 IRNN layers is optimal on VOC 2007 test. While stacking more convolution layers tends to make ConvNets perform better, the same is not always true for RNNs <ref type="bibr" target="#b18">[19]</ref>.</p><p>Finally, we were surprised to discover that removing the recurrent W hh transition performs almost as well as learning it <ref type="table">(Table 8</ref>). It seems that the input-to-hidden and hidden-to-output connections contain sufficient context that  <ref type="table">Table 8</ref>. Varying the hidden transition. We vary the number of units and try either learning recurrent transition W hh initialized to the identity, or entirely removing it (same as setting W hh = I).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Variation mAP</head><p>Our method 76.5 (a) Left-right then up-down 76.5 (b) Pool out of both IRNNs 75.9 (c) Combine 2x stacked 512x3x3 conv and IRNN 76.5 <ref type="table">Table 9</ref>. Other variations. Metric: VOC07 test mAP. We list some other variations that all perform about the same. the recurrent transition can be removed and replaced with an addition, saving a large matrix multiply.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.">Other variations</head><p>There are some other variations on our architecture that perform almost as well, which we summarize in <ref type="table">Table 9</ref>. For example, (a) the first IRNN only processes two directions left/right and the second IRNN only processes up/down. This kind of operation was explored in ReNet <ref type="bibr" target="#b41">[42]</ref> and performs the same as modeling all four directions in both IRNN layers. We also explored (b) pooling out of both IRNNs, and (c) pooling out of both stacked convolutions and the IRNNs. None of these variations perform better than our main method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>This paper introduces the Inside-Outside Net (ION), an architecture that leverages context and multi-scale knowledge for object detection. Our architecture uses a 2x stacked 4-directional IRNN for context, and multi-layer ROI pooling with normalization for improved object description. To justify our design choices, we conducted extensive experiments evaluating choices like the number of layers combined, using segmentation loss, normalizing feature amplitudes, different IRNN architectures, and other variations. We achieve state-of-the-art results on both PASCAL VOC and COCO, and find our proposed architecture is particularly effective at improving detection of small objects.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Inside-Outside Net (ION)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Interpretation of the first IRNN output. Each cell in the output summarizes the features to the left/right/top/bottom.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>VOC 2007 normalized AP by size. Left to right: increasing complexity. Left-most bar in each group: Fast R-CNN;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Receptive field of different layer types. When considering a single cell in the input, what output cells depend on it? (a) If we add two stacked 3x3 convolutions on top of conv5, then a cell in the input influences a 5x5 window in the output. (b) Similarly, for a 5x5 convolution, one cell influences a 9x9 window in the output. (c) For global average pooling, every cell in the output depends on the entire input, but the output is the same value repeated. (d) For IRNNs, every cell in the output depends on the entire input, but also varies spatially.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>and C. L. Zitnick are now at Facebook AI Research.</figDesc><table>scale 
1x1 
conv 

fc 
fc 

softmax 

bbox 

For each ROI 

fc 

fc 
L2 normalize 

concat 

ROI Pooling 

conv2 
conv1 
conv5 
conv4 
conv3 

4-dir 
IRNN 

4-dir 
IRNN 

context 
features 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>1 .</head><label>1</label><figDesc>We introduce the ION architecture that leverages context and multi-scale skip pooling for object detection. 2. We achieve state-of-the-art results on PASCAL VOC 2007, with a mAP of 80.1%, VOC 2012, with a mAP of 77.9%, and on COCO, with a mAP of 33.1%. 3. We conduct extensive experiments evaluating choices like the number of layers combined, using a segmentation loss, normalizing feature amplitudes, different RNN architectures, and other variations. 4. We analyze ION's performance and find improved accuracy across the board, especially for small objects.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head></head><label></label><figDesc>Method Boxes R W D TrainTime mAP aero bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tv Table 2. Detection results on VOC 2012 test. Legend: 07+12: 07 trainval + 12 trainval, 07++12: 07 trainvaltest + 12 trainval, 07+12+S: 07+12 plus SBD segmentation labels<ref type="bibr" target="#b11">[12]</ref>, R: include 2x stacked 4-dir IRNN (context features), W: two rounds of bounding box regression and weighted voting<ref type="bibr" target="#b7">[8]</ref>, D: remove all dropout, SS: SelectiveSearch<ref type="bibr" target="#b39">[40]</ref>, EB: EdgeBoxes<ref type="bibr" target="#b42">[43]</ref>, RPN: region proposal network<ref type="bibr" target="#b31">[32]</ref>.</figDesc><table>FRCN [9] SS 
07+12 
0.3s 
70.0 77.0 78.1 69.3 59.4 38.3 81.6 78.6 86.7 42.8 78.8 68.9 84.7 82.0 76.6 69.9 31.8 70.1 74.8 80.4 70.4 
Faster [32] RPN 
07+12 
0.2s 
73.2 76.5 79.0 70.9 65.5 52.1 83.1 84.7 86.4 52.0 81.9 65.7 84.8 84.6 77.5 76.7 38.8 73.6 73.9 83.0 72.6 
MR-CNN [8] SS+EB 
07+12 
30s 
78.2 80.3 84.1 78.5 70.8 68.5 88.0 85.9 87.8 60.3 85.2 73.7 87.2 86.5 85.0 76.4 48.5 76.3 75.5 85.0 81.0 

ION [ours] SS 
07+12 
0.8s 
74.6 78.2 79.1 76.8 61.5 54.7 81.9 84.3 88.3 53.1 78.3 71.6 85.9 84.8 81.6 74.3 45.6 75.3 72.1 82.6 81.4 
ION [ours] SS 
07+12 
0.8s 
75.6 79.2 83.1 77.6 65.6 54.9 85.4 85.1 87.0 54.4 80.6 73.8 85.3 82.2 82.2 74.4 47.1 75.8 72.7 84.2 80.4 
ION [ours] SS 
07+12 
1.2s 
77.6 79.7 83.4 78.1 65.7 62.0 86.5 85.8 88.8 60.2 83.4 75.1 86.5 87.3 82.1 79.7 48.3 77.0 75.3 85.3 82.4 
ION [ours] SS+EB 
07+12 
2.0s 
79.4 82.5 86.2 79.9 71.3 67.2 88.6 87.5 88.7 60.8 84.7 72.3 87.6 87.7 83.6 82.1 53.8 81.9 74.9 85.8 81.2 

ION [ours] SS 
07+12+S 0.8s 
76.5 79.2 79.2 77.4 69.8 55.7 85.2 84.2 89.8 57.5 78.5 73.8 87.8 85.9 81.3 75.3 49.7 76.9 74.6 85.2 82.1 
ION [ours] SS 
07+12+S 1.2s 
78.5 80.2 84.7 78.8 72.4 61.9 86.2 86.7 89.5 59.1 84.1 74.7 88.9 86.9 81.3 80.0 50.9 80.4 74.1 86.6 83.3 
ION [ours] SS 
07+12+S 1.2s 
79.2 80.2 85.2 78.8 70.9 62.6 86.6 86.9 89.8 61.7 86.9 76.5 88.4 87.5 83.4 80.5 52.4 78.1 77.2 86.9 83.5 
ION [ours] SS+EB 
07+12+S 2.0s 
80.1 84.2 87.2 82.1 74.8 67.1 85.0 88.0 89.3 60.4 86.1 76.3 88.7 86.3 83.5 82.2 55.5 80.5 75.3 86.5 83.3 

Table 1. Detection results on VOC 2007 test. Legend: 07+12: 07 trainval + 12 trainval, 07+12+S: 07+12 plus SBD segmentation 
labels [12], R: include 2x stacked 4-dir IRNN (context features), W: two rounds of box regression and weighted voting [8], D: remove all 
dropout, SS: SelectiveSearch [40], EB: EdgeBoxes [43], RPN: region proposal net. [32], Time: per image, excluding proposal generation. 

Method 
Boxes R W D Train 
mAP aero bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tv 

FRCN [9] 
SS 
07++12 
68.4 82.3 78.4 70.8 52.3 38.7 77.8 71.6 89.3 44.2 73.0 55.0 87.5 80.5 80.8 72.0 35.1 68.3 65.7 80.4 64.2 
Faster [32] 
RPN 
07++12 
70.4 84.9 79.8 74.3 53.9 49.8 77.5 75.9 88.5 45.6 77.1 55.3 86.9 81.7 80.9 79.6 40.1 72.6 60.9 81.2 61.5 
FRCN+YOLO [31] SS 
07++12 
70.4 83.0 78.5 73.7 55.8 43.1 78.3 73.0 89.2 49.1 74.3 56.6 87.2 80.5 80.5 74.7 42.1 70.8 68.3 81.5 67.0 
HyperNet [21] RPN 
07++12 
71.4 84.2 78.5 73.6 55.6 53.7 78.7 79.8 87.7 49.6 74.9 52.1 86.0 81.7 83.3 81.8 48.6 73.5 59.4 79.9 65.7 
MR-CNN [8] 
SS+EB 
07+12 
73.9 85.5 82.9 76.6 57.8 62.7 79.4 77.2 86.6 55.0 79.1 62.2 87.0 83.4 84.7 78.9 45.3 73.4 65.8 80.3 74.0 

ION [ours] 
SS 
07+12 
74.7 86.9 84.5 75.2 58.2 57.7 80.5 78.3 90.4 54.4 79.9 60.5 88.4 83.0 83.0 81.2 50.7 77.3 67.6 83.5 72.3 
ION [ours] 
SS+EB 
07+12 
76.4 88.0 84.6 77.7 63.7 63.6 80.8 80.8 90.9 55.5 81.9 60.9 89.1 84.9 84.2 83.9 53.2 79.8 67.4 84.4 72.9 

ION [ours] 
SS 
07+12+S 76.4 87.5 84.7 76.8 63.8 58.3 82.6 79.0 90.9 57.8 82.0 64.7 88.9 86.5 84.7 82.3 51.4 78.2 69.2 85.2 73.5 
ION [ours] 
SS+EB 
07+12+S 77.9 88.3 85.7 80.5 67.2 63.6 82.5 82.0 91.4 58.2 84.1 65.3 90.1 87.3 85.0 84.4 53.6 80.7 69.1 84.6 74.7 

Method 
Boxes 
RW D 
Train 
Avg. Precision, IoU: 
Avg. Precision, Area: 
Avg. Recall, # Dets: 
Avg. Recall, Area: 

0.5:0.95 

0.50 0.75 
Small 
Med. 
Large 
1 
10 
100 Small 
Med. 
Large 

FRCN [9]* SS 
train 
20.5 39.9 19.4 
4.1 
20.0 
35.8 
21.3 29.5 
30.1 
7.3 
32.1 
52.0 
FRCN [9]* SS 
train 
20.0 40.3 18.1 
4.1 
19.6 
34.5 
20.8 29.1 
29.8 
7.4 
31.9 
50.9 

ION [ours] SS 
train 
23.0 42.0 23.0 
6.0 
23.8 
37.3 
23.0 32.4 
33.0 
9.7 
37.0 
53.5 
ION [ours] SS 
train 
23.6 43.2 23.6 
6.4 
24.1 
38.3 
23.2 32.7 
33.5 
10.1 
37.7 
53.6 
ION [ours] SS 
train+S 
24.9 44.7 25.3 
7.0 
26.1 
40.1 
23.9 33.5 
34.1 
10.7 
38.8 
54.1 
ION [ours] SS 
train+S 
24.6 46.3 23.3 
7.4 
26.2 
38.8 
23.7 33.9 
34.6 
11.7 
40.0 
53.8 

ION comp.  † MCG+RPN 

trainval35k+S 

31.2 53.4 32.3 
12.8 
32.9 
45.2 
27.8 43.1 
45.6 
23.6 
50.0 
63.2 
ION post.  † MCG+RPN 

trainval35k+S 

33.1 55.7 34.6 
14.5 
35.2 
47.2 
28.9 44.8 
47.4 
25.5 
52.4 
64.3 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head></head><label></label><figDesc>CAL VOC 2007. All numbers in this section are VOC 2007 test mAP, trained on 2007 trainval + 2012 trainval, with the settings described in Section 4.1. Note that for this section, we use dropout in all networks, and a single round of bounding box regression at test time.</figDesc><table>ROI pooling from: Merge features using: 
C2 C3 C4 C5 
1x1 L2+Scale+1x1 

*70.8 
71.5 
69.7 
74.4 
63.6 
74.6 
59.3 
74.6 

Table 4. Combining features from different layers. Metric: De-
tection mAP on VOC07 test. Training set: 07 trainval + 12 train-
val. 1x1: combine features from different layers using a 1x1 con-
volution. L2+Scale+1x1: use L2 normalization, scaling (initial-
ized to 1000), and 1x1 convolution, as described in section 3.1. 
These results do not include "context features." *This entry is the 
same as Fast R-CNN [9], but trained with our hyperparameters. 

L2 Normalization method Seg. 
Scale: 
Learned Fixed 

Sum across channels 
76.4 
76.2 
Sum over all entries 
76.5 
76.6 

Table 5. Approaches to normalizing feature amplitude. Metric: 
VOC07 test mAP. All items regularized with segmentation labels. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="true"><head>Table 5</head><label>5</label><figDesc></figDesc><table>ROI pooling from: 
Use seg. loss? 
C2 C3 C4 C5 IRNN 
No 
Yes 

69.9 
70.6 
73.9 
74.2 
75.1 
76.2 
75.6 
76.5 
74.9 
76.8 

Table 6. Effect of segmentation loss. Metric: detection mAP on 
VOC07 test. Adding segmentation loss tends to improve detection 
performance by about 1 mAP, with no test-time penalty. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head></head><label></label><figDesc>ROI pooling from: Seg. # units Include W hh ? C3 C4 C5 IRNN Yes No</figDesc><table>128 
76.4 
75.5 
256 
76.5 
75.3 
512 
76.5 
76.1 
1024 
76.2 
76.4 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/BVLC/caffe/wiki/Model-Zoo</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">On a single Titan X GPU, excluding proposal generation 3 Anonymous URL: http://host.robots.ox.ac.uk:8080/anonymous/B3VFLE.html</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">"Small" means area ≤ 32 2 px; about 40% of COCO is "small."</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements This project started as a Microsoft Research (MSR) internship, and was later supported by a NSERC PGS-D scholarship. We thank NVIDIA for the donation of K40 GPUs, and Abhinav Shrivastava and Ishan Misra for helpful discussions while at MSR.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Analyzing the performance of multilayer neural networks for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Hierarchical Neural Networks for Image Interpretation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Behnke</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
		<respStmt>
			<orgName>Freie Universität Berlin</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Byeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Breuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Raue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liwicki</surname></persName>
		</author>
		<title level="m">Scene labeling with LSTM recurrent neural networks. In CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Return of the devil in the details: Delving deep into convolutional nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chatfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">On the properties of neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1259</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">Encoderdecoder approaches. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">An empirical study of context in object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">The PASCAL Visual Object Classes (VOC) Challenge. IJCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Object detection via a multiregion &amp; semantic segmentation-aware CNN model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Offline handwriting recognition with multidimensional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Semantic contours from inverse detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Hypercolumns for object segmentation and fine-grained localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Diagnosing error in object detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chodpathumwan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">What makes for effective detection proposals?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Hosang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.05082</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<title level="m">Caffe: Convolutional architecture for fast feature embedding</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-F</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.02078</idno>
		<title level="m">Visualizing and understanding recurrent networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Representation of local geometry in the visual system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Koenderink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Van Doorn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bio. cybernetics</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">HyperNet: towards accurate region proposal generation and joint object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">A simple way to initialize recurrent networks of rectified linear units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.00941</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1405.0312</idno>
		<title level="m">Microsoft COCO: common objects in context. arXiv e-prints</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note>cs.CV</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">ParseNet: Looking wider to see better</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.04579</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv e-prints</note>
	<note>cs.CV</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Modeling the shape of the scene: a holistic representation of the spatial envelope</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCV</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Exploring tiny images: The roles of appearance and contextual information for machine and human object recognition. PAMI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning to segment object candidates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">O</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Multiscale combinatorial grouping for image segmentation and object proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.00848</idno>
		<imprint>
			<date type="published" when="2015-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.02640</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0575</idno>
		<title level="m">ImageNet Large Scale Visual Recognition Challenge</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Paliwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Pedestrian detection with unsupervised multi-stage feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">U. the difficulty of training deep feedforward neural networks. Xavier glorot and yoshua bengio</title>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Contextual priming for object detection. IJCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Selective search for object recognition. IJCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smeulders</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Original approach for the localisation of objects in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vaillant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Monrocq</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEE Proc. on Vision, Image, and Signal Processing</title>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Visin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kastner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Matteucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.00393</idno>
		<title level="m">ReNet: A recurrent neural network based alternative to convolutional networks. arXiv e-prints</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note>cs.CV</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Edge boxes: Locating object proposals from edges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
