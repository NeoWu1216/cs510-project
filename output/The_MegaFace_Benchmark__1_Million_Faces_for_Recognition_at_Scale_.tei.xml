<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">The MegaFace Benchmark: 1 Million Faces for Recognition at Scale</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ira</forename><surname>Kemelmacher</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shlizerman</forename><surname>Steven</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Seitz</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Miller</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Brossard</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">The MegaFace Benchmark: 1 Million Faces for Recognition at Scale</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:47+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>. We use two different probe sets (a) FaceScrub-photos of celebrities, (b) FGNET-photos with a large variation in age per person. We present rank-1 identification of state of the art algorithms that participated in our challenge. On the left side of each plot is current major benchmark LFW scale (i.e., 10 distractors, see how all the top algorithms are clustered above 95%). On the right is mega-scale (with a million distractors). Observe that rates drop with increasing numbers of distractors, even though the probe set is fixed, and that algorithms trained on larger sets (dashed lines) generally perform better. Participate at: http://megaface.cs.washington.edu.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Face recognition has seen major breakthroughs in the last couple of years, with new results by multiple groups <ref type="bibr" target="#b25">[25,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b27">27]</ref> surpassing human performance on the leading Labeled Faces in the Wild (LFW) benchmark <ref type="bibr" target="#b15">[15]</ref> and achieving near perfect results.</p><p>Is face recognition solved? Many applications require accurate identification at planetary scale, i.e., finding the best matching face in a database of billions of people. This is truly like finding a needle in a haystack. Face recognition algorithms did not deliver when the police were searching for the suspect of the Boston marathon bombing <ref type="bibr" target="#b17">[17]</ref>. Similarly, do you believe that current cell-phone face unlocking programs will protect you against anyone on the planet who might find your lost phone? These and other face recognition applications require finding the true positive match(es) with negligible false positives. They also require training and testing on datasets that contain vast numbers of different people.</p><p>In this paper, we introduce the MegaFace dataset and benchmark to evaluate and encourage development of face recognition algorithms at scale. The goal of MegaFace is to evaluate the performance of current face recognition algorithms with up to a million distractors, i.e., up to a million people who are not in the test set. Our key objectives for assembling the dataset are that 1) it should contain a million photos "in the wild", i.e., with unconstrained pose, expression, lighting, and exposure, 2) be broad rather than deep, i.e., contain many different people rather than many photos of a small number of people, and most importantly 3) it will be publicly available, to enable benchmarking and distribution within the research community.</p><p>While recent face datasets have leveraged celebrity photos crawled from the web, such datasets have been limited to a few thousand unique individuals; it is challenging to find a million or more unique celebrities. Instead, we leverage Yahoo's recently released database of Flickr photos <ref type="bibr" target="#b31">[31]</ref>. The Yahoo dataset includes 100M creative commons photographs and hence can be released for research. While these photos are unconstrained and do not target face recognition research per se, they capture a large number of faces. Our algorithm samples the Flickr set searching for faces while optimizing for large number of unique people via analysis of Flickr user IDs and group photos. MegaFace includes 1 Million photos of more than 690,000 unique subjects.</p><p>The MegaFace challenge evaluates how face recognition algorithms perform with a very large number of "distractors," i.e., individuals that are not in the probe set. MegaFace is used as the gallery; the two probe sets we use are FaceScrub <ref type="bibr" target="#b22">[22]</ref> and FG-NET <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b16">16]</ref>. We address fundamental questions and introduce the following key findings ( <ref type="figure" target="#fig_0">Fig. 1</ref>):</p><p>• How well do current face recognition algorithms scale? Algorithms that achieve above 95% performance on LFW (equivalent of 10 distractors in our plots), achieve <ref type="bibr" target="#b35">35</ref> found that the performance with 10 distractors for FGNET as a probe set is lower than for FaceScrub, and the drop off spread is much bigger ( <ref type="figure" target="#fig_0">Fig. 1 (b)</ref>) . A deeper analysis also reveals that children (below age 20) are more challenging to recognize than adults, possibly due to training data availability, and that larger gaps in age (between gallery and probe) are similarly more challenging to recognize. These observations become evident by analyzing at large scale.</p><p>• How does pose affect recognition performance? Recognition drops for larger variation in pose between matching probe and gallery, and the effect is much more significant at scale.</p><p>In the following sections we describe how the MegaFace database was created, explain the challenge, and describe the outcomes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Benchmarks</head><p>Early work in face recognition focused on controlled datasets where subsets of lighting, pose, or facial expression were kept fixed, e.g., <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b11">11]</ref>. With the advance of algorithms, the focus moved to unconstrained scenarios with a number of important benchmarks appearing, e.g., FRGC, Caltech Faces, and many more (see <ref type="bibr" target="#b15">[15]</ref>, <ref type="figure" target="#fig_1">Fig. 3</ref>, for a list of all the datasets), and thorough evaluations <ref type="bibr" target="#b13">[13,</ref><ref type="bibr" target="#b37">37]</ref>. A big challenge, however, was to collect photos of large number of individuals.</p><p>Large scale evaluations were previously performed on controlled datasets (visa photographs, mugshots, lab captured photos) by NIST <ref type="bibr" target="#b13">[13]</ref>, and report recognition results of 90% on 1.6 million people. However, these results are not representative of photos in the wild.</p><p>In 2007, Huang et al. <ref type="bibr" target="#b15">[15]</ref> created the benchmark Labeled Faces in the Wild (LFW). The LFW database includes 13K photos of 5K different people. It was collected by running Viola-Jones face detection <ref type="bibr" target="#b32">[32]</ref> on Yahoo News photos. LFW captures celebrities photographed under unconstrained conditions (arbitrary lighting, pose, and expression) and it has been an amazing resource for the face analysis community (more than 1K citations). Since 2007, a number of databases appeared that include larger numbers of photos per person (LFW has 1620 people with more than 2 photos), video information, and even 3D information, e.g., <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b6">6,</ref><ref type="bibr" target="#b22">22]</ref>. However, LFW remains the leading benchmark on which all state of the art recognition methods are evaluated and compared. Indeed, just in the last year a number of methods (11 methods at the time of writing this paper), e.g., <ref type="bibr" target="#b25">[25,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b30">30]</ref> reported recognition rates above 99%+ <ref type="bibr" target="#b14">[14]</ref> (better than human recognition rates estimated on the same dataset by <ref type="bibr" target="#b19">[19]</ref>). The perfect recognition rate on LFW is 99.9% (it is not 100% since there are 5 pairs of photos that are mislabeled), and current top performer reports 99.77%. Recently, IJB-A dataset was released, it includes a large variation within an individual's photos, however is not large scale (26K photos total).  <ref type="figure">Figure 2</ref>. Representative sample of recent face recognition datasets (in addition to LFW). Current public datasets include up to 10K unique people, and a total of 500K photos. Several companies have access to orders of magnitude more photos and subjects, these however are subject to privacy constraints and are not public. MegaFace (this paper) includes 1M photos of more than 690K unique subjects, collected from Flickr (from creative commons photos), and is available publicly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Datasets</head><p>While, some companies have access to massive photo collections, e.g., Google in <ref type="bibr" target="#b25">[25]</ref> trained on 200 Million photos of 8 Million people (and more recently on 500M of 10M), these datasets are not available to the public and were used only for training and not testing.</p><p>The largest public data set is CASIA-WebFace <ref type="bibr" target="#b36">[36]</ref> that includes 500K photos of 10K celebrities, crawled from the web. While CASIA is a great resource, it contains only 10K individuals, and does not have an associated benchmark (i.e., it's used for training not testing).</p><p>Ortiz et al. <ref type="bibr" target="#b23">[23]</ref> experimented with large scale identification from Facebook photos assuming there is more than one gallery photo per person. Similarly Stone et al. <ref type="bibr" target="#b26">[26]</ref> show that social network's context improves large scale face recognition. Parkhi et al. <ref type="bibr" target="#b24">[24]</ref> assembled a dataset of 2.6 Million of 2600 people, and used it for training (testing was done on the smaller scale LFW and YouTube Faces <ref type="bibr" target="#b34">[34]</ref>). Wang et al. <ref type="bibr" target="#b33">[33]</ref> propose a hierarchical approach on top of commercial recognizer to enable fast search in a dataset of 80 million faces. Finally, <ref type="bibr" target="#b4">[4]</ref> experimented with a million distractors. Unfortunately, however, none of these efforts have produced publicly available datasets or public benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Related Studies</head><p>Age-invariant recognition is an important problem that has been studied in the literature, e.g., <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b20">20]</ref>. FG-NET <ref type="bibr" target="#b7">[7]</ref> includes 975 photos of 82 people, each with several photos spanning many ages. More recently, Chen et al. <ref type="bibr" target="#b5">[5]</ref> created a dataset of 160k photos of 2k celebrities across many ages, and Eidinger et al. <ref type="bibr" target="#b9">[9]</ref> created a dataset of 27K photos of 2.3K Flickr user to facilitate age and gender recognition. Since most modern face recognition algorithms have not been evaluated for age-invariance we rectify this by including an FG-NET test (augmented with a million distractors) in our benchmark. In the future, datasets like <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b9">9]</ref> can be easily incorporated into our benchmark.</p><p>Other recent studies have considered both identification as well as verification results on LFW <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b27">27]</ref>. Finally, Best-Rowden et al. <ref type="bibr" target="#b1">[2]</ref> performed an interesting Mechanical Turk study to evaluate human recognition rates on LFW and YouTube Faces datasets. They report that humans are better than computers when recognizing from videos due to additional cues, e.g., temporal information, familiarity with the subject (celebrity), workers' country of origin (USA vs. others), and also discovered errors in labeling of YouTube Faces via crowdsourcing. In the future, we will use this study's useful conclusions to help annotate MegaFace and create a training set in addition to the currently provided distractor set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Assembling MegaFace</head><p>In this section, we provide an overview of the MegaFace dataset, how it was assembled, and its statistics. We created MegaFace to evaluate and drive the development of face recognition algorithms that work at scale. As motivated in Section 1, we sought to create a public dataset, free of licensing restrictions, that captures photos taken with unconstrained imaging conditions, and with close to a million unique identities. After exploring a number of avenues for data collection, we decided to leverage Yahoo's 100M Flickr set <ref type="bibr" target="#b31">[31]</ref>. Yahoo's set was not created with face analysis in mind, however, it includes a very large number of faces and satisfies our requirements.</p><p>Optimizing for large number of unique identities. Our strategy for maximizing the number of unique identities is based on two techniques: 1) drawing photos from many different Flickr users-there are 500K unique user IDsand 2) assuming that two or more faces appear in the same photo, they are likely different identities. Note that these assumptions do not need to be infallible, as our goal is to produce a very diverse distractor set-it is not a problem if we have a small number of photos of the same person. Our algorithm for detecting and downloading faces is as follows.</p><p>We generated a list of images and user IDs in a round-robin fashion, by going through each of the 500K users and selecting the first photo with a face larger than 50 × 50 and adding it to the dataset. If the photo contains multiple faces above that resolution, we add them all, given that they are different people with high probability. We then repeated this process (choosing the second, then the third, etc. photo from each user), until a sufficient number of faces were assembled. Based on our experiments face detection can have up to 20% false positive rate. Therefore, to ensure that our final set includes a million faces, the process was terminated once 1, 296, 079 faces were downloaded. Once face detection was done, we ran additional stricter detection, and removed blurry faces. We assembled a total of 690, 572 faces in this manner that have a high probability of being unique individuals. While not guaranteed, the remaining 310K in our dataset likely also contain additional unique identities. <ref type="figure" target="#fig_1">Figure 3</ref> presents a histogram of number of faces per photo.</p><p>Face processing. We downloaded the highest resolution available per photo. The faces are detected using the Head-Hunter 2 algorithm by Mathias et al. <ref type="bibr" target="#b21">[21]</ref>, which reported state of the art results in face detection, and is especially robust to a wide range of head poses including profiles. We crop detected faces such that the face spans 50% of the photo height, thus including the full head <ref type="figure" target="#fig_1">(Fig. 3)</ref>. We further estimate 49 fiducial points and yaw and pitch angles, as computed by the IntraFace 3 landmark model <ref type="bibr" target="#b35">[35]</ref>.</p><p>Dataset statistics. <ref type="figure" target="#fig_1">Figure 3</ref> presents MegaFace's statistics:</p><p>• Representative photographs and bounding boxes. Observe that the photographs contain people from different countries, gender, variety of poses, glasses/no glasses, and many more variations.</p><p>detection/ 3 http://www.humansensing.cs.cmu.edu/intraface/ • Distribution of Flickr tags that accompanied the downloaded photos. Tags range from 'instagram' to 'wedding,' suggesting a range of photos from selfies to high quality portraits (prominence of '2013' likely due to timing of when the Flickr dataset was released). • GPS locations demonstrate photos taken all over the world. • Camera types dominated by DSLRs (over mobile phones), perhaps correlated with creative commons publishers, as well as our preference for higher resolution faces. • 3D pose information: more than 197K of the faces have yaw angles larger than ±40 degrees. Typically unconstrained face datasets include yaw angles of less than ±30 degrees. • Number of faces per photo, to indicate the number of group photos. • Face resolution: more than 50% (514K) of the photos in MegaFace have resolution more than 40 pixels interocular distance (40 IOD corresponds to 100x100 face size, the resolution in LFW). We believe that this dataset is extremely useful for a variety of research areas in recognition and face modeling, and we plan to maintain and expand it in the future. In the next section, we describe the MegaFace challenge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">The MegaFace Challenge</head><p>In this section, we describe the challenge and evaluation protocols. Our goal is to test performance of face recognition algorithms with up to a million distractors, i.e., faces of unknown people. In each test, a probe image is compared against a gallery of up to a million faces drawn from the Megaface dataset.</p><p>Recognition scenarios The first scenario is identification: given a probe photo, and a gallery containing at least one photo of the same person, the algorithm rank-orders all photos in the gallery based on similarity to the probe. Specifically, the probe set includes N people; for each person we have M photos. We then test each of the M photos (denote by i) per person by adding it the gallery of distractors and use each of the other M − 1 photos as a probe. Results are presented with Cumulative Match Characteristics (CMC) curves-the probability that a correct gallery image will be chosen for a random probe by rank = K.</p><p>The second scenario is verification, i.e., a pair of photos is given and the algorithm should output whether the person in the two photos is the same or not. To evaluate verification we computed all pairs between the probe dataset and the Megaface distractor dataset. Our verification experiment has in total 4 billion negative pairs. We report verification results with ROC curves; this explores the trade off between falsely accepting non-match pairs and falsely rejecting match pairs.</p><p>Until now, verification received most of the focus in face recognition research since it was tested by the LFW benchmark <ref type="bibr" target="#b15">[15]</ref>. Recently, a number of groups, e.g., <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b27">27]</ref> also performed identification experiments on LFW. The relation between the identification and verification protocols was studied by Grother and Phillips <ref type="bibr" target="#b12">[12]</ref> and DeCann and Ross <ref type="bibr" target="#b8">[8]</ref>. In our challenge, we evaluate both scenarios with an emphasis on very large number of distractors. For comparison, testing identification on LFW is equivalent to 10 distractors in our challenge.</p><p>Probe set. MegaFace is used to create a gallery with a large number of distractors. For the probe set (testing known identities), we use two sets:</p><p>1. The FaceScrub dataset <ref type="bibr" target="#b22">[22]</ref>, which includes 100K photos of 530 celebrities, is available online. Face-Scrub has a similar number of male and female photos (55,742 photos of 265 males and 52,076 photos of 265 females) and a large variation across photos of the same individual which reduces possible bias, e.g., due to backgrounds and hair style <ref type="bibr" target="#b19">[19]</ref>, that may occur in LFW. For efficiency, the evaluation was done on a subset of FaceScrub which includes 80 identities (40 females and 40 males) by randomly selecting from a set of people that had more than 50 images each (from which 50 random photos per person were used).</p><p>2. The FG-NET aging dataset <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b16">16]</ref>: it includes 975 photos of 82 people. For some of the people the age range in photos is more than 40 years.</p><p>Evaluation and Baselines. Challenge participants were asked to calculate their features on MegaFace, full Face-Scrub, and FGNET. We provided code that runs identification and verification on the FaceScrub set. After the results were submitted by all groups we re-ran the experiments with FaceScrub and 3 different random distractor sets per gallery size. We further ran the FGNET experiments on all methods 4 and each of the three random MegaFace subsets per gallery size. The metric for comparison is L 2 distance. Participants were asked not to train on FaceScrub or FGNET. As a baseline, we implemented two simple recognition algorithms: 1) comparison by LBP <ref type="bibr" target="#b0">[1]</ref> features-it achieves 70% recognition rates on LFW, and uses no training, 2) a Joint Bayesian (JB) approach represents each face as the sum of two Gaussian variables x = µ + ǫ where µ is identity and ǫ is inter-personal variation. To determine whether two faces, x 1 and x 2 belong to the same identity, we calculate P (x 1 , x 2 |H 1 ) and P (x 1 , x 2 |H 2 ) where H 1 is the hypothesis that the two faces are the same and H 2 is the hypothesis that the two faces are different. These distributions can also be written as normal distributions, which allows for efficient inference via a log-likelihood test. JB algorithm was trained on the CASIA-WebFace dataset <ref type="bibr" target="#b36">[36]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results</head><p>This section describes the results and analysis of the challenge. Our challenge was released on Sep 30, 2015. Groups were given three weeks to finish their evaluations. More than 100 groups registered to participate. We present results from 5 groups that uploaded all their features by the deadline. We keep maintaining the challenge and datacurrently 20 more groups are working on their submissions.</p><p>Participating algorithms In addition to baseline algorithms LBP, and Joint Bayes, we present results of the following methods (some provided more than 1 model):</p><p>1. Google's FaceNet: achieves 99.6% on LFW, was trained on more than 500M photos of 10M people (newer version of <ref type="bibr" target="#b25">[25]</ref>  <ref type="figure">Figure 4</ref> summarizes the models, training sizes (240K-500M photos, 5K-10M people) and availability of the training data. Below we describe all the experiments and key conclusions.    <ref type="figure">Figure 6</ref>. Rank-1 identification results (in%) with 1M distractors on the two probe sets.</p><p>Verification results. <ref type="figure" target="#fig_2">Fig. 5</ref> shows results of the verification experiment for our two probe sets, (a) and (b) show results on FaceScrub and (c) and (d) on FGNET. We present results of one random fixed set of distractors per gallery size (see the other two in the supplementary).</p><p>We see that, for FaceScrub, at lower false accept rates the performance of algorithms drops by about 40% on average. FaceNet and FaceN lead with only about 15%. Interestingly, FaceN that was trained on 18M photos is able to achieve comparable results to FaceNet that was trained on 500M. Striving to perform well at low false accept rate is important with large datasets. Even though the chance of a false accept on the small benchmark is acceptable, it does not scale to even moderately sized galleries. Results at LFW are typically reported at equal error rate which implies false accept rate of 1%-5% for top algorithms, while for a large set like MegaFace, only FAR of 10 − 5 or 10 − 6 is meaningful.</p><p>For FGNET the drop in performance is striking-about 60% for everyone but FaceNet, the latter achieving impressive performance across the board. One factor may be the type of training used by different groups (celebrities vs. photos across ages, etc.).</p><p>Verification rate stays similar when scaling up the gallery, e.g., compare (a) and (b). The intuition is that verification rate is normalized by the size of the dataset, so that if a probe face is matched incorrectly to 100 other faces in a 1000 faces dataset, assuming uniform distribution of the data, the rate will stay the same, and so in a dataset of a million faces one can expect to find 10,000 matches at the same false accept rate (FAR).    Identification results. In <ref type="figure">Fig. 7</ref> we show the performance with respect to different ranks, i.e., rank-1 means that the correct match got the best score from the whole database, rank-10 that the correct match is in the first 10 matches, <ref type="figure">etc. (a,b,c)</ref> show performance for the FaceScrub dataset and (d,e,f) for FGNET. We observe that rates drop for all algorithms as the gallery size gets larger. This is visualized in <ref type="figure" target="#fig_0">Fig. 1</ref>, the actual accuracies are in <ref type="figure">Fig. 6</ref>. The curves also suggest that when evaluated on more than 1M distractors (e.g., 100M), rates will be even lower. Testing on FGNET at scale reveals a dramatic performance gap. All algorithms perform much worse, except for FaceNet that has a similar performance to its results on FaceScrub.</p><p>Training set size. Dashed lines in all plots represent algorithms that were trained on data larger than 500K photos and 20K people. We can see that these generally perform better than others.</p><p>Age. Evaluating performance using FGNET as a probe set also reveals a major drop in performance for most algorithms when attempting to match across differences in  <ref type="figure" target="#fig_0">Figure 10</ref>. Examples pairs from FGNET using top performing FaceNet with 10 distractors. Each consecutive left-right pair of images is the same person. All algorithms match better with smaller age differences.</p><p>age. We present a number of results: <ref type="figure">Fig. 8</ref> shows differences in performances with varying age across gallery and probe. Each column represents a different algorithm, rows present results for 1K and 1M distractors. Red colors indicate higher identification rate, blue lower rate. We make two key observations: 1) algorithms perform better when the difference in age between gallery and probe is small (along the diagonal), and 2) adults are more accurately matched than children, at scale. <ref type="figure" target="#fig_0">Fig. 10</ref> shows examples of matched pairs (true positives and false negatives) using FaceNet and 10 distractors. Notice that false negatives have a bigger age gap relative to true positives. It is impressive, however, that the algorithm was able to these and many other true positives, given the variety in lighting, pose, and quality of the photo in addition to age changes.</p><p>Pose. <ref type="figure" target="#fig_5">Fig. 9</ref> evaluates error in recognition as a function of difference in yaw between the probe and gallery. The results are normalized by the total number of pairs for each pose difference. We can see that recognition accuracy depends strongly on pose and this difference is revealed more prominently when evaluated at scale. Top row show results of three different algorithms (representative of others) with 1K distractors. Red colors indicate that identification is very high and mostly independent of pose. However, once evaluated at scale (bottom row) with 1M distractors we can see that variation across algorithms as well as poses is more dramatic. Specifically, similar poses identified better, and more frontal (center of the circle) poses are easier to recognize.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Discussion</head><p>An ultimate face recognition algorithm should perform with billions of people in a dataset. While testing with billions is still challenging, we have done the first step and created a benchmark of a million faces. MegaFace is available to researchers and we presented results from state of the art methods. Our key discoveries are 1) algorithms' performance degrades given a large gallery even though the probe set stays fixed, 2) testing at scale allows to uncover the differences across algorithms (which at smaller scale appear to perform similarly), 3) age differences across probe and gallery are still more challenging for recognition. We will keep maintaining and updating the MegaFace benchmark online, as well as, create more challenges in the future. Below are topics we think are exciting to explore. First, we plan to release all the detected faces from the 100M Flickr dataset. Second, companies like Google and Facebook have a head start due to availability of enormous amounts of data. We are interested to level the playing field and provide large training data to the research community that will be assembled from our Flickr data. Finally, the significant number of high resolution faces in our Flickr database will also allow to explore resolution in more depth. Currently, it is mostly untouched topic in face recognition literature due to lack of data.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>The MegaFace challenge evaluates identification and verification as a function of increasing number of gallery distractors (going from 10 to 1 Million)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>MegaFace statistics. We present randomly selected photographs (with provided detections in red), along with distributions of Flickr tags, GPS locations, and camera types. We also show the pose distribution (yaw and roll), number of faces per photograph, and number of faces for different resolutions (compared to LFW in which faces are approximately 100x100).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 .</head><label>5</label><figDesc>Verification performance with (a,c) 1 Million and (b,d) 10K distractors on both probe sets. Note the performance at low false accept rates (left side of each plot).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 .Figure 8 .</head><label>78</label><figDesc>Identification performance for all methods with (a,d) 1M distractors and (b,e) 10K distractors, and (c,f) rank-10 for both probe sets.Fig. 1also shows rank-1 performance as a function of number of distractors on both probe sets. barebones: rank 1: 1K dist. Analysis of rank-1 identification with respect to varying ages of gallery and probe. Columns represent five algorithms, rows 1K and 1M distractors. X-axis represents a person's age in the gallery photo and Y-axis age in the probe. The colors represent identification accuracy going from 0(=blue)-none of the true pairs were matched to 1(=red)-all possible combinations of probe and gallery were matched per probe and gallery ages. Lower scores on left and bottom indicate worse performance on children, and higher scores along the diagonal indicate that methods are better at matching across small age differences.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 9 .</head><label>9</label><figDesc>Analysis of rank-1 identification with varying poses of gallery and probe, for three algorithms. Top: 1K distractors, Bottom: 1M distractors. The colors represent identification accuracy going from 0 (blue) to 1 (red), where 0 means that none of the true pairs were matched, and 1 means that all possible combinations of probe and gallery were matched per probe and gallery ages. White color indicates combinations of poses that did not exist in our test set. We can see that evaluation at scale (bottom) reveals large differences in performance, which is not visible at smaller scale (top): frontal poses and smaller difference in poses is easier for identification.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>How does age affect recognition performance? We</figDesc><table>-75% identification rates with 1M 
distractors. Baselines (Joint Bayes and LBP) while 
achieving reasonable results on LFW drop to less than 
10%. 
• Is the size of training data important? We observe 
that algorithms that were trained on larger sets (top two 
are FaceNet that was trained on more than 500M pho-
tos of 10M people, and FaceN that was trained on 18M 
of 200K people) tend to perform better at scale. Inter-
estingly, however, FaceN (trained on 18M) compares 
favorably to FaceNet (trained on 500M) on the Face-
Scrub set. 
• </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head></head><label></label><figDesc>3DiVi.com: was trained on 240K photos of 5K people.</figDesc><table>). 
2. FaceAll (Beijing University of Post and Telecommu-
nication), was trained on 838K photos of 17K people, 
and provided two types of features. 
3. NTechLAB.com (FaceN algorithm): provided two 
models (small and large)-small was trained on 494K 
photos of 10K people, large on more than 18M of 
200K. 
4. BareBonesFR (University group): was trained on 
365K photos of 5K people. 
5. </table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">http://markusmathias.bitbucket.org/2014_eccv_face_</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Google's FaceNet was ran by the authors since their features could not be uploaded due to licensing conditions</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments This work was funded in part by Samsung, Google's Faculty Research Award, and by NSF/Intel grant #1538613. We would like to thank the early challenge participants for great feedback on the baseline code and data.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Face description with local binary patterns: Application to face recognition. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ahonen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hadid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pietikainen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2037" to="2041" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Unconstrained face recognition: Identifying a person of interest from a media collection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Best-Rowden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Otto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Klare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Beveridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Bolme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Draper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">H</forename><surname>Givens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">M</forename><surname>Lui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">N</forename><surname>Teli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The challenge of face recognition from digital point-and-shoot cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">W</forename><surname>Scruggs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bowyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Biometrics: Theory, Applications and Systems (BTAS), 2013 IEEE Sixth International Conference on</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Some faces are more equal than others: Hierarchical organization for accurate and efficient large-scale identity-based face retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bhattarai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jurie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pérez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2014 Workshops</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="160" to="172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Cross-age reference coding for age-invariant face recognition and retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">H</forename><surname>Hsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Bayesian face revisited: A joint formulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2012</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="566" to="579" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">The fg-net aging database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cootes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lanitis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Can a poor verification system be a good identification system? a preliminary study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Decann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Information Forensics and Security (WIFS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="31" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Age and gender estimation of unfiltered faces. Information Forensics and Security</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Eidinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Enbar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2170" to="2179" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Yale face database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Georghiades</surname></persName>
		</author>
		<ptr target="http://cvc.yale.edu/projects/yalefaces/yalefa" />
		<imprint>
			<date type="published" when="1997" />
		</imprint>
		<respStmt>
			<orgName>Center for computational Vision and Control at Yale University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Multi-pie. Image and Vision Computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="807" to="813" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Models of large population recognition performance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Grother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Phillips</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2004 IEEE Computer Society Conference on</title>
		<meeting>the 2004 IEEE Computer Society Conference on</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2004" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">68</biblScope>
		</imprint>
	</monogr>
	<note>Computer Vision and Pattern Recognition</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Report on the evaluation of 2d still-image face recognition algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Grother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Quinn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Phillips</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIST interagency report</title>
		<imprint>
			<biblScope unit="volume">7709</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">106</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">When face recognition meets with deep learning: an evaluation of convolutional neural networks for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Christmas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hospedales</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.02351</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Labeled faces in the wild: A database for studying face recognition in unconstrained environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
		<idno>07-49</idno>
		<imprint>
			<date type="published" when="2005" />
			<pubPlace>Amherst</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Massachusetts</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Illumination-aware age progression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kemelmacher-Shlizerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Suwajanakorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note>IEEE</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A case study on unconstrained facial recognition using the boston marathon bombings suspects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Klontz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Tech. Rep</title>
		<imprint>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">120</biblScope>
			<date type="published" when="2013" />
		</imprint>
		<respStmt>
			<orgName>Michigan State University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Attribute and simile classifiers for face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">N</forename><surname>Belhumeur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 12th International Conference on</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="365" to="372" />
		</imprint>
	</monogr>
	<note>Computer Vision</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Describable visual attributes for face verification and image search. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">N</forename><surname>Belhumeur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">10</biblScope>
			<date type="published" when="1962" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A discriminative model for age invariant face recognition. Information Forensics and Security</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1028" to="1037" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Face detection without bells and whistles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pedersoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">A data-driven approach to cleaning large face datasets. people</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-W</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Winkler</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">265</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Face recognition for webscale datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">G</forename><surname>Ortiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Becker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">118</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="153" to="170" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lenc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lenc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision</title>
		<meeting>the British Machine Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.03832</idno>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Autotagging facebook: Social network context improves photo annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zickler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
	<note>CVPRW&apos;08</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.00873</idno>
		<title level="m">Deepid3: Face recognition with very deep neural networks</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Deeply learned face representations are sparse, selective, and robust</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.1265</idno>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deepface: Closing the gap to human-level performance in face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1701" to="1708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Webscale training for face identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.5266</idno>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Thomee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Friedland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Elizalde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Poland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Borth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.01817</idno>
		<title level="m">The new data and new challenges in multimedia research</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Robust real-time face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="137" to="154" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Face search at scale: 80 million gallery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Otto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.07242</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Face recognition in unconstrained videos with matched background similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Maoz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Supervised descent method and its applications to face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>De La</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torre</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2013 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="532" to="539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Learning face representation from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.7923</idno>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Face recognition: A literature survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rosenfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acm Computing Surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="399" to="458" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
