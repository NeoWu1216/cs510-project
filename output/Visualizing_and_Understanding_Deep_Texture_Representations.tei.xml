<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Visualizing and Understanding Deep Texture Representations</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yu</forename><surname>Lin</surname></persName>
							<email>tsungyulin@cs.umass.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Massachusetts</orgName>
								<address>
									<settlement>Amherst</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
							<email>smaji@cs.umass.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Massachusetts</orgName>
								<address>
									<settlement>Amherst</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Visualizing and Understanding Deep Texture Representations</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:48+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A number of recent approaches have used deep convolutional neural networks (CNNs) to build texture representations. Nevertheless, it is still unclear how these models represent texture and invariances to categorical variations. This work conducts a systematic evaluation of recent CNN-based texture descriptors for recognition and attempts to understand the nature of invariances captured by these representations. First we show that the recently proposed bilinear CNN model <ref type="bibr" target="#b24">[25]</ref> is an excellent generalpurpose texture descriptor and compares favorably to other CNN-based descriptors on various texture and scene recognition benchmarks. The model is translationally invariant and obtains better accuracy on the ImageNet dataset without requiring spatial jittering of data compared to corresponding models trained with spatial jittering. Based on recent work <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b27">28]</ref> we propose a technique to visualize pre-images, providing a means for understanding categorical properties that are captured by these representations. Finally, we show preliminary results on how a unified parametric model of texture analysis and synthesis can be used for attribute-based image manipulation, e.g. to make an image more swirly, honeycombed, or knitted. The source code and additional visualizations are available at</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The study of texture has inspired many of the early representations of images. The idea of representing texture using the statistics of image patches have led to the development of "textons" <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b23">24]</ref>, the popular "bag-of-words" models <ref type="bibr" target="#b5">[6]</ref> and their variants such as the Fisher vector <ref type="bibr" target="#b29">[30]</ref> and VLAD <ref type="bibr" target="#b18">[19]</ref>. These fell out of favor when the latest generation of deep Convolutional Neural Networks (CNNs) showed significant improvements in recognition performance over a wide range of visual tasks <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b32">33]</ref>. Recently however, the interest in texture descriptors have been revived by architectures that combine aspects of texture representations with CNNs. For instance, Cimpoi et al. <ref type="bibr" target="#b3">[4]</ref> showed that Fisher vectors built on top of CNN activations <ref type="bibr">dotted</ref> water landromat honeycombed wood bookstore <ref type="figure">Figure 1</ref>. How is texture represented in deep models? Visualizing various categories by inverting the bilinear CNN model <ref type="bibr" target="#b24">[25]</ref> trained on DTD <ref type="bibr" target="#b2">[3]</ref>, FMD <ref type="bibr" target="#b33">[34]</ref>, and MIT Indoor dataset <ref type="bibr" target="#b31">[32]</ref> (each column from left to right). These images were obtained by starting from a random image and adjusting it though gradient descent to obtain high log-likelihood for the given category label using a multi-layer bilinear CNN model (See Sect. 2 for details). Best viewed in color and with zoom.</p><p>lead to better accuracy and improved domain adaptation not only for texture recognition, but also for scene categorization, object classification, and fine-grained recognition. Despite their success little is known how these models represent invariances at the image and category level. Recently, several attempts have been made in order to understand CNNs by visualizing the layers of a trained model <ref type="bibr" target="#b39">[40]</ref>, studying the invariances by inverting the model <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b34">35]</ref>, and evaluating the performance of CNNs for various recognition tasks. In this work we attempt to provide a similar understanding for CNN-based texture representations. Our starting point is the bilinear CNN model of our previous work <ref type="bibr" target="#b24">[25]</ref>. The technique builds an orderless image representation by taking the location-wise outer product of image features extracted from CNNs and aggregating them by averaging. The model is closely related to Fisher vectors but has the advantage that gradients of the model can be easily computed allowing fine-tuning and inversion. Moreover, when the two CNNs are identical the bi-linear features capture correlations between filter channels, similar to the early work in parametric texture representation of Portilla and Simoncelli <ref type="bibr" target="#b30">[31]</ref>.</p><p>Our first contribution is a systematic study of bilinear CNN features for texture recognition. Using the Flickr Material Dataset (FMD) <ref type="bibr" target="#b33">[34]</ref>, Describable Texture Dataset (DTD) <ref type="bibr" target="#b2">[3]</ref> and KTH-T2b <ref type="bibr" target="#b0">[1]</ref> we show that it performs favorably to Fisher vector CNN model <ref type="bibr" target="#b3">[4]</ref>, which is the current state of the art. Similar results are also reported for scene classification on the MIT indoor scene dataset <ref type="bibr" target="#b31">[32]</ref>. We also evaluate the role of different layers, effect of scale, and fine-tuning for texture recognition. Our experiments reveal that multi-scale information helps, but also that features from different layers are complementary and can be combined to improve performance in agreement with recent work <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b28">29]</ref>.</p><p>Our second contribution is to investigate the role of translational invariance of these models due to the orderless pooling of local descriptors. Recently, we showed <ref type="bibr" target="#b24">[25]</ref> that bilinear CNNs initialized by pre-training a standard CNN (e.g., VGG-M [2]) on ImageNet, truncating the network at a lower convolutional layer (e.g., conv5), adding bilinear pooling modules, followed by domain-specific fine-tuning leads to significant improvements in accuracy for a number of fine-grained recognition tasks. These models capture localized feature interactions in a translationally invariant manner which is useful for making fine-grained distinctions between species of birds or models of cars. However, it remains unclear what the tradeoffs are between explicit translational invariance in these models versus implicit invariance obtained by spatial jittering of data during training. To this end we conduct experiments on the ImageNet LSVRC 2012 dataset <ref type="bibr" target="#b6">[7]</ref> by training several models using different amounts of data augmentation. Our experiments reveal that bilinear CNN models can be trained from scratch, resulting in better accuracy without requiring spatial jittering of data than the corresponding CNN architectures that consist of standard "fully-connected" layers trained with jittering.</p><p>Our third contribution is a technique to "invert" these models to construct invariant inputs and visualize preimages for categories. <ref type="figure">Fig. 1</ref> shows the inverse images for various categoriesmaterials such as wood and water, describable attributes such as honeycombed and dotted, and scene categories such as laundromat and bookstore. These images reveal what categorical properties are learned by these texture models. Recently, Gatys et al. <ref type="bibr" target="#b12">[13]</ref> showed that bilinear features (they call it the Gram matrix representation) extracted from various layers of the "verydeep VGG network" <ref type="bibr" target="#b35">[36]</ref> can be inverted for texture synthesis. The synthesized results are visually appealing, demonstrating that the convolutional layers of a CNN capture textural properties significantly better than the first and second order statistics of wavelet coefficients of Portilla and Simoncelli. However, the approach remains impractical since it requires hundreds of CNN evaluations and is orders of magnitude slower than non-parametric patch-based methods such as image quilting <ref type="bibr" target="#b8">[9]</ref>. We show that the two approaches are complementary and one can significantly speed up the convergence of gradient-based inversion by initializing the inverse image using image quilting. The global adjustment of the image through gradient descent also removes many artifacts that quilting introduces.</p><p>Finally, we show a novel application of our approach for image manipulation with texture attributes. A unified parametric model of texture representation and recognition allows us to adjust an image with high-level attributes -to make an image more swirly or honeycombed, or generate hybrid images that are a combination of multiple texture attributes, e.g., chequered and interlaced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Related work</head><p>Texture recognition is a widely studied area. Current state-of-the-art results on texture and material recognition are obtained by hybrid approaches that build orderless representations on top of CNN activations. Cimpoi et al. <ref type="bibr" target="#b3">[4]</ref> use Fisher vector pooling for material and scene recognition, Gong et al. <ref type="bibr" target="#b14">[15]</ref> use VLAD pooling for scene recognition, etc. Our previous work <ref type="bibr" target="#b24">[25]</ref> proposed a general orderless pooling architecture called the bilinear CNN that outperforms Fisher vector on many fine-grained datasets. These descriptors are inspired by early work on texture representations <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b18">19</ref>] that were built on top of wavelet coefficients, linear filter bank responses, or SIFT features <ref type="bibr" target="#b26">[27]</ref>.</p><p>Texture synthesis has received attention from both the vision and graphics communities due to its numerous applications. Heeger and Bergen <ref type="bibr" target="#b16">[17]</ref> synthesized texture images by histogram matching. Portilla and Simoncelli were one of the early proponents of parametric approaches. The idea is to represent texture as the first and second order statistics of various filter bank responses (e.g., wavelets, steerable pyramids, etc.). However, these approaches were outperformed by simpler non-parametric approaches. For instance, Efros and Lueng <ref type="bibr" target="#b9">[10]</ref> proposed a pixel-by-pixel synthesis approach based on sampling similar patches -the method was simple and effective for a wide range of textures. Later, Efros and Freeman proposed a quilting-based approach that was significantly faster <ref type="bibr" target="#b8">[9]</ref>. A number of other non-parametric approaches have been proposed for this problem <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b38">39]</ref>. Recently, <ref type="bibr">Gatys et al.</ref> showed that replacing the linear filterbanks by CNN filterbanks results in better reconstructions. Notably, the Gram matrix representation used in their approach is identical to the bilinear CNN features of Lin et al., suggesting that these features might be good for texture recognition as well. However for synthesis, the parametric approaches remain impractical as they are orders of magnitude slower than non-parametric approaches.</p><p>Understanding CNNs through visualizations has also been widely studied given their remarkable performance. Zieler and Fergus <ref type="bibr" target="#b39">[40]</ref> visualize CNNs using the top activations of filters and show per-pixel heatmaps by tracking the position of the highest responses. Simonyan and Zisserman <ref type="bibr" target="#b34">[35]</ref> visualize parts of the image that cause the highest change in class labels computed by back-propagation. Mahendran and Vedaldi <ref type="bibr" target="#b27">[28]</ref> extend this approach by introducing natural image priors which result in inverse images that have fewer artifacts. Dosovitskiy and Brox <ref type="bibr" target="#b7">[8]</ref> propose a "deconvolutional network" to invert a CNN in a feed-forward manner. However, the approach tends to produce blurry images since the inverse is not uniquely defined.</p><p>Our approach is also related to prior work on editing images based on example images. Ideas from patch-based texture synthesis have been extended in a number of ways to modify the style of the image based on an example <ref type="bibr" target="#b17">[18]</ref>, adjusting texture synthesis based on the content of another image <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b8">9]</ref>, etc. Recently, in a separate work, Gatys et al. <ref type="bibr" target="#b11">[12]</ref> proposed a "neural style" approach that combines ideas from inverting CNNs with their work on texture synthesis. They generate images that match the style and content of two different images producing compelling results. Although the approach is not practical compared to existing patch-based methods for editing styles, it provides a basis for a rich parametric model of texture. We describe an novel approach to manipulate images with high-level attributes and show several examples of editing images with texture attributes. There is relatively little prior work on manipulating the content of an image using semantic attributes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Methodology and overview</head><p>We describe our framework for parametric texture recognition, synthesis, inversion, and attribute-based manipulation using CNNs. For an image I one can compute the activations of the CNN at a given layer r i to obtain a set of features F ri = {f j } indexed by their location j. The bilinear feature B ri (I) of F ri is obtained by computing the outer product of each feature f j with itself and aggregating them across locations by averaging, i.e.,</p><formula xml:id="formula_0">B ri (I) = 1 N N j=1 f j f T j . (1)</formula><p>The bilinear feature (or the Gram matrix representation) is an orderless representation of the input image and hence is suitable for modeling texture. Let r i , i = 1, . . . , n, be the index of the i th layer with the bilinear feature representation B ri . Gatys et al. <ref type="bibr" target="#b12">[13]</ref> propose a method for texture synthesis from an input image I by obtaining an image x ∈ R H×W ×C that matches the bilinear features at various layers by solving the following optimization:</p><formula xml:id="formula_1">min x n i=1 α i L 1 B ri ,B ri + γΓ(x).<label>(2)</label></formula><p>Here,B ri = B ri (I), α i is the weight of the i th layer, Γ(x) is a natural image prior such as the total variation norm (TV norm), and γ is the weight on the prior. Note that we have dropped the implicit dependence of B ri on x for brevity. Using the squared loss-function L 1 (x, y) = (x i − y i ) 2 and starting from a random image where each pixel initialized with a i.i.d zero mean Gaussian, a local optimum is reached through gradient descent. The authors employ L-BFGS, but any other optimization method can be used (e.g., Mahendran and Vedaldi <ref type="bibr" target="#b27">[28]</ref> use stochastic gradient descent with momentum).</p><p>Prior work on minimizing the reconstruction error with respect to the "un-pooled" features F ri has shown that the content of the image in terms of the color and spatial structure is well-preserved even in the higher convolutional layers. Recently, Gatys et al. in a separate work <ref type="bibr" target="#b11">[12]</ref> synthesize images that match the style and content of two different images I and I ′ respectively by minimizing a weighted sum of the texture and content reconstruction errors:</p><formula xml:id="formula_2">min x λL 1 F s ,F s + n i=1 α i L 1 B ri ,B ri + γΓ(x). (3)</formula><p>HereF s = F s (I ′ ) are features from a layer s from which the target content features are computed for an image I ′ .</p><p>The bilinear features can also be used for predicting attributes by first normalizing the features (signed square-root and ℓ 2 ) and training a linear classifier in a supervised manner <ref type="bibr" target="#b24">[25]</ref>. Let l i : i = 1, . . . , m be the index of the i th layer from which we obtain attribute prediction probabilities C li . The prediction layers may be different from those used for texture synthesis. Given a target attributeĈ we can obtain an image that matches the target label and is similar to the texture of a given image I by solving the following optimization:</p><formula xml:id="formula_3">min x n i=1 α i L 1 B ri ,B ri + β m i=1 L 2 C li ,Ĉ + γΓ(x).</formula><p>(4) Here, L 2 is a loss function such as the negative loglikelihood of the labelĈ and β is a tradeoff parameter. If multiple targetsĈ j are available then the losses can be blended with weights β j resulting in the following optimization:</p><formula xml:id="formula_4">min x n i=1 α i L 1 B ri ,B ri +β j m i=1,j L 2 C li ,Ĉ j +γΓ(x).<label>(5)</label></formula><p>Implementation details. We use the 16-layer VGG network <ref type="bibr" target="#b35">[36]</ref> trained on ImageNet for all our experiments. For the image prior Γ(x) we use the TV β norm with β = 2:</p><formula xml:id="formula_5">Γ(x) = i,j (x i,j+1 − x i,j ) 2 + (x i+1,j − x i,j ) 2 β 2 . (6)</formula><p>The exponent β = 2 was empirically found to lead to better reconstructions in <ref type="bibr" target="#b27">[28]</ref> as it leads to fewer "spike" artifacts than β = 1. In all our experiments, given an input image we resize it to 224×224 pixels before computing the target bilinear features and solve for x ∈ R 224×224×3 . This is primarily for speed since the size of the bilinear features are independent of the size of the image. Hence, an output of any size can be obtained by minimizing Eqn. 5. We use L-BFGS for optimization and compute the gradients of the objective with respect to x using back-propagation. One detail we found to be critical for good reconstructions is that we ℓ 1 normalize the gradients with respect to each of the L 1 loss terms to balance the losses during optimization. Mahendran and Vedaldi <ref type="bibr" target="#b27">[28]</ref> suggest normalizing each L 1 loss term by the ℓ 2 norm of the target featureB ri . Without some from of normalization the losses from different layers are of vastly different scales leading to numerical stability issues during optimization.</p><p>Using this framework we: (i) study the effectiveness of bilinear features B ri extracted from various layers of a network for texture and scene recognition (Sect. 3), (ii) investigate the nature of invariances of these features by evaluating the effect of training with different amounts of data augmentation (Sect. 4), (iii) provide insights into the learned models by inverting them (Sect. 5), and (iv) show results for modifying the content of an image with texture attributes (Sect. 6). We conclude in Sect. 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Texture recognition</head><p>In this section we evaluate the bilinear CNN (B-CNN) representation for texture recognition and scene recognition.</p><p>Datasets and evaluation. We experiment on three texture datasets -the Describable Texture Dataset (DTD) <ref type="bibr" target="#b2">[3]</ref>, Flickr Material Dataset (FMD) <ref type="bibr" target="#b33">[34]</ref>, and KTH-TISP2-b (KTH-T2b) <ref type="bibr" target="#b0">[1]</ref>. DTD consists of 5640 images labeled with 47 describable texture attributes. FMD consists of 10 material categories, each of which contains 100 images. Unlike DTD and FMD where images are collected from the Internet, KTH-T2b contains 4752 images of 11 materials captured under controlled scale, pose, and illumination. The KTH-T2b dataset splits the images into four samples for each category. We follow the standard protocol by training on one sample and test on the remaining three. On DTD and FMD, we randomly divide the dataset into 10 splits and report the mean accuracy across splits. Besides these, we also evaluate our models on MIT indoor scene dataset <ref type="bibr" target="#b31">[32]</ref>.</p><p>Indoor scenes are weakly structured and orderless texture representations have been shown to be effective here. The dataset consists of 67 indoor categories and a defined training and test split.</p><p>Descriptor details and training protocol. Our features are based on the "verydeep VGG network" <ref type="bibr" target="#b35">[36]</ref> consisting of 16 convolutional layers pre-trained on the ImageNet dataset. The FV-CNN builds a Fisher Vector representation by extracting CNN filterbank responses from a particular layer of the CNN using 64 Gaussian mixture components, identical to setup of Cimpoi et al. <ref type="bibr" target="#b3">[4]</ref>. The B-CNN features are similarly built by taking the location-wise outer product of the filterbank responses and average pooling across all locations (identical to B-CNN [D,D] in Lin et al. <ref type="bibr" target="#b24">[25]</ref>). Both these features are passed through signed square-root and ℓ 2 normalization which has been shown to improve performance. During training we learn one-vs-all SVMs (trained with SVM hyperparameter C = 1) and weights scaled such that the median positive and negative class scores in the training data is +1 and −1 respectively. At test time we assign the class with the highest score. Our code in implemented using MatConvNet <ref type="bibr" target="#b37">[38]</ref> and VLFEAT <ref type="bibr" target="#b36">[37]</ref> libraries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Experiments</head><p>The following are the main conclusions of the experiments:  <ref type="bibr" target="#b3">[4]</ref> for all datasets except KTH-T2b (−4%). These differences in results are likely due to the choice of the CNN 1 and the range of scales. These results show that the bilinear pooling is as good as the Fisher vector pooling for texture recognition. One drawback is that the FV-CNN features with 64 GMM components has half as many dimensions (64×2×256) as the bilinear features (256×256). However, it is known that these features are highly redundant and their dimensionality can be reduced by an order of magnitude without loss in performance <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b24">25]</ref>.</p><p>2. Multi-scale analysis improves performance. Tab. 1 shows the results by combining features from multiple scales 2 s , s ∈ {1.5:-0.5:-3} relative to the 224×224 image. We discard scales for which the image is smaller than the size of the receptive fields of the filters, or larger than 1024 2 pixels for efficiency. Multiple scales consistently lead to an improvement in accuracy. 3. Higher layers perform better. Tab. 2 shows the performance using various layers of the CNN. The accuracy improves using the higher layers in agreement with <ref type="bibr" target="#b3">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.</head><p>Multi-layer features improve performance. By combining features from all the layers we observe a small but significant improvement in accuracy on DTD 69.9% → 70.7% and on MIT indoor from 72.8% → 74.9%. This suggests that the features from multiple layers capture complementary information and can be combined to improve performance. This is in agreement with the "hypercolumn" approach of Hariharan et al. <ref type="bibr" target="#b15">[16]</ref>.</p><p>5. Fine-tuning leads to a small improvement. On the MIT indoor dataset fine-tuning the network using the B-CNN architecture leads to a small improvement 72.8% → 73.8% using relu5 3 and s = 1. Fine-tuning on texture datasets led to insignificant improvements which might be attributed to their small size. On larger and specialized datasets, such as fine-grained recognition, the effect of finetuning can be significant <ref type="bibr" target="#b24">[25]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">The role of translational invariance</head><p>Earlier experiments on B-CNN and FV-CNN were reported using pre-trained networks. Here we experiment with training a B-CNN model from scratch on the ImageNet LSRVC 2012 dataset. We experimenting with the effect of spatial jittering of training data on the classification performance. For these experiments we use the VGG-M <ref type="bibr" target="#b1">[2]</ref> architecture which performs better than AlexNet <ref type="bibr" target="#b21">[22]</ref> with a moderate decrease in classification speed. For the B-CNN model we replace the last two fully-connected layers with a linear classifier and softmax layer on the outputs of the square-root and ℓ 2 normalized bilinear features of the relu5 layer outputs. The evaluation speed for B-CNN is 80% of that of the standard CNN, hence the overall training times for both architectures are comparable.</p><p>We train various models with different amounts of spatial jittering -"f1" for flip, "f5" for flip + 5 translations and "f25" for flip + 25 translations. In each case the training is done using stochastic sampling where one of the jittered copies is randomly selected for each example. The network parameters are randomly initialized and trained using stochastic gradient descent with momentum for a number of epochs. We start with a high learning rate and reduce it by a factor of 10 when the validation error stops decreasing. We stop training when the validation error stops decreasing. <ref type="figure" target="#fig_1">Fig. 2</ref> shows the "top1" validation errors and compares the B-CNN network to the standard VGG-M model. The validation error is reported on a single center cropped image. Note that we train all networks with neither PCA color jittering nor batch normalization and our baseline results are within 2% of the top1 errors reported in <ref type="bibr" target="#b1">[2]</ref>. The VGG-M model achieves 46.4% top1 error with flip augmentation during training. The performance improves significantly to 39.6% with f25 augmentation. As fully connected layers in a standard CNN network encode spatial information, the model loses performance without spatial jittering. For B-CNN network, the model achieves 38.7% top1 error with f1 augmentation, outperforming VGG-M with f25 augmentation. With more augmentations, B-CNN model improves top1 error by 1.6% (38.7% → 37.1%). Going from f5 to f25, B-CNN model improves marginally by &lt; 1%. The results show that B-CNN feature is discriminative and robust to translation. With a small amount of data jittering, B-CNN network achieves fairly good performance, suggesting that explicit translation invariance might be preferable to the implicit invariance obtained by data jittering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Understanding texture representations</head><p>In this section we aim to understand B-CNN texture representation by synthesizing invariant images, i.e. images that are nearly identical to a given image according to the bilinear features, and inverse images for a given category.</p><p>Visualizing invariant images for objects. We use <ref type="figure" target="#fig_1">relu1 1, relu2 1, relu3 1, relu4 1, relu5 1</ref> layers for texture representation. <ref type="figure" target="#fig_2">Fig. 3</ref> shows several invariant images to the image on the top left, i.e. these images are virtually identical as far as the bilinear features for these layers are con-  cerned. Translational invariance manifests as shuffling of patches but important local structure is preserved within the images. These images were obtained using γ = 1e − 6 and α i = 1 ∀i in Eqn. 5. We found that as long as some higher and lower layers are used together the synthesized textures look reasonable, similar to the observations of Gatys et al.</p><p>Role of initialization on texture synthesis. Although the same approach can be used for texture synthesis, it is not practical since it requires several hundreds of CNN evaluations, which takes several minutes on a high-end GPU. In comparison, non-parametric patch-based approaches such as image quilting <ref type="bibr" target="#b8">[9]</ref> are orders of magnitude faster. Quilting introduces artifacts when adjacent patches do not align with each other. The original paper proposed an approach where a one-dimensional cut is found that minimizes artifacts. However, this can fail since local adjustments cannot remove large structural errors in the synthesis. We instead investigate the use of quilting to initialize the gradient-based synthesis approach. <ref type="figure" target="#fig_5">Fig. 5</ref> shows the objective through iterations of L-BFGS starting from a random and quiltingbased initialization. Quilting starts at a lower objective and reaches the final objective of the random initialization significantly faster. Moreover, the global adjustments of the image through gradient descent remove many artifacts that quilting introduces (digitally zoom in to the onion image to see this). <ref type="figure">Fig. 6</ref> show the results using image quilting as initialization for style transfer <ref type="bibr" target="#b11">[12]</ref>. Here two images are given as input, one for content measured as the conv4 2 layer output, and one for style measured as the bilinear features. Similar to texture synthesis, the quilting-based initialization starts from lower objective value and the optimization converges faster. These experiments suggest that patch-based and parametric approaches for texture synthesis are complementary and can be combined effectively.</p><p>Visualizing texture categories. We learn linear classifiers to predict categories using bilinear features from relu2 2, relu3 3, relu4 3, relu5 3 layers of the CNN on various datasets and visualize images that produce high prediction scores for each class. <ref type="figure">Fig. 1</ref> shows some example inverse images for various categories for the DTD, FMD and MIT indoor datasets. These images were obtained by setting β = 100, γ = 1e−6, andĈ to various class labels in Eqn. 5. These images reveal how the model represents texture and scene categories. For instance, the dotted category of DTD contains images of various colors and dot sizes and the inverse image is composed of multi-scale multi-colored dots. The inverse images of water and wood from FMD are    <ref type="figure">Figure 6</ref>. Effect of initialization on style transfer. Given a content and a style image the style transfer reached using L-BFGS after 100 iterations starting from a random image: tranf(rand), and image quilting: tranf(quilt). The results using image quilting <ref type="bibr" target="#b8">[9]</ref> are shown as quilt. On the bottom right is the objective function for the optimization for 5 random initializations.</p><p>highly representative of these categories. Note that these images cannot be obtained by simply averaging instances within a category which is likely to produce a blurry image. The orderless nature of the texture descriptor is essential to produce such sharp images. The inverse scene images from the MIT indoor dataset reveal key properties that the model learns -a bookstore is visualized as racks of books while a laundromat has laundry machines at various scales and locations. In <ref type="figure" target="#fig_3">Fig. 4</ref> we visualize reconstructions by incrementally adding layers in the texture representation. Lower layers preserve color and small-scale structure and combining all the layers leads to better reconstructions. Even though the relu5 3 layer provides the best recognition accuracy, simply using that layer did not produce good inverse images (not shown). Notably, color information is discarded in the upper layers. <ref type="figure">Fig. 7</ref> shows visualizations of some other categories across datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Manipulating images with texture attributes</head><p>Our framework can be used to edit images with texture attributes. For instance, we can make a texture or the content of an image more honeycombed or swirly. <ref type="figure" target="#fig_7">Fig. 8</ref> shows some examples where we have modified images with various attributes. The top two rows of images were obtained by setting α i = 1 ∀i, β = 1000 and γ = 1e − 6 and varyinĝ C to represent the target class. The bottom row is obtained by setting α i = 0 ∀i, and using the relu4 2 layer for content reconstruction with weight λ = 5e − 8.</p><p>The difference between the two is that in the content reconstruction the overall structure of the image is preserved. The approach is similar to the neural style approach <ref type="bibr" target="#b11">[12]</ref>, but instead of providing a style image we adjust the image with attributes. This leads to interesting results. For instance, when the face image is adjusted with the interlaced attribute ( <ref type="figure" target="#fig_7">Fig. 8 bottom row)</ref> the result matches the scale and orientation of the underlying image. No single image in the DTD dataset has all these variations but the categorical representation does. The approach can be used to modify an image with other high-level attributes such as artistic styles by learning style classifiers.  <ref type="figure">Figure 7</ref>. Examples of texture inverses <ref type="figure">(Fig. 1 cont.</ref>) Visualizing various categories by inverting the bilinear CNN model <ref type="bibr" target="#b24">[25]</ref> trained on DTD <ref type="bibr" target="#b2">[3]</ref>, FMD <ref type="bibr" target="#b33">[34]</ref>, and MIT Indoor dataset <ref type="bibr" target="#b31">[32]</ref> (two columns each from left to right). Best viewed in color and with zoom. We can also blend texture categories using weights β j of the targetsĈ j . <ref type="figure" target="#fig_8">Fig. 9</ref> shows some examples. On the left is the first category, on the right is the second category, and in the middle is where a transition occurs (selected manually).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>We present a systematic study of recent CNN-based texture representations by investigating their effectiveness on recognition tasks and studying their invariances by inverting them. The main conclusion is that translational invariance is a useful property not only for texture and scene recognition, but also for general object classification on the ImageNet dataset. The resulting models provide a rich parametric approach for texture synthesis and manipulation of content of images using texture attributes. The key challenge is that the approach is computationally expensive, and we present an initialization scheme based on image quilting that significantly speeds up the convergence and also removes many structural artifacts that quilting introduces. The complementary qualities of patch-based and gradient-based methods may be useful for other applications.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>1. B-CNN compares favorably to FV-CNN. Tab. 1 shows results using the B-CNN and FV-CNN on various datasets. Across all scales of the input image the performance using B-CNN and FV-CNN is virtually identical. The FV-CNN multi-scale results reported here are comparable (±1%) to the results reported in Cimpoi et al.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Effect of spatial jittering on ImageNet LRVC 2012 classification. The top1 validation error on a single center crop on ImageNet dataset using the VGG-M network and the corresponding B-CNN model. The networks are trained with different levels of data jittering: "f1", "f5", and "f25" indicating flip, flip + 5 translations, and flip + 25 translations respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Invariant inputs. These six images are virtually identical when compared using the bilinear features of layers relu1 1, relu2 1, relu3 1, relu4 1, relu5 1 of the VGG network<ref type="bibr" target="#b35">[36]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Effect of layers on inversion. Pre-images obtained by inverting class labels using different layers. The leftmost column shows inverses using predictions of relu2 2 only. In the following columns we add layers relu3 3, relu4 3, and relu5 3 one by one.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Effect of initialization on texture synthesis. Given an input image, the solution reached by the L-BFGS after 250 iterations starting from a random image: syn(rand), and image quilting: syn(quilt). The results using image quilting<ref type="bibr" target="#b8">[9]</ref> are shown as quilt. On the right is the objective function for the optimization for 5 random initializations. Quilting-based initialization starts at a lower objective value and matches the final objective of the random initialization in far fewer iterations. Moreover, many artifacts of quilting are removed in the final solution (e.g., the top row</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 .</head><label>8</label><figDesc>Manipulating images with attributes. Given an image we synthesize a new image that matches its texture (top two rows) or its content (bottom two rows) according to a given attribute (shown in the image).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 .</head><label>9</label><figDesc>Hybrid textures obtained by blending the texture on the left and right according to weights β1 and β2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>Table 2. Layer by layer performance. The classification accuracy using B-CNN features based on the outputs of different layers on several datasets using input at s = 1, i.e. 224×224 pixels. The numbers are reported on the first split of all datasets.</figDesc><table>FV-CNN 

B-CNN 
Dataset 
s = 1 s = 2 ms s = 1 s = 2 ms 
DTD 
67.8 70.6 73.6 69.6 71.5 72.9 

±0.9 
±0.9 
±1.0 
±0.7 
±0.8 
±0.8 

FMD 
75.1 79.0 80.8 77.8 80.7 81.6 

±2.3 
±1.4 
±1.7 
±1.9 
±1.5 
±1.7 

KTH-T2b 
74.8 75.9 77.9 75.1 76.4 77.9 

±2.6 
±2.4 
±2.0 
±2.8 
±3.5 
±3.1 

MIT indoor 70.1 78.2 78.5 72.8 77.6 79.0 

Table 1. Comparison of B-CNN and FV-CNN. We report mean 
per-class accuracy on DTD, FMD, KTH-T2b and MIT indoor 
datasets using FV-CNN and B-CNN representations constructed 
on top of relu5 3 layer outputs of the 16-layer VGG network [36]. 
Results are reported using input images at different scales: s = 1, 
s = 2 and ms are with images resized to 224×224, 448×448 and 
pooled across multiple scales respectively. 

Dataset 
relu2 2 relu3 3 relu4 3 relu5 3 
DTD 
42.9 
59.0 
68.8 
69.9 
FMD 
49.6 
62.2 
73.4 
80.2 
KTH-T2b 
59.9 
71.3 
78.8 
79.0 
MIT indoor 
32.2 
54.5 
71.1 
72.8 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>). Best viewed with digital zoom. Images are obtained from http://www.textures.com.</figDesc><table>content 
style 
tranf(rand) 

quilt 
tranf(quilt) 

content 
style 
tranf(rand) 

quilt 
tranf(quilt) 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">they use the conv5 4 layer of the 19-layer VGG network.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgement The GPUs used in this research were generously donated by NVIDIA.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Class-specific material categorisation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hayman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mallikarjuna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Return of the devil in the details: Delving deep into convolutional nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chatfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Describing textures in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cimpoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Deep filter banks for texture recognition, description, and segmentation. IJCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cimpoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="1" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Region filling and object removal by exemplar-based image inpainting. Image Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pérez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">9</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Visual categorization with bags of keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Csurka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Dance</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Willamowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV Workshop on Stat. Learn. in Comp. Vision</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Inverting visual representations with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Image quilting for texture synthesis and transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Texture synthesis by non-parametric sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Beijbom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06062</idno>
		<title level="m">Compact bilinear pooling</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Gatys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Ecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.06576</idno>
		<title level="m">A neural algorithm of artistic style</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Texture synthesis and the controlled generation of natural stimuli using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Gatys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Ecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multi-scale orderless pooling of deep convolutional activation features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Hypercolumns for object segmentation and fine-grained localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Pyramid-based texture analysis/synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Heeger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Bergen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH</title>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Image analogies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hertzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">E</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Salesin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Aggregating local descriptors into a compact image representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pérez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia (ACM MM)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Textons, the fundamental elements in preattentive vision and perception of textures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Julesz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Bergen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bell System Technical Journal</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1619" to="1645" />
			<date type="published" when="1983-08" />
		</imprint>
	</monogr>
	<note>Pt 3</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Graphcut textures: image and video synthesis using graph cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kwatra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schödl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Essa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Turk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bobick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Representing and recognizing the visual appearance of materials using three-dimensional textons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="29" to="44" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Bilinear CNN Models for Fine-grained Visual Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roychowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Object recognition from local scale-invariant features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Understanding deep image representations by inverting them</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mahendran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Feedforward semantic segmentation with zoom-out features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mostajabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yadollahpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shakhnarovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Fisher kernels on visual vocabularies for image categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Dance</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A parametric texture model based on joint statistics of complex wavelet coefficients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Portilla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="49" to="70" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Recognizing indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Quattoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Cnn features off-the-shelf: An astounding baseline for recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Razavin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Azizpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Carlsson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>In DeepVision workshop</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Material perceprion: What can you see in a brief glance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rosenholtz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Adelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Vision</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">784</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deep inside convolutional networks: Visualising image classification models and saliency maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR workshop</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">VLFeat: an open and portable library of computer vision algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fulkerson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia (ACM MM)</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">MatConvNet: convolutional neural networks for matlab</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lenc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia (ACM MM)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Fast texture synthesis using treestructured vector quantization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Levoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
