<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Semi-supervised Vocabulary-informed Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
							<email>y.fu@qmul.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Disney Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Sigal</surname></persName>
							<email>lsigal@disneyresearch.com</email>
							<affiliation key="aff0">
								<orgName type="department">Disney Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Semi-supervised Vocabulary-informed Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:45+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Despite significant progress in object categorization, in recent years, a number of important challenges remain; mainly, ability to learn from limited labeled data and ability to recognize object classes within large, potentially open, set of labels. Zero-shot learning is one way of addressing these challenges, but it has only been shown to work with limited sized class vocabularies and typically requires separation between supervised and unsupervised classes, allowing former to inform the latter but not vice versa. We propose the notion of semi-supervised vocabulary-informed learning to alleviate the above mentioned challenges and address problems of supervised, zero-shot and open set recognition using a unified framework. Specifically, we propose a maximum margin framework for semantic manifoldbased recognition that incorporates distance constraints from (both supervised and unsupervised) vocabulary atoms, ensuring that labeled samples are projected closest to their correct prototypes, in the embedding space, than to others.</p><p>We show that resulting model shows improvements in supervised, zero-shot, and large open set recognition, with up to 310K class vocabulary on AwA and ImageNet datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Object recognition, and more specifically object categorization, has seen unprecedented advances in recent years with development of convolutional neural networks (CNNs) <ref type="bibr" target="#b22">[23]</ref>. However, most successful recognition models, to date, are formulated as supervised learning problems, in many cases requiring hundreds, if not thousands, labeled instances to learn a given concept class <ref type="bibr" target="#b9">[10]</ref>. This exuberant need for large labeled datasets has limited recognition models to domains with 100's to few 1000's of classes. Humans, on the other hand, are able to distinguish beyond 30, 000 basic level categories <ref type="bibr" target="#b4">[5]</ref>. What is more impressive, is the fact that humans can learn from few examples, by effectively leveraging information from other object category classes, and even recognize objects without ever seeing them (e.g., by reading about them on the Internet). This ability has spawned research in few-shot and zero-shot learning. In both cases, t-SNE visualization is used to illustrate samples from 4 source/auxiliary classes (denoted by ×) and 2 target/zeroshot classed (denoted by •) from the ImageNet dataset. Decision boundaries, illustrated by dashed lines, are drawn by hand for visualization. Note, that (i) large margin constraints in SS-Voc, both among the source/target classes and the external vocabulary atoms (denoted by arrows and words), and (ii) fine-tuning of the semantic word space, lead to a better embedding with more compact and separated classes (e.g., see truck and car or unicycle and tricycle).</p><p>Zero-shot learning (ZSL) has now been widely studied in a variety of research areas including neural decoding by fMRI images <ref type="bibr" target="#b30">[31]</ref>, character recognition <ref type="bibr" target="#b25">[26]</ref>, face verification <ref type="bibr" target="#b23">[24]</ref>, object recognition <ref type="bibr" target="#b24">[25]</ref>, and video understanding <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b44">45]</ref>. Typically, zero-shot learning approaches aim to recognize instances from the unseen or unknown testing target categories by transferring information, through intermediate-level semantic representations, from known observed source (or auxiliary) categories for which many labeled instances exist. In other words, supervised classes/instances, are used as context for recognition of classes that contain no visual instances at training time, but that can be put in some correspondence with supervised classes/instances. As such, a general experimental setting of ZSL is that the classes in target and source (auxiliary) dataset are disjoint and typically the learning is done on the source dataset and then information is transferred to the target dataset, with performance measured on the latter. This setting has a few important drawbacks: <ref type="bibr" target="#b0">(1)</ref> it assumes that target classes cannot be mis-classified as source classes and vice versa; this greatly and unrealistically simplifies the problem; (2) the target label set is often relatively small, between ten <ref type="bibr" target="#b24">[25]</ref> and several thousand unknown labels <ref type="bibr" target="#b13">[14]</ref>, compared to at least 30, 000 entry level categories that humans can distinguish; (3) large amounts of data in the source (auxiliary) classes are required, which is problematic as it has been shown that most object classes have only few instances (long-tailed distribution of objects in the world <ref type="bibr" target="#b39">[40]</ref>); and (4) the vast open set vocabulary from semantic knowledge, defined as part of ZSL <ref type="bibr" target="#b30">[31]</ref>, is not leveraged in any way to inform the learning or source class recognition.</p><p>A few works recently looked at resolving (1) through class-incremental learning <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b38">39]</ref> which is designed to distinguish between seen (source) and unseen (target) classes at the testing time and apply an appropriate model -supervised for the former and ZSL for the latter. However, (2)-(4) remain largely unresolved. In particular, while <ref type="formula" target="#formula_3">(2)</ref> and (3) are artifacts of the ZSL setting, (4) is more fundamental. For example, consider learning about a car by looking at image instances in <ref type="figure" target="#fig_0">Fig.1</ref>. Not knowing that other motor vehicles exist in the world, one may be tempted to call anything that has 4-wheels a car. As a result the zero-shot class truck may have large overlap with the car class (see <ref type="figure" target="#fig_0">Fig.1</ref> [SVR]). However, imagine knowing that there also exist many other motor vehicles (trucks, mini-vans, etc). Even without having visually seen such objects, the very basic knowledge that they exist in the world and are closely related to a car should, in principal, alter the criterion for recognizing instance as a car (making the recognition criterion stricter in this case). Encoding this in our [SS-Voc] model results in better separation among classes.</p><p>To tackle the limitations of ZSL and towards the goal of generic open set recognition, we propose the idea of semisupervised vocabulary-informed learning. Specifically, assuming we have few labeled training instances and a large open set vocabulary/semantic dictionary (along with textual sources from which statistical semantic relations among vocabulary atoms can be learned), the task of semi-supervised vocabulary-informed learning is to learn a model that utilizes semantic dictionary to help train better classifiers for observed (source) classes and unobserved (target) classes in supervised, zero-shot and open set image recognition settings. Different from standard semi-supervised learning, we do not assume unlabeled data is available, to help train classifier, and only vocabulary over the target classes is known.</p><p>Contributions: Our main contribution is to propose a novel paradigm for potentially open set image recognition: semisupervised vocabulary-informed learning (SS-Voc), which is capable of utilizing vocabulary over unsupervised items, during training, to improve recognition. A unified maximum margin framework is used to encode this idea in practice. Particularly, classification is done through nearestneighbor distance to class prototypes in the semantic embedding space, and we encode a set of constraints ensuring that labeled images project into semantic space such that they end up closer to the correct class prototypes than to incorrect ones (whether those prototypes are part of the source or target classes). We show that word embedding (word2vec) can be used effectively to initialize the semantic space. Experimentally, we illustrate that through this paradigm: we can achieve competitive supervised (on source classes) and ZSL (on target classes) performance, as well as open set image recognition performance with large number of unobserved vocabulary entities (up to 300, 000); effective learning with few samples is also illustrated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>One-shot Learning: While most of machine learningbased object recognition algorithms require large amount of training data, one-shot learning <ref type="bibr" target="#b11">[12]</ref> aims to learn object classifiers from one, or only few examples. To compensate for the lack of training instances and enable oneshot learning, knowledge much be transferred from other sources, for example, by sharing features <ref type="bibr" target="#b2">[3]</ref>, semantic attributes <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35]</ref>, or contextual information <ref type="bibr" target="#b40">[41]</ref>. However, none of previous works had used the open set vocabulary to help learn the object classifiers.</p><p>Zero-shot Learning: ZSL aims to recognize novel classes with no training instance by transferring knowledge from source classes. ZSL was first explored with use of attributebased semantic representations <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b31">32]</ref>. This required pre-defined attribute vector prototypes for each class, which is costly for a large-scale dataset. Recently, semantic word vectors were proposed as a way to embed any class name without human annotation effort; they can therefore serve as an alternative semantic representation <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b29">30]</ref> for ZSL. Semantic word vectors are learned from large-scale text corpus by language models, such as word2vec <ref type="bibr" target="#b28">[29]</ref>, or GloVec <ref type="bibr" target="#b32">[33]</ref>. However, most of previous work only use word vectors as semantic representations in ZSL setting, but have neither (1) utilized semantic word vectors explicitly for learning better classifiers; nor (2) for extending ZSL setting towards open set image recognition. A notable exception is <ref type="bibr" target="#b29">[30]</ref> which aims to recognize 21K zero-shot classes given a modest vocabulary of 1K source classes; we explore vocabularies that are up to an order of the magnitude larger -310K.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Open-set Recognition:</head><p>The term "open set recognition" was initially defined in <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b37">38]</ref> and formalized in <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b35">36]</ref> which mainly aims at identifying whether an image belongs to a seen or unseen classes. It is also known as classincremental learning. However, none of them can further identify classes for unseen instances. An exception is <ref type="bibr" target="#b29">[30]</ref> which augments zero-shot (unseen) class labels with source (seen) labels in some of their experimental settings. Similarly, we define the open set image recognition as the problems of recognizing the class name of an image from a potentially very large open set vocabulary (including, but not limited to source and target labels). Note that methods like <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b37">38]</ref> are orthogonal but potentially useful here -it is still worth identifying seen or unseen instances to be recognized with different label sets as shown in experiments. Conceptually similar, but different in formulation and task, open-vocabulary object retrieval <ref type="bibr" target="#b19">[20]</ref> focused on retrieving objects using natural language open-vocabulary queries.</p><p>Visual-semantic Embedding: Mapping between visual features and semantic entities has been explored in two ways: (1) directly learning the embedding by regressing from visual features to the semantic space using Support Vector Regressors (SVR) <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b24">25]</ref> or neural network <ref type="bibr" target="#b38">[39]</ref>;</p><p>(2) projecting visual features and semantic entities into a common new space, such as SJE <ref type="bibr" target="#b1">[2]</ref>, WSABIE <ref type="bibr" target="#b43">[44]</ref>, ALE <ref type="bibr" target="#b0">[1]</ref>, DeViSE <ref type="bibr" target="#b13">[14]</ref>, and CCA <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b17">18]</ref>. In contrast, our model trains a better visual-semantic embedding from only few training instances with the help of large amount of open set vocabulary items (using a maximum margin strategy). Our formulation is inspired by the unified semantic embedding model of <ref type="bibr" target="#b20">[21]</ref>, however, unlike <ref type="bibr" target="#b20">[21]</ref>, our formulation is built on word vector representation, contains a data term, and incorporates constraints to unlabeled vocabulary prototypes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Vocabulary-informed Learning</head><formula xml:id="formula_0">Assume a labeled source dataset D s = {x i , z i } Ns i=1 of N s samples, where x i ∈ R p</formula><p>is the image feature representation of image i and z i ∈ W s is a class label taken from a set of English words or phrases W; consequently, |W s | is the number of source classes. Further, suppose another set of class labels for target classes W t , such that W s ∩ W t = ∅, for which no labeled samples are available. We note that potentially |W t | &gt;&gt; |W s |. Given a new test image feature vector x * the goal is then to learn a function z * = f (x * ), using all available information, that predicts a class label z * . Note that the form of the problem changes drastically depending on which label set assumed for z * : Supervised learning: z * ∈ W s ; Zero-shot learning: z * ∈ W t ; Open set recognition: z * ∈ {W s , W t } or, more generally, z * ∈ W. We posit that a single unified f (x * ) can be learned for all three cases. We formalize the definition of semi-supervised vocabulary-informed learning (SS-Voc) as follows:</p><p>Definition 3.1. Semi-supervised Vocabulary-informed Learning (SS-Voc): is a learning setting that makes use of complete vocabulary data (W) during training. Unlike a more traditional ZSL that typically makes use of the vocabulary (e.g., semantic embedding) at test time, SS-Voc utilizes exactly the same data during training. Notably, SS-Voc requires no additional annotations or semantic knowledge; it simply shifts the burden from testing to training, leveraging the vocabulary to learn a better model.</p><p>The vocabulary W can come from a semantic embedding space learned by word2vec <ref type="bibr" target="#b28">[29]</ref> or GloVec <ref type="bibr" target="#b32">[33]</ref> on largescale corpus; each vocabulary entity w ∈ W is represented as a distributed semantic vector u ∈ R d . Semantics of embedding space help with knowledge transfer among classes, and allow ZSL and open set image recognition. Note that such semantic embedding spaces are equivalent to the "semantic knowledge base" for ZSL defined in <ref type="bibr" target="#b30">[31]</ref> and hence make it appropriate to use SS-Voc in ZSL setting.</p><p>Assuming we can learn a mapping g : R p → R d , from image features to this semantic space, recognition can be carried out using simple nearest neighbor distance, e.g., f (x * ) = car if g(x * ) is closer to u car than to any other word vector; u j in this context can be interpreted as the prototype of the class j. Thus the core question is then how to learn the mapping g(x) and what form of inference is optimal in the semantic space. For learning we propose discriminative maximum margin criterion that ensures that labeled samples x i project closer to their corresponding class prototypes u zi than to any other prototype u i in the open set vocabulary i ∈ W \ z i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Learning Embedding</head><p>Our maximum margin vocabulary-informed embedding learns the mapping g(x) : R p → R d , from low-level features x to the semantic word space by utilizing maximum margin strategy. Specifically, consider g(x) = W T x, where 1 W ⊆ R p×d . Ideally we want to estimate W such that u zi = W T x i for all labeled instances in D s (we would obviously want this to hold for instances belonging to unobserved classes as well, but we cannot enforce this explicitly in the optimization as we have no labeled samples for them).</p><p>Data Term: The easiest way to enforce the above objective is to minimize Euclidian distance between sample projections and appropriate prototypes in the embedding space 2 :</p><formula xml:id="formula_1">D (x i , u zi ) = W T x i − u zi 2 2 .<label>(1)</label></formula><p>We need to minimize this term with respect to each instance</p><formula xml:id="formula_2">(x i , u zi ), where z i is the class label of instance x i in D s .</formula><p>To prevent overfitting, we further regularize the solution:</p><formula xml:id="formula_3">L (x i , u zi ) = D (x i , u zi ) + λ W 2 F ,<label>(2)</label></formula><p>where · F indicates the Frobenius Norm. Solution to the Eq.(2) can be obtained through ridge regression. <ref type="bibr" target="#b0">1</ref> Generalizing to a kernel version is straightforward, see <ref type="bibr" target="#b42">[43]</ref>. <ref type="bibr" target="#b1">2</ref> Eq.(1) is also called data embedding <ref type="bibr" target="#b20">[21]</ref> / compatibility function <ref type="bibr" target="#b1">[2]</ref>.</p><p>Nevertheless, to make the embedding more comparable to support vector regression (SVR), we employ the maximal margin strategy -ǫ−insensitive smooth SVR (ǫ−SSVR) <ref type="bibr" target="#b26">[27]</ref> to replace the least square term in Eq. <ref type="bibr" target="#b1">(2)</ref>. That is,</p><formula xml:id="formula_4">L (x i , u zi ) = L ǫ (x i , u zi ) + λ W 2 F ,<label>(3)</label></formula><p>where</p><formula xml:id="formula_5">L ǫ (x i , u zi ) = 1 T | ξ | 2 ǫ ; λ is regularization coef- ficient. (|ξ| ǫ ) j = max 0, |W T ⋆j x i − (u zi ) j | − ǫ , |ξ| ǫ ∈ R d ,</formula><p>and () j indicates the j-th value of corresponding vector. W ⋆j is the j-th column of W . The conventional ǫ−SVR is formulated as a constrained minimization problem, i.e., convex quadratic programming problem, while ǫ−SSVR employs quadratic smoothing <ref type="bibr" target="#b46">[47]</ref> to make Eq.(3) differentiable everywhere, and thus ǫ−SSVR can be solved as an unconstrained minimization problem directly 3 .</p><p>Pairwise Term: Data term above only ensures that labelled samples project close to their correct prototypes. However, since it is doing so for many samples and over a number of classes, it is unlikely that all the data constraints can be satisfied exactly. Specifically, consider the following case, if u zi is in the part of the semantic space where no other entities live (i.e., distance from u zi to any other prototype in the embedding space is large), then projecting x i further away from u zi is asymptomatic, i.e., will not result in misclassification. However, if the u zi is close to other prototypes then minor error in regression may result in misclassification.</p><p>To embed this intuition into our learning, we enforce more discriminative constraints in the learned semantic embedding space. Specifically, the distance of D (x i , u zi ) should not only be as close as possible, but should also be smaller than the distance D (x i , u a ), ∀a = z i . Formally, we define the vocabulary pairwise maximal margin term 4 :</p><formula xml:id="formula_6">M V (x i , u zi ) = 1 2 A V a=1 C + 1 2 D (x i , u zi ) − 1 2 D (x i , u a ) 2 + (4) where a ∈ W t is selected from the open vocabulary; C is the margin gap constant. Here, [·]</formula><p>2 + indicates the quadratically smooth hinge loss <ref type="bibr" target="#b46">[47]</ref> which is convex and has the gradient at every point. To speedup computation, we use the closest A V target prototypes to each source/auxiliary prototype u zi in the semantic space. We also define similar constraints for the source prototype pairs:</p><formula xml:id="formula_7">M S (x i , u zi ) = 1 2 B S b=1 C + 1 2 D (x i , u zi ) − 1 2 D (x i , u b ) 2 +<label>(5)</label></formula><p>where b ∈ W s is selected from source/auxiliary dataset vocabulary. This term enforces that D (x i , u zi ) should be smaller than the distance D (x i , u b ), ∀b = z i . To facilitate the computation, we similarly use closest B S prototypes that are closest to each prototype u zi in the source classes. Our complete pairwise maximum margin term is:</p><formula xml:id="formula_8">M (x i , u zi ) = M V (x i , u zi ) + M S (x i , u zi ) .<label>(6)</label></formula><p>We note that the form of rank hinge loss in Eq.(4) and Eq. <ref type="formula" target="#formula_7">(5)</ref> is similar to DeViSE <ref type="bibr" target="#b13">[14]</ref>, but DeViSE only considers loss with respect to source/auxiliary data and prototypes.</p><p>Vocabulary-informed Embedding: The complete combined objective can now be written as:</p><formula xml:id="formula_9">W = argmin W n T i=1 (αL ǫ (x i , u yi ) + (1 − α)M (x i , u zi )) + λ W 2 F ,<label>(7)</label></formula><p>where α ∈ [0, 1] is ratio coefficient of two terms. One practical advantage is that the objective function in Eq. <ref type="formula" target="#formula_9">(7)</ref> is an unconstrained minimization problem which is differentiable and can be solved with L-BFGS. W is initialized with all zeros and converges in 10 − 20 iterations.</p><p>Fine-tuning Word Vector Space: Above formulation works well assuming semantic space is well laid out and linear mapping is sufficient. However, we posit that word vector space itself is not necessarily optimal for visual discrimination. Consider the following case: two visually similar categories may appear far away in the semantic space.</p><p>In such a case, it would be difficult to learn a linear mapping that matches instances with category prototypes properly. Inspired by this intuition, which has also been expressed in natural language models <ref type="bibr" target="#b5">[6]</ref>, we propose to fine-tune the word vector representation for better visual discriminability. One can potentially fine-tune the representation by optimizing u i directly, in an alternating optimization (e.g., as in <ref type="bibr" target="#b20">[21]</ref>). However, this is only possible for source/auxiliary class prototypes and would break regularities in the semantic space, reducing ability to transfer knowledge from source/auxilary to target classes. Alternatively, we propose optimizing a global warping, V , on the word vector space:</p><formula xml:id="formula_10">{W, V } = argmin W,V n T i=1 (αL ǫ (x i , u yi V ) + (1 − α) M (x i , u zi V )) + λ W 2 F +µ V 2 F ,<label>(8)</label></formula><p>where µ is regularization coefficient. Eq.(8) can still be solved using L-BFGS and V is initialized using an identity matrix. The algorithm first updates W and then V ; typically the step of updating V can converge within 10 iterations and the corresponding class prototypes used for final classification are updated to be u zi V .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Maximum Margin Embedding Recognition</head><p>Once embedding model is learned, recognition in the semantic space can be done in a variety of ways. We explore a simple alternative to classify the testing instance x ⋆ ,</p><formula xml:id="formula_11">z * = argmin i W x * − φ (u i , V, W, x * ) 2 2 .<label>(9)</label></formula><p>Nearest Neighbor (NN) classifier directly measures the distance between predicted semantic vectors with the prototypes in semantic space, i.e., φ (u i , V, W, x * ) = u i V . We further employ the k-nearest neighbors (KNN) of testing instances to average the predictions, i.e., φ (·) is averaging the KNN instances of predicted semantic vectors. 5 Competitors. We compare the following models, <ref type="bibr" target="#b4">5</ref> This strategy is known as Rocchio algorithm in information retrieval. Rocchio algorithm is a method for relevance feedback by using more relevant instances to update the query instances for better recall and possibly precision in vector space (Chap 14 in <ref type="bibr" target="#b27">[28]</ref>). It was first suggested for use on ZSL in <ref type="bibr" target="#b16">[17]</ref>; more sophisticated algorithms <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b33">34]</ref> are also possible. SVM: SVM classifier trained directly on the training instances of source data, without the use of semantic embedding. This is the standard (SUPERVISED) learning setting and the learned classifier can only predict the labels in testing data of source classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>SVR-Map: SVR is used to learn W and the recognition is done in the resulting semantic manifold. This corresponds to only using Eq.(3) to learn W .</p><p>DeVise, ConSE, AMP: To compare with state-of-the-art large-scale zero-shot learning approaches we implement DeViSE <ref type="bibr" target="#b13">[14]</ref> and ConSE <ref type="bibr" target="#b29">[30]</ref> 6 . ConSE uses a multi-class logistic regression classifier for predicting class probabilities of source instances; and the parameter T (number of top-T nearest embeddings for a given instance) was selected from {1, 10, 100, 1000} that gives the best results. ConSE method in supervised setting works the same as SVR. We use the AMP code provided on the author webpage <ref type="bibr" target="#b18">[19]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SS-Voc:</head><p>We test three different variants of our method.</p><p>closed is a variant of our maximum margin leaning of W with the vocabulary-informed constraints only from known classes (i.e., closed set W s ). W corresponds to our full model with maximum margin constraints coming from both W s and W t (or W). We compute W using Eq. <ref type="formula" target="#formula_9">(7)</ref>, but without optimizing V . full further fine-tunes the word vector space by also optimizing V using Eq. <ref type="bibr" target="#b7">(8)</ref>.</p><p>Open set vocabulary. We use google word2vec to learn the open set vocabulary set from a large text corpus of around 7 billion words: UMBC WebBase (3 billion words), the latest Wikipedia articles (3 billion words) and other web documents (1 billion words). Some rare (low frequency) words and high frequency stopping words were pruned in the vocabulary set: we remove words with the frequency &lt; 300 or &gt; 10 million times. The result is a vocabulary of around 310K words/phrases with openness ≈ 1, which is defined as openness = 1 − (2 × |W s |) / (|W|). <ref type="bibr" target="#b37">[38]</ref>. Computational and parameters selection and scalability. All experiments are repeated 10 times, to avoid noise due to small training set size, and we report an average across all runs. For all the experiments, the mean accuracy is reported, i.e., the mean of the diagonal of the confusion matrix on the prediction of testing data. We fix the parameters µ and λ as 0.01 and α = 0.6 in our experiments when only few training instances are available for AwA (5 instances per class) and ImageNet (3 instances per class). Varying values of λ, µ and α leads to &lt; 1% variances on AwA and &lt; 0.2% variances on ImageNet dataset; but the experimental conclusions still hold. Cross-validation is conducted when more training instances are available. A V and B S are set to 5 to balance computational cost and efficiency of pairwise constraints.</p><p>To solve Eq.(8) at a scale, one can use Stochastic Gradient Descent (SGD) which makes great progress initially, but often is slow when approaches a solution. In contrast, the L-BFGS method mentioned above can achieve steady convergence at the cost of computing the full objective and gradient at each iteration. L-BFGS can usually achieve better results than SGD with good initialization, however, is computationally expensive. To leverage benefits of both of these methods, we utilize a hybrid method to solve Eq. <ref type="bibr" target="#b7">(8)</ref> in large-scale datasets: the solver is initialized with few instances to approximate the gradients using SGD first, then gradually more instances are used and switch to L-BFGS is made with iterations. This solver is motivated by Friedlander et al. <ref type="bibr" target="#b12">[13]</ref>, who theoretically analyzed and proved the convergence for the hybrid optimization methods. In practice, we use L-BFGS and the Hybrid algorithms for AwA and ImageNet respectively. The hybrid algorithm can save between 20 ∼ 50% training time as compared with L-BFGS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental results on AwA dataset</head><p>We report AwA experimental results in Tab. 1, which uses 100/1000-dimensional word2vec representation (i.e., d = 100/1000). We highlight the following observations: (1) SS-Voc variants have better classification accuracy than SVM and SVR. This validates the effectiveness of our model. Particularly, the results of our SS-Voc:full are 1.8/2% and 9/10.9% higher than those of SVR/SVM on supervised and zero-shot recognition respectively. Note that though the results of SVM/SVR are good for supervised recognition tasks (52.1 and 51.4/57.1 respectively), we can further improve them, which we attribute to the more dis- Akata et al. <ref type="bibr" target="#b1">[2]</ref> A+W CN N GoogleLeNet 73.9 TMV-BLP <ref type="bibr" target="#b15">[16]</ref> A+W CN N OverFeat 69.9 AMP (SR+SE) <ref type="bibr" target="#b18">[19]</ref> A+W CN N OverFeat 66.0 DAP <ref type="bibr" target="#b24">[25]</ref> A CN N VGG19 57.5 PST <ref type="bibr" target="#b33">[34]</ref> A+W CN N OverFeat 54.1 DAP <ref type="bibr" target="#b24">[25]</ref> A CN N OverFeat 53.2 DS <ref type="bibr" target="#b34">[35]</ref> W/A CN N OverFeat 52.7 Jayaraman et al. <ref type="bibr" target="#b21">[22]</ref> A low-level 48.7 Yu et al. <ref type="bibr" target="#b45">[46]</ref> A low-level 48.3 IAP <ref type="bibr" target="#b24">[25]</ref> A CN N OverFeat 44.5 HEX <ref type="bibr" target="#b8">[9]</ref> A CN N DECAF</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="44.2">AHLE [1]</head><p>A low-level 43.5 <ref type="table">Table 2</ref>.</p><p>Zero-shot comparison on AwA. We compare the state-of-the-art ZSL results using different semantic spaces (S. Sp) including word vector (W) and attribute (A). 1000 dimension word2vec dictionary is used for SS-Voc. (Chance-level =10%). Different types of CNN and hand-crafted low-level feature are used by different methods. Except SS-Voc (200/800), all instances of source data (24295 images) are used for training. As a general reference, the classification accuracy on ImageNet: CN NDECAF &lt; CN NOverFeat &lt; CN NVGG19 &lt; CN NGoogleLeNet. stances / class). Our model achieves 78.3% accuracy, which is remarkably higher than all previous methods. This is particularly impressive taking into account the fact that we use only a semantic space and no additional attribute representations that many other competitor methods utilize. Further, our results with 800 training instances, a small fraction of the 24, 295 instances used to train all other methods, already outperform all other approaches. We argue that much of our success and improvement comes from a more discriminative information obtained using an open set vocabulary and corresponding large margin constraints, rather than from the features, since our method improved 25.1% as compared with DAP <ref type="bibr" target="#b24">[25]</ref> which uses the same Over-Feat features. Note, our SS-Voc:full result is 4.4% higher than the closest competitor <ref type="bibr" target="#b1">[2]</ref>; this improvement is statistically significant. Comparing with our work, <ref type="bibr" target="#b1">[2]</ref> did not only use more powerful visual features (GoogLeNet Vs. OverFeat), but also employed more semantic embeddings (attributes, GloVe 7 and WordNet-derived similarity embeddings as compared to our word2vec).</p><p>Large-scale open set recognition: Here we focus on OPEN-SET 310K setting with the large vocabulary of approximately 310K entities; as such the chance performance of the task is much much lower. In addition, to study the effect of performance as a function of the open vocabulary set, we also conduct two additional experiments with different label sets: (1) OPEN-SET 1K−N N : the 1000 labels from nearest neighbor set of ground-truth class prototypes are selected from the complete dictionary of 310K labels. This corresponds to an open set fine grained recognition; (2) OPEN-SET 1K−RN D : 1000 label names randomly sampled from 310K set. The results are shown in <ref type="figure" target="#fig_2">Fig. 2</ref>. Also note that we did not fine-tune the word vector space (i.e., V is an Identity matrix) on OPEN-SET 310K setting since Eq (8) can optimize a better visual discriminability only on a relative small subset as compared with the 310K vocabulary. While our OPEN-SET variants do not assume that test data comes from either source/auxiliary domain or target domain, we split the two cases to mimic SUPERVISED and ZERO-SHOT scenarios for easier analysis.</p><p>On SUPERVISED-like setting, <ref type="figure" target="#fig_2">Fig. 2</ref> (left), our accuracy is better than that of SVR-Map on all the three different label sets and at all hit rates. The better results are largely due to the better embedding matrix W learned by enforcing maximum margins between training class name and open set vocabulary on source training data.</p><p>On ZERO SHOT-like setting, our method still has a notable advantage over that of SVR-Map method on Top-k (k &gt; 5) accuracy, again thanks to the better embedding W learned by Eq. <ref type="bibr" target="#b6">(7)</ref>. However, we notice that our top-1 accuracy on ZERO SHOT-like setting is lower than SVR-Map method. We find that our method tends to label some instances from target data with their nearest classes from within source label set. For example, "humpback whale" from testing data is more likely to be labeled as "blue whale". However, when considering Top-k (k &gt; 5) accuracy, our method still has advantages over baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Experimental results on ImageNet dataset</head><p>We further validate our findings on large-scale ImageNet 2012/2010 dataset; 1000-dimensional word2vec representation is used here since this dataset has larger number of classes than AwA. We highlight that our results are still better than those of two baselines -SVR-Map and SVM on (SUPERVISED) and (ZERO-SHOT) settings respectively as shown in Tab. <ref type="bibr" target="#b2">3</ref>. The open set image recognition results are shown in <ref type="figure" target="#fig_4">Fig. 4</ref>. On both SUPERVISED-like and ZERO-SHOT-like settings, clearly our framework still has advantages over the baseline which directly matches the nearest neighbors from the vocabulary by using predicted semantic word vectors of each testing instance.</p><p>We note that SUPERVISED SVM results (34.61%) on Im-ageNet are lower than 63.30% reported in <ref type="bibr" target="#b6">[7]</ref>, despite us- Open set recognition results on AwA dataset: Openness=0.9839. Chance=3.2e − 4%. Ground truth label is extended for its variants. For example, we count a correct label if a 'pig' image is labeled as 'pigs'. ⋆, †:different 1000 label settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Testing Classes</head><p>ing the same features. This is because only few, 3 samples per class, are used to train our models to mimic human performance of learning from few examples and illustrate ability of our model to learn with little data. However, our semi-supervised vocabulary-informed learning can improve the recognition accuracy on all settings. On open set image recognition, the performance has dropped from 37.12% (SUPERVISED) and 8.92% (ZERO-SHOT) to around 9% and 1% respectively <ref type="figure" target="#fig_4">(Fig. 4)</ref>. This drop is caused by the intrinsic difficulty of the open set image recognition task (≈ 300× increase in vocabulary) on a large-scale dataset. However, our performance is still better than the SVR-Map baseline which in turn significantly better than the chance-level.</p><p>We also evaluated our model with larger number of training instances (&gt; 3 per class). We observe that for standard supervised learning setting, the improvements achieved using vocabulary-informed learning tend to somewhat diminish as the number of training instances substantially grows. With large number of training instances, the mapping between low-level image features and semantic words, g(x), becomes better behaved and effect of additional constraints, due to the open-vocabulary, becomes less pronounced.</p><p>Comparing to state-of-the-art on ZSL. We compare our results to several state-of-the-art large-scale zero-shot recognition models. Our results, SS-Voc:full, are better than those of ConSE, DeViSE and AMP on both T-1 and T-5 metrics with a very significant margin (improvement over best competitor, ConSE, is 3.43 percentage points or nearly 62% with 3, 000 training samples). Poor results of DeViSE with 3, 000 training instances are largely due to the inefficient learning of visual-semantic embedding matrix. AMP algorithm also relies on the embedding matrix from DeViSE, which explains similar poor performance of AMP   with 3, 000 training instances. In contrast, our SS-Voc:full can leverage discriminative information from open vocabulary and max-margin constraints, which helps improve performance. For DeViSE with all ImageNet instances, we confirm the observation in <ref type="bibr" target="#b29">[30]</ref> that results of ConSE are much better than those of DeViSE. Our results are a further significant improved from ConSE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Qualitative results of open set image recognition</head><p>t-SNE visualization of AwA 10 target testing classes is shown in <ref type="figure">Fig. 3</ref>. We compare our SS-Voc:full with SS-Voc:closed and SVR. We note that (1) the distributions of 10 classes obtained using SS-Voc are more centered and more separable than those of SVR (e.g., rat, persian cat and pig), due to the data and pairwise maximum margin terms that help improve the generalization of g (x) learned;</p><p>(2) the distribution of different classes obtained using the full model SS-Voc:full are also more separable than those of SS-Voc:closed, e.g., rat, persian cat and raccoon. This can be attributed to the addition of the open-vocabularyinformed constraints during learning of g (x), which further improves generalization. <ref type="bibr">For</ref>   <ref type="table">Table 4</ref>. ImageNet comparison to state-of-the-art on ZSL: We compare the results of using 3, 000/all training instances for all methods; T-1 (top 1) and T-5 (top 5) classification in % is reported. set recognition example image of "persian cat", which is wrongly classified as a "hamster" by SS-Voc:closed.</p><p>Partial illustration of the embeddings learned for the Im-ageNet2012/2010 dataset are illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>, where 4 source/auxiliary and 2 target/zero-shot classes are shown. Again better separation among classes is largely attributed to open-set max-margin constraints introduced in our SS-Voc:full model. Additional examples of miss-classified instances are available in the supplemental material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion and Future Work</head><p>This paper introduces the problem of semi-supervised vocabulary-informed learning, by utilizing open set semantic vocabulary to help train better classifiers for observed and unobserved classes in supervised learning, ZSL and open set image recognition settings. We formulate semisupervised vocabulary-informed learning in the maximum margin framework. Extensive experimental results illustrate the efficacy of such learning paradigm. Strikingly, it achieves competitive performance with only few training instances and is relatively robust to large open set vocabulary of up to 310, 000 class labels.</p><p>We rely on word2vec to transfer information between observed and unobserved classes. In future, other linguistic or visual semantic embeddings could be explored instead, or in combination, as part of vocabulary-informed learning.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Illustration of the semantic embeddings learned (left) using support vector regression (SVR) and (right) using the proposed semi-supervised vocabulary-informed (SS-Voc) approach.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>criminative classification boundary informed by the vocabulary. (2) SS-Voc:W significantly, by up to 8.1%, improves zero-shot recognition results of SS-Voc:closed. This validates the importance of information from open vocabulary. (3) SS-Voc benefits more from open set vocabulary as compared to word vector space fine-tuneing. The results of supervised and zero-shot recognition of SS-Voc:full are 1/0.9% and 2.5/8.6% higher than those of SS-Voc:closed.Comparing to state-of-the-art on ZSL: We compare our results with the state-of-the-art ZSL results on AwA dataset in Tab. 2. We compare SS-Voc:full trained with all source instances, 800 (20 instances / class), and 200 instances (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Map(OPEN SET − 1K−NN) SVR−Map(OPEN SET − 1K−RND) SVR−Map (OPEN SET − 310K) SS-Voc:W (OPEN SET − 1K−NN) SS-Voc:W (OPEN SET − 1K−RND) SSoVoc:W (OPEN SET − 310K)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Open set recognition results on ImageNet 2012/2010 dataset: Openness=0.9839. Chance=3.2e − 4%. We use the synsets of each class-a set of synonymous (word or prhase) terms as the ground truth names for each instance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>|W s | = 40) and 10 target/test classes (|W t | = 10) is introduced. We follow this split for supervised and zeroshot learning. We use OverFeat features (downloaded from<ref type="bibr" target="#b18">[19]</ref>) on AwA to make the results more easily comparable to state-of-the-art. ImageNet 2012/2010 dataset is a large-scale dataset. We use 1000 (|W s | = 1000) classes of ILSVRC 2012 as the source/auxiliary classes and 360</figDesc><table>Datasets. We conduct our experiments on Animals with At-
tributes (AwA) dataset, and ImageNet 2012/2010 dataset. 
AwA consists of 50 classes of animals (30, 475 images in 
total). In [25] standard split into 40 source/auxiliary classes 
((|W t | = 360) classes of ILSVRC 2010 that are not used 
in ILSVRC 2012 as target data. We use pre-trained VGG-
19 model [7] to extract deep features for ImageNet. On 
both dataset, we use few instances from source dataset to 
mimic human performance of learning from few examples 
and ability to generalize. 

Recognition tasks. We consider three different settings in 
a variety of experiments (in each experiment we carefully 
denote which setting is used): 
SUPERVISED recognition, where learning is on source 
classes and we assume test instances come from same 
classes with W s as recognition vocabulary; 

ZERO-SHOT recognition, where learning is on source 
classes and we assume test instances coming from tar-
get dataset with W t as recognition vocabulary; 

OPEN-SET recognition, where we use entirely open vocab-
ulary with |W| ≈ 310K and use test images from both 
source and target splits. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>Table 1. Classification accuracy (%) on AwA dataset for SUPERVISED and ZERO-SHOT settings for 100/1000-dim word2vec representation.</figDesc><table>Testing Classes 

SS-Voc 
Aux 
Targ. 
Total 
Vocab 
Chance 
SVM 
SVR 
closed 
W 
full 
SUPERVISED 
40 
40 
2.5 
52.1 
51.4/57.1 
52.9/58.2 53.6/58.6 53.9/59.1 
ZERO-SHOT 
10 
10 
10 
-
52.1/58.0 
58.6/60.3 59.5/68.4 61.1/68.9 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head></head><label></label><figDesc>SS-Voc:full : persian_cat, siamese_cat, hamster, weasel, rabbit, monkey, zebra, owl, anthropomorphized, cat SS-Voc:closed: hamster, persian_cat, siamese_cat, rabbit, monkey, weasel, squirrel, anteater, cat, stuffed_toy SVR-Map: hamster, squirrel, rabbit, raccoon, kitten, siamese_cat, stuffed_toy, persian_cat, ladybug, puppyFigure 3. t-SNE visualization of AwA 10 testing classes. Please refer to Supplementary material for larger figure. . The classification accuracy (%) of ImageNet 2012/2010 dataset on SUPERVISED and ZERO-SHOT settings.</figDesc><table>−20 

0 

20 

40 

60 

80 

100 
−100 
−80 
−60 
−40 
−20 
0 
20 
40 
60 
80 
100 

SVR-Map 

−120 −100 
−80 
−60 
−40 
−20 
0 
20 
40 
60 
80 
−100 

−50 

0 

50 

100 

150 

SS-Voc:full 

persian cat 
hippopotamus 
leopard 
humpback whale 
seal 
chimpanzee 
rat 
giant panda 
pig 
raccoon 

−50 

0 

50 

100 

−100 
−80 
−60 
−40 
−20 
0 
20 
40 
60 
80 
100 

SS-Voc: closed 

Testing Classes 
SS-Voc 
Aux 
Targ. 
Total 
Vocab 
Chance 
SVM 
SVR 
closed 
W 
full 
SUPERVISED 
1000 
1000 
0.1 
33.8 
25.6 
34.2 
36.3 
37.1 
ZERO-SHOT 
360 
360 
0.278 
-
4.1 
8.0 
8.2 
8.9 
Table 3Testing Classes 
ImageNet Data Aux. 
Targ. 
Total 
Vocab 
OPEN-SET 310K 
(left) (right) 1000 / 360 310K 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head></head><label></label><figDesc>example, we show an open</figDesc><table>Methods 
S. Sp 
Feat. 
T-1 
T-5 

SS-Voc:full 

W 
CN NOverFeat 
8.9/9.5 
14.9/16.8 
ConSE [30] 
W 
CN NOverFeat 
5.5/7.8 
13.1/15.5 
DeViSE [14] 
W 
CN NOverFeat 
3.7/5.2 
11.8/12.8 
AMP [19] 
W 
CN NOverFeat 
3.5/6.1 
10.5/13.1 
Chance 
-
-
2.78e-3 
-
</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">We found Eq.<ref type="bibr" target="#b1">(2)</ref> and Eq.(3) have similar results, on average, but formulation in Eq.(3) is more stable and has lower variance.<ref type="bibr" target="#b3">4</ref> Crammer and Singer loss<ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b7">8]</ref> is the upper bound of Eq (4) and (5) which we use to tolerate variants of uz i (e.g. 'pigs' Vs. 'pig' inFig. 2) and thus are better for our tasks.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">Code for<ref type="bibr" target="#b13">[14]</ref> and<ref type="bibr" target="#b29">[30]</ref> is not publicly available.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">GloVe<ref type="bibr" target="#b32">[33]</ref> can be taken as an improved version of word2vec.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Labelembedding for attribute-based classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Evaluation of output embeddings for fine-grained image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Walter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Cross-generalization: learning novel classes from a single example by feature replacement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Bart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ullman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Towards open world recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bendale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Boult</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Recognition by components -a theory of human image understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Biederman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Learning distributed word representations for natural logic reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno>abs/1410.4176</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Return of the devil in the details: Delving deep into convolutional nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chatfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">On the algorithmic implementation of multiclass kernel-based vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Large-scale object classification using label relation graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Neven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Describing objects by their attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Endres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">One-shot learning of object categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Hybrid deterministicstochastic methods for data fitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Friedlander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schmidt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Scientific Computing</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">DeViSE: A deep visualsemantic embedding model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Attribute learning for understanding unstructured social activity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Transductive multi-view embedding for zero-shot recognition and annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning multi-modal latent attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Transductive multi-view zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">zero-shot object recognition by semantic manifold distance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kodirov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Open-vocabulary object retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rodner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Farrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics Science and Systems (RSS)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A unified semantic embedding: relating taxonomies and attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Zero shot recognition with unreliable attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jayaraman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Attribute and simile classifiers for face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">N</forename><surname>Belhumeur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Attributebased classification for zero-shot visual object categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nickisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harmeling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Zero-data learning of new tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">ǫ-SSVR: A smooth support vector machine for ǫ-insensitive regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-F</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-M</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TKDE</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Introduction to Information Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schutze</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Zero-shot learning by convex combination of semantic embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Zero-shot learning with semantic output codes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Palatucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pomerleau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Relative attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Transfer learning in a transductive setting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">What helps where -and why? semantic relatedness for knowledge transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Szarvas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gurevych</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Prediction of search targets from fixations in open-world settings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sattar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bulling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Probability models for open set recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Scheirer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">P</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">E</forename><surname>Boult</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Towards open set recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Scheirer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rocha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sapkota</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">E</forename><surname>Boult</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Zero-shot learning through cross-modal transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ganjoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bastani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">80 million tiny images: A large data set for nonparametric object and scene recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Using the forest to see the trees: Exploiting context for visual object detection and localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">P</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Large margin methods for structured and interdependent output variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Tsochantaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Altun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Efficient additive kernels via explicit feature maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE TPAMI</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Wsabie: Scaling up to large vocabulary image annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Harnessing object and scene semantics for large-scale video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Designing category-level attributes for discriminative visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Solving large scale linear prediction problems using stochastic gradient descent algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
