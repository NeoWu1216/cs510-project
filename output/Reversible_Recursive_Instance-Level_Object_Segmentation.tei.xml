<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Reversible Recursive Instance-level Object Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Sun Yat-sen University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Adobe Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zequn</forename><surname>Jie</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Sun Yat-sen University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">360 AI Institute</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Reversible Recursive Instance-level Object Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:45+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this work, we propose a novel Reversible Recursive Instance-level Object Segmentation (R2-IOS) framework to address the challenging instance-level object segmentation task. R2-IOS consists of a reversible proposal refinement sub-network that predicts bounding box offsets for refining the object proposal locations, and an instance-level segmentation sub-network that generates the foreground mask of the dominant object instance in each proposal. By being recursive, R2-IOS iteratively optimizes the two subnetworks during joint training, in which the refined object proposals and improved segmentation predictions are alternately fed into each other to progressively increase the network capabilities. By being reversible, the proposal refinement sub-network adaptively determines an optimal number of refinement iterations required for each proposal during both training and testing. Furthermore, to handle multiple overlapped instances within a proposal, an instance-aware denoising autoencoder is introduced into the segmentation sub-network to distinguish the dominant object from other distracting instances. Extensive experiments on the challenging PASCAL VOC 2012 benchmark well demonstrate the superiority of R2-IOS over other state-of-the-art methods. In particular, the AP r over 20 classes at 0.5 IoU achieves 66.7%, which significantly outperforms the results of 58.7% by PFN [17]  and 46.3% by <ref type="bibr" target="#b21">[22]</ref>.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recently, beyond the traditional object detection <ref type="bibr" target="#b6">[7]</ref> <ref type="bibr" target="#b28">[29]</ref>[5] <ref type="bibr" target="#b14">[15]</ref>[4] <ref type="bibr" target="#b29">[30]</ref> and semantic segmentation tasks <ref type="bibr" target="#b0">[1]</ref>[23] <ref type="bibr" target="#b1">[2]</ref>[37] <ref type="bibr" target="#b18">[19]</ref> <ref type="bibr" target="#b17">[18]</ref> <ref type="bibr" target="#b20">[21]</ref>[20] <ref type="bibr" target="#b13">[14]</ref> <ref type="bibr" target="#b15">[16]</ref>, instancelevel object segmentation has attracted much attention <ref type="bibr" target="#b9">[10]</ref> <ref type="bibr" target="#b10">[11]</ref> <ref type="bibr" target="#b21">[22]</ref>[31] <ref type="bibr" target="#b35">[36]</ref> <ref type="bibr" target="#b16">[17]</ref>. It aims at joint object detection and semantic segmentation, and requires the pixel-wise semantic labeling for each object instance in the image. Therefore, it is very challenging for existing computer vision techniques since instances of a semantic * Corresponding author is Liang Lin (E-mail: linliang@ieee.org). This work was done when the first author worked as an intern at NUS. category may present arbitrary scales, various poses, heavy occlusion or obscured boundaries. Most of the recent advances <ref type="bibr" target="#b9">[10]</ref>[11] <ref type="bibr" target="#b21">[22]</ref> in instancelevel object segmentation are driven by the rapidly developing object proposal methods <ref type="bibr" target="#b26">[27]</ref> <ref type="bibr" target="#b33">[34]</ref>. A typical pipeline of solving this task starts with an object proposal generation method and then resorts to tailored Convolutional Neural Networks (CNN) architectures <ref type="bibr" target="#b12">[13]</ref>[32] <ref type="bibr" target="#b32">[33]</ref> and postprocessing steps (e.g. graphical inference <ref type="bibr" target="#b21">[22]</ref>). As a result, the network training and the accuracy of segmentation results are largely limited by the quality of object proposals generated by existing methods. Some efforts have been made in refining the object proposals by bounding box regressions <ref type="bibr" target="#b6">[7]</ref> <ref type="bibr" target="#b28">[29]</ref> and iterative localizations <ref type="bibr" target="#b4">[5]</ref> during testing. However, their strategies did not explicitly utilize additional information such as more fine-grained segmentation masks during training to boost the network capability. Intuitively, object proposal refinement and proposal-based segmentation should be jointly tackled as they are complementary to each other. Specifically, the semantic category information and pixel-wise semantic labeling can provide more high-level cues and local details to learn more accurate object proposal localizations, while the refined object proposals with higher recall rates would naturally lead to more accurate segmentation masks with an improved segmentation network. In addition, as illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>, different object proposals may require different extent of refinement depending on their initial localization precision and interactions with neighboring objects. Therefore the recursive refinement should be able to adaptively determine the optimal number of iterations for each proposal as opposed to performing a fixed number of iterations for all the proposals as in those previous methods.</p><p>Motivated by the above observations, in this work we propose a novel Reversible Recursive framework for Instance-level Object Segmentation (R2-IOS). R2-IOS integrates the instance-level object segmentation and object proposal refinement into a unified framework. Inspired by the recent success of recurrent neural network on visual attention <ref type="bibr" target="#b24">[25]</ref> <ref type="bibr" target="#b34">[35]</ref>, our R2-IOS updates instance-level segmentation results and object proposals by exploiting the previous information recursively. As illustrated in <ref type="figure">Figure 2</ref>, the instance-level segmentation sub-network produces the foreground mask of the dominant object in each proposal, while the proposal refinement sub-network predicts the confidences for all semantic categories as well as the bounding box offsets for refining the object proposals. To make the two sub-networks complementary to each other, the rich information in pixel-wise segmentation is utilized to update the proposal refinement sub-network by constructing a powerful segmentation-aware feature representation. The object proposals are therefore refined given the inferred bounding box offsets by the updated sub-networks and the previous locations, which are in turn fed into the two sub-networks for further updating. R2-IOS can be conveniently trained by back-propagation after unrolling the sub-networks <ref type="bibr" target="#b25">[26]</ref> and sharing the network parameters across different iterations.</p><p>To obtain a better refined bounding box for each proposal, the proposal refinement sub-network adaptively determines the number of iterations for refining each proposal in both training and testing, which is in spirit similar to the early stopping rules for iteratively training large networks <ref type="bibr" target="#b5">[6]</ref>. R2-IOS first recursively refines the proposal for all iterations, and then the optimal refinement iteration is determined where the highest category-level confidence is obtained across all iterations. The final results of the proposal can thus be obtained by reversing towards the results of the optimal iteration number. The optimization of the proposal will be stopped at the optimal iteration during training, and similarly the generated results in that iteration will be regarded as the final outputs during testing.</p><p>One major challenge in proposal-based instance segmentation methods is that there might be multiple overlapped objects, in many cases belonging to the same category and sharing similar appearance, in a single proposal. It is critical to correctly extract the mask of the dominant object with clear instance-level boundaries in such a proposal in order to achieve good instance-level segmentation performance. To handle this problem, a complete view of the whole proposal region becomes very important. In this work, an instance-aware denoising autoencoder embedded in the segmentation sub-network is proposed to gather global information to generate the dominant foreground masks, in which the noisy outputs from other distracting objects are largely reduced. The improved segmentation masks can accordingly further help update the proposal refinement subnetwork during our recursive learning.</p><p>The main contributions of the proposed R2-IOS can be summarized as: 1) To the best of our knowledge, our R2-IOS is the first research attempt to recursively refine object proposals based on the integrated instance-level segmentation and reversible proposal refinement sub-networks for instance-level object segmentation during both training and testing. 2) A novel reversible proposal refinement sub-network adaptively determines the optimal number of recursive refinement iterations for each proposal.</p><p>3) The instance-aware denoising autoencoder in the segmentation sub-network can generate more accurate foreground masks of dominant instances through global inference. 4) Extensive experiments on the PASCAL VOC 2012 benchmark demonstrate the effectiveness of R2-IOS which advances the state-of-the-art performance from 58.7% to 66.7%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Object Detection. Object detection aims to recognize and localize each object instance with a bounding box. Generally, most of the detection pipelines <ref type="bibr" target="#b28">[29]</ref>[7][5] <ref type="bibr" target="#b14">[15]</ref>[8] begin with producing object proposals from the input image, and then the classification and the bounding box regression are performed to identify the target objects. Many hand-designed approaches such as selective search <ref type="bibr" target="#b33">[34]</ref>, Edge Boxes <ref type="bibr" target="#b37">[38]</ref> and MCG <ref type="bibr" target="#b27">[28]</ref>, or CNN-based methods such as DeepMask <ref type="bibr" target="#b26">[27]</ref> and RPN <ref type="bibr" target="#b28">[29]</ref> have been proposed for object proposal extraction. Those detection approaches often treat the proposal generation and object detection as two separate techniques, yielding suboptimal results. In contrast, the proposed R2-IOS adaptively learns the optimal number of refinement iterations for each object proposal. Meanwhile, the reversible proposal refinement and instance-level segmentation sub-networks are jointly trained to mutually boost each other.  <ref type="bibr" target="#b16">[17]</ref> have developed algorithms on the challenging instance-level object segmentation. Most of these works take the object proposal methods as the prerequisite. For instance, Hariharan et al. <ref type="bibr" target="#b9">[10]</ref> proposed a joint framework for both object detection and instance-level segmentation. Founded on <ref type="bibr" target="#b9">[10]</ref>, complex post-processing methods, i.e. category-specific inference and shape prediction, were proposed by Chen et al. <ref type="bibr" target="#b21">[22]</ref> to further boost the segmentation performance. In contrast to these previous works that use fixed object proposals based on a single-pass  <ref type="figure">Figure 2</ref>. Detailed architecture of the proposed R2-IOS. R2-IOS recursively produces better object proposals to boost the capabilities of the instance-level segmentation sub-network and the reversible proposal refinement sub-network. The whole image is first fed into several convolutional layers to generate its feature maps. Then these feature maps along with the initial object proposals are passed into the two sub-networks to generate the confidences of all categories, the bounding box offsets, and the dominant foreground masks for all proposals. The ROI pooling layer extracts feature maps with fixed resolution to process proposals with diverse scales. The instance-aware denoising autoencoder in the segmentation sub-network then produces the foreground mask of the dominant object instance within each proposal. The two sub-networks can interact with each other by using the concatenated segmentation-aware features and refined proposals. In each iteration, the bounding box offsets are predicted by the updated sub-networks and then used to refine the object proposals for more precise instance-level segmentation. The optimal iteration number of recursive refinement is automatically determined for each proposal.</p><p>feed-forward scheme, the proposed R2-IOS recursively refines the bounding boxes of object proposals in each iteration. In addition, we proposed a new instance-level segmentation sub-network with an embedded instance-aware denoising autoencoder to better individualize the instances.</p><p>There also exist some works <ref type="bibr" target="#b35">[36]</ref>[17] that are independent of the object proposals and directly predict object-level masks. Particularly, Liang et al. <ref type="bibr" target="#b16">[17]</ref> predicted the instance numbers of different categories and the pixel-level coordinates of the object to which each pixel belongs. However, their performance is limited by the accuracy of instance number prediction, which is possibly low for cases with small objects. On the contrary, our R2-IOS can predict category-level confidences and segmentation masks for all the refined proposals, and better covers small objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Reversible Recursive Instance-level Object Segmentation (R2-IOS) Framework</head><p>As shown in <ref type="figure">Figure 2</ref>, built on the VGG-16 ImageNet model <ref type="bibr" target="#b31">[32]</ref>, R2-IOS takes an image and initial object proposals as inputs. An image first passes serveral convolutional layers and max pooling layers to generate its convolutional feature maps. Then the segmentation and reversible proposal refinement sub-networks take the feature maps as inputs, and their outputs are combined to generate instance-level segmentation results. To get the initial object proposals, the selective search method <ref type="bibr" target="#b33">[34]</ref> is used to extract around 2,000 object proposals in each image. In the following, we explain the key components of R2-IOS, including the instance-level segmentation sub-network, reversible proposal refinement sub-network, recursive learning and testing phase in more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Instance-level Segmentation Sub-network</head><p>Sub-network Structure. The structure of the segmentation sub-network is built upon the VGG-16 model <ref type="bibr" target="#b31">[32]</ref>. The original VGG-16 includes five max pooling layers. To retain more local details, we remove the last two max pooling layers in the segmentation sub-network. Following the common practice in semantic segmentation <ref type="bibr" target="#b23">[24]</ref>[1], we replace the last two fully-connected layers in VGG-16 with two fully-convolutional layers in order to obtain convolutional feature maps for the whole image. Padding is added when necessary to keep the resolution of feature maps. Then the convolutional feature maps of each object proposal pass through a region of interest (ROI) pooling layer <ref type="bibr" target="#b6">[7]</ref> to extract fixed-scale feature maps (40 × 40 in our case) for each proposal. Several 1 × 1 convolutional filters are then applied to generate confidence maps C for foreground and background classes. An instance-aware autoencoder is further appended to extract global information contained in the whole convolutional feature maps to infer the foreground mask of the dominant object within the object proposal.</p><p>Instance-aware Denoising Autoencoder. In real-world images, multiple overlapping object instances (especially those with similar appearances and in the same category) may appear in an object proposal. In order to obtain good instance-level segmentation results, it is very critical to segment out the dominant instance with clear instance-level boundaries and remove the noisy masks of other distracting instances for a proposal. Specifically, when an object proposal contains multiple object instances, we regard the mask of the object that has the largest overlap with the proposal bounding box as the dominant foreground mask. For example, in <ref type="figure">Figure 2</ref>, there are three human instances in-cluded in the given proposal (red rectangle). Apparently the rightmost person is the dominant instance in that proposal. We thus would like the segmentation sub-network to generate a clean binary mask over that instance as shown in <ref type="figure">Figure 2</ref>. Such appropriate pixel-wise prediction requires a global perspective on all the instances in the proposal to determine which instance is the dominant one. However, traditional fully-convolutional layers can only capture local information which makes it difficult to differentiate instances of the same category. To close this gap, R2-IOS introduces an instance-aware denoising autoencoder to gather global information from confidence maps C to accurately identify the dominant foreground mask within each proposal.</p><p>Formally, we vectorize C to a long vector ofC with a dimension of 40 × 40 × 2. Then the autoencoder takesC as the input and maps it to a hidden representation h = Φ(C), where Φ(·) denotes a non-linear operator. The produced hidden representation h is then mapped back (via a decoder) to a reconstructed vector v as v = Φ ′ (h). The compact hidden representation extracts global information based on the predictions from convolutional layers in the encoder, which guides the reconstruction of a denoised foreground mask of the dominant instance in the decoder. In our implementation, we use two fully connected layers along with ReLU non-linear operators to approximate the operators Φ and Φ ′ . The number of output units in the fully-connected layer for Φ is set as 512 and that of the fully-connected layer for Φ ′ is set as 3200. Finally the denoised prediction of v is reshaped to a map with the same size as C. A pixel-wise cross-entropy loss on v is employed to train the instancelevel segmentation sub-network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Reversible Proposal Refinement Sub-network</head><p>Sub-network Structure. The structure of the proposal refinement sub-network is built upon the VGG-16 model <ref type="bibr" target="#b31">[32]</ref>. Given an object proposal, the proposal refinement sub-network aims to refine the category recognition and the bounding box locations of the object, and accordingly generates the confidences over K + 1 categories, including K semantic classes and one background class, as well as the bounding-box regression offsets. Following the detection pipeline in Fast-RCNN <ref type="bibr" target="#b6">[7]</ref>, an ROI pooling layer is added to generate feature maps with a fixed size of 7 × 7. The maps are then fed into two fully-connected layers. Different from Fast R-CNN <ref type="bibr" target="#b6">[7]</ref>, segmentation-aware features are constructed to incorporate guidance from the pixel-wise segmentation information to predict the confidences and bounding box offsets of the proposal, as indicated by the dashed arrow in <ref type="figure">Figure 2</ref>. The foreground mask of the dominant object in each proposal can help better depict the boundaries of the instances, leading to better localization and categorization of each proposal. Thus, connected by segmentation-aware features and recursively re-fined proposals, the segmentation and proposal refinement sub-networks can be jointly optimized and benefit each other during training. Specifically, the segmentation-aware features are obtained by concatenating the confidence maps v from the instance-aware autoencoder with the features from the last fully-connected layer in the proposal refinement sub-network. Two output layers are then appended to these segmentation-aware features to predict category-level confidences and bounding-box regression offsets. The parameters of these predictors are optimized by minimizing soft-max loss and smooth L 1 loss <ref type="bibr" target="#b6">[7]</ref>.</p><p>Optimal Iteration Number. The best bounding box of each object proposal and consequently the most accurate segmentation mask may be generated at different iterations of R2-IOS during training and testing, depending on the accuracy of its initial bounding box and the interactions with other neighboring or overlapped instances. In the t-th iteration where t ∈ {1, . . . , T }, the r t is therefore introduced to determine the optimal number of refinement iterations performed for each proposal. While we can check the convergence of predicted bounding box offsets in each iteration, in practice we found that the predicted confidence of the semantic category is an easier and better indicator of the quality of each proposal. All the optimal iteration numbers are initialized with 0 which means an inactivated state. After performing all the T iterations for refining each proposal, the iteration with the highest category-level confidence score is regarded as the optimal iteration t ′ . Its corresponding r t ′ is then activated. Accordingly, we adopt the refinement results of the proposal at the t ′ -th iteration as the final results. We apply the optimal iteration number in both training and testing. During training, only the losses of this proposal in the first t ′ iterations are used for updating the parameters of the unrolled sub-networks, while the losses in the rest iterations are discarded.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Recursive Learning</head><p>The recursive learning seamlessly integrates instancelevel object segmentation and object proposal refinement into a unified framework. Specifically, denote the initial object proposal as l 0 where l 0 = (l x , l y , l w , l h ) contains the pixel coordinates of the center, width and height of the proposed bounding box. Assume each object proposal is labeled with its ground-truth location of the boundingbox, denoted asl = (l x ,l y ,l w ,l h ). In the t-th iteration, the bounding box location of the input proposal is denoted as l t−1 , produced by the two sub-networks in the (t − 1)-th iteration. After passing the input image I and the object proposal l t−1 into two sub-networks, the proposal refinement sub-network generates the predicted bounding box offsets o t,k = (o x t,k , o y t,k , o w t,k , o h t,k ) for each of the K object classes, and the category-level confidences p t = (p t,0 , . . . , p t,K ) for K + 1 categories. The ground-truth bounding box offsetsõ t are transformed asõ t = f l (l t−1 ,l).</p><p>We use the transformation strategy f l (·) given in <ref type="bibr" target="#b7">[8]</ref> to computeõ t , in whichõ t specifies a scale-invariant translation and log-space height/width shift relative to each object proposal. The segmentation sub-network generates the predicted foreground mask of the dominant object in the proposal as v t . We denote the associated ground-truth dominant foreground mask for the proposal asṽ t .</p><p>We adopt the following multi-loss J t for each object proposal to jointly train the instance-level segmentation subnetwork and the proposal refinement sub-network as</p><formula xml:id="formula_0">Jt = J cls (pt, g) + 1[g ≥ 1]J loc (ot,g,õt) + 1[g ≥ 1]Jseg(vt,ṽt),<label>(1)</label></formula><p>where J cls = − log p t,g is the log loss for the ground truth class g, J loc is a smooth L 1 loss proposed in <ref type="bibr" target="#b6">[7]</ref> and J seg is a pixel-wise cross-entropy loss. The indicator function 1[g ≥ 1] equals 1 when g ≥ 1 and 0 otherwise. For proposals that only contain background (i.e. g = 0), J loc and J seg are set to be 0. Following <ref type="bibr" target="#b6">[7]</ref>, only the object proposals that have at least 0.5 intersection over union (IoU) overlap with a ground-truth bounding box are labeled with a foreground object class, i.e. g ≥ 1. The remaining proposals are deemed as background samples and labeled with g = 0. The refined bounding box l t of the proposal can be calculated as f l −1 (l t−1 , o t,g ), where f l −1 (·) represents the inverse operation of f l (·) to calculate the refined bounding box given l t−1 and o t,g . Note that our R2-IOS adaptively adopts the results obtained by performing different number of refinement iterations for each proposal. If the optimal iteration number is the t ′ -th iteration as described in Sec. 3.2, the final refinement results for the proposal will be reversed towards the results of t ′ -th iteration. Thus R2-IOS updates the network parameters by adaptively minimizing the different number of multi-loss J t in Eqn. (1) for each proposal. The global loss of the proposal to update the networks is accordingly computed as J = t≤t ′ J t . R2-IOS can thus specify different number of iterations for each proposal to update the network capability and achieve better instance-level segmentation results. During training, it requires a reliable start of the prediction of category-level confidences for each proposal to produce the optimal iteration number for the refinement. We therefore first train the network parameters of R2-IOS without using the optimal iteration number in which the results after performing all T iterations of the refinement are adopted for all proposals. Then our complete R2-IOS is fine-tuned on these pre-trained network parameters by using the optimal iteration numbers for all proposals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Testing</head><p>R2-IOS first takes the whole image and the initial object proposals with locations l 0 as the input, and recursively passes them into the proposal refinement and segmentation sub-networks. In the t-th iteration, based on the confidence <ref type="table">Table 1</ref>. Comparison of instance-level segmentation performance with two state-of-the-arts using mean AP r metric over 20 classes at 0.5 and 0.7 IoU, when evaluated with the ground-truth annotations from SBD dataset. All numbers are in %. scores p t of all categories, the category for each proposalĝ t is predicted by taking the maximum of the p t . For the proposals predicted as background, the locations of proposals are not updated. For the remaining proposals predicted as a specific object class, the locations of object proposals l t are refined by the predicted offsets o t,ĝt and previous location l t−1 . Based on the predicted confidence scores p t,ĝt of the refined proposal in all T iterations, the optimal number of refinement iterations for each proposal can be accordingly determined. We denote the optimal number of refinement iterations of each proposal as t ′ . The final outputs for each object proposal can be reversed towards the results at the t ′th iteration, including the predicted categoryĝ t ′ , the refined locations l t ′ and the dominant foreground mask v t ′ . The final instance-level segmentation results can be accordingly generated by combining the outputs of all proposals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Settings</head><p>Dataset and Evaluation Metrics. To make fair comparison with four state-of-the-art methods <ref type="bibr" target="#b16">[17]</ref> [10] <ref type="bibr" target="#b21">[22]</ref> [11], we evaluate the proposed R2-IOS framework on the PAS-CAL VOC 2012 validation segmentation benchmark <ref type="bibr" target="#b2">[3]</ref>. For comparing with <ref type="bibr" target="#b10">[11]</ref>, we evaluate the performance on VOC 2012 main validation set, including 5732 images. The comparison results are reported in <ref type="table">Table 1</ref>. For comparing with <ref type="bibr" target="#b16">[17]</ref>  <ref type="bibr" target="#b21">[22]</ref>, the results are evaluated on VOC 2012 segmentation validation set, including 1449 images, and reported in <ref type="table" target="#tab_3">Table 2 and Table 3</ref>. Note that, VOC 2012 provides very elaborated segmentation annotations for each instance (e.g. carefully labeled skeletons for a bicycle) while SBD just gives the whole region (e.g. rough region for a bicycle). Since Chen et al. <ref type="bibr" target="#b21">[22]</ref> re-evaluated the performance of the method in <ref type="bibr" target="#b9">[10]</ref> with the annotations from VOC 2012 validation set, most of our evaluations are thus performed with the annotations from VOC 2012 segmentation validation set <ref type="bibr" target="#b2">[3]</ref> when comparing with <ref type="bibr" target="#b16">[17]</ref> [10] <ref type="bibr" target="#b21">[22]</ref>. We use standard AP r metric for evaluation, which calculates the average precision under different IoU scores with the ground-truth segmentation map.</p><p>Implementation Details. We fine-tune the R2-IOS based on the pre-trained VGG-16 model <ref type="bibr" target="#b31">[32]</ref> and our code is based on the publicly available Fast R-CNN framework <ref type="bibr" target="#b6">[7]</ref> on Caffe platform <ref type="bibr" target="#b11">[12]</ref>. During fine-tuning, each SGD mini-batch contains 64 selected object proposals from    each training image. Following <ref type="bibr" target="#b6">[7]</ref>, in each mini-batch, 25% of object proposals are foreground that have IoU overlap with a ground truth bounding box of at least 0.5, and the rest are background. During training, images are randomly selected for horizontal flipping with a probability of 0.5 to augment the training set. The maximal number of refinement iterations for all proposals is set as T = 4, since only minor improvement with more iterations is observed.</p><p>In the reversible proposal refinement sub-network, parameters in the fully-connected layers used for softmax classification and bounding box regression are randomly initialized with zero-mean Gaussian distributions with standard deviations of 0.01 and 0.001, respectively. In the segmentation sub-network, the last two convolutional layers used for pixel-wise semantic labeling and the fully-connected layers in the instance-aware denoising autoencoder are all initialized from zero-mean Gaussian distributions with standard deviations 0.001. All values of initial bias are set as 0. The learning rate of pre-trained layers is set as 0.0001.</p><p>For training, we first run SGD for 120k iterations for training the network parameters of R2-IOS without using optimal iteration numbers on a NVIDIA GeForce Titan X GPU and Intel Core i7-4930K CPU @3.40GHz. Then our R2-IOS is fine-tuned on the pre-trained network paramters for 100k iterations. For testing, on average, the R2-IOS framework processes one image within 1 second (excluding object proposal time). <ref type="table">Table 1</ref> provides the results of SDS <ref type="bibr" target="#b9">[10]</ref>, HC <ref type="bibr" target="#b10">[11]</ref> and our R2-IOS for instance-level segmentation with the annotations from SBD dataset <ref type="bibr" target="#b8">[9]</ref>. R2-IOS outperforms the previous state-of-the-art approaches by a significant margin, in average 19.1% better than SDS <ref type="bibr" target="#b9">[10]</ref> and 8.8% better than HC <ref type="bibr" target="#b10">[11]</ref> in terms of mean AP r metric at 0.5 IoU score. When evaluating on 0.7 IoU score, 7.1% improvement in AP r can be observed when comparing our R2-IOS with HC <ref type="bibr" target="#b10">[11]</ref>. We can only compare the results evaluated at 0.5 to 0.7 IoU scores, since no other results evaluated at higher IoU scores have been reported for the baselines. When evaluated with the annotations from VOC 2012 dataset, <ref type="table" target="#tab_3">Table 2</ref> and <ref type="table" target="#tab_4">Table 3</ref> present the comparison of the proposed R2-IOS with three state-of-the-art methods <ref type="bibr" target="#b9">[10]</ref>[22] <ref type="bibr" target="#b16">[17]</ref> using AP r metric at IoU score 0.5, 0.6 and 0.7, respectively. Evaluating with much higher IoU score requires high accuracy for predicted segmentation masks of object instances. R2-IOS significantly outperforms the three baselines: 66.7% vs 43.8% of SDS <ref type="bibr" target="#b9">[10]</ref>, 46.3% of Chen et al. <ref type="bibr" target="#b21">[22]</ref> and 58.7% of PFN <ref type="bibr" target="#b16">[17]</ref> in mean AP r metric. Furthermore, <ref type="table" target="#tab_4">Table 3</ref> shows that R2-IOS also substantially outperforms the three baselines evaluated at higher IoU scores 0.6 and 0.7. In general, R2-IOS shows dramatically higher performance than the baselines, demonstrating its superiority in predicting accurate instance-level segmentation masks benefiting from its coherent recursive learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Performance Comparisons</head><p>Several examples of the instance-level segmentation results (with respect to the ground truth) are visualized in <ref type="figure" target="#fig_3">Figure 3</ref>. Because no publicly released codes are available for other baselines, we only compare with visual results from SDS <ref type="bibr" target="#b9">[10]</ref>. Generally, R2-IOS generates more accurate segmentation results for object instances of different object categories, various scales and heavy occlusion, while SDS <ref type="bibr" target="#b9">[10]</ref> may fail to localize and segment out the object instances due to the suboptimal localized object proposals. For example, in the first image of the second row, the region of the leg is wrongly included in the predicted mask of the cat by SDS <ref type="bibr" target="#b9">[10]</ref>, while R2-IOS precisely segments out the mask of the cat without being distracted by other object instances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Studies on Proposed R2-IOS</head><p>We further evaluate the effectiveness of the four important components of R2-IOS, i.e. the recursive learning, the reversible gate (i.e., optimal iteration number), the instance-aware denoising autoencoder and the segmentation-aware feature representation. The performance over all 20 classes from eight variants of R2-IOS is reported in <ref type="table" target="#tab_3">Table 2</ref>.</p><p>Recursive Learning. The proposed R2-IOS uses the maximal 4 iterations to refine all object proposals. To justify the necessity of using multiple iterations, we evaluate the performance of R2-IOS with different numbers of iterations during training and testing stages. Note that all the following results are obtained without using the reversible gates. In our experimental results, "R2-IOS recursive 1" indicates the performance of using only 1 iteration, which is equivalent to the model without any recursive refinement. "R2-IOS recursive 2 and "R2-IOS recursive 3" represents the models of using 2 and 3 iterations. By comparing "R2-IOS recursive 4" with the three variants, one can observe considerable improvement on segmentation performance when using more iterations. This shows that R2-IOS can generate more precise instance-level segmentation results benefiting from recursively refined object proposals and segmentation predictions. We do not observe a noticeable increase in the performance by adding more iterations, thus the setting of 4 iterations is employed throughout our experiments.</p><p>In addition, we also report the results of the R2-IOS variant where the recursive process is only performed during testing and no recursive training is used, as "R2-IOS recursive only testing". By comparing with "R2-IOS recursive 4", a 3.3% decrease is observed, which verifies the advantage of using recursive learning during training to jointly improve the network capabilities of two sub-networks.</p><p>We also provide several examples for qualitative comparison of R2-IOS variants with different numbers of iterations in <ref type="figure" target="#fig_5">Figure 4</ref>. We can observe that the proposed R2-IOS is able to gradually produce better instance-level segmentation results with more iterations. For instance, in the first row,  by using only 1 iteration, R2-IOS can only segment out one part of the sofa with salient appearance with respect to background. After refining object proposals with 4 iterations, the complete sofa mask can be predicted by R2-IOS. Similarly, significant improvement by R2-IOS with more iterations can be observed in accurately locating and segmenting the object with heavy occlusion (in the second row).</p><p>Reversible Gate. We also verify the effectiveness of the reversible gate to adaptively determine the optimal number of refinement iterations for each proposal. "R2-IOS (ours)" offers a 1.5% increase by incorporating the reversible gates into the reversible proposal refinement sub-network, compared to the version "R2-IOS recursive 4". This demonstrates that performing adaptive number of refinement iterations for each proposal can help produce more accurate bounding boxes and instance-level object segmentation results for all proposals. Similar improvement is also seen at 0.6 and 0.7 IoU scores, as reported in <ref type="table" target="#tab_4">Table 3</ref>.</p><p>Instance-aware Autoencoder. We also evaluate the effectiveness of using the instance-aware denoising autoencoder to predict the foreground mask for the dominant object in each proposal. In <ref type="table" target="#tab_3">Table 2</ref>, "R2-IOS (w/o autoencoder)" represents the performance of the R2-IOS variant without the instance-aware autoencoder where the dominant foreground mask for each proposal is directly generated by the last convolutional layer. As shown by "R2-IOS (w/o autoencoder)" and "R2-IOS (ours)", using the instance-aware autoencoder, over 12.5% performance improvement can be observed. This substantial gain verifies that the instance-aware autoencoder can help determine the dominant object instance by explicitly harnessing global information within each proposal. In addition, another alternative strategy of gathering global information is to simply use fully-connected layers. We thus report the results of the R2-IOS variant using two fully-connected layers with 3200 outputs stacked on the convolutional layers, named as "R2-IOS (fully w/o autoencoder)". Our R2-IOS also gives favorable performance over "R2-IOS (fully w/o autoencoder)", showing that using intermediate compact features within the instance-aware autoencoder can help introduce more discriminative and higher-level representations for predicting the dominant foreground mask. <ref type="figure" target="#fig_6">Figure 5</ref> shows some segmentation results obtained by "R2-IOS (w/o autoencoder)" and "R2-IOS (ours)". "R2-IOS (w/o autoencoder)" often fails to distinguish the dominant instances among multiple instances in an object proposal, and wrongly labels all ob- ject instances as foreground. For example, in the first row, the instance-aware autoencoder enables the model to distinguish the mask of a human instance from a motorcycle. Segmentation-aware Feature Representation. The benefit of incorporating the confidence maps predicted by the segmentation sub-network as part of the features in the reversible proposal refinement sub-network can be demonstrated by comparing "R2-IOS (w/o seg-aware)" with "R2-IOS (ours)". The improvement shows that the two subnetworks can mutually boost each other and help generate more accurate object proposals and segmentation masks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion and Future Work</head><p>In this paper, we proposed a novel Reversible Recursive Instance-level Object Segmentation (R2-IOS) framework to address the challenging instance-level object segmentation problem. R2-IOS recursively refines the locations of object proposals by leveraging the repeatedly updated segmentation sub-network and the reversible proposal refinement sub-network in each iteration. In turn, the refined object proposals provide better features of each proposal for training the two sub-networks. The reversible proposal refinement sub-network adaptively determines the optimal iteration number of the refinement for each proposal. In future, we will utilize Long Short-Term Memory (LSTM) recurrent networks to leverage long-term spatial contextual dependencies from neighboring objects and scenes in order to further boost the instance-level segmentation performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Example instance-level object segmentation results by our R2-IOS. R2-IOS first recursively refines each proposal for all iterations, and then the optimal number of refinement iterations for each proposal is determined by the predicted confidences in all iterations, denoted as the dashed green rectangles. The final segmentation results are obtained by reversing towards the results of the optimal iteration number. Better viewed in color pdf.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Top detection results (with respect to the ground truth) of SDS<ref type="bibr" target="#b9">[10]</ref> and the proposed R2-IOS on the PASCAL VOC 2012 segmentation validation dataset. Compared with SDS, the proposed R2-IOS obtains favorable segmentation results for different categories and object instances with various scales, heavy occlusion and background clutters. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>Examples of instance-level object segmentation results by our R2-IOS using different numbers of iterations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 .</head><label>5</label><figDesc>Comparison of instance-level segmentation results by our R2-IOS without and with the instance-aware autoencoder.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="true"><head>Table 2 .</head><label>2</label><figDesc>Comparison of instance-level segmentation performance with several architectural variants of our R2-IOS and three state-of-thearts using AP r metric over 20 classes at 0.5 IoU on the PASCAL VOC 2012 validation set, when evaluated with the annotations on VOC 2012 validation set. All numbers are in %.</figDesc><table>Settings 
Method 
plane 
bike 
bird 
boat 
bottle 
bus 
car 
cat 
chair 
cow 
table 
dog 
horse 
motor 
person 
plant 
sheep 
sofa 
train 
tv 
average 

SDS [10] 
58.8 0.5 60.1 34.4 29.5 60.6 40.0 73.6 6.5 52.4 31.7 62.0 49.1 45.6 47.9 22.6 43.5 26.9 66.2 66.1 43.8 

Baselines 
Chen et al. [22] 
63.6 0.3 61.5 43.9 33.8 67.3 46.9 74.4 8.6 52.3 31.3 63.5 48.8 47.9 48.3 26.3 40.1 33.5 66.7 67.8 46.3 

PFN [17] 
76.4 15.6 74.2 54.1 26.3 73.8 31.4 92.1 17.4 73.7 48.1 82.2 81.7 72.0 48.4 23.7 57.7 64.4 88.9 72.3 58.7 

recursive 1 
80.7 1.8 85.0 58.1 44.9 82.8 57.5 85.7 13.5 71.1 9.9 86.0 76.3 72.4 54.8 36.7 55.4 47.9 88.9 78.9 59.6 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 .</head><label>3</label><figDesc></figDesc><table>Per-class instance-level segmentation results using AP r metric over 20 classes at 0.6 and 0.7 IoU on the VOC 2012 validation set. 
All results are evaluated with the annotations on VOC 2012 validation set. All numbers are in %. 

IoU score 
Method 
plane 
bike 
bird 
boat 
bottle 
bus 
car 
cat 
chair 
cow 
table 
dog 
horse 
motor 
person 
plant 
sheep 
sofa 
train 
tv 
average 

0.6 

SDS [10] 
43.6 0 52.8 19.5 25.7 53.2 33.1 58.1 3.7 43.8 29.8 43.5 30.7 29.3 31.8 17.5 31.4 21.2 57.7 62.7 34.5 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Boxsup: Exploiting bounding boxes to supervise convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The pascal visual object classes challenge: A retrospective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="98" to="136" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained partbased models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Object detection via a multiregion &amp; semantic segmentation-aware cnn model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Overfitting in neural nets: Backpropagation, conjugate gradient, and early stopping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C S L L</forename><surname>Giles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">402</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Semantic contours from inverse detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="991" to="998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Simultaneous detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Hypercolumns for object segmentation and fine-grained localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="675" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep human parsing with active template regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2402" to="2414" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Towards computational baby learning: A weakly-supervised approach for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Semantic object parsing with local-global long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Proposal-free network for instance-level object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.02636</idno>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Human parsing with contextualized convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Efficient piecewise training of deep structured models for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">D</forename><surname>Rei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.01013</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Fashion parsing with video context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1347" to="1358" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Matching-CNN Meets KNN: Quasi-Parametric Human Parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Multi-instance object segmentation with occlusion handling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-T</forename><forename type="middle">C X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Semantic image segmentation via deep parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Recurrent models of visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2204" to="2212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1211.5063</idno>
		<title level="m">On the difficulty of training recurrent neural networks</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning to segment object candidates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">O</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Multiscale combinatorial grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.06066</idno>
		<title level="m">Object detection networks on convolutional feature maps</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Instance segmentation of indoor scenes using a coverage loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sontag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="616" to="631" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Selective search for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">E</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03044</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Monocular object instance segmentation and depth ordering with cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jayasumana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
		<title level="m">Conditional random fields as recurrent neural networks. In ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Edge boxes: Locating object proposals from edges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="391" to="405" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
