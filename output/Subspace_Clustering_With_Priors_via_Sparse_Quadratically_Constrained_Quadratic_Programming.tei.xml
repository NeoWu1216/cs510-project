<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Subspace Clustering with Priors via Sparse Quadratically Constrained Quadratic Programming *</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongfang</forename><surname>Cheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Electrical and Computer Engineering</orgName>
								<orgName type="institution">Northeastern University</orgName>
								<address>
									<postCode>02115</postCode>
									<settlement>Boston</settlement>
									<region>MA, US</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Wang</surname></persName>
							<email>wang.yin@husky.neu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Electrical and Computer Engineering</orgName>
								<orgName type="institution">Northeastern University</orgName>
								<address>
									<postCode>02115</postCode>
									<settlement>Boston</settlement>
									<region>MA, US</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Sznaier</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Electrical and Computer Engineering</orgName>
								<orgName type="institution">Northeastern University</orgName>
								<address>
									<postCode>02115</postCode>
									<settlement>Boston</settlement>
									<region>MA, US</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Octavia</forename><surname>Camps</surname></persName>
							<email>camps@coe.neu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Electrical and Computer Engineering</orgName>
								<orgName type="institution">Northeastern University</orgName>
								<address>
									<postCode>02115</postCode>
									<settlement>Boston</settlement>
									<region>MA, US</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Subspace Clustering with Priors via Sparse Quadratically Constrained Quadratic Programming *</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:46+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper considers the problem of recovering a subspace arrangement from noisy samples, potentially corrupted with outliers. Our main result shows that this problem can be formulated as a convex semi-definite optimization problem subject to an additional rank constrain that involves only a very small number of variables. This is established by first reducing the problem to a quadratically constrained quadratic problem and then using its special structure to find conditions guaranteeing that a suitably built convex relaxation is indeed exact. When combined with the standard nuclear norm relaxation for rank, the results above lead to computationally efficient algorithms with optimality guarantees. A salient feature of the proposed approach is its ability to incorporate existing a-priori information about the noise, co-ocurrences, and percentage of outliers. These results are illustrated with several examples.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Many practical problems involve fitting piecewise models to a given set of sample points. Examples of applications include image compression <ref type="bibr" target="#b10">[11]</ref>, face recognition <ref type="bibr" target="#b2">[3]</ref>, motion segmentation <ref type="bibr" target="#b20">[21]</ref>, video segmentation <ref type="bibr" target="#b18">[19]</ref> and system identification <ref type="bibr" target="#b14">[15]</ref>. Due to its importance, a substantial research effort has been devoted to this problem, leading to many algorithms, that can be roughly classified into statistical, algebraic and self-representation based.</p><p>RANdom SAmple Consensus (RANSAC) <ref type="bibr" target="#b7">[8]</ref> is an iterative approach that proceeds by fitting one subspace at each iteration to as many points as possible, using a sampling based approach, removing these inliers from the dataset and repeating the process, until a given threshold on the percentage of inliers has been exceeded. While in principle the al-gorithm provides robust estimates of the parameters of the subspaces, it may require a large number of iterations to do so. On the other hand, due to its random nature, limiting the number of iterations may lead to arbitrarily bad solutions.</p><p>Algebraic methods such as GPCA <ref type="bibr" target="#b15">[16]</ref>, exploit the properties of subspace arrangements by reducing the problem to estimating the coefficients of a multivariate polynomial from noisy measurements of its zeros. Once this polynomial has been found, the parameters of each subspace can be recovered via polynomial differentiation. While GPCA works well with clean data, its performance degrades quickly with the noise level. This drawback has motivated the approach in <ref type="bibr" target="#b17">[18]</ref>, where the original data is "cleaned" via rank minimization. Although this approach is shown to be capable of handling substantial noise level, its main drawback is its computational complexity. In addition, in the presence of noise there are no guarantees that the resulting polynomial can be factored as a product of linear forms (and hence the parameters of the subspaces are recovered).</p><p>Due to these drawbacks, several methods have been recently proposed to handle noisy samples by exploiting the geometric properties of subspaces to reduce the problem to that of looking for sparse or low rank solutions to a set of linear equations that encode the fact that subspaces are self-expressive (e.g. a point in a subspace can be expressed as a linear combination of other points in it). These methods include Sparse Subspace Clustering (SSC) <ref type="bibr" target="#b6">[7]</ref>, Robust PCA (RPCA) <ref type="bibr" target="#b4">[5]</ref>, Low Rank Representation (LRR) <ref type="bibr" target="#b12">[13]</ref>, Fixed Rank Representation (FRR) <ref type="bibr" target="#b13">[14]</ref> and Robust Subspace Clustering (RSC) <ref type="bibr" target="#b18">[19]</ref>. All of these methods typically involve using relaxations (such as nuclear norm for rank and the ℓ 1 norm for sparsity), in order to obtain tractable convex problems <ref type="bibr" target="#b0">1</ref> . While in the noiseless case these relaxations are exact under suitable conditions on the distribution of the data, in the presence of noise such guarantees are usually lost. Further, finding the parameters of the subspaces re-quires performing first a spectral clustering to cluster the data. Thus, there is no direct control on the fitting error.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Paper contributions:</head><p>Motivated by these difficulties, in this paper we propose an alternative method for recovering a subspace arrangement from noisy samples. Its main idea is to recast the problem as a rank constrained semi-definite program, which in turn is relaxed to a sequence of convex optimization problems by using a reweighted nuclear norm as a surrogate for rank. We also provide easily testable conditions certifying that the relaxation is exact. Specifically, the contributions of the paper are:</p><p>• Establishing that the problem of subspace clustering can be recast into a quadratically constrained quadratic program (QCQP).</p><p>• Exploiting the sparse structure underlying this QCQP to show that it is equivalent to a convex semi-definite program subject to an additional (non-convex) rank constraint that involves only a very small number of variables (roughly the number of parameters needed to characterize the subspaces). Notably, the size of this constraint is independent of the number of data points to be clustered.</p><p>• Using the results above, together with the special sparse structure of the problem, to obtain convex relaxations whose computational complexity scales linearly with the number of data points, along with conditions certifying optimality of these relaxations.</p><p>• Developing a clustering algorithm that, contrary to most existing techniques, directly identifies a set of subspace parameters that guarantees a fitting error lower than a given bound. Further, this algorithm can easily accommodate existing co-ocurrence information (points known to be in the same or different subspaces), bounds on the number of outliers, and priors on the relative frequency of each subspace, to improve clustering performance. To the best of our knowledge, this ability is not shared by any other existing method.</p><p>The above contributions are illustrated with several examples, including both synthetic and real data, where the ability to incorporate priors is key to obtaining the correct segmentation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Preliminaries</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Some properties of QCQP</head><p>In this paper we will reduce the subspace clustering problem to a QCQP of the form:</p><formula xml:id="formula_0">p * = min v∈R n f 0 (v) s.t. f i (v) ≤ 0, ∀ q i=1 Av ≤ b,<label>(1)</label></formula><formula xml:id="formula_1">where f i (v) = v T Q i v + c T i v + d i and Q i ∈ R n×n</formula><p>is a symmetric matrix, for ∀i = 0, 1, . . . , q, A ∈ R m×n and b ∈ R m . In the case where Q i 0 for each i, the QCQP (1) is a convex programming problem that can be solved in polynomial time <ref type="bibr" target="#b1">[2]</ref>. On the other hand, it is well known that, in the general case the problem is NP-hard. Nevertheless, since these problems are ubiquitous in a wide range of areas, extensive efforts have been devoted to developing tractable relaxations and associated optimality certificates (see for instance <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b1">2]</ref> and references therein). In particular, a relaxation of interest in this paper can be obtained by introducing a new variable V ∈ R n×n and rewriting (1) as:</p><formula xml:id="formula_2">p * = min v,V Tr(Q 0 V) + c T 0 v + d 0 s.t. Tr(Q i V) + c T i v + d i ≤ 0, ∀ q i=1 Av ≤ b V = vv T ,<label>(2)</label></formula><p>where the non-convexity appears now only in the last equality constraint. A convex relaxation of this problem that provides a lower bound of the cost can now be obtained by replacing V = vv T with the convex positive semidefiniteness constraint V − vv T 0, leading to the semidefinite program (SDP) <ref type="bibr" target="#b21">[22]</ref>:</p><formula xml:id="formula_3">p * = min v,V Tr(Q 0 V) + c T 0 v + d 0 s.t. Tr(Q i V) + c T i v + d i ≤ 0, ∀ q i=1 Av ≤ b 1 v T v V 0<label>(3)</label></formula><p>We will refer to this as the SDP relaxation of (1).</p><formula xml:id="formula_4">Remark 1. Since (3) is a relaxation of (1), thenp * ≤ p * . A trivial sufficient condition forp * = p * is V * = v * v * T , that is, rank of 1 v * T v * V * is 1.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">A reduced QCQP relaxation</head><p>While the relaxation (3) is convex, it has relatively poor scaling properties, due to the semi-definite constraint (recall that for n × n matrices, the computational complexity of these constraints scales as n 6 ). However, the problem often (as in this paper) has an underlying sparse structure that can be exploited to mitigate this growth. For notational simplicity and without loss of generality (by absorbing linear terms into quadratic ones), rewrite the problem as</p><formula xml:id="formula_5">p * = min v∈R n f 0 (v) s.t. f i (v) ≤ 0, ∀ q i=1 (4) where f i (v) = v T Q i v + c T i v + d i . Assume that {v k } l k=1 are subsets of v, satisfying ∪ l k=1 v k = v, each constraint f i (v) for i = 1, .</formula><p>. . , q, depends only on a subsets of variables v k , and that the objective function f 0 (v) can be partitioned into a sum of the form f 0 (v) = l k=1 p k (v k ). Problem (1) is said to satisfy the running intersection property <ref type="bibr" target="#b11">[12]</ref> if there exists a reordering v k ′ of v k such that for every k ′ = 1, . . . , l − 1:</p><formula xml:id="formula_6">v k ′ +1 ∩ (∪ k ′ j=1 v j ) ⊆ v s for some s ≤ k ′ .<label>(5)</label></formula><p>Then a convex relaxation of (1) can be obtained by replac-</p><formula xml:id="formula_7">ing the condition M . = 1 v T v V 0 with positive semi-</formula><p>definiteness of a collection of smaller matrices as follows:</p><formula xml:id="formula_8">p * sparse = min M k l k=1 Tr(Q k,0 M k ) s.t. Tr(Q i M i ) ≤ 0, ∀ q i=1 M k 0, ∀ l k=1 M i (I ij , I ij ) = M j (I ij , I ij ), ∀ l i,j=1 , i = j (6) whereQ k,0 andQ i are symmetric matrices built from {Q i , c i , d i }. M i (I ij , I ij ) denotes the block of M i with rows and columns corresponding to 1 v T i ∩ v T j T .</formula><p>In the sequel, we will refer to the relaxation above as the "reduced SDP" relaxation. Clearly,p * sparse ≤p * ≤ p * , since the relaxation (6) is looser than (3). However, under certain conditions, the equality holds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Theorem 1. If (4) satisfies the running intersection, then a sufficient condition forp</head><formula xml:id="formula_9">* sparse =p * = p * is that rank(M k ) = 1, k = 1, . . . , l.</formula><p>Proof. This is a special case of Theorem 3.7 in <ref type="bibr" target="#b11">[12]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Problem Statement</head><p>The goal of this paper is to estimate a set of subspaces from noisy samples such that certain priors are satisfied, or show that none exists. Formally, this can be stated as:</p><formula xml:id="formula_10">Problem 1. Given: • A set of noisy samples X = {x j ∈ R n : x j = x j + η j } Np j=1 , drawn from N s distinct linear subspaces {S i ⊂ R n } Ns i=1 of dimension n − 1 of the form S i = {x ∈ R n : r T ix = 0, r i ∈ R n , r i 2 = 1}</formula><p>. • A-priori information consisting of (i) a bound ǫ on the distance from the noisy sample to the subspace it is drawn from, (ii) a bound N o on the number of outliers, (iii) N fi , the relative frequency of each subspace, and (iv) point wise co-occurrence information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Establish whether the data is consistent with the a-priori assumptions and, if so, find a set of subspaces compatible with the a-priori information and assign the (inlier) samples to each subspace. That is, find {r</head><formula xml:id="formula_11">i ∈ R n } Ns i=1 and a partition of the samples in N s + 1 sets {X i } Ns i=1 , X o such that card(X o ) ≤ N o and card(X i ) = N fi N p for each i = 1, . . . , N s ,and |r T i x| ≤ ǫ holds for ∀x ∈ X i .<label>(7)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">A Convex Approach to Clustering</head><p>In this section we present the main result of this paper, a convex optimization approach to solving Problem 1. The main idea is to first recast the problem into an equivalent non-convex QCQP, which in turn can be reduced to an SDP subject to a non-convex rank constraint by exploiting the results outlined in section 2.2. Next, by exploiting the structure of the problem, we show that this non-convex rank constraint needs to be enforced only on a single matrix of a small size (substantially smaller than those involved in the reduced SDP relaxation). Finally, combining these results with standard nuclear norm surrogates for rank leads to the desired algorithm. For simplicity we will consider first the case with no outliers (N o = 0), and without constraints on the relative frequency and co-ocurrences. The handling of these cases will be covered in sections 4.4 and 4.5 after presenting the basic algorithm and supporting theory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Clustering as a Nonconvex QCQP</head><p>It can be easily shown that by introducing a set of binary variables {s i,j } that indicate whether x j is drawn from S i or not, Problem 1 is equivalent to: Problem 2. Determine the feasibility of the following set of quadratic inequalities:</p><formula xml:id="formula_12">                 |s i,j r T i x j | ≤ ǫs i,j , ∀ Ns i=1 ∀ Np j=1 (8a) s 2 i,j = s i,j , s i,j ≥ 0, ∀ Ns i=1 ∀ Np j=1 (8b) Σ Ns i=1 s i,j = 1, ∀ Np j=1 (8c) r T i r i = 1, ∀ Ns i=1 (8d) r 1 (1) ≥ r 2 (1) ≥ · · · ≥ r Ns (1) ≥ 0 (8e) Here, (8a) is equivalent to |r T i x j | ≤ ǫ if s i,j = 0 (hence x j ∈ X j )</formula><p>and trivially satisfied otherwise; (8b) imposes that s i,j ∈ {0, 1}; (8c) forces each sample x i to be assigned to exactly one subspace; and (8e) eliminates the symmetry of the solutions. Thus, if (8) is feasible, then the subspaces recovered are characterized by their normals {r i }. On the other hand, infeasibility of (8) invalidates the a-priori information given in Problem 1.</p><p>Clearly, Problem 2 is a QCQP of the form (1), albeit nonconvex due to the constraints (8a), (8b) and (8d). Applying the SDP relaxation outlined in Section 2.2 leads to the following result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Theorem 2. Problem 2 is equivalent to establishing feasibility of</head><formula xml:id="formula_13">     Tr(Q k M) ≤ 0, ∀ K k=1 (9a) M 0, M(1, 1) = 1 (9b) rank(M) = 1 (9c)</formula><p>where (9a) denotes the linear (in)equalities,and M(i, j) denotes the (i,j) entry of M.</p><p>Proof. It follows from Remark 1 by collecting all variables</p><formula xml:id="formula_14">in (8) in a vector v ∈ R Ns(n+Np) , that is, v . = [r T 1 , · · · ,</formula><p>r T Ns , s 1,1 , · · · , s Ns,1 , · · · , s 1,j , · · · , s Ns,j , · · · , s 1,Np , · · · , s Ns,Np ] T , and defining the rank</p><formula xml:id="formula_15">1 matrix M = 1 v T v vv T .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Corollary 1. Consider the convex SDP problem</head><formula xml:id="formula_16">Tr(Q k M) ≤ 0, ∀ K k=1 M 0, M(1, 1) = 1<label>(10)</label></formula><p>If <ref type="formula" target="#formula_0">(10)</ref> is infeasible, then Problem 2 is also infeasible. On the other hand, if this problem admits a rank 1 solution M * , then M * (2 : (n + N p )N s + 1, 1) is also a feasible solution to Problem 2.</p><p>In principle, from the results above, one could attempt to solve Problem 2 by solving <ref type="bibr" target="#b9">(10)</ref> or <ref type="formula">(9)</ref> where the rank constraint is relaxed to one involving minimizing the reweighted nuclear norm. Note however, that both (10) and (9) require solving SDPs whose computational complexity scales as [N s (n + N p ) + 1] 6 , limiting the use of the algorithm to relatively few points.As shown next, these difficulties can be circumvented by exploiting the sparse structure of the problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Exploiting the Sparse Structure</head><p>To exploit the sparse structure of the problem, partition the constraints in <ref type="bibr" target="#b7">(8)</ref> into the N p + 1 sets P j , j = 0, 1, . . . , N p :</p><formula xml:id="formula_17">P 0 : r T i r i = 1, ∀ Ns i=1 r 1 (1) ≥ r 2 (1) ≥ · · · ≥ r Ns (1) ≥ 0, ∀ Np j=1 , P j :      |s i,j r T i x j | ≤ ǫs i,j , ∀ Ns i=1 s 2 i,j = s i,j , s i,j ≥ 0, ∀ Ns i=1 Ns i=1 s i,j = 1.</formula><p>It is easily seen that P 0 is only associated with vari-</p><formula xml:id="formula_18">ables v 0 = [r T 1 , . . . , r T Ns ] T ∈ R nNs , for j = 1, . . . , N p , P j is only associated with variables v j = [v T 0 , s 1,j , .</formula><p>. . , s Ns,j ] T ∈ R (n+1)Ns , and that each P j can be reformulated as a set of quadratic constraints of the form</p><formula xml:id="formula_19">v T j Q k,j v j + c T k,j v j + d k,j ≤ 0, ∀ Kj k=1 ,<label>(11)</label></formula><p>where K j is the number of constraints in P j .</p><p>Notice that v j ∩ ∪ j−1 k=0 v k = v 0 holds for ∀j = 1, . . . , N p and that ∪ Np j=0 v j = v. Thus, (8) exhibits the running intersection property and the results in Section 2.3 allow to reformulate (9) involving positive semi-definiteness of matrices of substantially smaller size than that of M in <ref type="bibr" target="#b8">(9)</ref>. To this effect, introduce positive semi-definite matri-</p><formula xml:id="formula_20">ces of the form M j = 1 m j (v T j ) m j (v j ) m j (v j v T j ) for j = 0, 1, . . . , N p , where m j (•) is a variable locating in the same position as • in 1 v T j v j v j v T j</formula><p>, and consider the following rank constrained SDP problem: From Theorem 2 it follows that the problem above is equivalent to Problem 2. However, contrary to <ref type="bibr" target="#b9">(10)</ref>, it involves N p + 1 matrices of dimension around [(n + 1)N s + 1] × [(n + 1)N s + 1]. Hence its computational complexity grows as N p [(n + 1)N s ] 6 , that is, linearly with the number of data points. However, this formulation requires enforcing (N p + 1) rank constraints, a very challenging task. Surprisingly, as shown next, the special structure of the problem makes N p of these constraints redundant, allowing for developing an equivalent of Problem 2 with a single rank constraint imposed on a (nN s + 1) × (nN s + 1) matrix. </p><formula xml:id="formula_21">Problem 3. Determine the feasibility of              Tr(Q k,j M j ) ≤ 0, ∀ Kj k=1 , ∀ Np j=0 (12a) M j 0, M j (1, 1) = 1, ∀ Np j=0 (12b) M j (1 : nN s + 1, 1 : nN s + 1) = M 0 , ∀ Np j=1 (12c) rank(M j ) = 1, ∀ Np j=0 (12d) whereQ k,j . = d k,j 0.5c T k,j 0.5c k,j Q k,</formula><p>Proof. Given in the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">A Convex Optimization Based Algorithm</head><p>Theorem 3 suggests that a convex algorithm whose complexity scales linearly with the number of data points can be obtained by seeking rank-1 solutions to (13) via iterative minimization of a re-weighted nuclear norm of M 0 <ref type="bibr" target="#b16">[17]</ref> subject to (12a)-(12c). This idea leads to Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Subspace Clustering via QCQP</head><formula xml:id="formula_23">1: Initialize: k = 0, W (0) = I, 0 &lt; δ ≪ 1, k max 2: repeat 3: solve {M (k) j } = arg min Tr(W (k) M 0 ) s.t. (12a) − (12c) 4: update W (k+1) = [M (k) 0 +σ 2 (M (k) 0 )] −1 , k = k +1; 5: until σ 2 (M (k) 0 ) &lt; δσ 1 (M (k) 0 ) or k &gt; k max .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Handling Outliers</head><p>Outliers, defined as points x that lie beyond a given distance ǫ from every subspace in the arrangement, (e.g. such that min i |r T i x| &gt; ǫ) can be handled by simply relaxing the requirement that each point must be assigned to a certain subspace, leading to the following QCQP:</p><formula xml:id="formula_24">p * = max Np j=1 Ns i=1 si,j s.t. |si,jr T i xj| ≤ ǫsi,j, s 2 i,j = si,j ∀ Ns i=1 ∀ Np j=1 Ns i=1 si,j ≤ 1, ∀ Np j=1 r T i ri = 1, ∀ Ns i=1 r1(1) ≥ r2(1) ≥ · · · ≥ rN s (1) ≥ 0<label>(14)</label></formula><p>which seeks to maximize the number of inliers. Since (14) exhibits a sparsity pattern similar to that in (8), it can be solved by a modified version of Algorithm 1, where the equality constraint Ns i=1 s i,j = 1 is replaced by Ns i=1 s i,j ≤ 1 (to accommodate the case where x j is an outlier), and the objective function is replaced bỹ</p><formula xml:id="formula_25">p = Σ Ns i=1 Σ Np j=1 (1 − m j (s i,j )) + λTr(W (k) M 0 ),<label>(15)</label></formula><p>where λ &gt; 0 is a parameter. Thus, this objective function penalizes both the number of outliers and the rank of M 0 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Remark 2. A bound N o on the number of outliers can be handled via a constraint of the form</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ns i=1</head><p>Np j=1 s i,j ≥ N p − N o . However, since this constraint subverts the sparsity of the problem, the formulation (14) is preferable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Handling Additional A-Priori Information</head><p>In many scenarios of practical interest, a-priori information on the labels of some sample points is available. For instance, in surveillance videos, it is easy to identify points lying on buildings, and hence background, and often points belonging to moving targets. Similarly, in many situations, information is available about the size of the target, and thus on the relative frequency of points in the corresponding subspace. As shown below, this additional information can be incorporated into our formulation by simply adding suitable quadratic constraints on the variables s i,j . Specifically: (i) f % of X belongs to S i ⇐⇒ Np j=1 s i,j = 0.01f N p ; (ii) x m , x n belong to the same subspace ⇐⇒ s i,m = s i,n , ∀i = 1, · · · , N s ; (iii) x m , x n belong to different subspaces ⇐⇒ s i,m s i,n = 0, ∀i = 1, · · · , N s . The advantages of being able to exploit this information will be illustrated in Section 6.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Recovery Properties</head><p>A salient feature of the proposed approach is its ability to certify optimality of the solution provided by Algorithm 1. Specifically, from Theorem 3 it follows that, in the case of data corrupted by outliers, rank(M 0 ) = 1 certifies that the correct clustering has been found. Similarly, in the presence of noise, this condition guarantees that the recovered subspaces fit the inliers within the given noise bound ǫ, and are consistent with the given a-priori information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Further Complexity Reduction</head><p>As discussed in section 4.2, exploiting the sparse structure of the problem leads to Algorithm 1 which only requires imposing positive semi-definiteness on N p + 1 ma-</p><formula xml:id="formula_26">trices of dimension at most [(n+1)N s +1]×[(n+1)N s +1].</formula><p>Hence, its complexity scales as (nN s ) <ref type="bibr" target="#b5">6</ref> . Further computational complexity reduction can be achieved by proceeding in a greedy fashion where subspaces are determined step by step rather than simultaneously as in Section 4.3. At each step, samples drawn from a specific subspace are considered as inliers while all other points are considered outliers. Thus, instead of introducing N s binary variables {s i,j } Ns i=1 for each sample as in Section 4.2, here only one binary variable s j is needed to indicate whether x j is an inlier. At each step the resulting problem reduces to a QCQP of the form:</p><formula xml:id="formula_27">p * = max sj ,r Np j=1 s j s.t. |s j r T x j | ≤ ǫs j , s 2 j = s j , ∀ Np j=1 r T r = 1, r(1) ≥ 0<label>(16)</label></formula><p>Proceeding as in section 4.2 it can be shown that this problem also exhibits the running intersection property. Combining this observation with a reasoning similar to the one used in the proof of Theorem 3 leads to the following result:  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experiments</head><p>In this section, we demonstrate the advantage of the proposed method using both synthetic data and a non-trivial application: planar segmentation by homography learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Synthetic Data</head><p>We first investigate the performance of the QCQP-based subspace clustering algorithm on synthetic data as the number of subspaces, their dimensions and noise levels changed. For each set of experiments, we randomly generated 20 instances with sample points lying on a union of multiple linear subspaces corrupted by noise, normal to the corresponding subspace, and with uniform random magnitudes in [0.8ǫ, ǫ]. A comparison of the performance of Algorithm 1, implemented in Matlab using CVX <ref type="bibr" target="#b8">[9]</ref>, against existing state-of-the-art methods is summarized in <ref type="table">Table 1</ref> 2 . As shown there, in all cases the proposed method outperformed the others in terms of the worst-case fitting error of the identified subspaces, given by:</p><formula xml:id="formula_28">err f = max j∈{1,··· ,Np} min i∈{1,··· ,Ns} |r T i x j | s.t. ||r i || 2 = 1, ∀ Ns i=1</formula><p>where r i 's are the normals to the subspaces found by each algorithm. For algorithms that cannot obtain r i directly, like SSC and LRR, we calculated r i by fitting a subspace to each cluster of points produced by the algorithm.</p><p>Next, the effect of outliers was investigated, by randomly corrupting 4 to 6 points with noise of magnitude 3ǫ (the distribution of the data is shown in <ref type="figure" target="#fig_5">Fig. 1</ref>). Using Algorithm 1 <ref type="bibr" target="#b1">2</ref> In order to provide a meaningful comparison, for each set of experiments, the adjustable parameters of each of the algorithms listed in the table were experimentally tuned to minimize the misclassification rate.  <ref type="formula" target="#formula_0">(14)</ref> instead of (8) led to the results shown in the last row of <ref type="table">Table 1</ref>. As shown there, the proposed algorithm could detect the outliers (black dots) precisely and fit the inliers to subspaces within the given error bound.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Planar Segmentation</head><p>In this section we illustrate the advantages of taking into account prior information in subspace clustering by applying our algorithm to planar segmentation by homography estimation, which is an important problem in image registration and computation of camera motion <ref type="bibr" target="#b9">[10]</ref>. Given the homogeneous coordinates of N p correspondences from two images {(p j , p ′ j )} Np j=1 ∈ R 3 , assuming that these N p points are on the same plane, let p j = x j y j 1 T and</p><formula xml:id="formula_29">p ′ j = x ′ j y ′ j 1 T , and let H ∈ R 3×3 denote the homog- raphy. Then h . = vectorize(H T ) satisfies x T j x T j+Np h = 0, with x T j = p T j 01×3 −x ′ j p T j x T j+Np = 01×3 p T j −y ′ j p T j<label>(18)</label></formula><p>meaning h lies in the null space of the matrix X = x 1 x 2 · · · x 2Np T . Now assume that {(p j , p ′ j )} Np j=1 are distributed on N s different planes (shown as the purple dots in <ref type="figure">Figure 2</ref> ). In this case <ref type="bibr" target="#b17">(18)</ref> no longer holds for all j = 1, · · · , N p with a single h. Instead the planes can be segmented by clustering {x j } 2Np j=1 to N s subspaces S i , characterized by different normal vectors h i , i = 1, · · · , N s (shown as the blue dots and red dots in <ref type="figure">Fig. 2</ref>). Thus subspace clustering techniques can be applied to planar segmentation. Specifically, we can formulate this problem as seeking a feasible solution to:</p><formula xml:id="formula_30">             ||h i || 2 2 = 1, ∀ Ns i=1 (19a) |s i,j h T i x j | ≤ ǫs i,j , s 2 i,j = s i,j , ∀ Ns i=1 ∀ 2Np j=1 (19b) Σ Ns i=1 s i,j = 1, ∀ 2Np j=1 (19c) s i,j = s i,j+Np , ∀ Ns i=1 ∀ Np j=1 (19d)</formula><p>where (19a)-(19c) define a subspace clustering problem similar to <ref type="bibr">Problem 2,</ref><ref type="bibr">and (19d)</ref> represents the prior information that x j and x j+Np should be in the same subspace since they are derived from a single correspondence (p j , p ′ j ) as in <ref type="bibr" target="#b17">(18)</ref>, which cannot be enforced by the existing subspace clustering methods.  We tested on three pairs of real images Merton I, Merton II, and Wadham, given by VGG, University of Oxford. Given each pair of images, firstly VLfeat toolbox <ref type="bibr" target="#b22">[23]</ref> was used to obtain the SIFT features of two images, and correspondences were defined by those pairs of points whose ℓ 2 norm are less than 5. Among these correspondences, we randomly generated 20 instances with 30 correspondences on each plane and N s = 2. ǫ was determined by calculating a ground truth homography matrix for each plane, H 1 , H 2 and ǫ = max{erf 1 , erf 2 }, erf i = max j∈Si |x T j vec(H T i )|, i = 1, 2. Performance was evaluated by the misclassification rate err 1 among 2N p samples, with errl =</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of Points with Different</head><p>Labels from the Ground Truth Total Number of Sample Points 2Np × 100%. <ref type="bibr" target="#b19">(20)</ref> Analysis. As reported in <ref type="table" target="#tab_2">Table 2</ref>, our proposed approach outperformed GPCA and the denoised GPCA proposed in <ref type="bibr" target="#b17">[18]</ref>. For LRR and SSC given by</p><formula xml:id="formula_31">(LRR): min ||Z|| * + λ||E||2,1, s.t. X = XZ + E (SSC): min ||C||1 + 0.5τ ||E|| 2 F , s.t. X = XC + E, diag(C) = 0,</formula><p>we plotted the performance for λ ∈ [10 −4 , 1] and τ ∈ [10 −3 , 10] in <ref type="figure" target="#fig_2">Fig. 3</ref>, from which we can see that the range of λ (τ ) for LRR (SSC) to be competitive with the proposed approach in terms of classification accuracy, is quite small, roughly λ * ∈ [0.003, 0.01], τ * ∈ [0.4, 0.7]. Thus, in this case, LRR and SSC were quite sensitive to the parameters, as shown in <ref type="figure" target="#fig_2">Fig. 3</ref> and <ref type="figure" target="#fig_3">Fig. 4</ref>. In addition, such small values of λ * (or τ * ) placed virtually no penalty on the noise  terms. As a result, they yielded solutions where the denoised data X−E fitted poorly the actual data. For example, for λ = 0.01, the misclassification rate of LRR for Merton I was 3.33%. However, as shown in <ref type="figure" target="#fig_9">Fig. 5</ref>, LRR produced a solution where where X and E had roughly the same scale. A similar situation (also shown in <ref type="figure" target="#fig_9">Fig. 5</ref>) arised with SSC when τ = 0.55, the value yielding the lowest misclassification rate (5%). In contrast, the proposed method did not have to be tuned and yielded an accurate estimate of the homography parameters. The specific reason why the existing methods performed worse is the structure of the samples for <ref type="bibr" target="#b18">(19)</ref>. It to a subspace, and {x j } 2Np j=Np+1 to the second subspace. On the other hand, by enforcing the prior information that x j and x Np+j , for j = 1, . . . , N p , belong to the same subspace, the proposed approach has an extremely low misclassification rate. The running time of each method, averaged over 60 instances, is summarized in <ref type="table" target="#tab_3">Table 3</ref>.  </p><formula xml:id="formula_32">is easy to show that [ x1 x2 . . . x Np ] = * 0 3×Np * , and [ x Np +1 x Np +2 . . . x 2Np ] = 0 3×Np * * ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusions</head><p>In this paper we propose a new approach to the problem of identifying an arrangement of subspaces from noisy samples, potentially corrupted by outliers. The main idea is to recast the problem into a QCQP, which in turn can be solved by solving convex semi-definite programs. A salient feature of the proposed approach is its ability to exploit available a-priori information on the percentage of outliers, relative number of points in each subspace and partial labelings. These advantages were illustrated with several examples comparing the performance of the proposed method vis-à-vis existing ones. The main drawback of the proposed method stems from the need to solve semi-definite programs. However, exploiting the underlying sparse structure of the problem allows for imposing the semi-definite constraints only on a collection of smaller matrices, leading to an algorithm whose complexity scales linearly with the number of data points. Research is currently underway seeking to further reduce the computational burden.</p><p>A. Proof of Theorem 3 (Necessity) Suppose v * is a feasible solution to <ref type="bibr" target="#b7">(8)</ref>, then the rank-1 matrices</p><formula xml:id="formula_33">M * j = 1 v * T j v * j v * j v * T j , j = 0, .., N p ,</formula><p>are a feasible solution to <ref type="bibr" target="#b11">(12)</ref>.</p><p>(Sufficiency) Suppose M * j , j = 0, . . . , N p , is a feasible solution to <ref type="bibr" target="#b11">(12)</ref>. Then from rank(M * 0 ) = 1, we know m 0 (r i (k) 2 ) * = m 0 (r i (k)) * 2 , ∀ Ns i=1 ∀ n k=1 .</p><p>Combining <ref type="formula" target="#formula_0">(21)</ref> and m j (s 2 i,j ) * = m j (s i,j ) * , from M * j 0, for j = 1, . . . , N p , it follows that its principal minor M j,i,k 0, k = 1, .., n, i = 1, . . . , N s ,</p><formula xml:id="formula_35">M j,i,k =   1</formula><p>mj(ri(k)) * mj(si,j) * mj(ri(k)) * mj(ri(k)) * 2 mj(ri(k)si,j) * mj(si,j) * mj(ri(k)si,j) * mj(si,j) *   , and its Schur complement of the first block is also positive semi-definite, that is, mj(ri(k)) * 2 mj(ri(k)si,j) * mj(ri(k)si,j) * mj(si,j) * − mj(ri(k)) * mj(si,j) * mj(ri(k)) * mj(si,j) * = 0 ∆ ∆ mj(si,j) * − mj(si,j) * 2 0</p><p>where ∆ = m j (r i (k)s i,j ) * − m j (r i (k)) * m j (s i,j ) * , which implies ∆ = 0, or m j (r i (k)s i,j ) * = m j (r i (k)) * m j (s i,j ) * . <ref type="formula" target="#formula_2">(23)</ref> Substituting <ref type="formula" target="#formula_2">(23)</ref> into the linear inequality constraints (12a) associated with (8a): m j (s i,j r i ) * T x j ≤ ǫm j (s i,j ) * m j (s i,j r i ) * T x j ≥ −ǫm j (s i,j ) * , leads to: m j (s i,j )m j (r i ) * T x j ≤ ǫm j (s i,j ) * m j (s i,j )m j (r i ) * T x j ≥ −ǫm j (s i,j ) * , which is equivalent to |m j (r i ) * T x j | ≤ ǫ,</p><p>for any m j (s i,j ) * &gt; 0. Thus, x j belongs to the subspace normal to m j (r i ) * . Finally, the conditions m j (s i,j ) * ≥ 0 and Ns i=1 m j (s i,j ) * = 1 guarantee that for each j = 1, . . . , N p , there exists at least one i 0 ∈ {1, . . . , N s }, such that m j (s i0,j ) * &gt; 0. Hence each point x j belongs to at least one of the subspaces characterized by the normals m 0 (r i ) * , i = 1, . . . , N s , which establishes feasibility of <ref type="bibr" target="#b7">(8)</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>of real number and non-negative integers I Identity matrix M N the matrix M − N is positive semidefinite σ i (A) the i-th largest singular value of matrix A Tr(A) trace of the square matrix A</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>j and M(1:j,1:j) denotes the sub matrix formed by the first j rows and columns of M.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Theorem 3 .</head><label>3</label><figDesc>Problem 2 is equivalent to checking the feasibility of (12a) − (12c) rank(M 0 ) = 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Theorem 4 .</head><label>4</label><figDesc>The problem<ref type="bibr" target="#b15">(16)</ref> is equivalent tõp * = max M j Np j=1 mj(sj) s.t. Tr(Q k,j Mj) ≤ 0, ∀ K j k=1 , ∀ Np j=0Mj 0, Mj(1, 1) = 1, ∀ Np j=0 Mj(1 : n + 1, 1 : n + 1) = M0, ∀ Np j=1 rank(M0) = 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>( 17 )J</head><label>17</label><figDesc>Remark 3. Compared to the nongreedy formulation<ref type="bibr" target="#b11">(12)</ref>, the number of variables and the size of the positive semidefinite matrix in<ref type="bibr" target="#b16">(17)</ref> are reduced to O(n 2 ) and O(n) respectively, independent of the number of subspaces N s .This result leads to Algorithm 2 for subspace clustering.Algorithm 2 Greedy Subspace Clustering by QCQP 1: Initialize: n s = 0, n o = N p , X o = X, N = {1, . . . , N o }; 2: while n o &gt; n ⊂ N is the union of j with s j = 1, then x j ∈ S ns , X o = X o \ {x j , j ∈ J}, N = N \ J, and n o = n o − cardinality(J); 6: end while</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 1 :</head><label>1</label><figDesc>Fitting inliers vs. Fitting both Inliers and Outliers modified to solve</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>1 :Figure 2 :</head><label>12</label><figDesc>Performance comparison for different synthetic data scenarios, n: dimension of data, di: Dimension of each subspace, Ni: Number of samples on each subspace, ǫ: error bound, µ and σ: mean and standard deviation of errf, (*): experiments with outliers. Images for Planar Segmentation: Merton I</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 3 :</head><label>3</label><figDesc>Average Performance of LRR and SSC over 20 Instances</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 4 :</head><label>4</label><figDesc>Variance of LRR and SSC over 20 Instances</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 5 :</head><label>5</label><figDesc>Denoised Data vs. Noise. (Left: LRR with λ = 0.01; Right: SSC with τ = 0.55)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table</head><label></label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>where * denotes the nonzero entries. Thus, without any prior information, the existing methods are likely to cluster {x j }</figDesc><table>Np 
j=1 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc>Average Performance for Planar Segmentation: (%)</figDesc><table>Dataset 
Alg. 1 
[18] 
GPCA 
Merton I 
4.29 (±3.45) 
49.12 (±3.91) 
50.00 (±0) 
Merton II 
2.33 (±2.85) 
49.58 (±0.66) 
46.04 (±10.99) 
Wadham 
1.33 (±2.19) 
49.74 (±0.92) 
45.69 (±7.02) 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc>Running Time (sec)</figDesc><table>Alg. 1 
[18] 
GPCA 
SSC 
LRR 
683.74 
730.94 
0.1421 
0.1904 
0.3553 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">FRR uses directly a non-convex formulation, but it is shown that, for noiseless data, the global optimum has a closed form solution.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Second-order cone programming. Mathematical programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Alizadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Goldfarb</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">95</biblScope>
			<biblScope unit="page" from="3" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">On convex relaxations for quadratically constrained quadratic programming. Mathematical programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Anstreicher</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">136</biblScope>
			<biblScope unit="page" from="233" to="251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Lambertian reflectance and linear subspaces. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Basri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="218" to="233" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Convex optimization. Cambridge university press</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Vandenberghe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Robust principal component analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Candès</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the ACM (JACM)</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Relaxations and randomized methods for nonconvex qcqps. EE392o Class Notes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Daspremont</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Boyd</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
		<respStmt>
			<orgName>Stanford University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Sparse subspace clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Elhamifar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="2790" to="2797" />
		</imprint>
	</monogr>
	<note>CVPR 2009. IEEE Conference on</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Fischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Bolles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="381" to="395" />
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Cvx: Matlab software for disciplined convex programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Grant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ye</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Multiple view geometry in computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>Cambridge university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multiscale hybrid linear models for lossy image representation. Image Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3655" to="3671" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Convergent sdp-relaxations in polynomial optimization with sparsity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Lasserre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Optimization</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="822" to="843" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Robust recovery of subspace structures by low-rank representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Fixed-rank representation for unsupervised visual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>De La Torre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="598" to="605" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Identification of deterministic switched arx systems via identification of algebraic varieties</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Hybrid Systems: Computation and Control</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="449" to="465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Estimation of subspace arrangements with applications in modeling and segmenting mixed data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Derksen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fossum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM review</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="413" to="458" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Iterative reweighted algorithms for matrix rank minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fazel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="3441" to="3473" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Gpca with denoising: A moments-based convex approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ozay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sznaier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lagoa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Camps</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="3209" to="3216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Robust subspace clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Soltanolkotabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Elhamifar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Candes</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.2603</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Greedy algorithms for structurally constrained high dimensional problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tewari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ravikumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="882" to="890" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A benchmark for the comparison of 3-d motion segmentation algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition, 2007. CVPR&apos;07. IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Vandenberghe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Boyd</surname></persName>
		</author>
		<title level="m">Semidefinite programming. SIAM review</title>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="49" to="95" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Vlfeat: An open and portable library of computer vision algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fulkerson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the international conference on Multimedia</title>
		<meeting>the international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1469" to="1472" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
