<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TenSR: Multi-Dimensional Tensor Sparse Representation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Na</forename><surname>Qi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Beijing Key Laboratory of Multimedia and Intelligent Software Technology</orgName>
								<orgName type="department" key="dep2">College of Metropolitan Transportation</orgName>
								<orgName type="institution">Beijing University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhui</forename><surname>Shi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Beijing Key Laboratory of Multimedia and Intelligent Software Technology</orgName>
								<orgName type="department" key="dep2">College of Metropolitan Transportation</orgName>
								<orgName type="institution">Beijing University of Technology</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyan</forename><surname>Sun</surname></persName>
							<email>xysun@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="department">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baocai</forename><surname>Yin</surname></persName>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Faculty of Electronic Information and Electrical Engineering</orgName>
								<orgName type="institution">Dalian University of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">TenSR: Multi-Dimensional Tensor Sparse Representation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:47+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The conventional sparse model relies on data representation in the form of vectors. It represents the vector-valued or vectorized one dimensional (1D) version of an signal as a highly sparse linear combination of basis atoms from a large dictionary. The 1D modeling, though simple, ignores the inherent structure and breaks the local correlation inside multidimensional (MD) signals. It also dramatically increases the demand of memory as well as computational resources especially when dealing with high dimensional signals. In this paper, we propose a new sparse model TenSR based on tensor for MD data representation along with the corresponding MD sparse coding and MD dictionary learning algorithms. The proposed TenSR model is able to well approximate the structure in each mode inherent in MD signals with a series of adaptive separable structure dictionaries via dictionary learning. The proposed MD sparse coding algorithm by proximal method further reduces the computational cost significantly. Experimental results with real world MD signals, i.e. 3D Multi-spectral images, show the proposed TenSR greatly reduces both the computational and memory costs with competitive performance in comparison with the state-of-the-art sparse representation methods. We believe our proposed TenSR model is a promising way to empower the sparse representation especially for large scale high order signals.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In the past decade, sparse representation has been widely used in a variety of tasks in computer vision such as image denoising <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b7">8]</ref>, image super-resolution <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b35">36]</ref>, face recognition <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b38">39]</ref>, and pattern recognition <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b12">13]</ref>. Generally speaking, a classic sparse model represents a vector-valued signal by a linear combination of certain atoms of an overcomplete dictionary. Higher-order signals (e.g. images and videos) need to be dealt with primarily by vectorizing them and applying any of the available vector techniques <ref type="bibr" target="#b28">[29]</ref>. Researches on the conventional one dimensional (1D) sparse representation include the proposal of 1D sparse model <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b8">9]</ref>, sparse coding <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b2">3]</ref>, and dictionary learning algorithms <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b17">18]</ref>. Though simple, the 1D sparse model suffers from high memory as well as high computational costs especially when handling high dimensional data since the vectorized data will be quite long and must be measured using very large sampling matrices.</p><p>Recent research has demonstrated the advantages of maintaining the higher-order data in their original form <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b5">6]</ref>. For image data, the two dimensional (2D) sparse model is proposed to make use of the intrinsic 2D structure and local correlations within images and has been applied to image denoising <ref type="bibr" target="#b25">[26]</ref> and super-resolution <ref type="bibr" target="#b24">[25]</ref>. The 2D dictionary learning problem is solved by the two-phase block-coordinate-relaxation approach. Given the 2D dictionaries, the 1D sparse coding algorithms are extended to solve the 2D sparse coding problem <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b10">11]</ref> or converted to 1D problem and solved via the kronecker product <ref type="bibr" target="#b25">[26]</ref>. By learning 2D dictionaries for images, the 2D sparse model helps to greatly reduce the time complexity and memory cost for image processing <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b24">25]</ref>. On the other hand, the 2D sparse model is difficult to be extended for multidimensional (MD) sparse modeling due to the use of 1D sparse coding method.</p><p>Tensors are also introduced in the sparse representation of vectors to approximate the structure in each mode of MD signals. Due to the equivalence of the constrained Tucker model and the Kronecker representation of a tensor, the tensor is assumed to be represented by separable given dictionaries, known as Kronecker dictionaries, with a sparsity constraint, such as multi-way sparsity and block sparsity <ref type="bibr" target="#b4">[5]</ref>. The corresponding Kroneker-OMP and N-way Block OMP (N-BOMP) algorithms are also presented for recovery of MD signals with fixed dictionaries. Furthermore, dictionary learning method based on tensor factorization are proposed in <ref type="bibr" target="#b39">[40]</ref>, and some algorithms are presented to approximate tensor based on tensor decomposition <ref type="bibr" target="#b19">[20]</ref> or tensor low-rank approximation <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b23">24]</ref>. However, to the best of our knowledge, there is no unified framework for tensorbased sparse representation presented in literature.</p><p>In this paper, we propose the first Tensor Sparse model for MD signal Representation (TenSR in short) along with the corresponding sparse coding and dictionary learning algorithms. Our proposed sparse coding algorithm is a iterative shrinkage thresholding method, which can be easily implemented via the n-mode product of tensor by matrix and element-wise thresholding. We also formulate the dictionary learning problem as an optimization problem solved via a two-phase block-coordinate-relaxation approach including sparse coding and dictionary updating. Both dictionary learning and sparse coding are without Kronecker product so as to greatly reduce the computation burden. In addition, the efficiency of our sparse coding can be further improved through parallel computing. Dictionaries of every dimension (mode) can be updated by a series of quadratically constrained quadratic programming (QCQP) problem via Lagrange dual method. The advantages of our proposed TenSR model as well as the sparse coding and dictionary learning algorithms are demonstrated with the real world 3D signal processing. To summarize, this paper makes the following contributions:</p><p>• We propose the first tensor sparse model TenSR for MD signal representation. To the best of knowledge, this is the first paper presenting this theory along with the corresponding sparse coding and dictionary learning algorithms.</p><p>• We propose the novel sparse coding and dictionary learning algorithms based on tensor operation rather than Kronecker product, which help in not only revealing structure in each mode inherent in MD signals but also significantly reducing computational as well as memory cost for MD signal processing.</p><p>• Our proposed TenSR model is able to empower the sparse representation especially when dealing with high dimensional data (e.g. 3D multi-spectral images as demonstrated in experiments) by greatly reducing the processing cost but meanwhile achieving comparable performance with regard to the conventional 1D sparse model.</p><p>The rest of this paper is organized as follows. Section 2 reviews the related work on sparse representation for 1D, 2D, and MD signals. Section 3 presents our MD tensor sparse model TenSR, the corresponding MD sparse coding and dictionary learning algorithms, followed by the complexity analysis. In Section 4, we demonstrate the effectiveness of our TenSR model by simulation experiment and 3D multi-spectral image denoising problem. Finally, Section 5 concludes this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>In this section, we briefly review the related work on sparse representation towards 1D, 2D and MD signals.</p><p>1D signal (vector) The conventional 1D sparse model represents a vector x by the linear combination of a few atoms from a large dictionary D, denoted as x = Db, b 0 ≤ L, where L is the sparsity of b. The computational techniques for approximating sparse coefficient b under a given dictionary D and x includes greedy pursuit (e.g. OMP <ref type="bibr" target="#b22">[23]</ref>) and convex relaxation optimization, such as Lasso <ref type="bibr" target="#b31">[32]</ref> and FISTA <ref type="bibr" target="#b2">[3]</ref>. Rather than using fixed dictionaries, dictionary learning algorithms <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b21">22]</ref> are also investigated, which substantially improve the performance of sparse representation <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b15">16]</ref>. However, the efficiency of 1D sparse coding as well as dictionary learning degrades rapidly as the dimensionality increases.</p><p>2D signal (matrix) A matrix X is sparse modeled by two dictionaries D 1 , D 2 , and a sparse coefficient matrix</p><formula xml:id="formula_0">B, denoted as X = D 1 B T D T 2 , B 0 ≤ L,</formula><p>where L is the sparsity of B <ref type="bibr" target="#b25">[26]</ref>. Given dictionaries D 1 and D 2 , the 2D sparse model can be easily converted to 1D sparse model x = Db, b 0 ≤ L, where D = D 2 ⊗D 1 . The dictionaries D 1 and D 2 are learned by the two-phase block-coordinaterelaxation approach via sparse coding and dictionary updating <ref type="bibr" target="#b25">[26]</ref> while the sparse coding problem can be solved by 2DSL0 through steepest ascent <ref type="bibr" target="#b11">[12]</ref> or the greedy algorithm 2D-OMP <ref type="bibr" target="#b10">[11]</ref>. The presented 2D sparse model is able to facilitate 2D signal processing. However,we notice that the sparse coding and dictionary learning algorithms presented for 2D sparse representation are not capable enough for MD signals. On the one hand, the 2D sparse coding problem is recast to 1D one by converting the 2D signal to a long vector via the Kronecker product during dictionary learning <ref type="bibr" target="#b25">[26]</ref>. On the other hand, the Riemannian conjugate gradient algorithm and the manifold-based dictionary learning in <ref type="bibr" target="#b13">[14]</ref> are quite complex to compute for high order data.</p><p>MD signal (tensor) A tensor X can be represented by a series of known Kroneker dictionaries {D n } N n=1 and the core tensor B with the multi-way sparsity or block sparsity constraint, denoted as the Tucker model X = B× 1 D 1 × 2 D 2 · · · × N D N <ref type="bibr" target="#b4">[5]</ref>. Given the fixed Kronecker dictionaries ( such as DCT, DWT, and DFT), the Kroneker-OMP and N-BOMP algorithms are also presented in <ref type="bibr" target="#b4">[5]</ref> to recovery MD signals possessing Kronecker structure and block-sparsity. However, the Kronecker-OMP and N-OMP algorithm is relatively complicated due to the Kronecker product operation. In addition, some other algorithms are presented to approximate tensor based on tensor decomposition, such as PARAFAC <ref type="bibr" target="#b19">[20]</ref> and tensor factorization under Tucker model <ref type="bibr" target="#b39">[40]</ref>, or tensor low-rank approximation <ref type="bibr" target="#b18">[19]</ref>, such as LRTA <ref type="bibr" target="#b29">[30]</ref>, HOSVD <ref type="bibr" target="#b27">[28]</ref>, and Tensor-DL <ref type="bibr" target="#b23">[24]</ref>. However, they only decompose and approximate the tensor itself rather than model the tensor.</p><p>Different from all previous tensor-based methods, in this paper, we not only propose the tenser sparse model for MD signal representation, but also present the corresponding sparse coding and dictionary learning algorithms. Moreover, since no Kronecker product is used, our proposed scheme is much light-weighted in terms of computational and memory costs for MD signal processing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">MD Tensor Sparse Representation</head><p>In this section, we present our TenSR model for MD signal representation followed by the corresponding sparse coding and dictionary learning algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Notations</head><p>For easy understanding, we would like to first introduce some notations used in this paper.</p><p>A tensor of order N is denoted as X ∈ R I1×I2···×I N . The l 0 , l 1 and l F norms of a N -order tensor X are denoted as</p><formula xml:id="formula_1">X 0 = #{X (i 1 , i 2 , · · · , i N ) = 0}, X 1 = i1 i2 · · · i N |X (i 1 , i 2 , · · · , i N )|, and X F = ( i1 i2 · · · i N X (i 1 , i 2 , · · · , i N ) 2 ) 1/2 , re- spectively, where X (i 1 , i 2 , · · · , i N ) is the (i 1 , i 2 , · · · , i N )- element of X .</formula><p>n-mode vectors are obtained by fixing every index but the one in the mode n. The n-mode unfolding matrix X (n) ∈ R In×I1I2···In−1In+1···I N is defined by arranging all the n-mode vectors as columns of a matrix. Following the formulation of tensor multiplication in <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b16">17]</ref>, we denote the n-mode product of tensor X and matrix U ∈ R Jn×In as X × n U, which is also a N -order tensor Y ∈ R I1×···×In−1×Jn×In+1×···×I N , whose entries Y(i 1 , · · · , i n−1 , j n , i n+1 , · · · , i N ) are computed by</p><formula xml:id="formula_2">In in=1 X (i 1 , i 2 , · · · , i N )U(j n , i n ).</formula><p>The inner product of two same-sized tensors X and Y is the sum of the products of their entries, i.e.,</p><formula xml:id="formula_3">X , Y = I1 i1=1 I2 i2=1 · · · I N i N =1 X (i 1 , i 2 , · · · , i N )Y(i 1 , i 2 , · · · , i N )</formula><p>Operator ⊗ represents the Kronecker product. The vectorization of X is x.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">The Proposed TenSR model</head><p>Let a N -order tensor X ∈ R I1×I2···×I N denotes a MD signal. In order to approximate the structure and exploit the correlations in every dimension in the MD signal X , we propose the MD TenSR model as</p><formula xml:id="formula_4">X = B× 1 D 1 × 2 D 2 · · · × N D N , B 0 ≤ K,<label>(1)</label></formula><p>which formulates the tensor X as a n-mode product of a N −order sparse tensor B and a series of matrix D n ∈ R In×Mn , I n ≤ M n . Here D n is defined as the n-th dimensional dictionary (or dictionary at mode n) and K is the sparsity denoting the number of the non-zero entries in B. It is seen that there is a formal resemblance between the Tucker model in <ref type="bibr" target="#b4">[5]</ref> and our TenSR model in <ref type="formula" target="#formula_4">(1)</ref>; they are in fact quite different. <ref type="bibr" target="#b4">[5]</ref> uses Tucker model to only approximate a tensor itself based on given Kronecker dictionaries. Our TenSR model, on the other hand, explores the features and structures of tensors in different dimensions by adaptive MD dictionaries rather than determined analytical dictionaries to model MD signals (tensors). The dictionaries D n in (1) can be learned by unfolding the tensor X in n-mode, resulting the equivalent unfolded matrix representation <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b16">17]</ref> </p><formula xml:id="formula_5">X (n) = D n B (n) (D N · · · ⊗D n+1 ⊗ D n−1 · · · ⊗D 1 ) T . (2) Let A (n) = B (n) (D N ⊗ · · · ⊗D n+1 ⊗ D n−1 ⊗ · · · ⊗D 1 ) T .</formula><p>Then, the dictionary learning problem can be solved on the basis of X (n) = D n A (n) , where D n is the dictionary of X (n) that reflects the correlation of X in the n-th dimension and A (n) is the corresponding representation on the n-th dimension. However, as mentioned before, the high complexity of the computation of A (n) by kronecker product will prevent the dictionary learning method from high order data processing. Therefore, a dedicated MD dictionary learning method should be studied for our TenSR model for MD signal representation.</p><p>Given learned dictionaries {D n } N n=1 , the TenSR model (1) can be easily converted to the traditional 1D sparse model as</p><formula xml:id="formula_6">x = Db, b 0 ≤ K. (3) where D = D N ⊗D N −1 ⊗ · · · ⊗D 1 , D ∈ R I×M , I = N n=1 I n , and M = N n=1 M n .</formula><p>x and b are the vectorization of X and B, respectively. However, with the increase of the dimension of MD signal, the size of the dictionary D will be exponentially expanded. No matter how large the dictionary is, the correlations that inherently exist in each domain of MD signals are ignored due to the vectorization in 1D sparse model. The sparse coding method presented in <ref type="bibr" target="#b4">[5]</ref> may also be adopted here but still with Kronecker product. Thus a new sparse coding method is presented in the following subsection to solve these problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">MD sparse coding</head><p>In this subsection, we discuss how to calculate the sparse coefficient B given dictionaries {D n } N n=1 under our TenSR model.</p><p>Given all the sparse dictionaries {D n } N n=1 , calculating B from X is a MD sparse coding problem formulated as</p><formula xml:id="formula_7">min B 1 2 X − B × 1 D 1 × 2 D 2 · · · × N D N 2 F + λ B 0 ,<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Tensor-based Iterative Shrinkage Thresholding</head><p>Initialization:</p><formula xml:id="formula_8">Set C 1 = B 0 ∈ R M1×M2···×M N , t 1 = 1 For k = 1 : num do Set L k = η k N n=1 D T n D n 2 Compute ∇f (C k ) via Eq.(9) Compute B k via P λ/L k (C k − 1 L k ∇f (C k )) t k+1 = 1+ √ 1+4t 2 k 2 C k+1 = B k + t k −1 t k+1 (B k − B k−1 ) End For Output: Sparse coefficient B</formula><p>where λ is a parameter to balance the fidelity and sparsity. The non-convex l 0 constraint in (4) can be relaxed to the l 1 norm to yield a convex optimization problem as</p><formula xml:id="formula_9">min B 1 2 X − B × 1 D 1 × 2 D 2 · · · × N D N 2 F + λ B 1 . (5)</formula><p>Rather than using 1D sparse coding or Kronecker product based methods, we propose a new sparse coding algorithm − Tensor-based Iterative Shrinkage Thresholding Algorithm (TISTA) to solve (4) as well as <ref type="formula">(5)</ref> directly. We first rewrite the objective function w.r.t B in (4) or <ref type="formula">(5)</ref> as</p><formula xml:id="formula_10">min B f (B) + λg(B),<label>(6)</label></formula><p>where f (B) stands for the data-fitting term</p><formula xml:id="formula_11">1 2 X − B × 1 D 1 × 2 D 2 · · · × N D N 2 F and g(B)</formula><p>stands for the sparsity constraint term B 1 or B 0 . We take a iterative shrinkage algorithm to solve the non-smooth regularized problem <ref type="formula" target="#formula_10">(6)</ref>, which can be rewritten as a linearized function f around the previous estimate B k−1 with the proximal regularization <ref type="bibr" target="#b14">[15]</ref> and the non-smooth regularization. Thus, at the k-th iteration, B k can be updated by</p><formula xml:id="formula_12">B k = argmin B f (B k−1 ) + ∇f (B k−1 ), B − B k−1 + L k 2 B − B k−1 2 F + λg(B),<label>(7)</label></formula><p>where L k &gt; 0 is a Lipschitz constant <ref type="bibr" target="#b14">[15]</ref> and ∇f (B) is a gradient defined on the tensor-field. Then this problem can be rewritten as, </p><formula xml:id="formula_13">B k = argmin B 1 2 B−(B k−1 − 1 L k ∇f (B k−1 )) 2 F + λ L k g(B).<label>(8)</label></formula><formula xml:id="formula_14">X − B × 1 D 1 × 2 D 2 · · · × N D N 2 F in our TenSR satisfies −∇f (B) = X × 1 D T 1 × 2 D T 2 · · · × N D T N −B × 1 D T 1 D 1 × 2 D T 2 D 2 · · · × N D T N D N .<label>(9)</label></formula><p>We argue that our solution via <ref type="formula" target="#formula_14">(9)</ref> is equivalent to the corresponding 1D sparse coding by proximal method. Note that the problems <ref type="formula" target="#formula_7">(4)</ref> and <ref type="formula">(5)</ref> can be converted to the corresponding 1D sparse coding problems as</p><formula xml:id="formula_15">min b 1 2 x − Db 2 2 + λ b 0 . (10) min b 1 2 x − Db 2 2 + λ b 1 .<label>(11)</label></formula><p>By using the formula of tensor and kronecker product in <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b16">17]</ref>, the right term in <ref type="formula" target="#formula_14">(9)</ref> can be converted as</p><formula xml:id="formula_16">(D T N ⊗D T N −1 ⊗ · · · ⊗D T 1 )x −(D T N D N ⊗ D T N −1 D N −1 · · · ⊗ D T 1 D 1 )b,<label>(12)</label></formula><p>which can be derived to <ref type="formula" target="#formula_14">(9)</ref> is equivalent to the gradient of the data-fitting term <ref type="bibr">1 2</ref> x − Db 2 2 at vector b in 1D sparse coding problems <ref type="formula" target="#formula_4">(10)</ref> and <ref type="formula" target="#formula_4">(11)</ref>, thus making these two solutions equivalent.</p><formula xml:id="formula_17">D T x − D T Db,<label>(13)</label></formula><formula xml:id="formula_18">with the equal term (D N ⊗D N −1 ⊗ · · · ⊗D 1 ) T x − (D T N ⊗ D T N −1 · · · ⊗ D T 1 )(D N ⊗ D N −1 · · · ⊗ D 1 )b. It can be ob- served that −(D T x − D T Db) in</formula><p>We then discuss how to determine the Lipschitz constant L k in <ref type="bibr" target="#b7">(8)</ref>. We assume f is a smooth convex function of the type C 1,1 . That is, for every</p><formula xml:id="formula_19">B, C ∈ R M1×M2···×M N , f is continuously differentiable with Lipschitz continuous gradient L(f ) satisfying ∇f (B) − ∇f (C) ≤ L(f ) B − C .<label>(14)</label></formula><p>where · denotes the l F norm on N -order tensor and L(f ) is the Lipschitz constant of ∇f . Substitute <ref type="bibr" target="#b8">(9)</ref> to <ref type="formula" target="#formula_4">(14)</ref>, we have</p><formula xml:id="formula_20">∇f (B) − ∇f (C) F = (B − C) × 1 D T 1 D 1 × 2 D T 2 D 2 · · · × N D T N D N F = (D T N D N ⊗ D T N −1 D N −1 · · · ⊗ D T 1 D 1 )(b − c) 2 ≤ D T N D N ⊗ D T N −1 D N −1 · · · ⊗ D T 1 D 1 2 b − c 2 = D T N D N 2 D T N −1 D N −1 2 · · · D T 1 D 1 2 b − c 2 = D T N D N 2 D T N −1 D N −1 2 · · · D T 1 D 1 2 B − C F .<label>(15)</label></formula><p>Thus the smallest Lipschitz constant of the gradient ∇f is</p><formula xml:id="formula_21">L(f ) = N n=1 D T n D n 2 .</formula><p>Then, in our iteration process, L k = η k N n=1 D T n D n 2 with η ≥ 1, i.e. L k ≥ L(f ). Finally, we present the solution of (8) with different sparsity constraint g(B). With the regularization term g(B) = B 1 , the proximal operator P τ (·) for solving <ref type="bibr" target="#b7">(8)</ref> is the (elementwise) soft-thresholding operator. Thus, the unique solution of (8) is S λ/L k (B k−1 − 1 L k ∇f (B k−1 )), where S τ (·) is the soft-thresholding operator S τ (·) −→ sign(·) max(| · | − τ, 0). In case of the l 0 norm sparsity constraint, i.e., g(B) = B 0 , the solution of (8) is </p><formula xml:id="formula_22">A = J × 1 D 1 × 2 D 2 · · · D n−1 × n+1 D n+1 · · · × N D N Get A (n) and Update D n via Eq.(19). End For Output: Learned dictionaries {D n } N n=1 H λ/L k (B k−1 − 1 L k ∇f (B k−1 )), where H τ (·)</formula><p>is the hardthresholding operator H τ (·) −→ max(· − τ, 0). For convenience of algorithm description, we denote P λ/L k (B k−1 − 1 L k ∇f (B k−1 )) as the solution of (8) with either l 1 norm or l 0 norm.</p><p>We further speed up the convergence of the iterative shrinkage algorithm by employing the iterative shrinkage operator at the point C k where</p><formula xml:id="formula_23">C k = B k−1 + ζ k (B k−1 − B k−2 )<label>(16)</label></formula><p>and ζ k &gt; 0 is a suitable step size, rather than at the point</p><formula xml:id="formula_24">B k−1 . We also set ζ k = (t k − 1)/t k+1 where t k+1 = 1+ √ 1+4t 2 k 2</formula><p>[3] and this extrapolation significantly accelerates the proximal gradient method for convex composite problem <ref type="bibr" target="#b34">[35]</ref>. Algorithm 1 summarizes our proposed Tensor-based Iterative Shrinkage Thresholding Algorithm for MD sparse coding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">MD dictionary learning</head><p>Given a set of training samples I = (X 1 , X 2 , · · · , X S ) ∈ R I1×I2···×I N ×S , where S denotes the number of N -order tensors X j ∈ R I1×I2···×I N , we formulate our MD dictionary learning problem as</p><formula xml:id="formula_25">min {Dn} N n=1 ,J 1 2 I − J × 1 D 1 × 2 D 2 · · · × N D N 2 F + λ J 1 s.t. D n (:, r) 2 2 = 1, 1 ≤ j ≤ S, 1 ≤ n ≤ N, 1 ≤ r ≤ M n ,<label>(17)</label></formula><p>where J = (B 1 , B 2 , · · · , B S ) ∈ R M1×M2···×M N ×S denotes the set of sparse coefficients of all the training samples I, λ is a parameter to balance the fidelity and sparsity, and {D n } N n=1 are targeted MD separate dictionaries. The problem (17) can be solved by using a two-phase block-coordinate-relaxation approach via sparse coding and dictionary updating. The process repeats until certain stop criterion is satisfied, e.g. the relative error of the objective function at adjacent two iteration is below some predetermined level ǫ. Algorithm 2 summarizes our Tensor-based Dictionary Learning method.</p><p>Sparse coding aims to approximate the sparse coefficient J of the training set I with fixed {D i } N i=1 by solving</p><formula xml:id="formula_26">min J 1 2 I − J × 1 D 1 × 2 D 2 · · · × N D N 2 F + λ J 1 .<label>(18)</label></formula><p>We are able to directly solve (18) by the MD sparse coding algorithm described in Sec. 3.3, rather than solving S independent MD sparse coding optimization problems with respect to each N -order signal X j <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b25">26]</ref>. In addition, we can divide all the samples to different subsets and solve the sparse coding problem for each subset in parallel to generate the final sparse coefficient J . Thus our sparse coding process runs much faster than the other related solutions.</p><p>Dictionary update tries to update {D n } N n=1 using the computed sparse coefficients J . The optimization procedures for {D n } N n=1 are similar. Without loss of generality, we take the updating of D n as an example to present our dictionary update method. Due to the interchangeability of n-mode product in our TenSR model <ref type="formula" target="#formula_4">(1)</ref>,</p><formula xml:id="formula_27">each tensor X j satisfies X j = A j × n D n with A j = B j × 1 D 1 × 2 D 2 · · · × n−1 D n−1 × n+1 D n+1 · · · × N D N , thus A j</formula><p>(n) can be easily obtained by unfolding the tensor A j rather than in the way by kronecker product mentioned in Sec. 3.2. Therefore, we first calculate A ∈ R M1×M2···×Mn−1×In×Mn+1×···×M N ×S 1 by J × 1 D 1 × 2 D 2 · · · × n−1 D n−1 × n+1 D n+1 · · · × N D N to make sure I ≈ A × n D n , and then unfold A in n-mode to obtain A (n) to guarantee I (n) ≈ D n A (n) . Thus, D n can be updated by </p><p>It is a quadratically constrained quadratic programming (QCQP) problem, where I (n) ∈ R In×Hn and A (n) ∈ R Mn×Hn are the mode-n unfolding matrix of I and A, respectively. Here H n = I 1 I 2 · · · I n−1 I n+1 · · · I N S. The problem <ref type="bibr" target="#b18">(19)</ref> can be resolved via the Lagrange dual <ref type="bibr" target="#b17">[18]</ref>.</p><formula xml:id="formula_29">The Lagrangian L here is L(D n , λ) = trace((I (n) − D n A (n) ) T (I (n) − D n A (n) ) + Mn j=1 λ j ( In i=1 D n (i, j) 2 − 1)),</formula><p>where each λ j ≥ 0 is a dual variable. Thus, the Lagrange dual function D(λ) = min Dn L(D n , λ) can be optimized by the Newton's method or conjugate gradient. After maximizing D(λ), we obtain the optimal bases D T n = (A (n) A T (n) + Λ) −1 (I (n) A T (n) ) T , where Λ = diag(λ). Compared with <ref type="bibr" target="#b39">[40]</ref> and <ref type="bibr" target="#b25">[26]</ref>, the new way of computing A (n) without Kronecker product can greatly reduce the computation complexity of our dictionary updating.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Complexity Analysis</head><p>In this subsection, we discuss the complexity as well as the memory usage of our sparse coding and dictionary </p><formula xml:id="formula_30">SC 1D D T x − D T Db O(IM + IM + M IM + M M ) O(IM 2 ) MD ∇f (B) O( N n=1 ( n i=1 M i N j=n I j + O( N n=1 M n M ) M n I n M n + M n M )) DU 1D min D I − DB 2 F O(M SM + M 3 + ISM + M M I) O(M 2 S) MD A (n) by kronecker product O(IM/(M n I n ) + IM/M n S) O( N n=1 N k=1 k =n k i=1 M i N j=k I j I n S) * A (n) by n-mode product O( N k=1 k =n ( k i=1 M i N j=k I j I n S)) min Dn I (n) − D n A (n) 2 F O(M 2</formula><p>n H n ) + + Hn = I 1 I 2 · · · I n−1 I n+1 · · · I N S * The n-mode product method for A (n) is less complicated than the Kronecker Product one. Thus, here only summarize the complexity of A (n) by n-mode product and min Dn I (n) − DnA (n) 2 F . </p><formula xml:id="formula_31">Time Complexity Memory SC DU 1D O(c 2N d 3N ) O(c 2N d 2N S) N n=1 M n I n MD O(N c N +1 d N +1 ) O(N c N d N +2 S) N n=1 M n I n</formula><p>learning algorithms with regard to those of conventional 1D counterparts.</p><p>We first analyze complexities of the main components of MD and 1D sparse coding (SC) and dictionary updating (DU) algorithms and summarized in <ref type="table" target="#tab_2">Table 1</ref>. In terms of SC, <ref type="table" target="#tab_2">Table 1</ref> shows the complexity of calculating ∇f (B) and D T x − D T Db, which cost most of time in SC step at each iteration. For a N -order signal X ∈ R I1×I2···×I N , the MD sparse coefficient B ∈ R M1×M2···×M N is computed with fixed dictionaries {D n } N n=1 , where D n ∈ R In×Mn . Correspondingly, the 1D sparse coefficient b ∈ R M is sparse approximated by the 1D dictionary D ∈ R I×M and x ∈ R I , where I = N n=1 I n , and M = N n=1 M n . In terms of DU, given a set of training samples I = (X 1 , X 2 , · · · , X S ) ∈ R I1×I2···×I N ×S , we learn MD dictionaries {D n } N n=1 , where D n ∈ R In×Mn . In order to update D n via (19), we need calculate A (n) in our scheme. In fact, A (n) can be computed in two ways, a) n−mode product which directly unfolds the tensor</p><formula xml:id="formula_32">A = J × 1 D 1 × 2 D 2 · · · × n−1 D n−1 × n+1 D n+1 · · · × N D N , and b) Kronecker product, A (n) = [A 1 (n) , A 2 (n) , · · · , A S (n) ] where A j (n) = B j (n) (D N · · · ⊗D n+1 ⊗ D n−1 · · · ⊗D 1 ) T [40]</formula><p>. The complexity of these two ways are all given in <ref type="table" target="#tab_2">Table 1</ref>. Clearly, our n−mode product method is less complicated than the Kronecker product one. For 1D dictionary learning, the correspondingly 1D training set is <ref type="table" target="#tab_3">Table 2</ref> summarizes the total time complexity of SC and DU for 1D and MD sparse model. Without loss of generality, we assume I n = d and M n is c times of I n , denoted as M n = cd, where c reflects the redundancy rate of dic- tionary D n . We can observe that our proposed MD sparse coding and dictionary learning algorithms will greatly reduce the time complexity especially for high order signals. In addition, the memory usage of our MD model is also significantly less than that of the 1D model.</p><formula xml:id="formula_33">I = [x 1 , x 2 , · · · , x S ] ∈ R I×S , thus the 1D dictionar- ies D ∈ R I×M is updated by min D I − DB 2 F , where B = [b 1 , b 2 , · · · , b S ] ∈ R M ×S .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results</head><p>We demonstrate the effectiveness of our TenSR model by first discussing the convergence of our dictionary learning and sparse coding algorithms in the Simulation Experiment and then evaluate the performance on 3D Multispectral Image (MSI) Denoising. <ref type="figure" target="#fig_1">Fig. 1</ref> shows the convergence rate of our sparse coding algorithm Tensor-based Iterative Shrinkage Thresholding Algorithm (TISTA) with regard to that of the classic 1D sparse coding method FISTA <ref type="bibr" target="#b2">[3]</ref>. Two sets of convergences curves are shown in <ref type="figure" target="#fig_1">Fig 1 (a)</ref>   times (in logarithmic coordinates) are quite different. Our TISTA method converge much more rapidly. The higher the dimension as well as data size, the higher the acceleration of our sparse coding algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Simulation Experiment</head><p>We further evaluate the time efficiency of our TISTA for recovering a series of MD signals in comparison with that of 1D FISTA in <ref type="table" target="#tab_4">Table 3</ref>. In this simulation, we sample cubes of size 5 × 5 × 5 from a 3D sub-MSI of size L × W × H(12 × 12 × 31, 16 × 16 × 31, and 32 × 32 × 31). ODCT dictionaries {D n } 3 n=1 , where D n ∈ R 5×10 , are used for the reconstructions. As illustrated in <ref type="figure" target="#fig_1">Fig. 1</ref>, TISTA and FISTA are similar in precision at each iteration. We thus measure the time efficiencies of these two methods by the running time of reconstructing a same number of sampled cubes at iteration num = 50 and λ = 1. As shown in Table 3, three set of time complexities are provided for TISTA when the cubes are recovered sequentially (Single), in batch of 500 (Batch), and altogether (All), respectively. We provide the complexity comparisons in sequential in both <ref type="figure" target="#fig_1">Fig. 1</ref> and Tab. 3, respectively. It is clear that our sparse coding is much fast in this case. Moreover, our scheme supports parallel naturally and can be easily speeded up as shown in Tab. <ref type="bibr" target="#b2">3</ref>.</p><p>The convergence of the presented MD dictionary learning algorithm is evaluated in <ref type="figure" target="#fig_4">Fig. 3</ref>. Here we train three dictionaries D 1 , D 2 , D 3 of size 5 × 10 from 40, 000 cubes, which are of size 5 × 5 × 5 randomly sampled from the 3D Multi-spectral images 'beads' <ref type="bibr" target="#b37">[38]</ref>. The learned 3D dictionaries D 1 , D 2 , D 3 are illustrated <ref type="figure" target="#fig_3">Fig. 2</ref>. These two figures show that our dictionary learning method is able to capture the feature at each dimension along with the convergence property.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Multi-spectral Image Denoising</head><p>In this subsection, we evaluate the performance of our TenSR model using 3D real-word examples − MSI images in Columbia MSI Database <ref type="bibr" target="#b37">[38]</ref> 2 . The denoising problem which has been widely studied in sparse representation is used as the target application. We add Gaussian white noise to these images at different noise levels σ = 5, 10, 20, 30, 50. In our TenSR-based denoising method, the 3D dictionaries D 1 , D 2 , D 3 of size 5 × 10 are <ref type="bibr" target="#b1">2</ref> The dataset contains 32 real-world scenes at a spatial resolution of 512 × 512 and a spectral resolution of 31 ranging from 400nm to 700nm.  initialized by ODCT and trained iteratively (≤ 50 times) in the same configuration of <ref type="figure" target="#fig_4">Fig. 3</ref>. Then we use the learned dictionaries to denoise the MSI images, with overlap of 3 pixels between adjacent cubes of size 5 × 5 × 5. Parameters in our scheme are λ = 9, 20, 45, 70, 160 for σ = 5, 10, 20, 30, 50, respectively. <ref type="table" target="#tab_5">Table 4</ref> shows the comparison results in terms of average PSNR and SSIM. There are 6 state-of-the-art MSI denoising methods are involved, including tensor dictionary learning (Tensor-DL) method <ref type="bibr" target="#b23">[24]</ref>, BM4D method <ref type="bibr" target="#b20">[21]</ref>, PARAFAC method <ref type="bibr" target="#b19">[20]</ref>, low-rank tensor approximation (LRTA) method <ref type="bibr" target="#b29">[30]</ref>, band-wise KSVD (BwK-SVD) method <ref type="bibr" target="#b9">[10]</ref> and 3D-cube KSVD (3DK-SVD) method <ref type="bibr" target="#b9">[10]</ref>  <ref type="bibr" target="#b2">3</ref> . We further classify these methods in two categories (1) without any extra constraint, e.g. nonlocal similarity, and (2) with additional prior like nonlocal similarity. As shown in <ref type="table" target="#tab_5">Table 4</ref>, our current solution belongs to category (1). Our scheme outperforms most of methods and is comparable with LRTA in category (1). Due to lack of additional constraint, all the algorithms in category (1) achieves lower PSNR and SSIM values than those in category <ref type="bibr" target="#b1">(2)</ref>. <ref type="figure" target="#fig_5">Fig. 4</ref> shows one exemplified visual result of a portion of the MSI image 'cloth' at the 420nm band with Gaussian noise at σ = 10.</p><p>We would like to point out that the size of our dictionary is the smallest among those of all sparse-based test methods including Bw-KSVD <ref type="bibr" target="#b9">[10]</ref>, 3DK-SVD <ref type="bibr" target="#b9">[10]</ref>, and Tensor-DL <ref type="bibr" target="#b23">[24]</ref>. The total size of dictionary of our method is 3 × 5 × 10 as we learned three small dictionaries of size 5×10. The total size of learned dictionaries of Bw-KSVD is 64 × 128 × 31, where a dictionary of size 64 × 128 is trained for each band image of all the 31 frame images. A dictio-   <ref type="bibr" target="#b37">[38]</ref>. From left to right: Original image at 420nm band, PARAFAC <ref type="bibr" target="#b19">[20]</ref>, BwK-SVD <ref type="bibr" target="#b9">[10]</ref>, 3DK-SVD <ref type="bibr" target="#b9">[10]</ref>, LRTA <ref type="bibr" target="#b29">[30]</ref>, BM4D <ref type="bibr" target="#b20">[21]</ref>, TensorDL <ref type="bibr" target="#b23">[24]</ref>, and Ours. nary of size 448 × 500 is learned in 3DK-SVD <ref type="bibr" target="#b9">[10]</ref> with each cube size 8 × 8 × 7. The Tensor-DL <ref type="bibr" target="#b23">[24]</ref> has a dictionary of size 8 × 8 × 31 × 648 by first building 162 groups of 3D band patches with each cube of size 8 × 8 × 31 and then obtaining a dictionary with 4 atoms for each groups via Tucker decomposition. The total dictionary size of Bw-KSVD <ref type="bibr" target="#b9">[10]</ref>, 3DK-SVD <ref type="bibr" target="#b9">[10]</ref>, and Tensor-DL <ref type="bibr" target="#b23">[24]</ref> are 1693, 1493, and 8570 times of our TenSR model, respectively. We believe that if we integrate nonlocal similarity to our model and train dictionaries for each group of MD signals, our performance for denoising will be significantly improved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we propose the first tensor sparse model TenSR to capture features and explore the correlations inherent in MD signals. We also propose the corresponding formulations as well as algorithms for calculating MD sparse coefficients and learning MD dictionaries. The proposed MD sparse coding algorithm by proximal method reduces the time complexity significantly so as to facilitate the dictionary learning and the recovery problem for high order data. The presented dictionary learning method is capable of approximating structures in each dimension via a series of adaptive separable structure dictionaries. We further analyze the properties of the TenSR model in terms of convergence as well as complexity. The presented TenSR model is applied to 3D multi-spectral image denoising and achieves competitive performance with the state-of-the-art related methods but with much lower time complexity and memory cost.</p><p>On the other hand, as shown in the denoising results, the performance of our current solution is not as good as the ones with additional prior, e.g. nonlocal similarity. We would like to further improve the performance of our algorithm by introducing prior, e.g. non-local similarities, in our future work. Moreover, in our current TenSR model, we assign each dimension a dictionary which may not be adaptive enough. For dimensions who have similar structures or strong correlations, we may support much flexible combinations of dimensions in our future dictionary learning algorithm.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>t. D n (:, r) 2 2 = 1, 1 ≤ r ≤ M n .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>Convergence rates of sparse coding algorithms 1D FISTA[3] and our MD TISTA. Y −label is objective function value of (5), X−label is the computational time in the log10(t) coordinate. (a) shows the case of 2D patch (N = 2) of size d × d and (b) is the one of 3D cube (N = 3) of size d × d × d, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>and (b) for 2D patch (N = 2) at sizes d × d (d = 8, 16, 20) and 3D cube (N = 3) at sizes d × d × d (d = 4, 6, 8), respectively. The dictionaries used in both simulations are Overcomplete DCT (ODCT) dictionaries {D n } N n=1 , where D n ∈ R d×cd , c = 2 (definitions of parameters can be found in subsection 3.5). This figure shows that the reconstruction precisions determined by (5) of these two methods are similar whereas the convergence</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 .</head><label>2</label><figDesc>Exemplified dictionary in our TenSR model. (a) Learned dictionaries D1, D2, D3 using TenSR model and (b) The Kroneker product D of learned dictionaries in (a) of arbitrary dimensions, where each column of D1, D2, D3 is an atom of each dimension of the cube and each square of D is an atom of size 5 × 5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 .</head><label>3</label><figDesc>Convergent Analysis. The X-label is the iteration number and the Y -label is the objective function of Eq.<ref type="bibr" target="#b16">(17)</ref>. It is shown the our Tensor-based dictionary learning algorithm is convergent.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>Visual comparison of reconstruction results by different methods on 'cloth' in dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>Algorithm 2 Tensor-based Dictionary LearningInput: Training Set I, number of iteration num Initialization: Set the dictionary {D n } N n=1 .</figDesc><table>For l = 1 : num 
Sparse coding Step: 
Compute J via Eq.(18) according to Algorithm 1. 
Dictionary Update Step: 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 1 .</head><label>1</label><figDesc>Complexity Analysis of Sparse Coding (SC) and Dictionary Update (DU) for MD and 1D Sparse Model</figDesc><table>Operation 
Complexity in Detail 
Complexity 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 .</head><label>2</label><figDesc>Time Complexity of SC and DU, and Memory Usage of Dictionary for MD and 1D Sparse Model</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="true"><head>Table 3 .</head><label>3</label><figDesc>Time complexity (in seconds) of recovering three sets of sampling cubes of 1D FISTA as well as our TISTA. Here Single, Batch, and All denote that the reconstruction are performed sequentially, in batch of 500, and altogether, respectively.</figDesc><table>FISTA 
TISTA 
Cube Size (cubes) 
Single 
Single Batch 
All 
12 × 12 × 31(1758) 
15674 
247.7 
16.9 
16.1 
16 × 16 × 31(3888) 
35912 
556.2 
36.4 
35.4 
32 × 32 × 31(21168) 193490 3038.7 200.7 189.0 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 4 .</head><label>4</label><figDesc>Average PSNR and SSIM results of the different methods for different noise levels on the set of test multispectral images. (1) Methods without nonlocal similarity, (2) Methods with nonlocal similarity and additional priority. TensorDL [24] 47.29 0.9896 44.05 0.9800 40.57 0.9638 38.53 0.9482 35.86 0.9139</figDesc><table>method 
σ = 5 
σ = 10 
σ = 20 
σ = 30 
σ = 50 

(1) 

PARAFAC [20] 32.77 0.8368 32.72 0.8344 32.48 0.8235 32.15 0.8052 30.22 0.7051 
BwK-SVD [10] 37.79 0.8873 34.11 0.7854 30.99 0.6571 29.34 0.5727 27.35 0.4614 
3DK-SVD [10] 39.47 0.9199 36.33 0.8612 33.47 0.7927 31.80 0.7457 29.63 0.6761 
LRTA [30] 
43.69 0.9664 40.56 0.9421 37.29 0.9018 35.29 0.8661 32.71 0.8030 
TenSR 
43.74 0.9750 39.05 0.9264 35.01 0.8473 33.31 0.7837 31.38 0.7778 

(2) 
BM4D [21] 
47.72 0.9894 44.33 0.9792 40.70 0.9560 38.46 0.9289 35.55 0.8687 
</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">A is a function of n, however the subscript n is omitted for brevity.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">We thank all the authors of<ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b9">10]</ref> for providing their source codes in their websites.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">An algorithm for designing overcomplete dictionaries for sparse representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aharon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bruckstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>K-Svd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Process</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="4311" to="4322" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Algorithm 862: Matlab tensor classes for fast algorithm prototyping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">W</forename><surname>Bader</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">G</forename><surname>Kolda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Math. Software</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="635" to="653" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A fast iterative shrinkagethresholding algorithm for linear inverse problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Teboulle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Imag. Sci</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="183" to="202" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">From sparse solutions of systems of equations to sparse modeling of signals and images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Bruckstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Donoho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Rev</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="34" to="81" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Computing sparse representations of multidimensional signals using kronecker bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">F</forename><surname>Caiafa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cichocki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="186" to="220" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Tensor decompositions for signal processing applications: From two-way to multiway component analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cichocki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mandic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>De Lathauwer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Caiafa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Phan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Mag</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="145" to="163" />
			<date type="published" when="2015-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Image denoising by sparse 3-d transform-domain collaborative filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dabov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Foi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Katkovnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Egiazarian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2080" to="2095" />
			<date type="published" when="2007-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Nonlocally Centralized Sparse Representation for Image Restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1620" to="1630" />
			<date type="published" when="2013-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Sparse and redundant representations -from theory to applications in signal and image processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Image denoising via sparse and redundant representations over learned dictionaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aharon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3736" to="3745" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">2D sparse signal recovery via 2d orthogonal matching pursuit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. China. Inf. Sci</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="889" to="897" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Sparse Decomposition of Two Dimensional Signals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ghaffari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Babaie-Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jutten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Acoust. Speech Signal Process</title>
		<meeting>IEEE Int. Conf. Acoust. Speech Signal ess</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="3157" to="3160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning sparse representations for human action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Guha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ward</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1576" to="1588" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Separable dictionary learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hawe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Seibert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kleinsteuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="438" to="445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Proximal methods for sparse hierarchical dictionary learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jenatton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">R</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">R</forename><surname>Obozinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 27th Annu, Int. Conf. Mach. Learn</title>
		<meeting>27th Annu, Int. Conf. Mach. Learn</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="487" to="494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning a discriminative dictionary for sparse coding via label consistent k-svd</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2011-06" />
			<biblScope unit="page" from="1697" to="1704" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Tensor decompositions and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">G</forename><surname>Kolda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">W</forename><surname>Bader</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Rev</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="455" to="500" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Efficient sparse coding algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Battle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Raina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="801" to="808" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Tensor completion for estimating missing values in visual data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Musialski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wonka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="208" to="220" />
			<date type="published" when="2013-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Denoising of hyperspectral images using the parafac model and statistical performance analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bourennane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fossati</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="3717" to="3724" />
			<date type="published" when="2012-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Nonlocal transform-domain filter for volumetric data denoising and reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maggioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Katkovnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Egiazarian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Foi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="119" to="133" />
			<date type="published" when="2013-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Non-local sparse models for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2009-09" />
			<biblScope unit="page" from="2272" to="2279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Orthogonal matching pursuit: Recursive function approximation with applications to wavelet decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rezaiifar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Krishnaprasad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 27th Asilomar Conf. Signals, Syst. and Comput</title>
		<meeting>27th Asilomar Conf. Signals, Syst. and Comput</meeting>
		<imprint>
			<date type="published" when="1993" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="40" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Decomposable nonlocal tensor dictionary learning for multispectral image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2014-06" />
			<biblScope unit="page" from="2949" to="2956" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Single image super-resolution via 2d sparse representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Multimedia Expo</title>
		<meeting>IEEE Int. Conf. Multimedia Expo</meeting>
		<imprint>
			<date type="published" when="2015-06" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Two dimensional synthesis sparse model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Multimedia Expo</title>
		<meeting>IEEE Int. Conf. Multimedia Expo</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A flexible tensor block coordinate ascent scheme for hypergraph matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Quynh Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Gautier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Image denoising using the higher order singular value decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rajwade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rangarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Banerjee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="849" to="862" />
			<date type="published" when="2013-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Tensor sparse coding for positive definite matrices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M N P</forename><surname>Sivalingam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Boley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Denoising and dimensionality reduction using multilinear tools for hyperspectral images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Renard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bourennane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Blanc-Talon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEE Geosci. Remote Sensing Letters</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="138" to="142" />
			<date type="published" when="2008-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Sparse image coding using a 3d non-negative tensor factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Tamir Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Polak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. IEEE. Int. Conf. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="50" to="57" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Regression Shrinkage and Selection Via the Lasso</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Roy. Statist. Soc.: Series B</title>
		<imprint>
			<biblScope unit="page" from="267" to="288" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Semi-Coupled Dictionary Learning with Applications to Image Super-Resolution and Photo-Sketch Synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2012-06" />
			<biblScope unit="page" from="2216" to="2223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Robust face recognition via sparse representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ganesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="210" to="227" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">A fast patch-dictionary method for whole image recovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.3740</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Coupled dictionary training for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3467" to="3478" />
			<date type="published" when="2012-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Image superresolution via sparse representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2861" to="2873" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Generalized Assorted Pixel Camera: Postcapture Control of Resolution, Dynamic Range, and Spectrum</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yasuma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mitsunaga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Iso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2241" to="2253" />
			<date type="published" when="2010-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Sparse representation or collaborative representation:which helps face recognition?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE. Int. Conf. Comput. Vis</title>
		<meeting>IEEE. Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="471" to="478" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Tensor dictionary learning with sparse tucker decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zubair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Digital Signal Process</title>
		<meeting>Int. Conf. Digital Signal ess</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
