<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Large-Scale Location Recognition and the Geometric Burstiness Problem</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torsten</forename><surname>Sattler</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">ETH Zürich</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Havlena</surname></persName>
							<email>havlena@vision.ee.ethz.ch</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Computer Vision Laboratory</orgName>
								<orgName type="institution">ETH Zürich</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konrad</forename><surname>Schindler</surname></persName>
							<email>schindler@geod.baug.ethz.ch</email>
							<affiliation key="aff2">
								<orgName type="department">Institute of Geodesy and Photogrammetry</orgName>
								<orgName type="institution">ETH Zürich</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Pollefeys</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">ETH Zürich</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Large-Scale Location Recognition and the Geometric Burstiness Problem</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:42+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Visual location recognition is the task of determining the place depicted in a query image from a given database of geo-tagged images. Location recognition is often cast as an image retrieval problem and recent research has almost exclusively focused on improving the chance that a relevant database image is ranked high enough after retrieval. The implicit assumption is that the number of inliers found by spatial verification can be used to distinguish between a related and an unrelated database photo with high precision. In this paper, we show that this assumption does not hold for large datasets due to the appearance of geometric bursts, i.e., sets of visual elements appearing in similar geometric configurations in unrelated database photos. We propose algorithms for detecting and handling geometric bursts. Although conceptually simple, using the proposed weighting schemes dramatically improves the recall that can be achieved when high precision is required compared to the standard re-ranking based on the inlier count. Our approach is easy to implement and can easily be integrated into existing location recognition systems.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Given a database of geo-tagged images, the task of a visual location recognition system is to determine the place depicted in a query photo <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35]</ref>. Knowing which database images show the same place as the query, the position (potentially also the orientation) from which the query photo was taken can either be approximated <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b36">37]</ref> or computed precisely <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b39">40]</ref> from the known positions of the matching database images. Location recognition techniques play an important role for several applications such as loopclosure in robotics <ref type="bibr" target="#b8">[9]</ref>, landmark recognition <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b6">7]</ref>, visual navigation <ref type="bibr" target="#b25">[26]</ref>, and image-based localization <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b28">29]</ref>.</p><p>Typically, two tasks must be accomplished to solve the visual location recognition problem: (i) find a set of database images visually similar to the query and (ii) de-termine which, if any, of the retrieved images depict the same place as the query. The first step is another canonical problem of computer vision, namely image retrieval <ref type="bibr" target="#b30">[31]</ref>. Consequently, most work on location recognition focuses on optimizing the retrieval step, with the aim to maximize the portion of queries where at least one relevant database photo is contained in the N most similar retrieved images, the so-called recall@N . Image retrieval largely ignores the spatial relations between features in the query image. Thus, the recall@N can be improved further through spatial verification: the (approximate) geometric transformation between the query and the top-ranked images after retrieval is estimated <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33]</ref> and the database images are re-ranked based on the number of inliers to the transformation.</p><p>Improving the retrieval step is clearly a key factor for solving the location recognition problem. Yet, only improving the recall@N is not sufficient for quite a few applications which require high precision. For example, to support loop-closure in SLAM systems one must recognize previously visited locations with high precision, since the loop closure (a.k.a. pose graph optimization) itself tolerates only a small number of mistakes <ref type="bibr" target="#b17">[18]</ref>. Similarly, tools which automatically annotate photos with the place where they were taken <ref type="bibr" target="#b10">[11]</ref> become useless if the user must search and correct too many mistakes. Thus, a second key capability of a location recognition system is to decide with high precision (i.e., low false-positive rate) which of the retrieved images actually depict the same place -ideally the bulk of the queries should satisfy recall@1.</p><p>As explained, the standard way to refine the raw list of retrieved images is geometric verification with a suitable transformation. But the re-ranking is surprisingly primitive: typically, images are simply re-ordered by the number of inliers to the transformation, respectively discarded if that number falls below some threshold. Interestingly, the results reported in previous work actually suggest that this is not a suitable strategy if high precision is required. E.g., <ref type="bibr" target="#b1">[2]</ref> reports a recall@1 of ≈70% on the large-scale Pittsburgh dataset <ref type="bibr" target="#b34">[35]</ref>, but a recall@50 of ≈90%. In other words, for  <ref type="figure">Figure 1</ref>. Geometric bursts, i.e., geometrically consistent structures of similar appearance, cause problems to location recognition: database images depicting an unrelated place can often attain more inliers to the estimated geometric model than photos of the same place.</p><p>20% of all query images an unrelated image has a higher inlier count than any photo taken at the same place, even after spatial verification! Somewhat surprisingly, this observation has received very little attention in the literature.</p><p>It is known that visual words appearing in visual bursts <ref type="bibr" target="#b15">[16]</ref> or words that are likely to co-occur together <ref type="bibr" target="#b7">[8]</ref> require special handling. In much the same way, large databases often contain geometric bursts, i.e., geometric configurations of visually similar features that are shared between different places. <ref type="figure">Fig. 1</ref> illustrates this phenomenon. By definition, geometric bursts appear in multiple locations of the database. Hence, they violate the basic assumption underlying geometric verification: even spatial configurations of several features are not always unique, making it impossible to distinguish between a correct and a wrong location.</p><p>In this paper, we investigate ways to explicitly handle geometric bursts by analyzing the geometric relations between the different database images retrieved by a query. Namely, we make the following contributions: (i) we introduce the concept of geometric burstiness. (ii) we demonstrate that geometric bursts are an important cause for false positives and have a significant impact on the precision of location recognition. (iii) we show how to dramatically increase the recall for a given precision with an appropriately weighted inlier count that better accounts for geometric bursts. (iv) our approach is designed such that it operates online at query time, and requires neither costly preprocessing nor any additional storage. It can be used as a drop-in replacement for the conventional inlier count, without any changes to the underlying retrieval system, and we make source code available at https://github.com/ tsattler/geometric_burstiness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Location recognition, also referred to as place recognition, relies on image retrieval techniques such as inverted files <ref type="bibr" target="#b30">[31]</ref>, quantized feature matching with large visual vocabularies <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b23">24]</ref>, vocabulary trees <ref type="bibr" target="#b22">[23]</ref>, and fast approximate spatial matching <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b32">33]</ref>. Hamming embedding <ref type="bibr" target="#b14">[15]</ref> simulates the similarity between two descriptors at little ad-ditional run-time and memory overhead by using compact binary representations. Thus, Hamming embedding allows to remove many of the unrelated votes caused by visual word quantization <ref type="bibr" target="#b28">[29]</ref>. To overcome the limited viewpoint invariance of modern features such as SIFT <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b25">26]</ref> rectify images prior to feature extraction, with the help of vanishing points. <ref type="bibr" target="#b6">[7]</ref> show that combining rectified and regular images increases the overall performance. Instead of using invariant features, <ref type="bibr" target="#b33">[34]</ref> densely sample the scene by generating synthetic renderings from novel viewpoints.</p><p>Recent work on place recognition focused on the problems caused by repetitive structures and uninformative features. Repetitive structures lead to bursts of visual elements, i.e., a visual word occurs very often in an image <ref type="bibr" target="#b15">[16]</ref>. While <ref type="bibr" target="#b15">[16]</ref> handle the repetitions of a single word, <ref type="bibr" target="#b7">[8]</ref> detect and handle sets of co-occurring features, showing that the classical tf-idf weighting cannot handle that case. <ref type="bibr" target="#b34">[35]</ref> recognize that repetitive structures are not only a nuisance, but can provide valuable information about a place. They propose to consider the features in a repetitive pattern as a soft assignment of a single visual element, and show that their explicit handling of repetitions outperforms the standard scheme <ref type="bibr" target="#b15">[16]</ref> that down-weights visual bursts.</p><p>In order to improve and accelerate the retrieval performance, <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b35">36]</ref> select only an informative subset of all database features that are repeatable and/or unique for each place. <ref type="bibr" target="#b16">[17]</ref> proceed more conservatively and only remove confusing features that are also found in unrelated places of the database. Both <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b11">12]</ref> learn SVM classifiers on top of the Bag-of-Words image representation for each place, so as to properly weight informative and confusing features. All these methods <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b35">36]</ref> must query every single database photo against the database. <ref type="bibr" target="#b1">[2]</ref> argue that this is infeasible for large databases due to its quadratic computational complexity. Instead, they propose to handle repetitions and uninformative features online at query time, by density estimation in the space of Hamming descriptors, which can be computed efficiently. All these methods aim to improve the retrieval stage before spatial verification. In contrast, we focus on providing a better measure for deciding between related and unrelated places after verification.</p><p>Instead of using visual words, <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b37">38]</ref> exploit the full feature descriptors for matching. They do not vote for individual database images, but instead use the geo-tags of all matching images to cast votes for the geo-position of the query image. <ref type="bibr" target="#b38">[39]</ref> take that idea one step further and use a 3D model of the scene to better constrain the voting, for both the position and orientation of the image. However, using full descriptors soon becomes infeasible at large scale.</p><p>Closely related to location recognition is the imagebased localization problem, where the goal is to recover the full camera pose of a given query image relative to a 3D scene model <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b27">28]</ref>. Image-based localization systems put emphasis on computing the camera pose with a high precision. They use the full feature descriptors for matching and more restrictive geometric models for spatial verification, while image retrieval-based approaches traditionally use visual words for matching and approximate geometric models for verification. As a result, image-based localization by large achieves a much higher recall than place recognition methods in the high precision regime, although <ref type="bibr" target="#b26">[27]</ref> recently showed that a similar recall at high precision can be achieved with quantized features as well. However, there are no theoretical reasons why location recognition approaches should perform worse. In this paper, we show that by handling geometric bursts, location recognition approaches can reach similar or better levels of recall in the high precision regime without using the full descriptors and using only an affine model for geometric verification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Geometric Burstiness</head><p>During conventional image retrieval the spatial configuration of features in query and database images is ignored. As a consequence, a retrieved database image may contain many visually similar features, but in a very different geometric configuration. The purpose of geometric verification is to detect images where the feature point locations are not consistent, i.e., they are unrelated and retrieved by mistake. The common assumption is that, if one fits a suitable image-to-image transformation to the feature matches, not many inliers will be found for unrelated images. The inlier count is used to re-rank the top-k retrieved images. Clearly, the assumption does not hold if the same geometric configuration occurs repeatedly in the database. Such nonunique configurations, geometric bursts, are more likely if the scene is large, and if it contains visually similar objects. The central message of this paper is that, other than what one might hope, geometric bursts do occur regularly in realistic databases. That means that one will encounter cases where unrelated images have the highest inlier counts (c.f . <ref type="figure">Fig. 1(left &amp; middle)</ref>). With standard re-ranking these attain the highest rank and lower the recall@N . Note that geometric bursts are not restricted to small image areas.</p><p>However, the impact of geometric bursts goes beyond a </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pittsburgh -precision/recall@1</head><p>Figure 2. Precision-recall curves obtained by a state-of-the-art localization recognition system <ref type="bibr" target="#b1">[2]</ref> when considering the top-ranked image after spatial verification (raw inlier count re-ranking).</p><p>reduced recall@N : as unrelated database images can have many inliers for some query images, the inlier count is also not a suitable measure to decide whether a place has been correctly recognized or not (c.f . <ref type="figure">Fig. 1(right)</ref>). <ref type="figure">Fig. 2</ref> shows that this can result in a much lower recall in the high precision regime, when one must set a high threshold, e.g., 90%, to the amount of the correct answers returned by the system. One obvious strategy to handle geometric bursts is to remove them in a pre-processing step, by detecting visual bursts for each database image, similar to the removal of confusing features proposed in <ref type="bibr" target="#b16">[17]</ref>. However, the computational complexity of such an offline process is quadratic in the number of database images: each image needs to be queried against the complete database. As pointed out by <ref type="bibr" target="#b1">[2]</ref>, such preprocessing quickly becomes infeasible as the database size grows. We propose to instead handle geometric bursts at query time. In <ref type="bibr" target="#b7">[8]</ref> it has been shown how to efficiently detect co-occurrence sets, i.e., sets of visual words likely to appear together, in the query image during retrieval. However, it is unclear how to distinguish cooccurrence sets between multiple images of the same location from geometric bursts that appear at unrelated locations. Moreover, removing bursts at retrieval time runs the risk of also losing the correct location <ref type="bibr" target="#b7">[8]</ref>. We thus prefer to handle geometric bursts at the stage where they cause problems, i.e., after spatial verification. By definition, geometric bursts visible in a query image will appear in multiple unrelated database images. Given the geo-tags of the database photos and the inlier matches detected for them, it is therefore rather simple to detect geometric bursts on demand and down-weight their influence on the image ranking. We will show in Sec. 5 that it is easier to distinguish related and unrelated images with that weighted inlier count. As a result, our approach greatly increases the recall at high precision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Detecting and Handling Geometric Bursts</head><p>Essentially, a geometric burst is a set of visual words that co-occur repeatedly in the same spatial configuration. <ref type="bibr" target="#b15">[16]</ref> show for visual bursts that appropriate down-weighting improves retrieval, and <ref type="bibr" target="#b7">[8]</ref> apply the same weighting scheme for co-occurrence sets (sets of co-occurring features in an arbitrary spatial configuration). In Sec. 4.1, we first review this weighting scheme and discuss how to adapt it for geometric burstiness. While this simple adaptation already improves the recall at high precision, it overestimates the importance of geometric bursts. Sec. 4.2 describes how to remedy this behavior. Sec. 4.3 then proposes to measure place popularity and include it into the weighting scheme.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Inter-Image Geometric Burstiness</head><p>According to <ref type="bibr" target="#b15">[16]</ref>, a visual burst is a visual word which violates the assumption that words appear independently of each other. Moreover, they distinguish between intra-image and inter-image burstiness. Intra-image bursts are caused by repetitive structures found in a single image, whereas interimage burstiness refers to visual elements shared between many database images. In terms of geometric burstiness, intra-image bursts can easily be handled by enforcing oneto-one correspondences for the inliers.</p><p>[16] handle inter-image visual burstiness as follows: let sim(Q i , D m j ) be the similarity score between the i th feature in the query image Q and the j th feature in the m th database image D m , e.g., computed via Hamming embedding. The sum of similarity scores for the i th query feature across all database images is thus given by</p><formula xml:id="formula_0">sim (Q i ) = m j sim(Q i , D m j ) .<label>(1)</label></formula><p>[16] use this sum to weight each similarity score</p><formula xml:id="formula_1">sim(Q i , D m j ) by multiplying with sim(Qi,D m j )</formula><p>sim (Qi) . Obviously, this weighting scheme can be adapted to the case of geometric bursts. A match (Q i , D m j ) contributes a value of 1 to the inlier count for image D m if it is an inlier to the estimated model and a value of 0 otherwise, i.e.,</p><formula xml:id="formula_2">sim geo (Q i , D m j ) = 1 if (Q i , D m j ) is an inlier 0 otherwise .<label>(2)</label></formula><p>The i th query feature Q i is a part of at most one inlier match for any database image, thus the geometric equivalent</p><formula xml:id="formula_3">sim geo, (Q i ) = k m=1 j sim geo (Q i , D m j )<label>(3)</label></formula><p>to Eqn. (1) is simply the number of database images for which Q i is an inlier of the geometric verification. Notice that while Eqn. (1) considers all database images, Eqn. <ref type="formula" target="#formula_3">(3)</ref> only includes the top-k ranked images after retrieval for which spatial verification is performed 1 . A query feature Q i participates in a geometric burst if it forms part of the inlier set for at least two database images D m = D l . To assign a lower weight to features from a geometric burst, we use an inter-image-weighted inlier count</p><formula xml:id="formula_4">I inter-image (D m ) = inlier match (Qi,D m j ) 1 sim geo, (Q i )<label>(4)</label></formula><p>over all verified matches (Q i , D m j ) from the query image Q to the database image D m . In Sec. 5, we will experiment with various weighting functions, as well as a variant that completely removes features from geometric bursts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Inter-Place Geometric Burstiness</head><p>Usually, place recognition databases contain multiple photos of each place, e.g., street-level panoramas taken at regular intervals as in online mapping services. Two images depicting the same location will inherently share common features. Thus, it is likely that a geometric burst detected within one of them is also detected in the other one. Eqn. (3) treats each view separately and thus overestimates the burstiness of the underlying features. Consequently, the weighted inlier count, Eqn. (4), underestimates the similarity between the query and database images. Rather than identifying geometric bursts on a per-image level, it would be more appropriate to identify bursts on a per-place level. In the following, we provide a workable definition of a "place" and with that definition compute an inter-place burstiness measure.</p><p>Defining places. <ref type="figure" target="#fig_2">Fig. 3</ref> shows a fundamental difficulty of visual location recognition. Two database images were taken at different places that are far apart, but they contain the same clock tower. Note the subtle problem: features on the front side of the tower in the two images depict the same physical points, nevertheless they form a geometric burst, since the tower is visible from multiple locations and can confuse place recognition. The example highlights a simple, but important fact: visual similarity alone is unsuitable to define a place (even if one had perfect descriptors that unambiguously encode 3D points), simply because vision is a long-range sensor. To solve this problem, we must exploit the geo-tags g(D m ) ∈ R 2 of each database image, obtained, e.g., from GPS or Structure-from-Motion.</p><p>One natural approach is to define a place as the set of all database images whose geo-tags fall into a pre-defined cell in scene space, e.g., on a regular lattice or a Voronoi tessellation found with k-means clustering. A regular grid is tempting, but will lead to quantization artifacts near the cell boundaries, because it ignores the distribution of the geotags. We therefore adaptively cluster at query time based on the spatially verified database images. At first glance, this approach seems suboptimal. However, we only need to consider a few spatially verified images 2 . Compared to the time required for retrieval and spatial verification, we found the time required for clustering to be negligible.</p><p>Our method is inspired by the initialization procedure of k-means++ clustering <ref type="bibr" target="#b2">[3]</ref>: D 1 is the database image with the largest number of inliers. Its geo-tag g(D 1 ) defines the center of the first cluster. We iteratively select the database image D m furthest away from all previously chosen cluster centers. This process is terminated once there exists no more image D m that is more than d max meters away from its closest cluster center, or all k verified images have been considered. The termination criterion is chosen to reflect that nearby images should belong to the same place. Next, we assign each verified database image D m to its closest cluster center c(D m ). Each cluster then defines one place.</p><p>Inter-place burstiness. Given the set of places obtained via clustering, we adapt the geometric burstiness weighting scheme to avoid overestimating the number of geometric bursts. For a feature Q i in the query image, let D(Q i ) be the set of database images containing an inlier match for Q i . The set of relevant places is then given by</p><formula xml:id="formula_5">c(Q i ) = {c(D m ) | D m ∈ D(Q i )} .<label>(5)</label></formula><p>We can now normalize over places rather than images to define an inter-place-weighted inlier count</p><formula xml:id="formula_6">I inter-place (D m ) = inlier match(Qi,D m j ) 1 |c(Q i )| .<label>(6)</label></formula><p>Compared to Eqn. (4), Eqn. (6) counts each geometric burst at most once per place to assess a query feature Q i . In the experiments, we show that this greatly improves recall at high precision. In Eqn. (6), we have dropped the square root as we found that the new criterion performs slightly better without it. We will experiment with different weighting functions in Sec. 5.</p><p>Exploiting metadata. Some datasets provide detailed metadata for each database image. For example, the San Francisco dataset <ref type="bibr" target="#b6">[7]</ref> provides a "carto id", a unique identifier for the building visible in each database image. Naturally, this information can be used as an alternative way of defining places. In Sec. 5 we show that using such metadata <ref type="bibr" target="#b1">2</ref> Typically, 10 to 1000 top-ranked images are spatially verified.</p><p>does not necessarily improve over the data-driven clustering, possibly because the "carto id" is somewhat ambiguous if more than one building is visible in the foreground.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Inter-Place Burstiness with Place Popularity</head><p>So far, we proposed a method which weights the individual features of Q differently in the computed inlier sum. Once we have the database images clustered to places, we can also use the popularity of the individual places to further refine the weighting scheme. For a database image D m , let C(D m ) be the set of images from its place. The place's popularity p(C(D m )) is given as the number of features from Q which are inliers for at least one of the images in C(D m ):</p><formula xml:id="formula_7">p(C(D m )) = |{i | D(Q i ) ∩ C(D m ) = ∅}| .<label>(7)</label></formula><p>The inter-place-popularity-weighted inlier count is then defined in the following way:</p><formula xml:id="formula_8">I inter-place + pop (D m ) = I inter-place (D m ) · p(C(D m )) max l p(C(D l ))</formula><p>. <ref type="formula">(8)</ref> Therefore, all retrieved database images not located at the most popular place are further down-weighted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Discussion</head><p>The weighting scheme proposed above is conceptually simple, and very easy to implement. It requires neither any additional matching or verification steps nor any external data. Notwithstanding its simplicity, re-weighting according to geometric burstiness brings drastic improvements compared to the traditional inlier count, as we will show in Sec. 5. The simplicity of our method naturally raises the question whether a different, possibly more sophisticated, way of handling bursts would perform even better.</p><p>In Sec. 5.2, we experiment with different weighting schemes for both Eqn. (4) and Eqn. <ref type="bibr" target="#b5">(6)</ref>. For example, we use |c(Q i )| instead of |c(Q i )| in Eqn. <ref type="bibr" target="#b5">(6)</ref>, to assign more importance to inliers found on geometric bursts. Our results will show that changing the weighting function has only a small impact on the overall performance of Eqn. <ref type="bibr" target="#b5">(6)</ref>, which is in agreement with the results of <ref type="bibr" target="#b15">[16]</ref> for visual bursts. At the same time, we observe a significant loss when Eqn. (4) is used instead. This suggests that detecting which geometric bursts come from the same scene structure is more important than the exact weighting function. We also show that the performance of the proposed method depends only little on the exact definition of what constitutes a place.</p><p>There is one obvious difference between visual and geometric bursts: visual bursts (and similarly co-occurrence sets) are defined independent of the feature's position in the image. In contrast, geometric bursts essentially correspond to geometrically consistent regions in the images. We tried to account for this difference by dividing the query image into tiles and counting the number of geometric bursts per tile rather than per feature, but this did not improve the results (improper setting of tile size even worsens the results).</p><p>There is one obvious situation in which the weighting scheme fails: consider the building in <ref type="figure" target="#fig_3">Fig. 4</ref>. If all inliers are found on a surface visible from many places, downweighting them will have no effect on the ranking. In such cases, higher-level information is needed, e.g., reasoning based on the outlines of the nearby buildings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experimental Evaluation</head><p>In this section, we evaluate the weighting schemes for geometric bursts proposed in Sec. 4 on two standard benchmark datasets for place recognition, San Francisco <ref type="bibr" target="#b6">[7]</ref> and Pittsburgh <ref type="bibr" target="#b34">[35]</ref>. We show that accounting for geometric bursts significantly improves the recall in the high precision regime as well as the overall recall.</p><p>San Francisco Landmarks dataset <ref type="bibr" target="#b6">[7]</ref>. The San Francisco dataset consists of 1.06M database images extracted from about 150k panoramic images captured by a vehicle driving through the streets of San Francisco. The 803 query images are taken with multiple mobile phones. Each database image is annotated with a "carto id" denoting the building visible in the image and a list of relevant "carto ids" is also provided for each query image. A query image is then considered to be successfully localized if the topranked database image is annotated with a relevant "carto id". We use the 2014 version of the ground truth <ref type="bibr" target="#b1">[2]</ref>.</p><p>Pittsburgh dataset <ref type="bibr" target="#b34">[35]</ref>. The database photos for the Pittsburgh dataset were obtained by extracting 254k perspective images from about 10.6k panoramas downloaded from Google Street View (which leads to a rather large distance between the panorama locations). 24k query images then come from a separate set of Google Street View panoramas taken from Google's Pittsburgh Research Dataset. Both sets of panoramas have quite accurate GPS coordinates which defines the localization task: A query image is considered being localized if the GPS position of the top-ranked database photo is within 25m of the query image's position.</p><p>Place recognition pipeline <ref type="bibr" target="#b1">[2]</ref>. We use our own implementation of a state-of-the-art location recognition system <ref type="bibr" target="#b1">[2]</ref>, referred to as DisLoc, to perform image retrieval and spatial verification. DisLoc uses 64-bit Hamming embedding <ref type="bibr" target="#b14">[15]</ref> to compute the similarity between a query and database feature, which is weighted based on the density of the descriptor space surrounding the database feature. As a result, less weight is assigned to matches found in dense parts of the descriptor space, effectively down-weighting visual elements that appear often. Inter-image visual burstiness weighting <ref type="bibr" target="#b15">[16]</ref> is used to handle visual burst. As in <ref type="bibr" target="#b1">[2]</ref>, upright Root-SIFT <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b19">20]</ref> descriptors are extracted from Hessian-Affine keypoints <ref type="bibr" target="#b20">[21]</ref> and are assigned to the closest out of 200k words. To lessen quantization artifacts, each query feature is assigned to its 5 closest words. As in <ref type="bibr" target="#b1">[2]</ref>, fast approximate spatial verification with an affine model <ref type="bibr" target="#b23">[24]</ref> is used to verify the top-200 images found by the retrieval step. <ref type="figure">Fig. 2</ref> shows that our implementation performs slightly worse than <ref type="bibr" target="#b1">[2]</ref>, i.e., the improvements reported in this paper do not come from a better implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Baseline Comparisons</head><p>First, we compare the weighting schemes for geometric bursts proposed in Sec. 4 with two baselines: the raw inlier count and the effective inlier count <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b26">27]</ref>. The latter measure is defined as follows: each inlier feature in the query image covers the area A i contained in a circle of radius r around itself, with r set to 12 pixels in our experiments. Given n inliers, the effective inlier count is computed as</p><formula xml:id="formula_9">I eff = | i A i | n i=1 |A i | · n ,<label>(9)</label></formula><p>where |A i | = π · r 2 denotes the size of the area covered by the i th inlier feature. Eqn. (9) thus compares the actual area covered by all inliers with the area that can be covered if none of the circles are overlapping. This measure down-weights inliers found in a small region of the query image. Following the setup from <ref type="bibr" target="#b6">[7]</ref>, we obtain precisionrecall curves by varying a threshold on the number of inliers for the raw count and thresholds on the weighted inlier counts for the effective and the two burstiness inlier counts. The goal of our first experiment is to show that both the inter-image-weighted inlier count I inter-image , Eqn. (4), and the inter-place-weighted inlier count I inter-place , Eqn. (6), enable us to find a better threshold to distinguish between correct and wrong place recognitions. Thus, in this experiment, we only consider the verified database images with the largest raw number of inliers found for each query image and do not re-rank based on the weighted inlier counts. As can be seen in <ref type="figure" target="#fig_4">Fig. 5(a-b)</ref>, the effective inlier count consistently outperforms the raw inlier count, while in turn both burstiness measures outperform the effective inlier count. The latter shows that geometric bursts are not restricted to small image regions.</p><p>The improvement in recall we gain by accounting for geometric bursts is dramatic: At 95% precision, the raw and effective inlier counts achieve 53.4% respectively 57.3% recall on San Francisco. In contrast, our weighting schemes for geometric burstiness achieve 63.5% and 70.1%. At 90% precision, the raw and effective counts obtain 7.2% and 18% recall on Pittsburgh while the inter-image count obtains 25.8% and the inter-place measure achieves 51.1% recall. These results clearly demonstrate the importance of handling geometric bursts. Interestingly, all measures perform poorly on the Pittsburgh dataset when a precision higher than 90% is required. We visually inspected over 400 out of 6381 query images for which the top-ranked database photo is unrelated but still receives a high weighted inlier score. One common failure case is that all inliers are solely found on geometric bursts, e.g., identical facades of a building or buildings seen from afar. As discussed in Sec. 4.4, such cases cannot be resolved by considering geometric bursts. <ref type="figure" target="#fig_4">Fig. 5</ref>(c-d) demonstrates that accounting for geometric bursts not only enables a better decision between correctly and incorrectly retrieved places. It also improves the overall recall when used for re-ranking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Ablation Study</head><p>In the next experiment, we evaluate the impact of different parameter settings for the burstiness schemes.</p><p>Different weighting schemes. We test the impact of different weighting schemes for the number of geometric bursts. For example, we replace the term 1/ sim geo, (Q i ) in Eqn. (4) with 1/sim geo, (Q i ) to give less weight to inliers participating in many geometric bursts. In addition, we experiment with removing inliers participating in geometric bursts, i.e., inliers to two or more images, respectively places. <ref type="figure">Fig. 6(a-b)</ref> shows the results from this ablation study. As can be seen, the weighting function used has a large impact on the inter-image count since, e.g., </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pittsburgh -precision/recall@1</head><p>(d) <ref type="figure">Figure 6</ref>. Ablation study for our method on the two datasets. All results are after re-ranking using the respective measure. Colors denote different inlier counts, while the line style, e.g., dashed or dotted, denotes different ways to compute these counts. sim geo, (Q i ) overestimates the number of geometric bursts for each query feature. Removing inliers participating in bursts further decreases the performance as it does not account for the fact that multiple database photos can depict the same place. The fact that the linear weighting performs similar as the square-root weighting on Pittsburgh comes from the fact that the database images are taken further apart, so there are fewer photos depicting the same place.</p><p>In contrast to the inter-image count, the inter-place count is much less sensitive to the weighting function used, with the linear weighting performing slightly better than the other weighting functions. This demonstrates that the main importance lies in detecting related bursts rather than in the way bursty inlier features are weighted.</p><p>Different place definitions. So far, we have only used the place clustering scheme described in Sec. 4.2, with the maximum distance set to d max = 25m. Next, we compare this scheme against using a regular grid of side length 25m. For San Francisco, we also compare against using the "carto id" of the database images for clustering. For the inter-place count, we use the linear weighting. The results of the experiments are shown in <ref type="figure">Fig. 6(c-d)</ref>. On San Francisco, where the database images are taken more densely, the adaptive clustering scheme performs better than the fixed grid, offering a recall of 71.2% at 95% precision compared to 65.6% for the fixed grid. However, using the "carto ids" to define places does not offer a significant advantage.</p><p>The sparser sampling on the Pittsburgh dataset leads to less quantization artifacts. As a result, using either the adaptive clustering or the regular grid results in virtually the same recall-precision curve. Independently of the place definition, the inter-place count gives better results than the inter-image count. This again demonstrates the importance of accounting for the fact that multiple database images can depict the same part of the scene.</p><p>Popularity-based weighting. <ref type="figure">Fig. 6(c-d)</ref> show the results obtained with the inter-place-popularity-weighted inlier count (using the place clustering scheme described in Sec. 4.2). On the San Francisco dataset, the improvement compared to the inter-place count is modest as the recall at 95% precision increases from 71.2% to 72.4%. However, the improvement measured on Pittsburgh is more significant as the recall at 90% precision increases from 54.3% to 59.4%. The inter-place-popularity count penalizes database images that do not come from the place with the largest number of inliers. The smaller improvement on San Francisco can be explained by the fact that location recognition performs better on this dataset, i.e., most of the correctly retrieved images come from the most popular place.</p><p>The maximum recall our implementation can achieve when verifying the 200 top-ranked images is 87.92% for San Francisco and 88.98% for Pittsburgh, respectively. Using the inter-place-popularity count, we achieve a recall@1 of 82.57% for San Francisco and 74.15% for Pittsburgh.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Comparison with State-of-the-Art</head><p>We compare our implementation of DisLoc+inter-placepopularity with state-of-the-art place recognition <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b34">35]</ref> and image-based localization approaches <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b38">39]</ref>. Since there is no 3D model for the Pittsburgh dataset, the comparison is only performed on San Francisco. <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b38">39]</ref> use a 3D model provided by <ref type="bibr" target="#b18">[19]</ref> while all other methods only use images. Whereas our approach considers the relationship between inliers in multiple images, all other methods score images and/or poses independently of each other. For <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b34">35]</ref>, we use results kindly provided by the authors to draw the precision-recall curves.</p><p>The Adaptive weights method from <ref type="bibr" target="#b34">[35]</ref> does not perform spatial verification. Thus, we use the similarity scores after retrieval to obtain the precision-recall curve. <ref type="bibr" target="#b6">[7]</ref> use histogram equalization before extracting upright SIFT features <ref type="bibr" target="#b19">[20]</ref> and a GPS prior (Hist.Eq. w/ GPS). The method from <ref type="bibr" target="#b38">[39]</ref> uses a 3D model to vote for the most likely camera pose, followed by a RANSAC-based refinement step <ref type="bibr" target="#b9">[10]</ref>. The Hyperpoints approach from <ref type="bibr" target="#b26">[27]</ref> uses a fine vocabulary of 16M words <ref type="bibr" target="#b21">[22]</ref> instead of the original point descriptors to obtain the 2D-3D matches required for pose estimation. For completeness, we also report the results originally obtained by <ref type="bibr" target="#b1">[2]</ref> with spatial verification (DisLoc+sp). <ref type="figure">Fig. 7</ref> shows the results of the comparison. As can be seen, <ref type="bibr" target="#b1">[2]</ref> can significantly outperform existing methods simply by accounting for geometric bursts. Compared to <ref type="bibr" target="#b6">[7]</ref>, we improve recall from 70.1% to 80.5% for 90% precision. For 95% precision, we improve the 63.5% recall achieved by <ref type="bibr" target="#b26">[27]</ref> to 72.4%. Our recall is close to the 74.2% obtained </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>San Francisco -precision/recall@1</head><p>Adaptive weights <ref type="bibr">[Torii'13]</ref> Hist.Eq. w/ GPS <ref type="bibr">[Chen'11]</ref> DisLoc+sp [Arandjelovic <ref type="bibr">'14]</ref> Hyperpoints <ref type="bibr">[Sattler'15]</ref> I inter-place + pop [proposed] <ref type="figure">Figure 7</ref>. Combining <ref type="bibr" target="#b1">[2]</ref> with our proposed scheme for handling geometric bursts not only provides significantly better results over the original method but also outperforms state-of-the-art methods for both place recognition and image-based localization. by <ref type="bibr" target="#b38">[39]</ref> with the help of a GPS prior and higher than the 67.5% reported by <ref type="bibr" target="#b38">[39]</ref> without a GPS prior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Image Retrieval Results</head><p>Finally, we show that handling geometric bursts also improves standard image retrieval performance. We trained vocabularies with 200k words on Paris6k <ref type="bibr" target="#b24">[25]</ref> and Ox-ford5k <ref type="bibr" target="#b23">[24]</ref> for Oxford105k <ref type="bibr" target="#b23">[24]</ref> and Paris106k <ref type="bibr" target="#b24">[25]</ref>, respectively. Spatial verification is performed for the 1000 top-ranked images. Due to a lack of geo-tags, we define places based on the filename prefixes of the images, e.g., "keble", which corresponds to the original Flickr queries. Tab. 1 shows the mean average precision (mAP) values obtained with the different (weighted) inlier counts. Both inter-place burstiness variants outperform the raw and effective inlier counts, while the inter-image scheme overestimates the number of geometric bursts and performs worse.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>In this paper, we have shown that geometric bursts, i.e., sets of visual elements that appear in a consistent spatial configuration in multiple unrelated database images, can significantly impact the recall that can be achieved by location recognition approaches. We have proposed a simple and easy-to-implement method for detecting and downweighting geometric bursts. Our approach can serve as a drop-in replacement for the classic re-ranking after spatial verification based on the number of inliers and our experimental results show that this simple approach dramatically increases the recall in the high precision regime. Just by using our weighting scheme, an existing place recognition method achieves state-of-the-art localization performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Two images from the San Francisco dataset depicting the same clock tower from different viewpoints.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Two images from the San Francisco dataset prominently displaying the same building from different sides.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Comparison against baseline measures: (a-b) without and (c-d) with re-ranking using the respective measure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>inliers I eff I inter-image I inter-place I inter-place+popTable 1. Image retrieval results reporting mean average precision.</figDesc><table>Oxford105k (mAP) 0.710 0.730 0.708 
0.735 
0.745 
Paris106k (mAP) 0.613 0.619 0.611 
0.649 
0.682 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">To avoid confusion, top-k will refer to the k images with the highest similarity score after retrieval. The recall@N measure then considers the N highest ranked images after applying spatial verification on the top-k images and re-ranking based on the (raw or weighted) number of inliers.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements.</head><p>This work was supported by Google's Project Tango and EC Horizon 2020 project REPLICATE (no. 687757). The authors thank Relja Arandjelović for his invaluable help with the DisLoc algorithm.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Three things everyone should know to improve object retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arandjelović</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">DisLocation: Scalable descriptor distinctiveness for location recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arandjelović</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">K-means++: The Advantages of Careful Seeding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Arthur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vassilvitskii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SODA</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Leveraging 3D City Models for Rotation Invariant Place-of-Interest Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Baatz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Köser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Grzeszczuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="315" to="334" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Large Scale Visual Geo-Localization of Images in Mountainous Terrain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Baatz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Saurer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Köser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="517" to="530" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Graph-Based Discriminative Learning for Location Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="239" to="254" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">City-scale landmark identification on mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Baatz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Köser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vedantham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pylvänäinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Roimela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Girod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Grzeszczuk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Unsupervised Discovery of Cooccurrence in Sparse High Dimensional Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">FAB-MAP: Probabilistic Localization and Mapping in the Space of Appearance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cummins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Newman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJRR</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="647" to="665" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bolles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comm. ACM</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="381" to="395" />
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">I Know What You Did Last Summer: Object-Level Auto-Annotation of Holiday Snaps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gammeter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Quack</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning perlocation classifiers for visual place recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gronat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Obozinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Efros. im2gps: estimating geographic information from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">From Structure-from-Motion Point Clouds to Fast Location Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Irschara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-M</forename><surname>Frahm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Hamming Embedding and Weak Geometric Consistency for Large Scale Image Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jegou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">On the burstiness of visual elements</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jegou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Avoiding confusing features in place recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Knopp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Robust Pose-Graph Loop-Closures with Expectation-Maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Fraundorfer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IROS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Worldwide Pose Estimation Using 3D Point Clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Huttenlocher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="63" to="86" />
		</imprint>
	</monogr>
	<note type="report_type">Scale &amp; Affine Invariant Interest Point Detectors. IJCV</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning vocabularies over a fine quantization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mikulík</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Perdoch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="163" to="175" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Scalable recognition with a vocabulary tree</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Nister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Stewenius</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Object retrieval with large vocabularies and fast spatial matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Lost in quantization: Improving particular object retrieval in large scale image databases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">An image-based system for urban navigation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Hyperpoints and Fine Vocabularies for Large-Scale Location Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sattler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Havlena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Radenovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Improving Image-Based Localization by Active Correspondence Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sattler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kobbelt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Image Retrieval for Image-Based Localization Revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sattler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kobbelt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">City-scale location recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Video Google: A text retrieval approach to object matching in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Size Matters: Exhaustive Geometric Verification for Image Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Stewénius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Gunderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pilet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Speeded-up Relaxed Spatial Matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tolias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Avrithis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">24/7 place recognition by view synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arandjelovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Okutomi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Visual Place Recognition with Repetitive Structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Okutomi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Better matching with fewer features: The selection of useful features in large database recognition problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Turcot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WS-LAVD</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Accurate Image Localization Based on Google Maps Street View</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Image Geo-localization Based on Multiple Nearest Neighbor Feature Matching using Generalized Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1546" to="1558" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Camera Pose Voting for Large-Scale Image-Based Localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zeisl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sattler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Image based localization in urban environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kosecka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3DPVT</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
