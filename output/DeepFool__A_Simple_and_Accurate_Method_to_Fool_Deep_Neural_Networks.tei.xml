<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DeepFool: a simple and accurate method to fool deep neural networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seyed-Mohsen</forename><surname>Moosavi-Dezfooli</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alhussein</forename><surname>Fawzi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Frossard</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ecole</forename><surname>Polytechnique</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fédérale</forename><surname>De Lausanne</surname></persName>
						</author>
						<title level="a" type="main">DeepFool: a simple and accurate method to fool deep neural networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:39+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>State-of-the-art deep neural networks have achieved impressive results on many image classification tasks. However, these same architectures have been shown to be unstable to small, well sought, perturbations of the images. Despite the importance of this phenomenon, no effective methods have been proposed to accurately compute the robustness of state-of-the-art deep classifiers to such perturbations on large-scale datasets. In this paper, we fill this gap and propose the DeepFool algorithm to efficiently compute perturbations that fool deep networks, and thus reliably quantify the robustness of these classifiers. Extensive experimental results show that our approach outperforms recent methods in the task of computing adversarial perturbations and making classifiers more robust. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Deep neural networks are powerful learning models that achieve state-of-the-art pattern recognition performance in many research areas such as bioinformatics <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b15">16]</ref>, speech <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b5">6]</ref>, and computer vision <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b7">8]</ref>. Though deep networks have exhibited very good performance in classification tasks, they have recently been shown to be particularly unstable to adversarial perturbations of the data <ref type="bibr" target="#b17">[18]</ref>. In fact, very small and often imperceptible perturbations of the data samples are sufficient to fool state-of-the-art classifiers and result in incorrect classification. (e.g., <ref type="figure" target="#fig_0">Figure 1</ref>). Formally, for a given classifier, we define an adversarial perturbation as the minimal perturbation r that is sufficient to change the estimated labelk(x):</p><p>∆(x;k) := min r r 2 subject tok(x + r) =k(x), <ref type="bibr" target="#b0">(1)</ref> where x is an image andk(x) is the estimated label. We call ∆(x;k) the robustness ofk at point x. The robustness of classifierk is then defined as First row: the original image x that is classified aŝ k(x)="whale". Second row: the image x + r classified ask(x + r)="turtle" and the corresponding perturbation r computed by DeepFool. Third row: the image classified as "turtle" and the corresponding perturbation computed by the fast gradient sign method <ref type="bibr" target="#b3">[4]</ref>. DeepFool leads to a smaller perturbation.</p><formula xml:id="formula_0">ρ adv (k) = E x ∆(x;k) x 2 ,<label>(2)</label></formula><p>where E x is the expectation over the distribution of data. The study of adversarial perturbations helps us understand what features are used by a classifier. The existence of such examples is seemingly in contradiction with the generalization ability of the learning algorithms. While deep networks achieve state-of-the-art performance in image classification tasks, they are not robust at all to small adversarial perturbations and tend to misclassify minimally perturbed data that looks visually similar to clean samples. Though adversarial attacks are specific to the classifier, it seems that the adversarial perturbations are generalizable across different models <ref type="bibr" target="#b17">[18]</ref>. This can actually become a real concern from a security point of view. An accurate method for finding the adversarial perturbations is thus necessary to study and compare the robustness of different classifiers to adversarial perturbations. It might be the key to a better understanding of the limits of current architectures and to design methods to increase robustness. Despite the importance of the vulnerability of state-ofthe-art classifiers to adversarial instability, no well-founded method has been proposed to compute adversarial perturbations and we fill this gap in this paper.</p><p>Our main contributions are the following:</p><p>• We propose a simple yet accurate method for computing and comparing the robustness of different classifiers to adversarial perturbations.</p><p>• We perform an extensive experimental comparison, and show that 1) our method computes adversarial perturbations more reliably and efficiently than existing methods 2) augmenting training data with adversarial examples significantly increases the robustness to adversarial perturbations.</p><p>• We show that using imprecise approaches for the computation of adversarial perturbations could lead to different and sometimes misleading conclusions about the robustness. Hence, our method provides a better understanding of this intriguing phenomenon and of its influence factors.</p><p>We now review some of the relevant work. The phenomenon of adversarial instability was first introduced and studied in <ref type="bibr" target="#b17">[18]</ref>. The authors estimated adversarial examples by solving penalized optimization problems and presented an analysis showing that the high complexity of neural networks might be a reason explaining the presence of adversarial examples. Unfortunately, the optimization method employed in <ref type="bibr" target="#b17">[18]</ref> is time-consuming and therefore does not scale to large datasets. In <ref type="bibr" target="#b13">[14]</ref>, the authors showed that convolutional networks are not invariant to some sort of transformations based on the experiments done on Pascal3D+ annotations. Recently, Tsai et al. <ref type="bibr" target="#b18">[19]</ref> provided a software to misclassify a given image in a specified class, without necessarily finding the smallest perturbation. Nguyen et al. <ref type="bibr" target="#b12">[13]</ref> generated synthetic unrecognizable images, which are classified with high confidence. The authors of <ref type="bibr" target="#b2">[3]</ref> also studied a related problem of finding the minimal geometric transformation that fools image classifiers, and provided quantitative measure of the robustness of classifiers to geometric transformations. Closer to our work, the authors of <ref type="bibr" target="#b3">[4]</ref> introduced the "fast gradient sign" method, which computes the adversarial perturbations for a given classifier very efficiently. Despite its efficiency, this method provides only a coarse approximation of the optimal perturbation vectors. In fact, it performs a unique gradient step, which often leads to sub-optimal solutions. Then in an attempt to build more robust classifiers to adversarial perturbations, <ref type="bibr" target="#b4">[5]</ref> introduced a smoothness penalty in the training procedure that allows to boost the robustness of the classifier. Notably, the method in <ref type="bibr" target="#b17">[18]</ref> was applied in order to generate adversarial perturbations. We should finally mention that the phenomenon of adversarial instability also led to theoretical work in <ref type="bibr" target="#b1">[2]</ref> that studied the problem of adversarial perturbations on some families of classifiers, and provided upper bounds on the robustness of these classifiers. A deeper understanding of the phenomenon of adversarial instability for more complex classifiers is however needed; the method proposed in this work can be seen as a baseline to efficiently and accurately generate adversarial perturbations in order to better understand this phenomenon.</p><p>The rest of paper is organized as follows. In Section 2, we introduce an efficient algorithm to find adversarial perturbations in a binary classifier. The extension to the multiclass problem is provided in Section 3. In Section 4, we propose extensive experiments that confirm the accuracy of our method and outline its benefits in building more robust classifiers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">DeepFool for binary classifiers</head><p>As a multiclass classifier can be viewed as aggregation of binary classifiers, we first propose the algorithm for binary classifiers. That is, we assume herek(x) = sign(f (x)), where f is an arbitrary scalar-valued image classification function f : R n → R. We also denote by F {x : f (x) = 0} the level set at zero of f . We begin by analyzing the case where f is an affine classifier f (x) = w T x + b, and then derive the general algorithm, which can be applied to any differentiable binary classifier f .</p><p>In the case where the classifier f is affine, it can easily be seen that the robustness of f at point x 0 , ∆(x 0 ; f ) 2 , is equal to the distance from x 0 to the separating affine hyper- <ref type="figure" target="#fig_1">Figure 2</ref>). The minimal perturbation to change the classifier's decision corresponds to the orthogonal projection of x 0 onto F . It is given by the closed-form formula:</p><formula xml:id="formula_1">F f (x) &lt; 0 f (x) &gt; 0 r * (x) ∆ ( x 0 ; f ) x 0</formula><formula xml:id="formula_2">plane F = {x : w T x + b = 0} (</formula><formula xml:id="formula_3">r * (x 0 ) := arg min r 2 (3) subject to sign (f (x 0 + r)) = sign(f (x 0 )) = − f (x 0 ) w 2 2 w.</formula><p>Assuming now that f is a general binary differentiable classifier, we adopt an iterative procedure to estimate the robustness ∆(x 0 ; f ). Specifically, at each iteration, f is linearized around the current point x i and the minimal perturbation of the linearized classifier is computed as</p><formula xml:id="formula_4">arg min ri r i 2 subject to f (x i ) + ∇f (x i ) T r i = 0. (4)</formula><p>The perturbation r i at iteration i of the algorithm is computed using the closed form solution in Eq. (3), and the next iterate x i+1 is updated. The algorithm stops when x i+1 changes sign of the classifier. The DeepFool algorithm for binary classifiers is summarized in Algorithm 1 and a geometric illustration of the method is shown in <ref type="figure">Figure 3</ref>.</p><p>In practice, the above algorithm can often converge to a point on the zero level set F . In order to reach the other side of the classification boundary, the final perturbation vector r is multiplied by a constant 1 + η, with η ≪ 1. In our experiments, we have used η = 0.02.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">DeepFool for multiclass classifiers</head><p>We now extend the DeepFool method to the multiclass case. The most common used scheme for multiclass classifiers is one-vs-all. Hence, we also propose our method Algorithm 1 DeepFool for binary classifiers 1: input: Image x, classifier f . 2: output: Perturbationr. <ref type="bibr" target="#b2">3</ref>:</p><formula xml:id="formula_5">Initialize x 0 ← x, i ← 0. 4: while sign(f (x i )) = sign(f (x 0 )) do 5: r i ← − f (xi) ∇f (xi) 2 2 ∇f (x i ), 6: x i+1 ← x i + r i , 7: i ← i + 1. 8: end while 9: returnr = i r i . x0 x1 F R n Figure 3: Illustration of Algorithm 1 for n = 2. As- sume x 0 ∈ R n . The green plane is the graph of x → f (x 0 )+∇f (x 0 ) T (x−x 0 ), which is tangent to the classifier function (wire-framed graph) x → f (x). The orange line indicates where f (x 0 ) + ∇f (x 0 ) T (x − x 0 ) = 0. x 1 is ob- tained from x 0 by projecting x 0 on the orange hyperplane of R n .</formula><p>based on this classification scheme. In this scheme, the classifier has c outputs where c is the number of classes. Therefore, a classifier can be defined as f : R n → R c and the classification is done by the following mapping:</p><formula xml:id="formula_6">k(x) = arg max k f k (x),<label>(5)</label></formula><p>where f k (x) is the output of f (x) that corresponds to the k th class. Similarly to the binary case, we first present the proposed approach for the linear case and then we generalize it to other classifiers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Affine multiclass classifier</head><p>Let f (x) be an affine classifier, i.e., f (x) = W ⊤ x + b for a given W and b. Since the mappingk is the outcome of a one-vs-all classification scheme, the minimal perturbation to fool the classifier can be rewritten as follows</p><formula xml:id="formula_7">arg min r r 2 s.t. ∃k : w ⊤ k (x 0 + r) + b k ≥ w ⊤ k(x0) (x 0 + r) + bk (x0) ,<label>(6)</label></formula><formula xml:id="formula_8">x 0 F 1 F 2 F 3 Figure 4: For x 0 belonging to class 4, let F k = {x : f k (x) − f 4 (x) = 0}.</formula><p>These hyperplanes are depicted in solid lines and the boundary of P is shown in green dotted line.</p><p>where w k is the k th column of W. Geometrically, the above problem corresponds to the computation of the distance between x 0 and the complement of the convex polyhedron P ,</p><formula xml:id="formula_9">P = c k=1 {x : fk (x0) (x) ≥ f k (x)},<label>(7)</label></formula><p>where x 0 is located inside P . We denote this distance by dist(x 0 , P c ). The polyhedron P defines the region of the space where f outputs the labelk(x 0 ). This setting is depicted in <ref type="figure">Figure 4</ref>. The solution to the problem in Eq. <ref type="formula" target="#formula_7">(6)</ref> can be computed in closed form as follows. Definel(x 0 ) to be the closest hyperplane of the boundary of P (e.g. l(x 0 ) = 3 in <ref type="figure">Figure 4</ref>). Formally,l(x 0 ) can be computed as followŝ</p><formula xml:id="formula_10">l(x 0 ) = arg min k =k(x0) f k (x 0 ) − fk (x0) (x 0 ) w k − wk (x0) 2 .<label>(8)</label></formula><p>The minimum perturbation r * (x 0 ) is the vector that projects x 0 on the hyperplane indexed byl(x 0 ), i.e.,</p><formula xml:id="formula_11">r * (x 0 ) = fl (x0) (x 0 ) − fk (x0) (x 0 ) wl (x0) − wk (x0) 2 2 (wl (x0) − wk (x0) ).</formula><p>(9) In other words, we find the closest projection of x 0 on faces of P .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">General classifier</head><p>We now extend the DeepFool algorithm to the general case of multiclass differentiable classifiers. For general non-linear classifiers, the set P in Eq. (7) that describes the region of the space where the classifier outputs labelk(x 0 ) is no longer a polyhedron. Following the explained iterative linearization procedure in the binary case, we approximate </p><formula xml:id="formula_12">P i = c k=1 x : f k (x i ) − fk (x0) (x i )<label>(10)</label></formula><formula xml:id="formula_13">+ ∇f k (x i ) ⊤ x − ∇fk (x0) (x i ) ⊤ x ≤ 0 .</formula><p>We then approximate, at iteration i, the distance between x i and the complement of P , dist(x i , P c ), by dist(x i ,P c i ). Specifically, at each iteration of the algorithm, the perturbation vector that reaches the boundary of the polyhedronP i is computed, and the current estimate updated. The method is given in Algorithm 2. It should be noted that the proposed algorithm operates in a greedy way and is not guaranteed to converge to the optimal perturbation in (1). However, we have observed in practice that our algorithm yields very small perturbations which are believed to be good approximations of the minimal perturbation.</p><p>It should be noted that the optimization strategy of Deep-Fool is strongly tied to existing optimization techniques. In the binary case, it can be seen as Newton's iterative algorithm for finding roots of a nonlinear system of equations in the underdetermined case <ref type="bibr" target="#b14">[15]</ref>. This algorithm is known as the normal flow method. The convergence analysis of this optimization technique can be found for example in <ref type="bibr" target="#b20">[21]</ref>. Our algorithm in the binary case can alternatively be seen as a gradient descent algorithm with an adaptive step size that is automatically chosen at each iteration. The linearization in Algorithm 2 is also similar to a sequential convex programming where the constraints are linearized at each step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Extension to ℓ p norm</head><p>In this paper, we have measured the perturbations using the ℓ 2 norm. Our framework is however not limited to this choice, and the proposed algorithm can simply be adapted </p><formula xml:id="formula_14">4: Initialize x 0 ← x, i ← 0. 5: whilek(x i ) =k(x 0 ) do 6: for k =k(x 0 ) do 7: w ′ k ← ∇f k (x i ) − ∇fk (x0) (x i ) 8: f ′ k ← f k (x i ) − fk (x0) (x i ) 9: end for 10:l ← arg min k =k(x0) |f ′ k | w ′ k 2 11: r i ← |f ′ l | w ′ l 2 2 w ′ l</formula><p>12:</p><p>x i+1 ← x i + r i 13:</p><p>i ← i + 1 14: end while 15: returnr = i r i to find minimal adversarial perturbations for any ℓ p norm (p ∈ [1, ∞)). To do so, the update steps in line 10 and 11 in Algorithm 2 must be respectively substituted by the following updateŝ</p><formula xml:id="formula_15">l ← arg min k =k(x0) |f ′ k | w ′ k q ,<label>(11)</label></formula><formula xml:id="formula_16">r i ← |f ′ l | w ′ l q q |w ′ l | q−1 ⊙ sign(w ′ l ),<label>(12)</label></formula><p>where ⊙ is the pointwise product and q = p p−1 . 3 In particular, when p = ∞ (i.e., the supremum norm ℓ ∞ ), these update steps becomê</p><formula xml:id="formula_17">l ← arg min k =k(x0) |f ′ k | w ′ k 1 ,<label>(13)</label></formula><formula xml:id="formula_18">r i ← |f ′ l | w ′ l 1 sign(w ′ l ).<label>(14)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Setup</head><p>We now test our DeepFool algorithm on deep convolutional neural networks architectures applied to MNIST, CIFAR-10, and ImageNet image classification datasets. We consider the following deep neural network architectures:</p><p>• MNIST: A two-layer fully connected network, and a two-layer LeNet convoluational neural network architecture <ref type="bibr" target="#b8">[9]</ref>. Both networks are trained with SGD with momentum using the MatConvNet <ref type="bibr" target="#b19">[20]</ref> package.</p><p>• CIFAR-10: We trained a three-layer LeNet architecture, as well as a Network In Network (NIN) architecture <ref type="bibr" target="#b10">[11]</ref>.</p><p>• ILSVRC 2012: We used CaffeNet <ref type="bibr" target="#b6">[7]</ref> and GoogLeNet <ref type="bibr" target="#b16">[17]</ref> pre-trained models.</p><p>In order to evaluate the robustness to adversarial perturbations of a classifier f , we compute the average robustnesŝ ρ adv (f ), defined bŷ</p><formula xml:id="formula_19">ρ adv (f ) = 1 |D| x∈D r(x) 2 x 2 ,<label>(15)</label></formula><p>wherer(x) is the estimated minimal perturbation obtained using DeepFool, and D denotes the test set <ref type="bibr" target="#b3">4</ref> . We compare the proposed DeepFool approach to stateof-the-art techniques to compute adversarial perturbations in <ref type="bibr" target="#b17">[18]</ref> and <ref type="bibr" target="#b3">[4]</ref>. The method in <ref type="bibr" target="#b17">[18]</ref> solves a series of penalized optimization problems to find the minimal perturbation, whereas <ref type="bibr" target="#b3">[4]</ref> estimates the minimal perturbation by taking the sign of the gradient</p><formula xml:id="formula_20">r(x) = ǫ sign (∇ x J(θ, x, y)) ,</formula><p>with J the cost used to train the neural network, θ is the model parameters, and y is the label of x. The method is called fast gradient sign method. In practice, in the absence of general rules to choose the parameter ǫ, we chose the smallest ǫ such that 90% of the data are misclassified after perturbation. <ref type="bibr" target="#b4">5</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Results</head><p>We report in <ref type="table">Table 1</ref> the accuracy and average robustnesŝ ρ adv of each classifier computed using different methods. We also show the running time required for each method to compute one adversarial sample. It can be seen that Deep-Fool estimates smaller perturbations (hence closer to minimal perturbation defined in (1)) than the ones computed using the competitive approaches. For example, the average perturbation obtained using DeepFool is 5 times lower than the one estimated with <ref type="bibr" target="#b3">[4]</ref>. On the ILSVRC2012 challenge dataset, the average perturbation is one order of magnitude smaller compared to the fast gradient method. It should be noted moreover that the proposed approach also yields slightly smaller perturbation vectors than the method in <ref type="bibr" target="#b17">[18]</ref>. The proposed approach is hence more accurate in detecting directions that can potentially fool neural networks. As a result, DeepFool can be used as a valuable tool to accurately assess the robustness of classifiers. On Classifier Test errorρ adv [DeepFool] timeρ adv <ref type="bibr" target="#b3">[4]</ref> timeρ adv <ref type="bibr" target="#b17">[18]</ref>  the complexity aspect, the proposed approach is substantially faster than the standard method proposed in <ref type="bibr" target="#b17">[18]</ref>. In fact, while the approach <ref type="bibr" target="#b17">[18]</ref> involves a costly minimization of a series of objective functions, we observed empirically that DeepFool converges in a few iterations (i.e., less than 3) to a perturbation vector that fools the classifier. Hence, the proposed approach reaches a more accurate perturbation vector compared to state-of-the-art methods, while being computationally efficient. This makes it readily suitable to be used as a baseline method to estimate the robustness of very deep neural networks on large-scale datasets. In that context, we provide the first quantitative evaluation of the robustness of state-of-the-art classifiers on the large-scale ImageNet dataset. It can be seen that despite their very good test accuracy, these methods are extremely unstable to adversarial perturbations: a perturbation that is 1000 smaller in magnitude than the original image is sufficient to fool state-of-the-art deep neural networks. We illustrate in <ref type="figure" target="#fig_0">Figure 1</ref> perturbed images generated by the fast gradient sign and DeepFool. It can be observed that the proposed method generates adversarial perturbations which are hardly perceptible, while the fast gradient sign method outputs a perturbation image with higher norm.</p><p>It should be noted that, when perturbations are measured using the ℓ ∞ norm, the above conclusions remain unchanged: DeepFool yields adversarial perturbations that are smaller (hence closer to the optimum) compared to other methods for computing adversarial examples. <ref type="table">Table 2</ref> reports the ℓ ∞ robustness to adversarial perturbations mea-</p><formula xml:id="formula_21">sured byρ ∞ adv (f ) = 1 |D| x∈D r(x) ∞</formula><p>x ∞ , wherer(x) is computed respectively using DeepFool (with p = ∞, see Section 3.3), and the Fast gradient sign method for MNIST and CIFAR-10 tasks.</p><p>Fine-tuning using adversarial examples In this section, we fine-tune the networks of <ref type="table">Table 1</ref>   <ref type="table">Table 2</ref>: Values ofρ ∞ adv for four different networks based on DeepFool (smallest l ∞ perturbation) and fast gradient sign method with 90% of misclassification. and CIFAR-10 tasks. Specifically, for each network, we performed two experiments: (i) Fine-tuning the network on DeepFool's adversarial examples, (ii) Fine-tuning the network on the fast gradient sign adversarial examples. We fine-tune the networks by performing 5 additional epochs, with a 50% decreased learning rate only on the perturbed training set. For each experiment, the same training data was used through all 5 extra epochs. For the sake of completeness, we also performed 5 extra epochs on the original data. The evolution ofρ adv for the different fine-tuning strategies is shown in <ref type="figure" target="#fig_5">Figures 6a to 6d</ref>, where the robustnessρ adv is estimated using DeepFool, since this is the most accurate method, as shown in <ref type="table">Table 1</ref>. Observe that finetuning with DeepFool adversarial examples significantly increases the robustness of the networks to adversarial perturbations even after one extra epoch. For example, the robustness of the networks on MNIST is improved by 50% and NIN's robustness is increased by about 40%. On the other hand, quite surprisingly, the method in <ref type="bibr" target="#b3">[4]</ref> can lead to a decreased robustness to adversarial perturbations of the network. We hypothesize that this behavior is due to the fact that perturbations estimated using the fast gradient sign method are much larger than minimal adversarial per-  turbations. Fine-tuning the network with overly perturbed images decreases the robustness of the networks to adversarial perturbations. To verify this hypothesis, we compare in <ref type="figure" target="#fig_6">Figure 7</ref> the adversarial robustness of a network that is fine-tuned with the adversarial examples obtained using DeepFool, where norms of perturbations have been deliberately multiplied by α = 1, 2, 3. Interestingly, we see that by magnifying the norms of the adversarial perturbations, the robustness of the fine-tuned network is decreased. This might explain why overly perturbed images decrease the robustness of MNIST networks: these perturbations can really change the class of the digits, hence fine-tuning based on these examples can lead to a drop of the robustness (for an illustration, see <ref type="figure">Figure 8</ref>). This lends credence to our hypothesis, and further shows the importance of designing accurate methods to compute minimal perturbations. <ref type="table" target="#tab_3">Table 3</ref> lists the accuracies of the fine-tuned networks. It can be seen that fine-tuning with DeepFool can improve the accuracy of the networks. Conversely, fine-tuning with the approach in <ref type="bibr" target="#b3">[4]</ref> has led to a decrease of the test accuracy in all our experiments. This confirms the explanation that the fast gradient sign method outputs overly perturbed images that lead to images that are unlikely to occur in the test data. Hence, it decreases the performance of the method as it acts as a regularizer that does not represent the distribution of the original data. This effect is analogous to geometric data augmentation schemes, where large transformations of the original samples have a counter-productive effect on gener- <ref type="figure">Figure 8</ref>: From "1" to "7" : original image classified as "1" and the DeepFool perturbed images classified as "7" using different values of α.  alization. <ref type="bibr" target="#b5">6</ref> To emphasize the importance of a correct estimation of the minimal perturbation, we now show that using approximate methods can lead to wrong conclusions regarding the adversarial robustness of networks. We fine-tune the NIN classifier on the fast gradient sign adversarial examples. We follow the procedure described earlier but this time, we decreased the learning rate by 90%. We have evaluated the adversarial robustness of this network at different extra epochs using DeepFool and the fast gradient sign method. As one can see in <ref type="figure" target="#fig_7">Figure 9</ref>, the red plot exaggerates the effect of training on the adversarial examples. Moreover, it is not sensitive enough to demonstrate the loss of robustness at the first extra epoch. These observations confirm that using an accurate tool to measure the robustness of classifiers is crucial to derive conclusions about the robustness of networks.</p><formula xml:id="formula_22">α =1 α =2 α =3 α =4</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work, we proposed an algorithm, DeepFool, to compute adversarial examples that fool state-of-the-art classifiers. It is based on an iterative linearization of the classifier to generate minimal perturbations that are sufficient to change classification labels. We provided extensive experimental evidence on three datasets and eight classifiers, showing the superiority of the proposed method over stateof-the-art methods to compute adversarial perturbations, as well as the efficiency of the proposed approach. Due to <ref type="bibr" target="#b5">6</ref> While the authors of <ref type="bibr" target="#b3">[4]</ref> reported an increased generalization performance on the MNIST task (from 0.94% to 0.84%) using adversarial regularization, it should be noted that the their experimental setup is significantly different as <ref type="bibr" target="#b3">[4]</ref> trained the network based on a modified cost function, while we performed straightforward fine-tuning. its accurate estimation of the adversarial perturbations, the proposed DeepFool algorithm provides an efficient and accurate way to evaluate the robustness of classifiers and to enhance their performance by proper fine-tuning. The proposed approach can therefore be used as a reliable tool to accurately estimate the minimal perturbation vectors, and build more robust classifiers.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>An example of adversarial perturbations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Adversarial examples for a linear binary classifier.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>For x 0 belonging to class 4, let F k = {x : f k (x) − f 4 (x) = 0}.The linearized zero level sets are shown in dashed lines and the boundary of the polyhedroñ P 0 in green. the set P at iteration i by a polyhedronP ĩ</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>on adversarial examples to build more robust classifiers for the MNIST</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>of fine-tuning on adversarial examples computed by two different methods for LeNet on MNIST. of fine-tuning on adversarial examples computed by two different methods for a fully-connected network on MNIST. Effect of fine-tuning on adversarial examples computed by two different methods for NIN on CIFAR-10. Effect of fine-tuning on adversarial examples computed by two different methods for LeNet on CIFAR-10.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6</head><label>6</label><figDesc>Figure 6</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Fine-tuning based on magnified DeepFool's adversarial perturbations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 :</head><label>9</label><figDesc>How the adversarial robustness is judged by different methods. The values are normalized by the correspondingρ adv s of the original network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>Algorithm 2 DeepFool: multi-class case 1: input: Image x, classifier f . 2: output: Perturbationr.</figDesc><table>3: 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>Table 1: The adversarial robustness of different classifiers on different datasets. The time required to compute one sample for each method is given in the time columns. The times are computed on a Mid-2015 MacBook Pro without CUDA support. The asterisk marks determines the values computed using a GTX 750 Ti GPU.</figDesc><table>time 

LeNet (MNIST) 
1% 
2.0 × 10 −1 
110 ms 
1.0 
20 ms 
2.5 × 10 −1 &gt; 4 s 

FC500-150-10 (MNIST) 
1.7% 
1.1 × 10 −1 
50 ms 
3.9 × 10 −1 10 ms 
1.2 × 10 −1 &gt; 2 s 

NIN (CIFAR-10) 
11.5% 
2.3 × 10 −2 
1100 ms 1.2 × 10 −1 180 ms 2.4 × 10 −2 &gt;50 s 

LeNet (CIFAR-10) 
22.6% 
3.0 × 10 −2 
220 ms 
1.3 × 10 −1 50 ms 
3.9 × 10 −2 &gt;7 s 

CaffeNet (ILSVRC2012) 
42.6% 
2.7 × 10 −3 
510 ms* 3.5 × 10 −2 50 ms* -
-

GoogLeNet (ILSVRC2012) 31.3% 
1.9 × 10 −3 
800 ms* 4.7 × 10 −2 80 ms* -
-

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc>The test error of networks after the fine-tuning on adversarial examples (after five epochs). Each columns correspond to a different type of augmented perturbation.</figDesc><table></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">To encourage reproducible research, the code of DeepFool is made available at http://github.com/lts4/deepfool</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">From now on, we refer to a classifier either by f or its corresponding discrete mappingk. Therefore, ρ adv (k) = ρ adv (f ) and ∆(x;k) = ∆(x; f ).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">To see this, one can apply Holder's inequality to obtain a lower bound on the ℓp norm of the perturbation.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">For ILSVRC2012, we used the validation data.<ref type="bibr" target="#b4">5</ref> Using this method, we observed empirically that one cannot reach 100% misclassification rate on some datasets. In fact, even by increasing ǫ to be very large, this method can fail in misclassifying all samples.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep autoencoder neural networks for gene ontology annotation predictions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chicco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sadowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Baldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Conference on Bioinformatics, Computational Biology, and Health Informatics</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="533" to="540" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Analysis of classifiers&apos; robustness to adversarial perturbations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Frossard</surname></persName>
		</author>
		<idno>abs/1502.02590</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Manitest: Are classifiers really invariant?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Frossard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="106" to="107" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Towards deep neural network architectures robust to adversarial examples. CoRR, abs/1412</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Rigazio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">5068</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kingsbury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Mag</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="82" to="97" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimedia (MM)</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="675" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems (NIPS)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Object recognition with gradient-based learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Shape, contour and grouping in computer vision</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="319" to="345" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Convolutional networks and applications in vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Farabet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Symposium on Circuits and Systems (ISCAS)</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="253" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Network in network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Strategies for training large scale neural network language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Deoras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Černockỳ</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="196" to="201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep neural networks are easily fooled: High confidence predictions for unrecognizable images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="427" to="436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">What is holding back convnets for detection?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Pepik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ritschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="517" to="528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Nonlinear optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Ruszczyński</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>Princeton university press</publisher>
			<biblScope unit="volume">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A deep learning network approach to ab initio protein secondary structure prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Spencer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Eickholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans. Comput. Biol. Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="103" to="112" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Intriguing properties of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Are deep learning algorithms easily hackable?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cox</surname></persName>
		</author>
		<ptr target="http://coxlab.github.io/ostrichinator" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Matconvnet: Convolutional neural networks for matlab</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lenc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimedia (MM)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="689" to="692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Least-change secant update methods for underdetermined systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">F</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">T</forename><surname>Watson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on numerical analysis</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1227" to="1262" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
