<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Continuous Occlusion Model for Road Scene Understanding</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikas</forename><surname>Dhiman</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc-Huy</forename><surname>Tran</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">NEC Laboratories America, Inc. Cupertino</orgName>
								<address>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manmohan</forename><surname>Chandraker</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">NEC Laboratories America, Inc. Cupertino</orgName>
								<address>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Michigan</orgName>
								<address>
									<settlement>Ann Arbor</settlement>
									<region>MI</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Continuous Occlusion Model for Road Scene Understanding</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:37+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a physically interpretable, continuous threedimensional (3D) model for handling occlusions with applications to road scene understanding. We probabilistically assign each point in space to an object with a theoretical modeling of the reflection and transmission probabilities for the corresponding camera ray. Our modeling is unified in handling occlusions across a variety of scenarios, such as associating structure from motion (SFM) point tracks with potentially occluding objects or modeling object detection scores in applications such as 3D localization. For point track association, our model uniformly handles static and dynamic objects, which is an advantage over motion segmentation approaches traditionally used in multibody SFM. Detailed experiments on the KITTI raw dataset show the superiority of the proposed method over both state-of-the-art motion segmentation and a baseline that heuristically uses detection bounding boxes for resolving occlusions. We also demonstrate how our continuous occlusion model may be applied to the task of 3D localization in road scenes.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>As a two-dimensional (2D) projection of the threedimensional (3D) world, image formation is associated with a loss of information. This is especially significant when objects in 3D space occlude each other with respect to the camera viewpoint. In recent years, we have seen remarkable progress in various aspects of scene understanding, such as structure from motion (SFM) and object detection. However, occlusions still present a challenge, with the difficulty of physically modeling them being a major bottleneck.</p><p>Our main contribution is a novel theoretical model for occlusion handling that is continuous and fully 3D. Our model is motivated by insights from computer graphics, whereby we represent objects as translucent 3D ellipsoids. In Section 3, we develop novel continuous models for representing transmission and reflection probabilities for each ray emanating from the camera. This allows assigning probabilities for <ref type="figure">Figure 1</ref>: We propose an occlusion model in 3D that is physicallyinspired and continuous. Given object detection and SFM point tracks, our unified model probabilistically assigns point tracks to objects and reasons about object detection scores and bounding boxes. It uniformly handles static and dynamic objects, thus, outperforms motion segmentation for association problems. We also demonstrate occlusion-aware 3D localization in road scenes. each point in space belonging to an object, which can explicitly explain image observations and reason about occlusions. This is in contrast to prior works that consider occlusions in 2D, or through discrete occluder patterns or models that are not physically interpretable <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35]</ref>.</p><p>A key advantage afforded by our occlusion model is unification. While previous approaches to handling occlusions are application-dependent, ours is physically-inspired, thus, flexible enough to be used in a variety of scenarios. In this paper, we show that our theory can be used for uniformly modeling the association of SFM point tracks with static or dynamic objects (Section 4.1), as well as modeling object detection scores in applications like 3D localization (Section 4.2). We demonstrate the application of our formulations for road scenes from the KITTI raw dataset <ref type="bibr" target="#b6">[7]</ref>.</p><p>In particular, assigning 2D point tracks to independent, but potentially occluding, objects is a fundamental challenge in computer vision problems such as multibody SFM <ref type="bibr" target="#b16">[17]</ref>. Recent works use motion segmentation <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b20">21]</ref> as a precursor to localizing objects, which often suffices for moving objects <ref type="bibr" target="#b24">[25]</ref> and has also been considered for multibody SFM <ref type="bibr" target="#b11">[12]</ref>. However, motion-based segmentation is not always applicable in road scenes, due to static parked cars, or dynamic cars that move with similar velocities. Occlusions make the problem more severe since point tracks get clustered together for static objects and may frequently appear to change association among dynamic objects in 2D. Indeed, we show in Section 5 that our point track association outperforms state-of-the-art motion segmentation methods, as well as a baseline that uses detection bounding boxes but does not consider occlusions. Another potential application of our proposed model is towards 3D localization in road scenes. Prior works such as <ref type="bibr" target="#b23">[24]</ref> combine information from point tracks and detection bounding boxes, but do not consider occlusions for either. In contrast, our unified occlusion model allows a probabilistic soft assignment of point tracks to objects, as well as an occlusion-aware interpretation of object detection outputs. Our model is continuous, so it remains amenable to the use of continuous optimization tools.</p><p>To summarize, our main contributions are:</p><p>• A novel theoretical model for handling occlusions that is continuous and formulated in 3D. • Unified occlusion handling for point tracks in SFM and bounding boxes and detection scores in object detection. • Application of our model to association of point tracks with both static and moving objects, improving over motion segmentation and occlusion-unaware baselines. • Application of our unified formulation to 3D localization of traffic participants in road scenes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Occlusion handling in detection Several works in object detection consider occlusion by training a detector on visible parts of the object <ref type="bibr" target="#b5">[6]</ref>. Occlusion reasoning based on 2D image silhouettes is used to improve detection performance in <ref type="bibr" target="#b9">[10]</ref>. On the other hand, our occlusion reasoning is based on 3D entities. In recent years, object detectors have also considered occlusion reasoning using 3D cues, often learned from a dataset of CAD models <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b29">30]</ref>. By necessity, such frameworks are often a discrete representation of occlusion behavior, for example, in the form of a collection of occlusion masks derived from object configurations discretized over viewpoint. In contrast to these works, our occlusion modeling is also fully 3D, but allows for a continuous representation. Further, to derive 3D information, we do not use CAD models, rather we derive a probabilistic formulation based on physical insights.</p><p>Occlusion handling in tracking Occlusions have also been handled in tracking-by-detection frameworks by considering occluder patterns in the image <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b28">29]</ref>. A notable exception is the work of Milan et al. <ref type="bibr" target="#b14">[15]</ref> that explicitly models occlusions in the continuous domain to determine a visibility ratio for each object in multi-target tracking. However, the occlusion model in <ref type="bibr" target="#b14">[15]</ref> is essentially the overlap of image projections of a Gaussian representation of the object. Our occlusion modeling, on the other hand, is fully 3D, based on physical modeling of object-ray intersections and much more general in determining the probability of a point in space as belonging to an object. While our model can also be used to determine a visibility ratio similar to <ref type="bibr" target="#b14">[15]</ref>, it has far more general applications and can be quantitatively evaluated, as shown by our experiments on point track associations.</p><p>Motion segmentation and multibody SFM An application for our occlusion modeling is to determine point track associations in scenes with multiple objects. For moving objects, this is within the purview of motion segmentation, which has been approached through algebraic factorization methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b25">26]</ref>, statistical methods <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b19">20]</ref> and clustering methods <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b7">8]</ref>. Some recent efforts include robust algebraic segmentation with hybrid perspective constraints <ref type="bibr" target="#b20">[21]</ref> and spectral clustering with point track spatial affinities <ref type="bibr" target="#b0">[1]</ref>. Unlike our work, such methods cannot handle static objects, or dynamic objects with little relative motion. Closer to our application, motion segmentation is also used within multibody SFM frameworks <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17]</ref>. In contrast to these works, our formulation does not distinguish between moving and static objects and also explicitly reasons about occlusions due to 3D object geometries for associating point tracks to individual objects.</p><p>3D localization One of the vital goals of 3D scene understanding is to localize 3D objects in complex scenes. Monocular frameworks like ours have also reasoned about occlusions, for instance, partial object detectors are considered in <ref type="bibr" target="#b27">[28]</ref>. A detailed part-based representation of objects based on annotated CAD models is used for monocular scene understanding in <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35]</ref>, which also allows reasoning about mutual occlusions between objects. In contrast to these works, our monocular framework uses a physical modeling of occlusion in continuous space and derives unified representations for SFM points and object detection bounding boxes. This makes our model more general, extensible and amenable for continuous optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Continuous Occlusion Model</head><p>A common parametric modeling for objects, especially traffic participants in road scene understanding, is as opaque Depth from camera(λ) Probability P reflection P transmission <ref type="figure">Figure 2</ref>: We represent objects as translucent ellipsoids, which leads to the formulation of transmission and reflection probabilities. This figure shows the reflection probability for the first object (in violet), which has high values around the camera-facing side of the object. Also, note that the transmission probability is inversely proportional to occupancy.</p><p>cuboids. <ref type="bibr" target="#b0">1</ref> However, such models introduce discontinuities in the problem formulation and do not adequately account for uncertainties in pose and dimensions. With this motivation, we introduce our representation of 3D objects and our modeling of object-object relationships, which lead to a continuous occlusion model that correctly accounts for uncertainties in position and dimensions. We refer the reader to <ref type="figure">Figure 2</ref> for an illustration of the proposed concepts.</p><p>Occupancy model for traffic participants Intuitively, we consider traffic participants to be regions of 3D space with a high probability of occupancy. We model the uncertainty in occupancy as a translucency function, with regions more likely to be occupied by an object considered more opaque and regions more likely to be free space considered more transparent. Based on this intuition, we model objects as translucent 3D ellipsoids whose opacity is maximum at the center and falls off towards the edges. In particular, we model the occupancy at 3D location x corresponding to an object O i centered at p i as</p><formula xml:id="formula_0">f i occ (x) = L(x; p i , Σ i ),<label>(1)</label></formula><p>where L(·) is the logistic function given by</p><formula xml:id="formula_1">L(x; p, Σ) = 1 1 + e −k(1−d(x,p)) ,<label>(2)</label></formula><formula xml:id="formula_2">with d(x, p) = (x−p) ⊤ Σ −1 (x−p)</formula><p>being the Mahalanobis distance. We set k = 10 ln(49) as the value that allows the logistic function L to drop to 0.98 at a distance d = 0.9 from the object center. The spread of the ellipsoid, determined by Σ i , depends on the dimensions of the object. Please refer to the supplementary material for the computation of Σ i from object dimensions.</p><p>Image formation Given the above occupancy representation of the scene, a point on an object is observed in the camera when precisely two conditions are satisfied. First, the backprojected ray from the observed image pixel is transmitted through free space until it reaches the object. Second, the ray encounters an opaque enough object surface and is reflected. More formally, the probability of observation of a point x j on object O i is given by</p><formula xml:id="formula_3">P ij observation = P ij reflection P j transmission .<label>(3)</label></formula><p>The reflection probability ensures the presence of an object to constitute the observation, while the transmission probability allows us to model occlusions. The forms of these two functions are described next.</p><p>Reflection probability Consider a 3D point x j observed in the image at pixel u j . Let K be the intrinsic calibration matrix for the camera andr j = K −1 u j K −1 u j be the unit vector along the backprojected ray from the camera center passing through u j . Then, the probability of reflection at depth λ along the rayr j , by an object O i , is determined by the gradient of the object's occupancy function f i occ as</p><formula xml:id="formula_4">P ij reflection (λ) = 1 Z (max{0, ∇f i occ (x j ) ⊤r j }) 2 .<label>(4)</label></formula><p>The gradient ∇f i occ (x j ) encourages the reflection probability to be high near object boundaries, the max ensures that negative probability due to the gradient in the direction opposite to the ray is clipped off and squaring allows the function to be smooth near zero. Here, Z denotes the normalization factor. We note that in the extreme case of an object being fully opaque (that is, ∇f i occ (x j ) ∈ {0, 1}), the above model reverts to a (squared) Lambertian reflection. <ref type="figure">Figure 2</ref> shows an example of the reflection probability.</p><p>Transmission probability Since we are modeling occupancy as transparency, we derive inspiration from optics for the modeling of translucent objects. A model for transmission of light across a distance α, through a medium of density ρ and opacity β is given by the Beer-Lambert law as</p><formula xml:id="formula_5">I(α) = I 0 e −βρα .<label>(5)</label></formula><p>In our formulation of scene occupancy, both opacity and density at a scene point x j are encapsulated within the total occupancy function summed over all objects,</p><formula xml:id="formula_6">f occ (x j ) = i f i occ (x j ). Further, the domain of our occupancy func- tion f occ (x j ) is [0, 1] instead of [0, ∞) for opacity β. Thus,</formula><p>we replace e −βρ by the transparency function 1 − f occ (x j ) and consequently, the transmission probability over a small distance dλ is given by</p><formula xml:id="formula_7">P j transmission (λ + dλ) = P j transmission (λ)(1 − f occ (x j )) dλ .<label>(6)</label></formula><p>Thus, for an image point u j to correspond to a 3D point x j at depth λ along the backprojected rayr j , the ray must be transmitted through space with the probability</p><formula xml:id="formula_8">P j transmission (λ) = λ c (1 − f occ (λr j )) dλ .<label>(7)</label></formula><p>Here, λ c represents a product integral from c to λ, where c is the position of camera screen, considered here to be equivalent to the focal length of the camera . <ref type="bibr" target="#b1">2</ref> In practice, the integral for transmission probability <ref type="formula" target="#formula_8">(7)</ref> is difficult to compute even numerically. So we choose a parameterization in the form of a product of sigmoid functions, which is a reasonable approximation to the behaviour of the transmission probability, as follows:</p><formula xml:id="formula_9">P j transmission (λ) = i (1 − L u (u; µ i , Γ i )L λ (λ; ν i )),<label>(8)</label></formula><p>where L u (.) is sigmoid in the image domain, with µ i and Γ i representing the elliptical projection of object O i in the image and L λ (.) is sigmoid in the depth domain, with ν i being the mean depth of object O i . That is,</p><formula xml:id="formula_10">L u (u; µ i , Γ i ) = 1 1 + e −ku(1−(u−µi) ⊤ Γ −1 i (u−µi)) ,<label>(9)</label></formula><formula xml:id="formula_11">L λ (λ; ν i ) = 1 1 + e −k d (λ−νi) .<label>(10)</label></formula><p>In <ref type="figure" target="#fig_0">Figure 3</ref>, we compare the exact and approximate formulations of transmission probability given by <ref type="formula" target="#formula_8">(7)</ref> and <ref type="formula" target="#formula_9">(8)</ref>, respectively. Note that the choice of mean depth of the object causes some deviation from the exact transmission probability. However, the shift of transmission probability anywhere through the object is still a reasonable approximation as occluded points can only lie outside the object. On the other hand, it yields significant computational savings since ray intersections with an ellipsoid are expensive to evaluate densely. Thus, we have modeled the transmission probability to effectively capture the effect of occlusion due to all traffic participants in a scene that lie along a particular ray. We reiterate that our reflection and transmission probabilities are continuous functions, which allows us to keep the problem formulation in the continuous domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Unified Occlusion Models</head><p>In this section, we highlight the versatility of our occlusion modeling by demonstrating its unified application to two different problems: associating point tracks with objects and 3D object localization using objects and point tracks. <ref type="table" target="#tab_0">Table 1</ref> summarizes inputs and outputs for these problems. p c (t) Position of camera at time t ω c (t)</p><p>Orientation of camera at time t </p><formula xml:id="formula_12">p i 0 (t) Initial position of object O i at time t ω i 0 (t) Initial orientation of object O i at time t B i 0 Initial 3D dimensions of object O i Output P ij assoc Probability of assigning feature track j to object O i p i (t) Position of object O i at time t ω i (t) Orientation of object O i at time t B i 3D dimensions of object O i</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Object-Point Association</head><p>Given 2D image points {u j } that are tracked between consecutive frames and a set of objects {O i } appearing in the frames, we aim to associate u j with O i . Based on our continuous occlusion model in Section 3, the association probability a ij (λ) between point track u j and object O i at depth λ can be defined as</p><formula xml:id="formula_13">a ij (λ) = P ij reflection (λ)P j transmission (λ),<label>(11)</label></formula><p>where P ij reflection (λ) and P j transmission (λ) are from <ref type="formula" target="#formula_4">(4)</ref> and <ref type="formula" target="#formula_9">(8)</ref> respectively. Note that the fraction a ij (λ), although called association probability, does not capture the entire information that we have available for computing the association of point tracks to objects.</p><p>Rather, to compute the association probability between point track u j and object O i , we should also use the reprojection error. When the association of point track u j and object O i is correct and the point of reflection is at depth λ, the corresponding reprojection error E ij reproj (λ) must be zero, otherwise the error becomes a measure of distance from the true solution. The error E ij reproj (λ) can be used for associating point tracks and objects by converting it to the probability domain as</p><formula xml:id="formula_14">P ij reproj (λ) = 1 Z ′ exp(−E ij reproj (λ)),<label>(12)</label></formula><p>where Z ′ is the normalization coefficient. Using both of the evidence terms in <ref type="formula" target="#formula_0">(11)</ref> and <ref type="formula" target="#formula_0">(12)</ref>, we can define the new association probability P ij assoc , as follows:</p><formula xml:id="formula_15">P ij assoc = 1 Z ′′ ∞ 0 a ij (λ) exp(−E ij reproj (λ))dλ,<label>(13)</label></formula><p>where Z ′′ is the new normalization coefficient. Once we have computed the association probability P ij assoc for every pair of point tracks and objects, we can assign each point track to the object with the highest association probability. The point tracks having very small association probabilities are assigned to the background. In contrast to the principled approach above, a heuristic baseline may simply assign a point track to the detection bounding box enclosing it (and the background if outside all bounding boxes). For regions where bounding boxes overlap, it may assign point tracks to the object that has the smallest mean depth among the competing bounding boxes. As we demonstrate in our experiments, such heuristics are suboptimal compared to using (13) from our occlusion model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">3D Object Localization</head><p>In this section, we exploit our continuous occlusion model for another application, namely, 3D object localization in road scenes, which further demonstrates its versatility. Given a set of 2D tracked feature points {u j (t)} and 2D detection bounding boxes {d i (t)} at frame t, the goal is to localize 3D traffic participants. In particular, for each traffic participant, we wish to estimate its position p i (t) and orientation ω i (t) on the ground plane and its 3D dimensions B i (t). Please refer to <ref type="table" target="#tab_0">Table 1</ref> for a summary of inputs and outputs.</p><p>We construct a graphical model for representing relationships among objects, as well as between objects and point tracks. <ref type="figure" target="#fig_1">Figure 4</ref> illustrates an example of the graph and energies. The negative log likelihood is decomposed as follows:</p><formula xml:id="formula_16">− log P ({p i (t), ω i (t), B i (t)}|{u j (t)}, {d i (t)}) = −Z + ei t=si λ track E ijt track + ei t=si N i=1 λ detect E it detect + λ dyn E it dyn + λ size E it size ,</formula><p>where E ijt track and E it detect reason about image observations such as point tracks and bounding boxes, while E it dyn and E it size impose smoothness constraints and size priors respectively. Here, λ track , λ detect , λ dyn , λ size are energy weights, N is the number of objects in the sequence, s i and t i are respectively the starting and ending frames of object O i , andZ is the normalization coefficient. Next, we present our unified continuous occlusion modeling for both point track and bounding box energies. Due to space constraints, we present the details of other energies in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Continuous point track energy with occlusion</head><p>Let Ω i (t) be the pose of object O i at time t in world coordinates, which is computed using the camera pose at time t and the relative pose of object O i with respect to the camera at time t. We denote π Ω i (t) (.) and π −1 Ω i (t−1) (.) as the forward and inverse projection functions that project a 3D point to the 2D image and vice versa. Then, the reprojection error for 2D point u j (t) with hypothesized depth λ, is given by</p><formula xml:id="formula_17">E ij reproj (λ) = u j (t) − π Ω i (t) π −1 Ω i (t−1) (u j (t − 1), λ) 2 . (14)</formula><p>Note that the inverse projection π −1 Ω i (t) (.) depends on both the 2D point u j (t) and the unknown depth λ. Also note that the inverse projection relies on the object pose at time t − 1 while the forward projection relies on the object pose at time t, which can be different.</p><p>For an object O i , let {Ω(t)} i be the poses of all occluding objects at time t (inclusive of object O i ) and {B} i be their corresponding 3D dimensions. Then, we model the continuous point track energy with explicit occlusion reasoning as the expected reprojection error over the association probability <ref type="bibr" target="#b14">(15)</ref> where N and M are, respectively, the number of objects and points and a ij (λ) is the association probability of point u j (t) with object O i at depth λ, given by <ref type="bibr" target="#b10">(11)</ref>.</p><formula xml:id="formula_18">E ijt track ({Ω(t)} i , {Ω(t − 1)} i , {B} i ) = N i=1 M j=1 ∞ 0 a ij (λ)E ij reproj (λ)dλ,</formula><p>Continuous bounding box energy with occlusion Object detection is usually followed by non-maximal suppression that results in discarding similar bounding boxes. When we are jointly optimizing detections with other cues, it is usually not desirable to use a single bounding box. To retain the entire detection output while maintaining the continuous form of our energies, we approximate the distribution of detection scores with a multi-modal sum of Gaussian-like logistic functions. In particular, let 2D bounding box d i (t) be parameterized as a 4D vector [x min , y min , x max , y max ] ⊤ . We fit a parametric function to the detection scores, of the form</p><formula xml:id="formula_19">S(d i (t)) = k A k exp −ǫ i k (t) ⊤ Λ −1 k ǫ i k (t) ,<label>(16)</label></formula><p>where A k is an amplitude and ǫ i k (t) = d i (t) − µ k , with µ k the mean and Λ k the covariance. We use a non-linear solver to minimize the above, with initialization from non-maximal suppressed outputs. The optimization is constrained by the symmetry and positive definiteness of Λ k , x max ≥ x min and y max ≥ y min .</p><p>Detection scores with occlusion reasoning With our model of P j transmission (λ) described in Section 3, we compute the probability of a point u in the image to be occluded, assuming the point is on object O i with mean depth ν i , as</p><formula xml:id="formula_20">Θ i (u, ν i ) = 1 − P transmission (ν i , u).<label>(17)</label></formula><p>If a portion of the proposed detection bounding box is known to be occluded, one would like to decrease the confidence in the detection score about the localization of that end of the object. Assuming that occlusions are more likely on the boundaries of the detection bounding box, we wish to decrease the confidence on the mean detection boundaries around the occluded boundaries. To re-model detection scores scaled by continuous occlusion, we sample Θ i (u, ν i ) at the hypothesized detection boundaries from the Gaussian mixture model (GMM) S(.) in <ref type="bibr" target="#b15">(16)</ref> and augment the detection boundary covariance matrix by  scores is given by Λ ′ k = P i + Λ k for all k. The detection score GMM S ′ (.) with explicit occlusion reasoning is given by replacing the covariance matrix, as follows:</p><formula xml:id="formula_21">P i = ρ i ρ ⊤ i , where ρ i = Θ i (u, ν i ).</formula><formula xml:id="formula_22">S ′ (d i (t)) = k A k exp −ǫ i k (t) ⊤ Λ ′ −1 k ǫ i k (t) .<label>(18)</label></formula><p>The energy of detection scores is simply taken to be the inverse of the above detection score, that is,</p><formula xml:id="formula_23">E it detect ({Ω i (t)} i , {B i } i ) = 1 S ′ (d i (t))</formula><p>.</p><p>Inference on graphical model We apply the Metropolis-Hastings method <ref type="bibr" target="#b13">[14]</ref> to perform inference on the graphical model. Since we optimize over continuous variables, we use the Gaussian distribution as the proposal function. We choose this over alternatives such as block-coordinate descent since they are slower in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>In this section, we benchmark our continuous occlusion model for point-to-object association against the baseline method using detection bounding boxes and state-of-the-art methods for motion segmentation <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b20">21]</ref>. We then show how the proposed model may be applied for 3D object localization in road scenes. For our experiments, we use 35 sequences of the KITTI raw dataset <ref type="bibr" target="#b6">[7]</ref>, which are recorded under a variety of driving conditions and include 10,088 frames and 634 object tracks in total.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Association Experiments</head><p>Setup We first perform the association experiment that compares the accuracy of point-to-object association using our proposed model against a heuristic baseline and state-ofthe-art motion segmentation methods. The detection bounding box baseline method (BBox) is as described at the end of Section 4.1. For motion segmentation, we use robust algebraic segmentation with hybrid perspective constraints (RAS) <ref type="bibr" target="#b20">[21]</ref> and spectral clustering with point track spatial affinities (BM) <ref type="bibr" target="#b0">[1]</ref>. For each sequence, the methods of <ref type="bibr" target="#b4">[5]</ref> and <ref type="bibr" target="#b1">[2]</ref> are used for computing detection bounding boxes and object tracklets, respectively. We then apply <ref type="bibr" target="#b31">[32]</ref> to extract point tracks. Note that our method can handle occlusions in both static and dynamic scenes, but motion segmentation focuses on dynamic scenes. For a complete evaluation, we organize the point tracks into four sets: all point tracks, occluded point tracks, dynamic point tracks and dynamic as well as occluded point tracks. The parameters (position, orientation, and dimensions) of all objects (cars) estimated by the method of <ref type="bibr" target="#b22">[23]</ref> are provided to our method (for computing association probability) and the baseline BBox method (for depth ordering). The number of objects is known a priori in our model (from object tracking <ref type="bibr" target="#b1">[2]</ref>) and is also provided to other methods such as BBox and RAS.  Results <ref type="figure">Figure 5</ref> shows the association errors -the percentages of point tracks incorrectly assigned to objects -for all methods on the four sets of input point tracks, for each sequence. The mean results over all sequences are summarized in <ref type="table" target="#tab_2">Table 2</ref>. From <ref type="figure">Figure 5</ref>, our method is usually the most accurate among all methods, leading to the best mean error on all sets of input point tracks in <ref type="table" target="#tab_2">Table 2</ref>, which is followed by the bounding box baseline method. This clearly shows the advantage of our continuous occlusion model over the simple baseline method for resolving occlusions. RAS and BM often have the highest errors in <ref type="figure">Figure 5</ref>, thus, the highest mean errors on all sets of input point tracks in <ref type="table" target="#tab_2">Table 2</ref>. More importantly, both RAS and BM rely on motions of objects for clustering point tracks, therefore they cannot work well with static point tracks (for example, point tracks that belong to parked cars). This fact can be observed in <ref type="table" target="#tab_2">Table 2</ref>, where there are large differences in the mean errors of both methods on data containing static point tracks (rows 2 and 4) and data consisting of dynamic point tracks only (rows 1 and 3). In contrast, our method and the baseline method are relatively independent of object motions, resulting in smaller performance gaps. Further, our method also outperforms motion segmentation on dynamic objects (row 3), which shows the effect of detection bounding boxes and by a more significant margin when occlusions are present (row 1), which shows the effect of our occlusion modeling.</p><p>Qualitative comparisons of point track associations from various methods are shown in <ref type="figure">Figure 6</ref>. We note the low errors using our occlusion model and the smooth transition of assignment across object boundaries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Localization Experiments</head><p>We report errors in translation and dimension estimates, measured in meters per car, in <ref type="table" target="#tab_4">Table 3</ref>. The average depth of cars in the dataset is approximately 20 meters. We compare four combinations of energies against the initialization using <ref type="bibr" target="#b22">[23]</ref> and a simple baseline which fits a 3D cuboid on the 3D point cloud reconstructed using SFM within detection bounding boxes in consecutive frames (for unobservable dimensions, such as when only the back of a car is visible, we rely on 3D size priors). The energy E ijt trackNoOcc represents the point track energy without accounting for occlusions, that is, we model E ijt track in the absence of a ij (λ). Similarly, E it detectNoOcc is the bounding box energy without the modification of Λ k that accounts for occlusion. We use λ track = 1, λ detect = 1, λ dyn = 10, λ size = 7. Please refer to the supplementary material for a detailed list of parameter settings.</p><p>From <ref type="table" target="#tab_4">Table 3</ref>, the baseline method has the highest errors, which is likely due to lack of point tracks and incorrect point-to-object associations (using detection bounding boxes). Moreover, minimizing different combinations of energies yields lower errors than the initialization with <ref type="bibr" target="#b22">[23]</ref>, which shows the advantage of our energy minimization. Finally, we observe that the use of the continuous occlusion model improves the localization accuracy in terms of the translation error, which is the most significant metric affected by all cues. Occlusion modeling for detection increases dimension error since we explicitly allow greater uncertainty in occluded edges of the bounding box. Note that none of our energies optimize yaw angles for static objects, which can be handled in practice through either the detector orientation or external information such as lane geometry.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions and Future Work</head><p>We have presented a theoretically novel continuous model for occlusion reasoning in 3D. A key advantage is its physical inspiration that lends flexibility towards occlusion reasoning for varied elements of scene understanding, such as point tracks, object detection bounding boxes and detection scores. We demonstrate unified modeling for different applications such as object-point track associations and 3D localization. Our occlusion model can uniformly handle static and dynamic objects, which is an advantage over motion segmentation methods for object-point association. A challenge is that inference for 3D localization is currently slow, requiring a few minutes per window of frames, which prevents exhaustive cross-validation for tuning of weights. Our future work will explore speeding up the inference, for example, by approximating the graph with a tree using the Chow-Liu method <ref type="bibr" target="#b2">[3]</ref>, which will allow belief propagation for fast inference. Another direction for future work is to replace a single ellipsoid by a set of spheres for modelling a translucent object <ref type="bibr" target="#b21">[22]</ref>, which will better capture object boundary and appearance while remaining a continuous model.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>2 A product integral is a simple integral in the log domain λ c (1 − focc(λr j )) dλ = e λ c ln (1−focc(λr j ))dλ . Comparisons between the approximate and exact for-mulations of P j transmission (λ).The drop in the approximate version is delayed because we assume drop at the object center rather than the camera-facing face of the object.Symbol DescriptionInput u j (t) 2D feature track j at time t d i (t)2D detection bounding box of object O i at time t Initialization with<ref type="bibr" target="#b22">[23]</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>(Top) A sample road scene with occlusions, where the unknowns of each object are modeled as random variables. (Bottom) The graphical model corresponding to the above frame. In particular, the numbered nodes denote the unknown state variables of each object (position, orientation, and dimensions), the shaded nodes are observed variables (detection bounding boxes and point tracks), and the colored squares represent various energies that capture object-object interactions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :Figure 6 :</head><label>56</label><figDesc>Association errors on different sets of input point tracks. Numbers on the x-axis represent sequence numbers in the KITTI raw dataset. Errors are in terms of average fractions of foreground points incorrectly associated to objects per sequence. Qualitative results of the association experiment. The "Associations" columns show the point track assignments to appropriate objects. Each color represents a different object to which point tracks can be associated to. The "Errors" columns show the probabilistic errors in association: low error points are in blue while high error points are in red. Note that our method changes smoothly at the object boundaries with intermediate probabilities, while the baseline method has merely 0 and 1 errors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc>Notation of inputs and outputs for object-point association and 3D object localization. Note that object dimensions are independent of time.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>The new covariance matrix for the detection</figDesc><table>Point tracks 
Ours BBox BM RAS 

Dynamic &amp; occluded 13.2 
21.3 30.9 30.1 
Occluded 
15.7 
19.8 39.5 37.8 
Dynamic 
6.6 
11.4 15.3 17.7 
All 
8.6 
12.6 21.9 21.5 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc>Mean association errors on different sets of input point</figDesc><table>tracks over all sequences. Errors are in terms of average fractions 
of foreground points incorrectly associated to objects per sequence. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc>Localization experiment results with different combinations of energies. We report translation error (t) and dimension error (dim) in meters per car. Yaw angles for static objects are not optimized by our model. These experiments use the set of occluded tracks to demonstrate the effect of our modeling.</figDesc><table></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Notable exceptions exist, such as<ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b34">35]</ref>, but we note that such models are expensive, application-specific and still discontinuous.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">4 1 1 {u j (t)} 1 1 1 1 1 d 6 (t) 1 d 2 (t) 1 1 1 d 5 (t)</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments This work was part of V. Dhiman's internship at NEC Labs America, in Cupertino. V. Dhiman and J. J. Corso were also supported by NSF NRI IIS 1522904.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Object segmentation by long term analysis of point trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multi-target tracking in world coordinate with single, minimally calibrated camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="553" to="567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Approximating discrete probability distributions with dependence trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="462" to="467" />
			<date type="published" when="1968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A multibody factorization method for independently moving objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Costeira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="159" to="179" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained partbased models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1627" to="1645" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A segmentation-aware object detection model with occlusion handling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Packer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1361" to="1368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? The KITTI vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Segmenting motions of different types by unsupervised manifold clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multibody factorization with uncertainty and missing data using the em algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gruber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="707" to="714" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Occlusion reasoning for object detection under arbitrary viewpoint</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hsiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3146" to="3153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Motion segmentation by subspace separation and model selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kanatani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="586" to="591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Realtime multibody visual SLAM with a smoothly moving monocular camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">V</forename><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2080" to="2087" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning occlusion with likelihoods for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1551" to="1558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Introduction to monte carlo methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Mackay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Learning in graphical models</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="175" to="204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Continuous energy minimization for multitarget tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="58" to="72" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Motion segmentation of multiple objects from a freely moving monocular camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Namdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="4092" to="4099" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multibody structure-from-motion in practice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">E</forename><surname>Ozden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1134" to="1141" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">3D2PM -3D deformable part models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Pepik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="356" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Occlusion patterns for object class detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Pepik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3286" to="3293" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Motion segmentation via robust subspace separation in the presence of outlying, incomplete, or corrupted trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Robust algebraic segmentation of mixed rigid-body and planar motions from two views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A versatile scene model with differentiable visibility applied to generative pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Robertini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Richardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="765" to="773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Robust scale correction in monocular SFM for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="1566" to="1573" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Joint SFM and detection cues for monocular 3D localization in road scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3734" to="3742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A benchmark for the comparison of 3D motion segmentation algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Motion segmentation with missing data using powerfactorization and gpca</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hartley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="310" to="316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Generalized principal component analysis (gpca)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sastry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="621" to="628" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Monocular visual scene understanding: Understanding multiobject traffic scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wojek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Walk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="882" to="897" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Detection and tracking of multiple, partially occluded humans by bayesian combination of edgelet based part detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="247" to="266" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Object detection by 3D aspectlets and occlusion reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshops</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="530" to="537" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A general framework for motion segmentation: Independent, articulated, rigid, non-rigid, degenerate and non-degenerate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="94" to="106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A duality based approach for realtime TV-L1 optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">DAGM on Pattern Recognition</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="214" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Explicit occlusion modeling for 3D object class representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3326" to="3333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Are cars just 3D boxes? -Jointly estimating the 3D shape of multiple objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Towards scene understanding with detailed 3d object representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Z</forename><surname>Zia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
