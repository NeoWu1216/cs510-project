<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Efficient Indexing of Billion-Scale datasets of deep descriptors</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Artem</forename><surname>Babenko</surname></persName>
							<email>artem.babenko@phystech.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Skolkovo Institute of Science and Technology (Skoltech)</orgName>
								<orgName type="institution">Yandex Moscow Institute of Physics and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
							<email>lempitsky@skoltech.ru</email>
							<affiliation key="aff0">
								<orgName type="department">Skolkovo Institute of Science and Technology (Skoltech)</orgName>
								<orgName type="institution">Yandex Moscow Institute of Physics and Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Efficient Indexing of Billion-Scale datasets of deep descriptors</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:40+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Existing billion-scale nearest neighbor search systems have mostly been compared on a single dataset of a billion of SIFT vectors, where systems based on the Inverted Multi-Index (IMI) have been performing very well, achieving state-of-the-art recall in several milliseconds. SIFT-like descriptors, however, are quickly being replaced with descriptors based on deep neural networks (DNN) that provide better performance for many computer vision tasks.</p><p>In this paper, we introduce a new dataset of one billion descriptors based on DNNs and reveal the relative inefficiency of IMI-based indexing for such descriptors compared to SIFT data. We then introduce two new indexing structures, the Non-Orthogonal Inverted Multi-Index (NO-IMI) and the Generalized Non-Orthogonal Inverted Multi-Index (GNO-IMI). We show that due to additional flexibility, the new structures are able to adapt to DNN descriptor distribution in a better way. In particular, extensive experiments on the new dataset demonstrate that these data structures provide considerably better trade-off between the speed of retrieval and recall, given similar amount of memory, as compared to the standard Inverted Multi-Index.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Modern image retrieval systems <ref type="bibr" target="#b18">[19]</ref> have to search through billion-scale databases in several milliseconds to respond to user queries. This imposes strict demands on scalability and efficiency of the underlying nearest-neighbor search algorithms. As exhaustive search is mostly infeasible at this scale, approximate nearest neighbor (ANN) methods that restrict the part of a dataset that is being considered in response to a query have to be used.</p><p>State-of-the-art ANN algorithms of this kind <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b4">5]</ref> avoid exhaustive search by organizing dataset points using an indexing structure. For a given query the indexing structure allows to retrieve a short-list of candidate points that are likely to be close to this query. The candidate points are then reranked according to their distances using such methods as Asymmetric Distance Computation (ADC) <ref type="bibr" target="#b11">[12]</ref>. In general, the reranking runtime is approximately linear in the size of the short-list. Consequently, the ability to obtain compact short-lists with high enough recall leads to fast reranking and overall time efficiency.</p><p>One of the first billion-scale retrieval systems for billionscale datasets have been presented by Jegou et al. <ref type="bibr" target="#b12">[13]</ref>. Their system called IVFADC divides data space into separate cells, corresponding to Voronoi regions obtained via k-means clustering. Given a query, IVFADC returns the short list with the points from the cells that are the closest to it. The Inverted Multi-Index (IMI) <ref type="bibr" target="#b1">[2]</ref> generalizes IVFADC by decomposing the dataspace into several subspaces and splitting each subspace into cells independently. The system <ref type="bibr" target="#b13">[14]</ref>, based on the IMI indexing structure and the Product Quantization (PQ) compression provides stateof-the-art performance on the SIFT1B dataset of one billion SIFT descriptors <ref type="bibr" target="#b12">[13]</ref>, which invariably serves as the testbed for such systems.</p><p>Recently, the focus in computer vision has shifted from SIFT descriptors to descriptors produced by deep neural networks (DNN). Several works <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b3">4]</ref> have shown that DNN-based descriptors significantly improve the state-of-the-art on common retrieval benchmarks. Furthermore, <ref type="bibr" target="#b6">[7]</ref> have shown that deep descriptors can be compressed by PCA with almost no loss in performance.</p><p>Despite such shift to deep descriptors, to the best of our knowledge, billion-scale ANN with deep descriptors has not been investigated. One potential problem with relying on the SIFT1B dataset is its particular suitability for IMIbased systems. This is because, the SIFT construction procedure uses different halves of a SIFT vector to describe disjoint regions of an image patch. Thus, the correlations between the two halves are likely to be small and the decomposition in the IMI is justified. Deep descriptors do not possess such structure and therefore the good performance of IMI might not be achieved.</p><p>This paper seeks to develop a good indexing structure for deep descriptors. We begin with experiments illustrating that for this type of data the Inverted Multi-Index indeed . For all structures parameter K was set to four, hence 16 centroids were produced. The left plot corresponds to the IMI structure and large blue points on the axes denote the codewords of the underlying PQ decomposition. The middle and right plots correspond to the NO-IMI and the GNO-IMI structures respectively. On the both plots green points correspond to the "first-order centroids" S1, . . . , S4. The GNO-IMI centroids represent the actual data distribution more accurately than the other structures.</p><p>produces a particularly large number of redundant cells that do not contain any points. This deficiency arises from the space decomposition, implicitly performed in the IMI. We conclude that for deep data the correlations between subspaces are significant and independent codebooks for each subspace cannot represent the global data distribution adequately. We then introduce two new indexing structures, the Non-Orthogonal Inverted Multi-Index (NO-IMI) and the Generalized Non-Orthogonal Inverted Multi-Index (GNO-IMI). The NO-IMI forms index cells centroids as sums of two vectors from two non-orthogonal codebooks, following the well-known residual vector quantization (RVQ) scheme <ref type="bibr" target="#b7">[8]</ref>, thus avoiding any decomposition into orthogonal subspaces. As a result, the NO-IMI provides more reasonable index cells with the centroids representing actual data distribution more accurately. The GNO-IMI pushes the representation accuracy even further by forming cell centroids as linear combinations of codewords. As the extra linear coefficients are learned from the data, the GNO-IMI exhibits higher efficiency while indexing complex data as compared to the NO-IMI.</p><p>We demonstrate that better indexing by the (G)NO-IMI results in more compact and precise short-lists and, therefore, more efficient retrieval. Overall, we show that for the same time budget the new structures provide higher recall compared to the existing top-performing schemes. Our evaluation is based on a new dataset of one billion deep descriptors with hold-out query and learning sets. The dataset is the first publicly available billion-scale dataset of deep descriptors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>We briefly describe several ideas from the previous work that are essential to the description of the our structures. Along the way, we introduce notation for this description.</p><p>Product quantization (PQ) is a lossy compression scheme for high-dimensional vectors <ref type="bibr" target="#b11">[12]</ref>. PQ encodes each vector x ∈ R D as a concatenation of M codewords from M D M -dimensional codebooks C 1 , . . . , C M , each containing K codewords. In other words, PQ decomposes a vector into M separate subvectors and applies vector quantization (VQ) to each subvector, while using a separate codebook. As a result each vector x is encoded by a tuple of codewords indices [i 1 , . . . , i M ] and approximated by</p><formula xml:id="formula_0">x ≈ [C 1 (i 1 ), . . . , C M (i M )]</formula><p>. Fast Euclidean distance computation becomes possible via efficient ADC procedure <ref type="bibr" target="#b11">[12]</ref> using lookup tables:</p><formula xml:id="formula_1">q − x 2 ≈ q − [C 1 (i 1 ), . . . , C M (i M )] 2 = (1) M m=1 q m − C m (i m ) 2</formula><p>where q m -mth subvector of a query q. This sum can be calculated in M additions and lookups given that distances from query subvectors to codewords are precomputed.</p><p>From the geometry viewpoint, PQ effectively partitions the original vector space into K M cells, each being a Cartesian product of M lower-dimensional cells. Such productbased approximation works better if the D M -dimensional components of vectors have independent distributions. The degree of dependence is affected by the choice of the splitting, and can be further improved by orthogonal transformation applied to vectors as preprocessing. Two recent works have therefore looked into finding an optimal transformation <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b15">16]</ref>. Following one of them, the modification of PQ corresponding to such pre-processing transformation is referred below as Optimized Product Quantization (OPQ).</p><p>Non-orthogonal quantizations. Several works <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b5">6]</ref> have proposed modifications of PQ that do not decompose the dataspace into orthogonal subspaces. In fact, they generalize the idea of Product Quantization by approximating each vector by a sum of M codewords instead of concatenation. In this case the ADC procedure is still efficient while the approximation accuracy is increased.</p><p>The first approach, Residual Vector Quantization <ref type="bibr" target="#b7">[8]</ref>, quantizes original vectors, and then iteratively quantizes the approximation residuals from the previous iteration. <ref type="bibr" target="#b7">[8]</ref> also introduces the IVFRVQ system that allows to perform non-exhaustive search. IVFRVQ prunes large portions of database points based on the distances from a query to the "coarse approximations", produced by the first several quantizers. Another approach, Additive Quantization (AQ) <ref type="bibr" target="#b2">[3]</ref>, is the most general as it does not impose any constraints on the codewords from the different codebooks. Usually, AQ provides the smallest compression errors, however, it is much slower than other methods, especially for large M . Composite Quantization (CQ) <ref type="bibr" target="#b20">[21]</ref> learns codebooks with a fixed value of scalar product between the codewords from different codebooks. Finally, Tree Quantization (TQ) <ref type="bibr" target="#b5">[6]</ref> performs decomposition into subspaces while constraining the non-orthogonality relations between different codebooks to form a tree graph. Under this constraint, efficient encoding is possible.</p><p>Overall, non-orthogonal quantizations achieve higher approximation accuracy than (O)PQ, especially for nonhistogram data. At the same time, they are slower and require more complex learning procedures.</p><p>IVFADC. One of the first systems capable of dealing with billion-scale datasets efficiently was IVFADC introduced in <ref type="bibr" target="#b12">[13]</ref>. This system combines the inverted index <ref type="bibr" target="#b18">[19]</ref> as indexing structure and Product Quantization for database compression. IVFADC first splits the space into K cells via standard K-means and then encodes displacements of each point from the centroid of a cell it belongs to. The encoding is performed via Product Quantization that uses global codebooks shared by all cells.</p><p>The Inverted Multi-Index and Multi-D-ADC. The Inverted Multi-Index (IMI) <ref type="bibr" target="#b1">[2]</ref> is an indexing algorithm for high-dimensional spaces and very large datasets. The IMI generalizes the inverted index by using product codebooks for cell centroids construction (typically, as few as two components in the product are considered). Thus the Inverted Multi-Index has two D 2 -dimensional product codebooks for different halves of the vector, each with K sub-codewords, thus effectively producing K 2 cells, which would typically be orders of magnitude bigger than the number of cells within the IVFADC system or other systems using inverted indices. Large number of cells provides very dense partitioning of the space, which means that a small fraction of dataset has to be traversed to achieve high recall (w.r.t. the true nearest neighbor). For dataset compression, <ref type="bibr" target="#b1">[2]</ref> followed the IVFADC system and used Product Quantization with global codebooks shared across all cells in order to encode the displacements of the vectors from centroids (this system is referred to as Multi-D-ADC).</p><p>Further improvements. <ref type="bibr" target="#b8">[9]</ref> improved the performance of Multi-D-ADC by replacing Product Quantization with Optimized Product Quantization for both indexing and compression (hence the name of their system OMulti-D-OADC). OMulti-D-OADC gives the state-of-the-art performance in terms of the search accuracy on the SIFT1B dataset with global codebooks for database compression.</p><p>Another system, called LOPQ <ref type="bibr" target="#b13">[14]</ref>, is a modification of IVFADC which uses separate local OPQ codebooks for database points compression in each cell. As local codebooks represent points distribution in the particular cell more precisely, the accuracy of reranking in LOPQ is much higher than in IVFADC. Also, <ref type="bibr" target="#b13">[14]</ref> introduces the Multi-LOPQ system, which uses local codebooks with the Inverted Multi-Index structure. Currently, Multi-LOPQ system provides the highest recall on the SIFT1B dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The Non-Orthogonal Inverted Multi-Index</head><p>In this section we introduce a new indexing structure, the Non-Orthogonal Inverted Multi-Index (NO-IMI). We first describe the design of the NO-IMI and the intuition behind it. We then provide a procedure of the NO-IMI codebooks learning. Finally, we explain how candidate lists are extracted and are reranked using the NO-IMI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">The NO-IMI structure</head><p>Let us assume that a database P = {p 1 , . . . , p N } of D-dimensional points is given. As well as the existing indexing structures the NO-IMI partitions the dataspace into a large number of cells and distributes database points over these cells. The cells in the NO-IMI are constructed based on two codebooks S = {S 1 , . . . , S K } and T = {T 1 , . . . , T K }, each containing K codewords. The codewords in the codebooks are D-dimensional, S, T ⊂ R D .</p><p>The NO-IMI splits the data space into K 2 cells with centroids c j i defined by the formula:</p><formula xml:id="formula_2">c j i = S i + T j , i, j = 1, . . . , K<label>(2)</label></formula><p>Thus, the NO-IMI cells are the regions of the data space defined by the following:</p><formula xml:id="formula_3">C j i = {x ∈ R D |i, j = arg min k,l x − (S k + T l ) 2 } (3)</formula><p>This construction is similar to how cells within the Inverted Multi-Index are constructed. We however aim to avoid the space decomposition, underlying the Inverted Multi-Index, which is based on product quantization and which ignores correlations between subspaces of the deep descriptors data. Based on this consideration, it is reasonable to replace PQ by some non-orthogonal quantization. Among non-orthogonal approaches, we chose Residual Vector Quantization (RVQ) as it allows to perform efficient short-lists extraction, as will be discussed below. With the RVQ method in mind, the NO-IMI codewords S 1 , . . . , S K can be interpreted as cluster centroids in the original dataspace and we refer to them as first-order centroids. Similarly, the codewords T 1 , . . . , T K can be interpreted as centroids in the space of displacements of data points from the first-order centroids. We refer to them as second-order centroids. As a result, the indexing structure has K 2 centroids in the form (S i + T j ), i, j = 1, . . . , K and the same number of cells, each assigned with the first-order index i and the second-order index j.</p><p>The construction <ref type="formula" target="#formula_2">(2)</ref> is also directly related to the wellknown Hierarchical K-Means (HKM) method <ref type="bibr" target="#b14">[15]</ref>. This method also clusters the original data space, producing K first-order centroids. It then clusters the displacements in each cluster separately, producing K second-order centroids in each cluster. For large K, the usage of HKM is infeasible as it would require to keep K 2 D-dimensional vectors in the main memory. Such enormous memory consumption can be avoided by using K second-order centroids that are shared by all clusters. Sharing the secondorder centroids makes Hierarchical K-Means equivalent to the RVQ scheme, described in the previous paragraph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">The generalized NO-IMI</head><p>The NO-IMI assumes that the displacements distributions within first-order clusters are similar, as it uses shared second-order centroids. However, this assumption may be too strong, as e.g. the spatial extent and the shape of the corresponding Voronoi cells can vary, especially when the data distribution has dense and sparse regions. To alleviate this assumption we augment the NO-IMI with an additional α-matrix, which incorporates cluster-wise information. In particular, an element α[i, j] is a factor for the j-th secondorder centroid T j in the i-th first-order cluster. After the incorporation of these factors, the expression for the corresponding centroid becomes:</p><formula xml:id="formula_4">c j i = S i + α[i, j]T j , i, j = 1, . . . , K<label>(4)</label></formula><p>The addition of α-matrix allows to represent data distribution adequately even with shared second-order centroids. Below we refer to the version of the NO-IMI with the α-matrix as Generalized Non-Orthogonal Inverted Multi-Index (GNO-IMI). We demonstrate an example of centroids produced by the original IMI, the NO-IMI, and the GNO-IMI on a toy 2D dataset in <ref type="figure" target="#fig_0">Figure 1.</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Indexing</head><p>The parameters of (G)NO-IMI, namely S, T, α, can all be learned from data as will be discussed below. Let us now assume that S, T, α are given and describe the process that distributes the dataset points p 1 , . . . , p N assigning them to different (G)NO-IMI cells. More formally, for each point p we have to find the closest centroid indices:</p><p>i, j = arg min</p><formula xml:id="formula_5">k,l p − (S k + α[k, l]T l ) 2<label>(5)</label></formula><p>Exhaustive minimization of this function would require the evaluation of K 2 possible solutions, which is impractical. Instead, we use a simple heuristic to cut out most of the suboptimal solutions. Let us rewrite the expression above:</p><formula xml:id="formula_6">p − (S k + α[k, l]T l ) 2 = p − S k 2 + α[k, l] 2 T l 2 −2α[k, l] p, T l + 2α[k, l] S k , T l<label>(6)</label></formula><p>Note, that the first term p − S k 2 is a distance from p to the first-order centroid S k . The RVQ heuristic is to inspect only r values k 1 , . . . , k r corresponding to the indices of centroids S k1 , . . . , S kr , which are the closest to p. With this pruning, only rK solutions have to be evaluated. Another speed optimization is to precompute the terms p − S k 2 and p, T l , which can be done in O(KD) operations. With these terms precomputed, each solution can be evaluated in O(1) operations. After all rK possibilities are evaluated, p is assigned to the cell, corresponding to the optimal index pair.</p><p>The complexity of point assignment is therefore a sum of complexities of precomputation and pairs evaluation steps, which are O(DK) and O(rK) respectively. Hence, the overall complexity is O(DK + rK), while the complexity in the Inverted Multi-Index equals O(DK). In our experiments r is several times smaller than D and the actual timings of indexing for both the IMI and the (G)NO-IMI are rather similar.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Codebooks learning</head><p>We now focus on the task of obtaining codebooks S, T and the α-matrix that fit a given data distribution well. We suppose that a training dataset P = {p 1 , . . . , p N } is provided. The learning is performed by the minimization of the reconstruction error for all training points:</p><formula xml:id="formula_7">N i=1 p i − (S k i + α[k i , l i ]T l i ) 2 → min S k ∈R D ,|S|=K, T k ∈R D ,|T |=K, α∈R D×D k i ,l i ∈{1,...,K}<label>(7)</label></formula><p>Overall, the optimization is performed over four groups of variables, where S, T and α variables are continuous, while the assignment variables {k i , l i } are discrete. We minimize the function using block-coordinate descent by optimizing over one variable group at a time with the other three groups fixed. Below we describe the optimization over each variable group separately.</p><p>Optimization over assignments. At this step the optimization is performed over {k i , l i } variables given codebooks S, T and α matrix. Thus, this problem is equivalent to the indexing task and we discuss an efficient method for that above in Section 3.3.</p><p>Optimization over α. We now fix the assignments {k i , l i } and the codebooks S, T and minimize over the elements of the α matrix. Let us show that it is possible to minimize over each matrix element α[k, l] independently. For that we decompose the target function as follows:</p><formula xml:id="formula_8">N i=1 p i − (S k i + α[k i , l i ]T l i ) 2 = = K k=1 K l=1 i:ki=k,li=l p i − S k − α[k, l]T l 2<label>(8)</label></formula><p>Note, that each term i:ki=k,li=l</p><formula xml:id="formula_9">p i − S k − α[k, l]T l 2</formula><p>depends on the only element α[k, l]. Hence, the minimization of the whole function is equivalent to the independent minimizations of each term:</p><p>i:ki=k,li=l</p><formula xml:id="formula_10">p i − S k − α[k, l]T l 2 → min α[k,l]<label>(9)</label></formula><p>This subproblem can be solved analytically and its closed-form solution is:</p><formula xml:id="formula_11">α[k, l] = i:ki=k,li=l p i − S k , T l i:ki=k,li=l T l 2<label>(10)</label></formula><p>Optimization over second-order codebook T . Similarly, we optimize over the second-order codewords T l by decomposing the target function again:</p><formula xml:id="formula_12">N i=1 p i − (S k i + α[k i , l i ]T l i ) 2 = K l=1 K k=1 i:ki=k,li=l p i − S k − α[k, l]T l 2<label>(11)</label></formula><p>Then each term K k=1 i:ki=k,li=l</p><formula xml:id="formula_13">p i − S k − α[k, l]T l 2 is</formula><p>minimized over T l independently. This minimization also can be solved analytically and the solution has the form:</p><formula xml:id="formula_14">T l = K k=1 α[k, l] i:ki=k,li=l (p i − S k ) K k=1 i:ki=k,li=l α[k, l] 2<label>(12)</label></formula><p>Optimization over first-order codebook S. This problem is also decomposable and the minimization is performed completely analogously to the previous step. The solution to this step has the form</p><formula xml:id="formula_15">S k = K l=1 i:ki=k,li=l (p i − α[k, l]T l ) K l=1 i:ki=k,li=l 1<label>(13)</label></formula><p>Initialization. We initialize the iterations of the GNO-IMI learning in the following way. The α-matrix is initialized by all ones and S is initialized by centroids obtained via K-means clustering of the dataset P . Second-order codebook T is initialized by centroids obtained via K-means clustering of the displacements from the database points to the closest first-order codewords.</p><p>Comments. In practice we observed that the method requires quite a few iterations to converge and we used ten iterations in our experiments. Parameter r was set to eight. Also note, that for some index pairs k, l there could be no points and then both the numerator and the denominator in the expression (10) vanish. In this case we set α[k, l] = 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Short-list extraction</head><p>Here we describe the procedure that gets a query q and produces an ordered sequence of cells that correspond to the centroids that are the closest to q. The sequence can be of any desired length L and the points assigned to these cells form the short-list that is returned by the GNO-IMI in response to the query. The procedure consists of three steps:</p><p>1. First, we compute the distances q − S k 2 between the query and the first-order centroids. Let us denote by k 1 , . . . , k r indices of r first-order centroids that appeared to be the closest to q.</p><formula xml:id="formula_16">distances[k] = q − S k 2 , k = 1, . . . , K k 1 , . . . , k r = distances.argsort()[1 : r]</formula><p>We then discard all the cells with first-order indices not belonging to the set {k 1 , . . . , k r }. The subsequent search is performed among the remaining rK cells only.</p><formula xml:id="formula_17">cells = {(k i ; l)} k i ∈ {k 1 , . . . , k r }, l = 1, . . . , K</formula><p>The complexity of this step is O(KD + K log K) with the two terms corresponding to distances calculation and sorting. 2. Secondly, the procedure creates an array of the length rK and fills it with the distances to the centroids of the promising cells selected in step one:</p><p>for each cell = (k i ; l) in cells :</p><formula xml:id="formula_18">cellDistances[(k i ; l)] = q − S ki 2 + α[k i , l] 2 T l 2 −2α[k i , l] q, T l + 2α[k i , l] S ki ; T l</formula><p>Note, that the terms q, T l can be precomputed once and reused for all k i and l. The terms S ki , T l and the norms T l 2 are precomputed before querying and are kept in lookup-tables. Thus, the complexity of this step is O(rK). 3. Finally, we apply partial sorting of distances array with linear average complexity 1 . For any input array this partial sorting guarantees that the L smallest elements will be placed in the first L position of the output array (possibly unordered), where L is the desired length of cells sequence. Then only L cells corresponding to the smallest distances are sorted and the sorted sequence is yielded as a procedure result. P artialSort(cellDistances[], L)</p><formula xml:id="formula_19">Output = Sort(cellDistances[1 : L])</formula><p>The complexity of this step is O(rK + L log L).</p><p>The total complexity of the procedure is a sum of complexities for the three steps and equals O(KD + K log K + rK + L log L). In this sum the terms O(KD) and O(K log K) are common for existing indexing structures as they also perform query quantization and sort distances to centroids. The term O(rK + L log L) is a computational overhead in the (G)NO-IMI but we show in the next section that it is insignificant and has no influence on the scheme applicability.</p><p>After forming the sorted sequence of cells as described above, the method starts to traverse the cells and collects points from these cells into a candidate list.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Reranking</head><p>After short-list extraction the (G)NO-IMI performs reranking of candidate points in the same manner as it was proposed in <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b13">14]</ref>. During the indexing stage we calculate the displacement of each point from its cell centroid and compress these displacements via Optimized Product Quantization (OPQ). In the experiments, we used K sets of local PQ codebooks, and each set was shared by the cells with the same first-order index. We used one global rotation matrix for all database points.</p><p>When querying, each candidate point is reconstructed from its OPQ code and the distance between this reconstruction and the given query is evaluated. Finally, candidates are sorted based on the calculated distances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7.">Connection to IVFRVQ</head><p>The proposed NO-IMI system is similar to the IVFRVQ scheme proposed in <ref type="bibr" target="#b7">[8]</ref>. Here we highlight the differences between the NO-IMI and IVFRVQ:</p><p>1. IVFRVQ is based on RVQ only, while NO-IMI uses OPQ to compress the data at the fine level. Not relying on RVQ at all levels is important for high perfromance, since RVQ degrades when it is applied with large number of codebooks <ref type="bibr" target="#b7">[8]</ref>.</p><p>2. The NO-IMI does not need to keep the norms of dataset points. IVFRVQ has to keep them, which needs extra memory and also extra operations during search.</p><p>3. IVFRVQ evaluates distances to all cell centroids and then explores only several inverted lists corresponding to the closest cells. The NO-IMI does not evaluate distances to all centroids and performs more efficient nonexhaustive evaluation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section we provide results of experiments that compare the Non-Orthogonal Inverted Multi-Indices (NO-IMI and GNO-IMI) with the standard Inverted Multi-Index. The experiments were performed on two datasets:</p><p>1. SIFT1B dataset <ref type="bibr" target="#b12">[13]</ref> that contains one billion of 128dimensional SIFT descriptors along with precomputed groundtruth for 10, 000 queries. A hold-out learning set of 100 million descriptors is also provided. 2. DEEP1B dataset. This is a new dataset that we introduce to the community 2 . Descriptors for DEEP1B were produced in a way similar to <ref type="bibr" target="#b6">[7]</ref>. Specifically, we took the outputs of the last fully-connected layer of a DNN for a billion images on the Web. Our DNN had the GoogLeNet <ref type="bibr" target="#b19">[20]</ref> architecture and was trained on the Ima-geNet dataset <ref type="bibr" target="#b0">[1]</ref>. The outputs were then compressed by PCA to 96 dimensions and l 2 -normalized. We also prepared hold-out sets containing 350 millions of descriptors for learning and 10, 000 for querying (with known ground truth for nearest neighbors in the main set).</p><p>For both datasets we compare three indexing structures:</p><p>1. the inverted multi-index (IMI) with global rotation before dataspace decomposition <ref type="bibr" target="#b8">[9]</ref>. The top-performing systems <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b4">5]</ref> use this indexing structure, so we use it as a baseline. We used the authors' implementation <ref type="bibr" target="#b4">[5]</ref>; 2. the Non-Orthogonal Inverted Multi-Index (NO-IMI); 3. the Generalized Non-Orthogonal Inverted Multi-Index (GNO-IMI).  For all the structures we used the same value of K = 2 14 , hence all the indices have the same number of cells. All the structures and the codebooks for database compression were optimized on the hold-out learning sets. Representation accuracy. We first check our intuition that the subspaces corresponding to different halves of deep data are more correlated for deep data compared to SIFT descriptors. As described in section 2, the IMI centroids are constructed using codewords from the two codebooks C 1 and C 2 , corresponding to orthogonal data subspaces. Let us introduce two discrete random variables I 1 , I 2 corresponding to the indices of the closest codewords from C 1 and C 2 respectively for a certain point. We can measure the normalized mutual information between I 1 and I 2 using all the points from the particular dataset. High values of mutual information would then reveal the correlation between the halves. <ref type="figure" target="#fig_1">Figure 3</ref> (left) shows values of the normalized mutual information (NMI) for both datasets. For DEEP1B dataset N M I(I 1 , I 2 ) is two times higher than for SIFT1B. This verifies the intuition that the decomposition for deep data can lead to low performance, even when decorrelating global rotation (as in OPQ) is fitted during preprocessing. <ref type="figure" target="#fig_1">Figure 3</ref> also shows NMI values for the NO-IMI scheme. In this case we measure the mutual information between the indices of the closest codewords in codebooks S and T . For both SIFT1B and DEEP1B the NMI is markedly smaller in the case of the NO-IMI index, suggesting better adaptivity of the RVQ model over PQ. We additionally calculate the percent of empty cells for both indexing structures, with the results shown in <ref type="figure" target="#fig_1">Figure 3</ref>(right). This further reveals that the NO-IMI significantly reduces a number of empty index cells compared to IMI, especially for DEEP1B data.</p><p>We further demonstrate the advantage of the (G)NO-IMI for deep data by measuring the average distance from the data points to the centroids of cells they belong to. This is a reliable indicator of the indexing quality as smaller distances mean that cells represent the actual data distribution better. Furthermore, the small distances mean that the displacements from the points to the closest centroids have small norms, hence they can be encoded more accurately, which results in better reranking. The average distances for two datasets and three systems are shown in <ref type="table">Table 1</ref>.</p><p>For the deep data the NO-IMI reduces average distance by 15% compared to the IMI scheme with the same number of cells. This means that the NO-IMI cell centroids represent data distribution better and we show below that it results in much more precise short-lists. The more powerful Generalized NO-IMI scheme provides even greater improvement by 21%. For SIFT1B, the usage of the NO-IMI allows to reduce average distance by just three percent, which means that in this metric our approach gives very small improvement for this dataset.</p><p>Short-list quality. We now compare the retrieval performance of different schemes. In most of experiments we use the commonly used measure Recall@R, which is calculated as a rate of queries for which true nearest neighbor is present in a short-list of a length R.</p><p>We plot the values of Recall@R for different values of R in <ref type="figure" target="#fig_2">Figure 2</ref>. For DEEP1B dataset (left), the NO-IMI and the GNO-IMI provide a great improvement in short-list quality over the original IMI. For instance, for the recall level 0.5 the GNO-IMI provides short-lists that are eight time more compact. For SIFT1B dataset (left), the benefit of the NO-IMI is negligible as short-lists are improved only by a very small margin. As the (G)NO-IMI has additional computational overhead comparing to the IMI, such a small improvement does not justify the usage of non-orthogonal  For all systems we used OPQ with local codebooks to compress database points. With few exceptions, for any given time budget above 11 ms the (G)NO-IMI provides considerably higher recall compared to the IMI-based scheme. multi-indices with SIFT data.</p><p>In this experiment we also compare with IVFADC. This scheme was shown to be less accurate than the IMI on SIFT1B, but it might potentially offer advantage on other data distributions, as it does not perform any space decomposition. For IVFADC we use a considerably larger codebook of size K = 2 17 , but it still produces noticeably less accurate short-lists compared to other schemes.</p><p>Reranking and runtime. Finally, we evaluate the (G)NO-IMI in combination with subsequent reranking of candidate lists. Our baseline here is the state-of-the-art Multi-LOPQ system (IMI+reranking), which provides the state-of-the-art on the SIFT1B dataset.</p><p>We perform experiments with M = 8 and 16 reranking codebooks, which corresponds to 8 and 16 bytes per point respectively. The parameter r in the (G)NO-IMI was set to 32 in all experiments. For both values of M we plot Recall@1, Recall@5 and Recall@10 for different lengths of candidate lists as functions of the corresponding search runtime. The results are summarized in <ref type="figure" target="#fig_4">Figure 4</ref>. We highlight several observations based on it:</p><p>• The timings for the (G)NO-IMI start from 11 milliseconds as this is the cost of the computational overhead. This time Multi-LOPQ allows to extract and to rerank a short-list of approximately 25 thousand points. <ref type="figure" target="#fig_2">Figure 2</ref> demonstrates that such a small list for the IMI would contain a true nearest neighbor only for 70% queries, hence such small lists are quite unreliable.</p><p>• For any given time more than eleven milliseconds the (G)NO-IMI produces higher recall values comparing to Multi-OPQ. The margin is up to six absolute percent that corresponds to 13% of relative improvement.</p><p>Memory consumption. We also provide the amount of total memory consumption for all systems. The NO-IMI requires one additional gigabyte to keep the terms S i , T j , i, j = 1, . . . , K. The GNO-IMI also requires one gigabyte to keep K 2 elements of α-matrix. Overall, the amount of additional memory is relatively small. For example, in the setting with 16 bytes, memory consumption equals 23 gigabytes for Multi-LOPQ and 25 gigabytes for the GNO-IMI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We investigated indexing structures for deep descriptors. We have shown that the original Inverted Multi-Index is suboptimal for deep data and have introduced two systems for billion-scale indexing, the Non-Orthogonal Inverted Multi-Index and the Generalized Non-Orthogonal Inverted Multi-Index, which provide more accurate indexing and more precise candidate lists. The advantages of the (G)NO-IMI come at a price of computational overhead, which is small for typical settings.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>The cell centroids (red points) produced by different indexing structures for the same sample of two-dimensional data (blue points)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Normalized mutual information between the indices of the closest codewords in two codebooks (left) and the percent of empty index cells (right) for the IMI and NO-IMI systems. The high value of NMI in the case of DEEP1B and the IMI means that the implicit assumption of independent subspaces in the IMI does not hold for DEEP1B. The large percent of empty cells also indicates that cell centroids in the IMI represent DEEP1B data distribution poorly.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Recall as a function of the candidate list length. On DEEP1B (left plot) we compare four systems: IVFADC with 2 17 codewords, the IMI with K = 2 14 and preliminary orthogonal transformation, the NO-IMI and the GNO-IMI with K = 2<ref type="bibr" target="#b13">14</ref> . For all recall levels the (G)NO-IMI provides much shorter candidate lists. For SIFT1B dataset (right plot) the advantage of the NO-IMI is negligible.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Comparison of the original IMI, NO-IMI and GNO-IMI in terms of recall after reranking, and runtime on the DEEP1B dataset.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">std::nth element from the standard STL library</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">The dataset and the project code are available on http://sites.skoltech.ru/compvision/noimi/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank Konstantin Lakhman for his help with data collection. Victor Lempitsky is supported by the Russian Ministry of Science and Education grant RFMEFI57914X0071.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Large scale visual recognition challenge (ILSVRC)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fei-Fei</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The inverted multi-index</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Babenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Additive quantization for extreme vector compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Babenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">S</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Aggregating deep convolutional features for image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Babenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">S</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision -ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The inverted multi-index</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Babenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">S</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Tree quantization for largescale similarity search and classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Babenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">S</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Neural codes for image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Babenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Slesarev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chigorin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">S</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision -ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Approximate nearest neighbor search by residual vector quantization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sensors</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Optimized product quantization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Optimized product quantization for approximate nearest neighbor search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multi-scale orderless pooling of deep convolutional activation features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">13th European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="392" to="407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Product quantization for nearest neighbor search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Searching in one billion vectors: Re-rank with source coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jegou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tavenard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Amsaleg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Locally optimized product quantization for approximate nearest neighbor search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Avrithis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Computer Vision and Pattern Recognition</title>
		<meeting>International Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Scalable recognition with a vocabulary tree</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Nistér</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Stewénius</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Cartesian k-means</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">CNN features off-the-shelf: An astounding baseline for recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Razavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Azizpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Carlsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR Workshops</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="512" to="519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Visual instance retrieval with deep convolutional networks. CoRR, abs/1412</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Razavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Maki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Carlsson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">6574</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Video google: A text retrieval approach to object matching in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Composite quantization for approximate nearest neighbor search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
