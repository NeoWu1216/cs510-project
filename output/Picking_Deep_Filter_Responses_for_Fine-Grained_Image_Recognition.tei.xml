<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Picking Deep Filter Responses for Fine-grained Image Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaopeng</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongkai</forename><surname>Xiong</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wengang</forename><surname>Zhou</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Science</orgName>
								<address>
									<country>Technology of China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyao</forename><surname>Lin</surname></persName>
							<email>wylin@sjtu.edu.cnzhwg@ustc.edu.cnqitian@cs.utsa.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Texas at San Antonio</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Picking Deep Filter Responses for Fine-grained Image Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:44+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recognizing fine-grained sub-categories such as birds and dogs is extremely challenging due to the highly localized and subtle differences in some specific parts. Most previous works rely on object / part level annotations to build part-based representation, which is demanding in practical applications. This paper proposes an automatic finegrained recognition approach which is free of any object / part annotation at both training and testing stages. Our method explores a unified framework based on two steps of deep filter response picking. The first picking step is to find distinctive filters which respond to specific patterns significantly and consistently, and learn a set of part detectors via iteratively alternating between new positive sample mining and part model retraining. The second picking step is to pool deep filter responses via spatially weighted combination of Fisher Vectors. We conditionally pick deep filter responses to encode them into the final representation, which considers the importance of filter responses themselves. Integrating all these techniques produces a much more powerful framework, and experiments conducted on CUB-200-2011  and Stanford Dogs demonstrate the superiority of our proposed algorithm over the existing methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>As an emerging research topic, fine-grained recognition aims at discriminating usually hundreds of sub-categories belonging to the same basic-level category. It lies between the basic-level category classification (e.g. categorizing bikes, boats, cars, and so on in Pascal VOC <ref type="bibr" target="#b7">[8]</ref>) and the identification of individual instances (e.g. face recognition). An inexperienced person can easily recognize basic-level categories like bikes or horses immediately since they are visually very dissimilar, while it is difficult for him / her to tell a black bird from a crow without specific expert guidance. As a matter of fact, fine-grained sub-categories often share the same parts (e.g., all birds should have wings, legs, etc.), and are often discriminated by the subtle differ-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Distinctive Filters</head><p>Nondistinctive Filters</p><p>Ch. 4</p><p>Ch. 92</p><p>Ch. 185</p><p>Ch. 99 We generate candidate patches with selective search <ref type="bibr" target="#b24">[25]</ref> and compute response of each patch at conv4 layer. We show several top responding patches of some channels and observe that there exist some filters which respond to specific patterns (e.g., the head or leg of bird), while most of them respond chaotically. This paper proposes to pick deep filters with significant and consistent responses, and learn a set of discriminative detectors for recognition. ences in texture and color properties of these parts (e.g. only the breast color counts when discriminating similar birds). Hence localizing and describing object and the corresponding parts become crucial for fine-grained recognition. In order to achieve accurate object and part locations, most existing works explicitly require object level or even part level annotations at both training and testing stages <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b32">[33]</ref>. However, such a requirement is demanding in practical applications. Some works consider a more reasonable setting, i.e. object / part level annotations at only training stage but not at testing time <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b31">[32]</ref>. However, even with such a setup, it still requires expensive annotations at training stage, and is especially hard for large scale recognition problems. Hence, one promising research direction is to free us from the tedious and subjective manual annotations for fine-grained recognition, which we refer to automatic part discovery. However, discovering parts automatically is a classical chicken-and-egg problem, i.e. without an accurate appearance model, examples of a part cannot be discovered, and an accurate appearance model cannot be learned without having part examples. Some pioneering works begin to consider this issue <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b26">[27]</ref>. How- . An overview of our proposed framework. Our approach consists of two picking steps. The first step aims at picking deep filters which respond to specific patterns significantly and consistently. Based on these picked filters, we choose positive samples and train a set of discriminative detectors iteratively. The second step is to pick filter responses via Spatially Weighted Fisher Vector (SWFV) encoding. We assign each Fisher Vector a weight and pool it into final image representation, which considers the importance of Fisher Vector itself. ever, these methods either needs a network trained from scratch <ref type="bibr" target="#b26">[27]</ref>, or suffers complex optimization <ref type="bibr" target="#b20">[21]</ref>, and the performance is limited.</p><p>As our first contribution, we propose an automatic part detection strategy for fine-grained recognition (Sec. 3), which is free of any object / part level annotation at both training and testing stages. Our detection method consists of two main contributions. First, we propose a novel initialization method for detector learning, which is based on the selectivity of deep filters. As illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>, which shows some top responding patches of some filters on CUB-200-2011. It can be found that some filters work as part detectors and respond to specific parts (i.e., the head of bird). However, these detectors are weak and most of them are not relevant to our task. The key insight of our initialization approach is to elaborately pick deep filters with significant and consistent responses. Second, we propose to learn a set of detectors via iteratively per-category positive sample mining and regularized part model retraining. We mine new positive samples by category and introduce a regularized term for each positive sample, which considers both the diversity and reliability of positive samples. The learned detectors tend to discover discriminative and consistent patches which are helpful for part-based recognition.</p><p>Feature representation is another key issue for finegrained recognition. Recently, Convolutional Neural Network (CNN) has been widely used for feature extraction. However, there exist two challenges for fine-grained representation. The first is that traditional CNN representation requires fixed size rectangle as input, which inevitably includes background information. However, background is unlikely to play any major role for fine-grained recognition since all sub-categories share similar background (e.g. all birds usually inhabit on the tree or fly in the sky). The sec-ond is the gap between detection and classification. Due to large pose variation and partial occlusion, detection may be unreliable and lose crucial details for recognition.</p><p>To address the above challenges, as our second contribution, we propose a new kind of feature which is suitable for fine-grained representation (Sec. 4). We regard deep filter responses of a CNN as localized descriptors, and encode them via Spatially Weighted Fisher Vector (SWFV-CNN).</p><p>The key insight is that not all filter responses are equally important for recognition. Our goal is to highlight the responses which are crucial for recognition and discount those which are less helpful. To this end, we propose a picking strategy which conditionally selects descriptors based on part saliency map, which indicates how likely a pixel belongs to a foreground part. Experimental results demonstrate that SWFV-CNN performs consistently better than traditional CNN, and is complementary with traditional CNN to further boost the performance.</p><p>• Framework overview. An overview of our proposed framework is shown in <ref type="figure" target="#fig_1">Fig. 2</ref>. Our approach consists of two picking steps. The first step aims at picking deep filters which respond to specific patterns significantly and consistently. Based on these filters, we elaborately select positive samples which are semantically similar and train a set of discriminative detectors. We use an iterative procedure which alternates between selecting positive samples and training classifier, while applying cross-validation at each step to prevent classifier from overfitting the initial positive samples. The trained detectors are used to discover parts for recognition. The second step is to pick CNN filters via Spatially Weighted combination of Fisher Vector, which we refer to SWFV-CNN. We compute spatial weights with part saliency map, which indicates how likely a pixel belongs to a foreground part. The part saliency map is used to weight each Fisher Vector and pool it into final image representation, which considers the importance of Fisher Vector itself.</p><p>The rest of this paper is organized as follows. Sec. 2 describes related work on fine-grained categorization. The details of our proposed part discovery strategy is elaborated in Sec. 3. In Sec. 4, we describe our proposed Spatially Weighted FV-CNN. Experimental results and discussions are given in Sec. 5. Sec. 6 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>Fine-grained recognition is a challenging problem and has recently emerged as a hot topic. In the following, we organize our discussion related to fine-grained recognition with two tasks: part localization and feature representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Part Localization</head><p>As fine-grained datasets are often provided with extra annotations of bounding box and part landmarks <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b25">[26]</ref>, most works rely on these annotations more or less.</p><p>Early works assume that annotations are available at both training and testing time. Among them the strongest supervised setting is to use both object and part level annotations <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b27">[28]</ref>. Obviously, this kind of setting is demanding and a more reasonable setting only assumes the availability of object bounding box. Chai et al. <ref type="bibr" target="#b2">[3]</ref> introduce techniques that improve both segmentation and part localization accuracy by simultaneous segmentation and detection. Gavves et al. <ref type="bibr" target="#b9">[10]</ref> propose a supervised alignment method which retrieves nearest neighbor training images for a test image, and regresses part locations from these neighboring training images to the test image.</p><p>Later works require annotations only during training, and no knowledge of annotations at testing time. These methods are supervised at the level of object and parts during training. Zhang et al. <ref type="bibr" target="#b31">[32]</ref> generalize the R-CNN <ref type="bibr" target="#b10">[11]</ref> framework to detect parts as well as the whole object. Branson et al. <ref type="bibr" target="#b1">[2]</ref> train a strongly supervised model in a pose normalized space. Further on, Krause et al. <ref type="bibr" target="#b14">[15]</ref> propose a method which only need object level annotations at training time, and is completely unsupervised at the level of parts.</p><p>Recently, there have been some emerging works which aim at a more general condition, e.g. without expecting any information about the location of fine-grained objects, neither during training nor testing time. This level of unsupervision is a big step towards making fine-grained recognition suitable for wide deployment. Xiao et al. <ref type="bibr" target="#b26">[27]</ref> propose to use two attention models with deep convolutional networks, one to select relevant patches to a certain object, and the other to localize discriminative parts. Simon et.al. <ref type="bibr" target="#b20">[21]</ref> propose to localize parts with constellation model, which incorporates CNN into deformable part model <ref type="bibr" target="#b8">[9]</ref>.</p><p>Our approach belongs to the last setting, which is free of any object / part level annotation at both training and testing stages. Different from previous works <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b26">[27]</ref>, we learn a set of discriminative detectors via elaborately selecting positive samples and iteratively updating part models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Feature Representation</head><p>For the description of image, CNN features have achieved breakthrough on a large number of benchmarks <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b30">[31]</ref>, etc. Different from traditional descriptors which explicitly encode local information and aggregate them for global representation, CNN features represent global information directly, and can alleviate the requirement of manually designing a feature extractor. Though not specifically designed to model sub-category level differences, CNN features capture such information well <ref type="bibr" target="#b6">[7]</ref>.</p><p>Most works choose the output of a CNN as feature representation directly <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b31">[32]</ref>. However, CNN features still preserve a great deal of global spatial information. As demonstrated in <ref type="bibr" target="#b30">[31]</ref>, the activations from the fifth maxpooling layer can be reconstructed to form an image which looks very similar to the original one. The requirements of invariance to translation and rotation are weakly ensured by max-pooling. Though max-pooling helps improve invariance to small-scale deformations, invariance to larger-scale deformations might be undermined by the preserved global spatial information. To solve this issue, Gong et al. <ref type="bibr" target="#b11">[12]</ref> propose to aggregate features of the fully connected layers via orderless VLAD pooling. Considering deeper layers are more domain specific and potentially less transferable than shallower layers, Cimpoi et al. <ref type="bibr" target="#b5">[6]</ref> pool features from the convolutional layers, and achieve considerable improvements for texture recognition.</p><p>Our approach regards responses from deep CNN filters as localized descriptors (similar with SIFT), and encodes these responses via Fisher Vector. Different from previous works which encode CNN descriptors globally <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b11">[12]</ref>, we project each response back to the original image and encode each part separately. Most importantly, we propose a picking strategy which conditionally selects responses based on their importance for recognition, and encodes them via spatially weighted combination of Fisher Vectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Learning Part Detectors</head><p>In this section, we target at learning a collection of discriminative detectors that automatically discover discriminative object / parts. Our strategy consists of three modules: positive sample initialization, regularized detector training, and detector selection. The first module generates initial parts, each of which is defined by a set of potentially positive samples of image patches. In the second module, we train detectors for each set of positive samples with a regularized iterative strategy. To remove those noisy detectors, the third module select good detectors by measuring their predictive power in terms of recognition accuracy. Note that  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Picking Filters: Positive Sample Initialization</head><p>Learning a part detector requires a set of part examples, which should be identified in the training data. Most previous works employ some form of unsupervised clustering, such as k-means <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>, or template matching <ref type="bibr" target="#b29">[30]</ref>, to initialize a part model. However, running k-means or template matching on mid-level patches does not return very good clusters, and often produces clustered instances which are in no way visually similar.</p><p>Different from previous works, we propose a picking strategy which elaborately selects distinctive and consistent patches based on the responses of CNN filter banks. The key insight is that different layers of a CNN are sensitive to specific patterns. e.g., the lower layers often respond to corners and other edge conjunctions, while the higher layers often correspond to semantically meaningful regions. In a sense, these convolutional filters work as part detectors. However, these detectors are usually weak, and most of them are not relevant to our fine-grained tasks.</p><p>In order to find which filters are distinctive for part discovery, we first generate a large pool of region proposals with selective search <ref type="bibr" target="#b24">[25]</ref>, and randomly sample a subset of one million patches. Each proposal is resized to a target size of 107 × 107, which makes the activation output of the 4th convolutional layer a single value (similar with detection score). Then, we sort responses over all channels and pick the top scored 10K responses. These responses are binned into corresponding channels according to which channel they respond most to. Finally, we get a response distribution of the top scored 10K regions. As shown in <ref type="figure" target="#fig_3">Fig.  3</ref>, the response distributions are sparse, with most responses focusing on only a few channels (e.g., for CUB-200-2011, over 90% responses focus on the top 5% channels). We refer to these channels as distinctive filters, which respond to specific patterns significantly. In our experiment, we select channels which include the top 90% responses as distinctive  <ref type="figure" target="#fig_0">Fig. 1</ref> visualizes some top responding regions for distinctive and non-distinctive channels. The responses of distinctive filters always focus on consistent parts, such as the head of birds. While non-distinctive filters pick up some cluttered samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Regularized Detector Training</head><p>With the initialization of positive samples, we learn the corresponding detector by optimizing a linear SVM classifier. We define the negatives based on Intersection over Union (IoU) overlap with the positives, and the regions with IoU overlap below 0.3 are treated as negative samples. Since negative samples are much larger than the positives, we adopt the standard hard negative mining method <ref type="bibr" target="#b8">[9]</ref>, which converges quickly after only a single pass over all images. Iterative update. Since the initial positives are not very good to begin with (as shown in the first row of <ref type="figure" target="#fig_6">Fig. 4</ref>, some samples are biased), we train SVM detector iteratively. During each iteration, the top 10% firings of previous round detector are used as new positive samples. However, doing this directly does not produce much improvement since the detector tends to overfit to the initial positives, and would prefer these positives during the next round of validation. To solve this issue, we divide the training samples into two equal, non-overlapping subsets, which enables us to achieve better generalization by training on one subset while validating on another. We then exchange the role of training and validation and repeat this whole process until convergence (the learned detector does not change). Regularized Loss Term. Another issue of training object / part detectors for all the fine-grained sub-categories is that the top detections always latch on a few easy detectable subcategories, and cannot discover positive samples from the majority of other sub-categories. Due to the large interclass variations among sub-categories, if a detector does not see any positive sample of one sub-category, it would localize badly on that one. However, including patches that do not correspond to the same part as the exemplars will decrease the localization and discrimination power of part  </p><formula xml:id="formula_0">min 1 2 ||ω t || 2 + C n i=1 β i t−1 ξ i s.t. y i (ω T t x i + b t ) ≥ 1 − ξ i , i = 1, ..., n ξ i ≥ 0, i = 1, ..., n,<label>(1)</label></formula><p>where</p><formula xml:id="formula_1">β i t−1 =        Ψ(ω T t−1 x i + b t−1 ), y i = 1 1, y i = −1,<label>(2)</label></formula><p>where Ψ[·] is a sigmoid function which maps the detection scores within range (0, 1), and C controls relative weights of the loss terms. Note that we introduce an extra regularized term β i t−1 for each positive sample x i , which measures the reliability of x i with detection score of previous round detector. The regularized term highlights the higher scored patches and downweights the lower scored patches.</p><p>Note that there are two benefits for our regularized detector learning. First, with per-category positive sample mining, the detector can see more diverse positives, which is beneficial for its generalization. Second, with the introduced regularized term β, the detector is able to avoid overfitting the less reliable positives, while focusing on the more reliable positives. <ref type="figure" target="#fig_6">Fig. 4</ref> shows some detector learning process in different iteration steps. Our algorithm is able to mine positive samples which are visually consistent, even though the initial positives are not well localized. As the iteration goes, the positives become more and more consistent, which in turn boosts the discriminative power of part model. The full approach for detector learning is summarized in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Detector Selection</head><p>Our algorithm produces tens of detectors, and there is no guarantee that the part mining procedure will not return bad detectors. In order to discard those detectors which are poorly localized, we measure the discriminative power of detectors in terms of recognition accuracy. We equally divide the labeled training samples into training and validation subsets. For each detector, classification is performed based on the top scored region. Finally, we discard detectors with recognition rate below 40%, which reduces the detectors to only a few (less than ten in our experiments).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Bag of Parts Image Representation</head><p>With the above trained detectors, we can identify patches corresponding to the parts from each image. One direct method for part representation is to extract CNN features directly from the detected parts, and concatenate them for final representation. This kind of features are usually obtained from the penultimate Fully-Connected (FC) layer of a CNN, and are widely used in previous works. However, there are two limitations of FC-CNN for fine-grained recognition. The first is the background disturbance, as CNN requires a fixed rectangle as input, which includes cluttered background inevitably. The second comes from the inaccuracy of detections, which may lose crucial details for partbased representation. To deal with these issues, instead of extracting FC-CNN within a tight rectangle, we propose to compute part saliency map and pool CNN features with Spatially Weighted Fisher Vector (SWFV-CNN). Part saliency map. The part saliency map is used to indicate how likely a pixel belongs to a foreground part. Our part saliency map consists of two sources, part map and saliency map. The part map indicates the spatial prior of a part, and is obtained simply from the top detection. The saliency map <ref type="bibr" target="#b12">[13]</ref> is a topographically arranged map that represents visual saliency of a corresponding scene. Since fine-grained images are not cluttered with many objects, and <ref type="figure">Figure 5</ref>. Sample detection results of our automatically discovered detectors. We select detections with top three recognition accuracies (shown in red, green, and blue in order), and overlay them to original image for better visualization <ref type="figure" target="#fig_0">(Row 1 and 3)</ref>. We also show the detections directly returned by the picked filters <ref type="figure" target="#fig_1">(Row 2 and 4)</ref>, which is similar with the method <ref type="bibr" target="#b26">[27]</ref>. Our detectors improve localization power via iterative training, while detectors directly from the filters are weak, and in most situations localize inaccurately. The top two rows for CUB-200-2011, and the bottom two rows for Stanford Dogs. The last three columns show some failure cases. the object of interest is always the most salient region, we choose saliency map S to measure the existence probability of foreground object. The final part saliency map M is obtained as follows: , v 1 , ..., u k , v k ], which is the stacking of mean derivation vectors u k and covariance deviation vectors v k for each of the K modes. Each entry of u k and v k can be rewritten as follows:</p><formula xml:id="formula_2">M(p) = S(p) k i=1 D i (p) Z ,<label>(3)</label></formula><formula xml:id="formula_3">u jk = N i=1 u i jk = N i=1 q ik N √ π k z ji − µ jk σ jk v jk = N i=1 v i jk = N i=1 q ik N √ 2π k        z ji − µ jk σ jk 2 − 1        ,<label>(4)</label></formula><p>where j = 1, ..., D spans the vector dimension. We formulate u jk and v jk as accumulated sum of the first and second order statistics of z i j , respectively. However, this kind of representation considers each z i equally important, which is often not the case. The vector z i may lie in non-salient regions, or less reliable detected regions. Considering this issue, we introduce a spatially weighted term M(p i ) for each vector z i , which indicates the importance of z i itself. The weighted results of u jk and v jk can be expressed as:</p><formula xml:id="formula_4">u w jk = N i=1 M(p i ) · u i jk , v w jk = N i=1 M(p i ) · v i jk ,<label>(5)</label></formula><p>with the introduced spatial weights, we are able to catch the important features for recognition. We would see its effectiveness in the following section. An illustration of how to compute SWFV-CNN of an image is shown in <ref type="figure" target="#fig_7">Fig. 6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Datasets</head><p>The empirical evaluation is performed on two benchmarks: Caltech-UCSD Birds-200-2011 (Birds) <ref type="bibr" target="#b25">[26]</ref> and S-tanford Dogs (Dogs) <ref type="bibr" target="#b13">[14]</ref>, which are the most extensive and competitive datasets in fine-grained literature. Birds dataset contains 11, 788 images spanning 200 sub-species, while Dogs dataset consists of 20, 580 images with 120 dog species. We use the default training / test split, which gives us around 30 training examples per class for Birds and around 100 training examples per class for Dogs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Network</head><p>• Supervised pre-training. For Birds, two different models are used in our experiments: VGG-M [4] and a more accurate but deeper one VGG-VD <ref type="bibr" target="#b21">[22]</ref>. Since Dogs dataset is a training subset of ILSVRC 2012, simply choosing the pre-trained network brings about cross-dataset redundancy. Considering this issue, we check ILSVRC 2012 training data and remove samples that are used as test in Dogs, then we train a network (AlexNet) from scratch to obtain the model specific to Dogs.</p><p>• Fine-tuning with saliency-based sampling. Finetuning is beneficial to adapt the network pretrained on Im-ageNet to our fine-grained tasks. Since most existing finegrained datasets only contain a few thousand training samples, which is far from enough for fine-tuning. A common strategy is to introduce many "jittered" samples around the ground truth bounding box <ref type="bibr" target="#b10">[11]</ref>. Instead, we propose a saliency-based sampling strategy without such annotation information. To this end, we compute a saliency map S <ref type="bibr" target="#b12">[13]</ref> of an image. For each region proposal x generated with selective search <ref type="bibr" target="#b24">[25]</ref>, we compute the saliency score with s(x | S ) = i∈x S i / S . The regions with saliency score above a threshold (set as 0.7 in our experiments, which expands the samples by approximately 20×) are chosen as augmented samples. This enables them to have high quality in containing the object of interest.</p><p>There are two benefits for network fine-tuning. First, the fine-tuned network is a better feature extractor for classification, e.g., when fine-tuning on VGG-M <ref type="bibr" target="#b3">[4]</ref>, our proposed saliency-based sampling strategy achieves an accuracy of 66.97% on Birds, which is even better than the bounding box based sampling method in <ref type="bibr" target="#b10">[11]</ref> (66.08%). This indicates that for fine-grained datasets, bounding box information is unnecessary for network fine-tuning. Second, the internal responses of convolutional filters are more domain specific, which helps for part selection in Sec. 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Implementation Details</head><p>• Detector learning. In Sec. 3, we choose pool5 features for detector training. In practice, the iteration process converges within several times, and we set the iteration times as 7. It only remains several detectors after selection (Sec. <ref type="bibr">3.3)</ref>, and the number is 6 for Birds and 5 for Dogs.</p><p>• FC-CNN. FC-CNN is extracted from the penultimate Fully-Connected (FC) layer of a CNN. The input image is  <ref type="bibr" target="#b15">[16]</ref> on Stanford Dogs. "BL" refers to baseline method which extracts features directly from the whole image, without any knowledge of object or parts. "PD" refers to our proposed part detection method in Sec. 3, and "SWFV-CNN" refers to our spatially weighted FV-CNN method proposed in Sec. <ref type="bibr">4.</ref> resized to fixed size and mean subtracted before propagating through the CNN. FC-CNN is widely used in previous works <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b31">[32]</ref>, etc., so we include it for fair comparison.</p><p>• FV-CNN. FV-CNN pools CNN features with Fisher Vector. We extract conv5 descriptors (512-d for VGG-M, VGG-VD, and 256-d for AlexNet) at 3 scales (s = {256, 384, 512}), with each image rescaled to the target size so that min (w, h) = s. We reduce the dimension to 128-d by PCA transformation and pool them into a FV representation with 256 Gaussian components, resulting in 65K-d features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Results and Comparisons</head><p>We first conduct a detailed analysis of our method with regard to part detection and recognition performance, and move on to compare with prior works.</p><p>• Part detection. <ref type="figure">Fig. 5</ref> shows some detection results (Row 1 and 3) of our learned detectors. We select detections with top three recognition accuracies (shown in red, green, and blue in order), and overlay them to the original image for better visualization. These detections exhibit surprisingly good visual consistency even without annotated training samples. For Birds, they fire consistently and represent a diverse set of parts (e.g., object, head, and leg). While for Dogs, they usually focus around head, mainly due to the fact that other parts are either highly deformable or partial occluded. We also show detections (Row 2 and 4) directly returned by the picked filters, which is similar with the method <ref type="bibr" target="#b26">[27]</ref>. These filters are not task relevant and usually return inferior localization results to ours, which demonstrates the effectiveness of our part detectors. Note that these detectors are redundant (e.g., both detectors respond to dog's head) to some extent, however, their features have different representation and can enrich each other.</p><p>• Recognition results. The performance of part detection can be further demonstrated in terms of recognition accuracy. As shown in <ref type="table">Table 1</ref>, we perform detailed analysis Method Train anno.</p><p>Test anno. Accuracy Ours PDFS n/a n/a 84.54% GPP <ref type="bibr" target="#b27">[28]</ref> bbox+parts bbox+parts 66.35% Symbolic <ref type="bibr" target="#b2">[3]</ref> bbox bbox 59.4% POOF <ref type="bibr" target="#b0">[1]</ref> bbox bbox 56.78%</p><p>Alignment <ref type="bibr" target="#b9">[10]</ref> bbox bbox 67% n/a n/a 53.6% PN-CNN <ref type="bibr" target="#b1">[2]</ref> bbox+parts bbox+parts 85.4% bbox+parts n/a 75.7%</p><p>Part R-CNN <ref type="bibr" target="#b31">[32]</ref> bbox+parts bbox+parts 76.37% bbox+parts n/a 73.89% FOAF <ref type="bibr" target="#b33">[34]</ref> bbox+parts bbox+parts 81.2% PG Alignment <ref type="bibr" target="#b14">[15]</ref> bbox bbox 82.8% NAC <ref type="bibr" target="#b20">[21]</ref> n/a n/a 81.01% TL Atten. <ref type="bibr" target="#b26">[27]</ref> n/a n/a 77.9% <ref type="table">Table 2</ref>. Recognition performance comparisons on Birds. "bbox" and "parts" refer to object bounding box and part annotations.</p><p>by comparing different variants of our method. "BL" refers to baseline method, which extracts features directly from the whole image, without any knowledge of object or parts. "PD" refers to our proposed part detection method (Sec. 3), and "SWFV-CNN" refers to our spatially weighted FV-CNN method (Sec. 4). From <ref type="table">Table 1</ref> we observe that: 1) Part detection boosts the performance significantly. Comparing with the baseline, PD brings about a nearly 10% (66.97% → 76.74%) improvement for Birds, and an 5.5% improvement for Dogs. Note that the performance improvement on Dogs is less than that on Birds, mainly due to the larger deformations and more frequent occlusions on Dogs.</p><p>2) FC-CNN is usually better than FV-CNN. FC-CNN usually outperforms FV-CNN by around 2% ∼ 3% (76.74% vs 73.83% for Birds, and 65.07% vs 63.11% for Dogs). This is because FV-CNN usually includes background information, which is confused for fine-grained recognition. While FC-CNN alleviates this influence by max-pooling.</p><p>3) SWFV-CNN performs consistently better than FV-CNN, and even better than FC-CNN. We find that SWFV-CNN brings about over 3% improvement comparing with FV-CNN, and is even better than FC-CNN. The reason is that SWFV-CNN focuses on features which are important for recognition, and deemphasizes those which are not helpful. The results demonstrate that SWFV-CNN is more suitable for fine-grained recognition. 4) SWFV-CNN complements with FC-CNN. When combining SWFV-CNN with FC-CNN, we obtain an accuracy of 80.26% for Birds, and 71.96% for Dogs, which demonstrates the complementation of these features. Replacing VGG-M with VGG-VD improves the performance in all the cases, with a final accuracy of 84.54% for Birds.</p><p>• Comparisons with prior works. <ref type="table">Table 2</ref> shows the comparison results of our method with prior works on Birds. We list the amount of annotations of each method for fair comparison. Early works <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b27">[28]</ref> choose SIFT</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Train anno. Test anno. Accuracy Ours PDFS n/a n/a 71.96% Temp. Match <ref type="bibr" target="#b28">[29]</ref> bbox bbox 38% Symbolic <ref type="bibr" target="#b2">[3]</ref> bbox bbox 45.6%</p><p>Alignment <ref type="bibr" target="#b9">[10]</ref> bbox bbox 57% n/a n/a 49% Selec. Pooling <ref type="bibr" target="#b4">[5]</ref> bbox bbox 52% FOAF <ref type="bibr" target="#b33">[34]</ref> bbox bbox 53.5% NAC <ref type="bibr" target="#b20">[21]</ref> n/a n/a 68.61% <ref type="table">Table 3</ref>. Recognition performance comparisons on Dogs.</p><p>as features, and the performance is limited. When switching to CNN features, our approach is best among methods under the same setting <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b26">[27]</ref>, and obtains a 18% error reduction comparing with the best performing result <ref type="bibr" target="#b20">[21]</ref> (81.01%). Moreover, our result even outperforms methods which use object <ref type="bibr" target="#b14">[15]</ref> (82.8%) or even part <ref type="bibr" target="#b31">[32]</ref> (76.37%), <ref type="bibr" target="#b33">[34]</ref> (81.2%) annotations, only beaten by <ref type="bibr" target="#b1">[2]</ref> (85.4%) which uses both object and part annotations at both training and testing time. Our method indicates that fully automatic finegrained recognition is within reach. <ref type="table">Table 3</ref> shows the comparison results on Dogs. Few works report results on this dataset, due to there are not off-the-shelf CNN models for feature extraction. The most comparable result with our method is <ref type="bibr" target="#b20">[21]</ref>, which also trains AlexNet model from scratch and obtain an accuracy of 68.61%. Our method improves it by over 3%, an error rate reduction of 10.7%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we propose a framework for fine-grained recognition which is free of any object / part annotation at both training and testing stages. Our method incorporates deep convolutional filters for both part detection and description. We claim two major contributions. Firstly, we propose to pick good filters which respond to specific parts significantly and consistently. Based on these picked filters, we elaborately choose positive samples and train a set of discriminative detectors iteratively. Secondly, we propose a simple but effective feature encoding method, which we call SWFV-CNN. SWFV-CNN packs local CNN descriptors via spatially weighted combination of Fisher Vectors. Integrating the above approaches produces a powerful framework, and shows notable performance improvements on CUB-200-2011 and Stanford Dogs.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Illustration of filter selectivity for a typical network VGG-M [4] on CUB-200-2011.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2</head><label>2</label><figDesc>Figure 2. An overview of our proposed framework. Our approach consists of two picking steps. The first step aims at picking deep filters which respond to specific patterns significantly and consistently. Based on these picked filters, we choose positive samples and train a set of discriminative detectors iteratively. The second step is to pick filter responses via Spatially Weighted Fisher Vector (SWFV) encoding. We assign each Fisher Vector a weight and pool it into final image representation, which considers the importance of Fisher Vector itself.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Response distributions of the top scored 10K patches on VGG-M (512 channels). The top scored responses only focus on a few channels. We remove the channels with lower response frequency for better visualization. the full procedure is weakly supervised, which only needs the labels of training examples, while does not need any object / part level annotation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Algorithm 1</head><label>1</label><figDesc>Learning Discriminative Part Detector Require: Disjoint training subsets {D 1 , D 2 }; 1: initialization N = {(x i , y i )} m i=1 ∈ D 1 , β = [1, ...1] m 2: while not converged do 3: Detector w ⇐ svm train (N, β) 4: [N new , β new ] ⇐ top (w, D 2 , m) per top (w, D 2 , k) 5: N ⇐ N new , β ⇐ β new 6: swap (D 1 , D 2 ) 7: end while 8: Return Detector w filters. For each distinctive filter, we select patches with the top m (m = 100) responses as initial positives for the corresponding part model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 .</head><label>4</label><figDesc>Positive samples during the detectors learning process in different iteration steps. The first row is the initial positive samples and rows 2-3 show new positive samples returned by the top detections of previous round detectors. Even though the initial positive samples are not well localized, our algorithm is able to mine new samples which exhibit visual consistency, and learn a set of discriminative detectors. model. To solve this issue, we mine per-category positive samples with regularized loss during each round of training. Specifically, the top 10% detections per-category are used as positives as well as the top 10% detections among all subcategories. Since these potential positives are not equally reliable, we assign a weight term β to each positive sample, which measure the reliability of each positive.DenoteD = {(x i , y i )} n i=1be the set of positive and negative training patches, and x i its corresponding feature vector of x i , where y i ∈ {−1, 1}. The part detector ω t during round t can be learned by minimizing the following function:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 .</head><label>6</label><figDesc>Illustration of how to compute SWFV-CNN. We first compute part saliency map with the top detections and saliency map. The part saliency map assign weight to each descriptor, and SWFV-CNN is the weighted combination of each Fisher Vector.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>where D i (p) = 1 when the ith detection contains the pixel p, otherwise D i (p) = 0. Z is a normalization constant which makes max M(p) = 1.Spatially weighted FV-CNN. The Fisher Vector models the distribution of a set of vectors with gaussian mixture models and represents an image by considering the gradient with respect to the model parameters. Let I = (z 1 , ..., z N ) be a set of D dimensional feature vectors extracted from an image. Define Θ = (µ k , Σ k , π k : k = 1, ..., K) be the parameters of a gaussian mixture model fitting the distribution of descriptors, and q ik be the posterior probability of each vec-tor z i (i = 1, ..., N) to a mode k in the mixture model. For an image I, the Fisher Vector Φ(I) = [u 1</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Poof: Part-based one-vs-one features for fine-grained categorization, face verification, and attribute estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">N</forename><surname>Belhumeur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Bird species categorization using pose normalized deep convolutional nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Van Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Symbiotic segmentation and part localization for fine-grained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Return of the devil in the details: Delving deep into convolutional nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chatfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Selective pooling vector for fine-grained recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">X</forename><surname>Han</surname></persName>
		</author>
		<editor>AWACV. IEEE</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep filter banks for texture recognition and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cimpoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Decaf: A deep convolutional activation feature for generic visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">E</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1310.1531</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A discriminatively trained, multiscale, deformable part model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Fine-grained categorization by alignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Smeulders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multi-scale orderless pooling of deep convolutional activation features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Graph-based visual saliency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Harel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Novel dataset for fine-grained image categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jayadevaprakash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR FGVC workshop</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Fine-grained recognition without part annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Dog breed classification using part localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Belhumeur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The truth about cats and dogs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jawahar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1427" to="1434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Cats and dogs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Cnn features off-the-shelf: An astounding baseline for recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Razavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Azizpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Carlsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR workshop</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Neural activation constellations: Unsupervised part model discovery with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rodner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Unsupervised discovery of mid-level discriminative patches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning discriminative part detectors for image classification and cosegmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Selective search for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">E</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="154" to="171" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">The Caltech-UCSD Birds-200-2011 Dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<idno>CNS-TR-2011-001</idno>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The application of twolevel attention models in deep convolutional neural network for fine-grained image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">E</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Hierarchical part matching for fine-grained visual categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Unsupervised template learning for fine-grained object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">G</forename><surname>Shapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIP-S</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A codebook-free and annotation-free approach for fine-grained image categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bradski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3466" to="3473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Partbased r-cnns for fine-grained category detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deformable part descriptors for fine-grained recognition and attribute prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Farrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Fused one-vs-all mid-level features for fine-grained visual categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
