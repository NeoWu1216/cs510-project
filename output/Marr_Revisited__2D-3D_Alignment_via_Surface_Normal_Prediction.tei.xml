<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Marr Revisited: 2D-3D Alignment via Surface Normal Prediction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aayush</forename><surname>Bansal</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Russell</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Adobe Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Marr Revisited: 2D-3D Alignment via Surface Normal Prediction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:43+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input&amp;Image&amp;</head><p>Predicted&amp;Surface&amp;Normal&amp; CAD&amp;Model&amp;Library&amp; Aligned&amp;Models&amp; <ref type="figure">Figure 1</ref>. Given a single 2D image, we predict surface normals that capture detailed object surfaces. We use the image and predicted surface normals to retrieve a 3D model from a large library of object CAD models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>We introduce an approach that leverages surface normal predictions, along with appearance cues, to retrieve 3D models for objects depicted in 2D still images from a large CAD object library. Critical to the success of our approach is the ability to recover accurate surface normals for objects in the depicted scene. We introduce a skip-network model built on the pre-trained Oxford VGG convolutional neural network (CNN) for surface normal prediction. Our model achieves state-of-the-art accuracy on the NYUv2 RGB-D dataset for surface normal prediction, and recovers fine object detail compared to previous methods. Furthermore, we develop a two-stream network over the input image and predicted surface normals that jointly learns pose and style for CAD model retrieval. When using the predicted surface normals, our two-stream network matches prior work using surface normals computed from RGB-D images on the task of pose prediction, and achieves state of the art when using RGB-D input. Finally, our two-stream network allows us to retrieve CAD models that better match the style and pose of a depicted object compared with baseline approaches.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Consider the images depicting objects shown in <ref type="figure">Figure 1</ref>. When we humans see the objects, we can not only recognize the semantic category they belong to, e.g., "chair", we can also predict the underlying 3D structure, such as the occluded legs and surfaces of the chair. How do we predict the underlying geometry? How do we even reason about invisible surfaces? These questions have been the core area of research in computer vision community from the beginning of the field. One of the most promising theories in the 1970-80's was provided by David Marr at MIT <ref type="bibr" target="#b29">[30]</ref>. Marr believed in a feed-forward sequential pipeline for object recognition. Specifically, he proposed that recognition involved several intermediate representations and steps. His hypothesis was that from a 2D image, humans infer the surface layout of visible pixels, a 2.5D representation. This 2.5D representation is then processed to generate a 3D volumetric representation of the object and finally, this volumetric representation is used to categorize the object into the semantic category. While Marr's theory was very popular and gained a lot of attention, it never materialized computationally because of three reasons: (a) estimating the surface normals for vis-ible pixels is a hard problem; (b) approaches to take 2.5D representations and estimate 3D volumetric representations are not generally reliable due to lack of 3D training data which is much harder to get; (c) finally, the success of 2D feature-based object detection approaches without any intermediate 3D representation precluded the need of this sequential pipeline. However, in recent years, there has been a lot of success in estimating 2.5D representation from single image <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b45">46]</ref>. Furthermore, there are stores of 3D models available for use in CAD repositories such as Trimble3D Warehouse and via capture from 3D sensor devices. These recent advancements raise an interesting question: is it possible to develop a computational framework for Marr's theory? In this paper, we propose to bring back the ideas put forth by Marr and develop a computational framework for extracting 2.5D representation followed by 3D volumetric estimation. Why sequential? Of course, one could ask why worry about Marr's framework? Most of the available data for training 3D representations is the CAD data (c.f. ShapeNet or ModelNet <ref type="bibr" target="#b46">[47]</ref>). While one could render the 3D models, there still remains a big domain gap between the CAD model renders and real 2D images. We believe Marr's 2.5D representation helps to bridge this gap. Specifically, we can train a 2D → 2.5D model using RGB-D data, and whose output can be aligned to an extracted 2.5D representation of the CAD models.</p><p>Inspired by this reasoning, we used off-the-shelf 2D-to-2.5D models to build our computational framework <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b45">46]</ref>. However, these models are optimized for global scene layout and local fine details in objects are surprisingly missing. To overcome this problem, we propose a new skip-network architecture for predicting surface normals in an image. Our skip network architecture is able to retrieve the fine details, such as the legs of a table or chair, which are missing in current ConvNet architectures. In order to build the next stage in Marr's pipeline, we train another ConvNet that learns a similarity metric between rendered CAD models and 2D images using both appearances and surface normal layout. A variant of this architecture is also trained to predict the pose of the object and yields state-of-the-art performance. Our Contributions: Our contributions include: (a) A skipnetwork architecture that achieves state-of-the-art performance on surface normal estimation; (b) A CNN architecture for CAD retrieval combining image and predicted surface normals. We achieve state-of-the-art accuracy on pose prediction using RGB-D input, and in fact our RGBonly model achieves performance comparable to prior work which used RGB-D images as input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Related Work</head><p>The problem of 3D scene understanding has rich history starting from the early works on blocks world <ref type="bibr" target="#b35">[36]</ref>, to generalized cylinders <ref type="bibr" target="#b4">[5]</ref>, to the work of geons <ref type="bibr" target="#b3">[4]</ref>. In recent years, most of the work in 3D scene understanding can be divided in two categories: (a) Recovering the 2.5D; (b) Recovering the 3D volumetric objects. The first category of approaches focus on recovering the geometric layout of everyday indoor scenes, e.g., living room, kitchen, bedroom, etc. The goal is to extract a 2.5D representation and extract surface layout <ref type="bibr" target="#b17">[18]</ref> or depth of the pixels in the scene. Prior work has sought to recover the overall global shape of the room by fitting a global parametric 3D box <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b38">39]</ref> or recovering informative edge maps <ref type="bibr" target="#b28">[29]</ref> that align to the shape of the room, typically based on Manhattan world constraints <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b20">21]</ref>. However, such techniques do not recover fine details of object surfaces in the scene. To recover fine details techniques have sought to output a 2.5D representation (i.e. surface normal and depth map) by reasoning about mid-level scene properties, such as discriminative 3D primitives <ref type="bibr" target="#b9">[10]</ref>, convex and concave edges <ref type="bibr" target="#b10">[11]</ref>, and style elements harvested by unsupervised learning <ref type="bibr" target="#b11">[12]</ref>. Recent approaches have sought to directly predict surface normals and depth via discriminative learning, e.g., with hand-crafted features <ref type="bibr" target="#b22">[23]</ref>. Most similar to our surface normal prediction approach is recent work that trains a CNN to directly predict depth <ref type="bibr" target="#b26">[27]</ref>, jointly predicts surface normals, depth, and object labels <ref type="bibr" target="#b8">[9]</ref>, or combines CNN features with the global room layout via a predicted 3D box <ref type="bibr" target="#b45">[46]</ref>.</p><p>The second category of approaches go beyond a 2.5D representation and attempt to extract a 3D volumetric representation <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b35">36]</ref>. This in line with traditional approaches for object recognition based on 3D model alignment <ref type="bibr" target="#b31">[32]</ref>. Parametric models, such as volumetric models <ref type="bibr" target="#b23">[24]</ref>, cuboids <ref type="bibr" target="#b47">[48]</ref>, joint cuboid and room layout <ref type="bibr" target="#b37">[38]</ref>, and support surfaces (in RGB-D) <ref type="bibr" target="#b12">[13]</ref> have been proposed. Rendered views of object CAD models over different (textured) backgrounds have been used as training images for CNN-based object detection <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b34">35]</ref> and viewpoint estimation <ref type="bibr" target="#b44">[45]</ref>. Most similar to us are approaches based on CAD retrieval and alignment. Approaches using captured RGB-D images from a depth sensor (e.g. Kinect) include exemplar detection by rendering depth from CAD and sliding in 3D <ref type="bibr" target="#b41">[42]</ref>, 3D model retrieval via exemplar regions matched to object proposals (while optimizing over room layout) <ref type="bibr" target="#b13">[14]</ref>, and training CNNs to predict pose for CAD model alignment <ref type="bibr" target="#b14">[15]</ref> and to predict object class, location, and pose over rendered CAD scenes <ref type="bibr" target="#b32">[33]</ref>. We address the harder case of alignment to single RGB images. Recent work include instance detection of a small set of IKEA objects via contour-based alignment <ref type="bibr" target="#b25">[26]</ref>, depth prediction by aligning to renders of 3D shapes via hand-crafted features <ref type="bibr" target="#b43">[44]</ref>, object class detection via exemplar matching with mid-level elements <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b6">7]</ref>, and alignment via composition from multiple 3D models using hand-crafted features <ref type="bibr" target="#b18">[19]</ref>. More recently CNN-based approaches have de-F" C CNN"Architecture" Output:"" Surface"Normal"" F" C F" C Input"(I):"" RGB"Image" Convolu?onal"Layers"(VGGD16)" h p (I)"" 1x4096" 1x4096" 1x3" c p j1 " c p j2 " h p (I)"="[c p j1 ,"c p j2 "…","c p jα ]" c p jα " " veloped, such as learning a mapping from CNN features to a 3D light-field embedding space for view-invariant shape retrieval <ref type="bibr" target="#b24">[25]</ref> and retrieval using AlexNet <ref type="bibr" target="#b21">[22]</ref> pool5 features <ref type="bibr" target="#b1">[2]</ref>. Also relevant is the approach of Bell and Bala <ref type="bibr" target="#b2">[3]</ref> that trains a Siamese network modeling style similarity to retrieve product images having similar style as a depicted object in an input photo.</p><formula xml:id="formula_0">…' p" N O R M" L2"</formula><p>Our work impacts both the categories and bridges the two. First, our skip-network approach (2D → 2.5D) uses features from all levels of ConvNet to preserve the fine level details. It provides state of the art performance on surface layout estimation. Our 2.5D→ 3D approach differs in its development of a CNN that jointly models appearance and predicted surface normals for viewpoint prediction and CAD retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.">Approach Overview</head><p>Our system takes as input a single 2D image and outputs a set of retrieved object models from a large CAD library matching the style and pose of the depicted objects. The system first predicts surface normals capturing the fine details of objects in the scene (Section 2). The image, along with the predicted surface normals, are used to retrieve models from the CAD library (Section 3). We train CNNs for both tasks using the NYU Depth v2 <ref type="bibr" target="#b39">[40]</ref> and rendered views from ModelNet <ref type="bibr" target="#b46">[47]</ref> for the surface normal prediction and CAD retrieval steps, respectively. We evaluate both steps and compare against the state-of-the-art in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Predicting Detailed Surface Normals</head><p>Our goal is, given a single 2D image I, to output a predicted surface normal map n for the image. This is a challenging problem due to the large appearance variation of objects, e.g., due to texture, lighting, and viewpoint.</p><p>Recently CNN-based approaches have been proposed for this task, achieving state of the art <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b45">46]</ref>. Wang et al <ref type="bibr" target="#b45">[46]</ref> trained a two-stream network that fuses top-down informa-tion about the global room layout with bottom-up information from local image patches. While the model recovered the majority of the scene layout, it tended to miss fine details present in the image due to the difficulty of fusing the two streams. Eigen and Fergus <ref type="bibr" target="#b8">[9]</ref> trained a feed-forward coarse-to-fine multi-scale CNN architecture. The convolutional layers of the first scale (coarse level) were initialized by training on the object classification task over Ima-geNet <ref type="bibr" target="#b36">[37]</ref>. The remaining network parameters for the mid and fine levels were trained from scratch on the surface normal prediction task using NYU depth <ref type="bibr" target="#b39">[40]</ref>. While their approach captured both coarse and fine details, the mid and fine levels of the network were trained on much less data than the coarse level, resulting in inaccurate predictions for many objects.</p><p>In light of the above, we seek to better leverage the rich feature representation learned by a CNN trained on largescale data tasks, such as object classification over Ima-geNet. Recently, Hariharan et al. <ref type="bibr" target="#b15">[16]</ref> introduced the hypercolumn representation for the tasks of object detection and segmentation, keypoint localization, and part labeling. Hypercolumn feature vectors h p (I) are formed for each pixel p by concatenating the convolutional responses of a CNN corresponding to pixel location p, and capture coarse, mid, and fine-level details. Such a representation belongs to the family of skip networks, which have been applied to pixel labeling <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b27">28]</ref> and edge detection <ref type="bibr" target="#b48">[49]</ref> tasks.</p><p>We seek to build on the above successes for surface normal prediction. Formally, we seek to learn a function n p (I; θ) that predicts surface normals for each pixel location p independently in image I given model parameters θ. Given a training set of N image and ground truth surface normal map pairs {(I i ,n i )} N i=1 , we optimize the following objective:</p><formula xml:id="formula_1">min θ N i=1 p ||n p (I i ; θ) −n i,p || 2 .<label>(1)</label></formula><p>We formulate n p (I; θ) as a regression network starting from hypercolumn feature h p (I). Let c j p (I) correspond to the outputs of pre-trained CNN layer j at pixel location p given input image I. The hypercolumn feature vector is a concatenation of the responses, h p (I) = c j1 p (I), . . . , c jα p (I) for layers j 1 , . . . , j α . As shown in <ref type="figure" target="#fig_0">Figure 2</ref>, we train a multi-layer perceptron starting from hypercolumn feature h p (I) as input. Note that the weights of the convolutional layers used to form h p (I) are updated during training. Also, we normalize the outputs of the last fully-connected layer, which results in minimizing a cosine loss.</p><p>Given input vector x and matrix-vector parameters A k and b k , each layer k produces as output:</p><formula xml:id="formula_2">f k (x) = ReLU (A k x + b k ),<label>(2)</label></formula><p>where element-wise operator ReLU (z) = max(0, z). For our experiments we use three layers in our regression network, setting the output of the last layer as the predicted surface normal n p (I; θ). Note that Hariharan et al. <ref type="bibr" target="#b15">[16]</ref> learnt weights for a single layer over hypercolumn features. We found that having multiple layers captures nonlinearities present in the data and further improves results (c.f. Section 4). Also, note that a fully-convolutional network <ref type="bibr" target="#b27">[28]</ref> fuses output class predictions from multiple layers via a directed acyclic graph, whereas we learn regression weights over a concatenation of the layer responses. Our work is similar to Mostajabi et al. <ref type="bibr" target="#b30">[31]</ref> where they save hypercolumn features to disk and train a multi-layer perceptron. In contrast, ours is an end-to-end pipeline that allows fine tuning of all layers in the network. Implementation details and optimization. Given training data, we optimized our network via stochastic gradient descent (SGD) using the publicly-available Caffe source code <ref type="bibr" target="#b19">[20]</ref>. We used a pre-trained VGG-16 network <ref type="bibr" target="#b40">[41]</ref> to initialize the weights of our convolutional layers. The VGG-16 network has 13 convolutional layers and 3 fullyconnected (fc) layers. We converted the network to a fullyconvolutional one following Long et al. <ref type="bibr" target="#b27">[28]</ref>. To avoid confusion with the fc layers of our multi-layer regression network, we denote fc-6 and fc-7 of VGG-16 as conv-6 and conv-7, respectively. We used a combination of six different convolutional layers in our hypercolumn feature (we analyze our choices in Section 4). We constructed mini-batches by resizing training images to 224 × 224 resolution and randomly sampled pixels from 5 images (1000 pixels were sampled per image). The random sampling not only ensures that memory remains in bound, but also reduces overfitting due to feature correlation of spatially-neighboring pixels. We employed dropout <ref type="bibr" target="#b42">[43]</ref> in the fully-connected layers of the regression network to further reduce overfitting. We set the starting learning rate to ǫ = 0.001, and back propagated through all layers of the network. The learning rate was reduced by a factor of 10 at every 50K iterations. For the current results, we stopped training at 60K iterations. At test time, an image is passed through the network and the output of the last layer are returned as the predicted surface normals. No further postprocessing (outside of ensuring the normals are unit length) is performed on the output surface normals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Learning Pose and Style for CAD Retrieval</head><p>Given a selected image region depicting an object of interest, along with a corresponding predicted surface normal map (Section 2), we seek to retrieve a 3D model from a large object CAD library matching the style and pose of the depicted object. This is a hard task given the large number of library models and possible viewpoints of the object. While prior work has performed retrieval by matching the image to rendered views of the CAD models <ref type="bibr" target="#b0">[1]</ref>, we seek to leverage both the image appearance information and the predicted surface normals.</p><p>We first propose a two-stream network to estimate the object pose. This two-stream network takes as input both the image appearance I and predicted surface normals n(I), illustrated in <ref type="figure" target="#fig_1">Figure 3</ref>(left). Each stream of the two stream network is similar in architecture to CaffeNet <ref type="bibr" target="#b21">[22]</ref> upto pool5 layer. We also initialize both the streams using pretrained ImageNet network.</p><p>Note that for surface normals there is no corresponding pre-trained CNN. Although the CaffeNet model has been trained on images, we have found experimentally (c.f. Section 4.2) that it can also represent well surface normals. As the surface normals are not in the same range as natural images, we found that it is important as a pre-processing step to transform them to be in the expected range.  normal values range from [−1, 1]. We map these scores of surface normals to [0, 255] to bring them in same range as natural images. A mean pixel subtraction is done before the image is fed forward to the network. The mean values for n x , n y , and n z are computed using the 381 images in train set of NYUD2. While one could use the pre-trained networks directly for retrieval, such a representation has not been optimized for retrieving CAD models with similar pose and style. We seek to optimize a network to predict pose and style given training data. For learning pose, we leverage the fact that the CAD models are registered to a canonical view so that viewpoint and surface normals are known for rendered views. We generate a training set of sampled rendered views and surface normal maps {(I i ,n i )} N i=1 for viewing angles {φ i } N i=1 for all CAD models in the library. We generate surface normals for each pixel by ray casting to the model faces, which allows us to compute view-based surface normalsn.</p><p>To model pose, we discretize the viewing angles φ and cast the problem as one of classifying into one of the discrete poses. We pass the concatenated CaffeNet "pool5" featuresc(I,n) through a sequence of two fully-connected layers, followed by a softmax layer to yield pose predictions g(I,n; Θ) for model parameters Θ. We optimize a softmax loss over model parameters Θ:</p><formula xml:id="formula_3">min Θ − N i=1 φ T i log(g(I i ,n i ; Θ)).<label>(3)</label></formula><p>Note that during training, we back propagate the loss through all the layers of CaffeNet as well. Given a trained pose predictor, at test time we pass in image I and predicted surface normals n(I) to yield pose predictions g(I, n(I); Θ) from the last fully connected layer. We can also run our network given RGB-D images, where surface normals are derived from the depth channel. We show poseprediction results for both types of inputs in Section 4.2.</p><p>Note that a similar network for pose prediction has been proposed for RGB-D input images <ref type="bibr" target="#b14">[15]</ref>. There, they train a network from scratch using normals from CAD for training and query using Kinect-based surface normals during prediction. We differ in our use of the pre-trained CaffeNet to represent surface normals and our two-stream network incorporating both surface normal and appearance information. We found that due to the differences in appearance of natural images and rendered views of CAD models, simply concatenating the pool5 CaffeNet features hurt performance. We augmented the data similar to <ref type="bibr" target="#b44">[45]</ref> by compositing our rendered views over backgrounds sampled from natural images during training, which improved performance.</p><p>From two-stream pose to siamese style network. While the output of the last fully-connected layer used for pose prediction can be used for retrieval, it has not yet been optimized for style. Inspired by <ref type="bibr" target="#b2">[3]</ref>, we seek to model style given a training set of hand-aligned similar and dissimilar CAD model-image pairs. Towards this goal, we extend our two-stream pose network to a Siamese two-stream network for this task, illustrated in <ref type="figure" target="#fig_1">Figure 3(right)</ref>. Specifically, let f be the response of the last fully-connected layer of the pose network above. Given similar image-model pairs (f p , f q ) and dissimilar pairs (f q , f n ), we optimize the contrastive loss:</p><formula xml:id="formula_4">L(Θ) = (q,p) L p (f q , f p ) + (q,n) L n (f q , f n ).</formula><p>(4)</p><p>We use the losses L p (f q , f p ) = ||f q − f p || 2 and L n (f q , f n ) = max (m − ||f q − f n || 2 , 0), where m = 1 is a parameter specifying the margin. As in <ref type="bibr" target="#b2">[3]</ref>, we optimize the above objective via a Siamese network. Note that we optimize over pose and style, while <ref type="bibr" target="#b2">[3]</ref> optimizes over object class and style for the task of product image retrieval. For optimization, we apply mini-batch SGD in training using the caffe framework. We followed the standard techniques to train a CaffeNet-like architecture, and backpropagate through all layers. The procedure for training and testing are described in the respective experiment section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We present an experimental analysis of each component of our pipeline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Surface Normal Estimation</head><p>The skip-network architecture described in Section 2 is used to estimate the surface normals. The VGG-16 network <ref type="bibr" target="#b40">[41]</ref> has 13 convolutional layers represented as {1 1 , 1 2 , 2 1 , 2 2 , 3 1 , 3 2 , 3 3 , 4 1 , 4 2 , 4 3 , 5 1 , 5 2 , 5 3 }, and three fully-connected layers {fc-6, fc-7, fc-8}. As mentioned in Section 2, we convert the pretrained fc-6 and fc-7 layers from VGG-16 to convolutional ones, denoted conv-6 and conv-7, respectively. We use a combination of {1 2 , 2 2 , 3 3 , 4 3 , 5 3 , 7 } convolutional layers from VGG-16. We evaluate our approach on NYU Depth v2 dataset <ref type="bibr" target="#b39">[40]</ref>. There are 795 training images and 654 test images in this dataset. Raw depth videos are also made available by <ref type="bibr" target="#b39">[40]</ref>. We use the frames extracted from these videos to train our network for the task of surface normal estimation.</p><p>For training and testing we use the surface normals computed from the Kinect depth channel by Ladicky et al. <ref type="bibr" target="#b22">[23]</ref> over the NYU trainval and test sets. As their surface normals are not available for the video frames in the training set, we compute normals (from depth data) using the approach of Wang et al. <ref type="bibr" target="#b45">[46]</ref> 1 .</p><p>We ignore pixels where depth data is not available during training and testing. As shown in <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b45">46]</ref> data augmentation during training can boost accuracy. We performed minimal data augmentation during training. We performed left-right flipping of the image and color augmentation, similar to <ref type="bibr" target="#b45">[46]</ref>, over the NYU trainval frames only; we did not perform augmentation over the video frames. This is much less augmentation than prior approaches <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b45">46]</ref>, and we believe we can get additional boost with further augmentation, e.g. by employing the suggestions in <ref type="bibr" target="#b5">[6]</ref>. Note that the proposed pixel-level optimization also achieves comparable results training on only the 795 images in the training set of the NYUD2 dataset. This is due to the variability provided by pixels in the image as now each pixel act as a data point. <ref type="figure" target="#fig_2">Figure 4</ref> shows qualitative results from our approach. Notice that the back of the sofa in row 1 is correctly captured and the fine details of the desk and chair in row 3 are more visible in our approach. For quantitative evaluation we use the criteria introduced by Fouhey et al. <ref type="bibr" target="#b9">[10]</ref> to compare our approach against prior work <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b45">46]</ref>. Six statistics are computed over the angular error between the predicted normals and depth-based normals -Mean, Median, RMSE, 11.25 • , 22.5 • , and 30 • -using the normals of Ladicky et al. as ground truth <ref type="bibr" target="#b22">[23]</ref>. The first three criteria capture the mean, median, and RMSE of angular error, where lower is better. The last three criteria capture the percentage of pixels within a given angular error, where higher is better.</p><p>In this work, our focus is to capture more detailed surface normal information from the images. We, therefore, not only evaluate our approach on the entire global scene layout as in <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b45">46]</ref>, but we also introduce an evaluation over objects (chair, sofa, and bed) in indoor scene categories. First we show the performance of our approach on the entire global scene layout and compare it with <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b45">46]</ref>. We then compare the surface normals for indoor scene furniture categories (chair, sofa, and bed) against <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b45">46]</ref>. Finally, we perform an ablative analysis to justify our architecture design choices.</p><p>Global Scene Layout: <ref type="table">Table 1</ref> compares our approach with existing work. We present our results both with and without Manhattan-world rectification to fairly compare against previous approaches, as <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b45">46]</ref> use it and <ref type="bibr" target="#b8">[9]</ref> do not. Similar to <ref type="bibr" target="#b9">[10]</ref>, we rectify our normals using the van- ishing point estimates from Hedau et al. <ref type="bibr" target="#b16">[17]</ref>. Interestingly, our approach performs worse with Manhattan-world rectification (unlike Fouhey et al. <ref type="bibr" target="#b9">[10]</ref>). Our network architecture predicts room layout automatically, and appears to be better than using vanishing point estimates. Though capturing scene layout was not our objective, our work out-performs previous approaches on all evaluation criteria.</p><p>Local Object Layout: The existing surface normal literature is focussed towards the scene layout. In this work, we stress the importance of fine details in the scene generally available around objects. We, therefore, evaluated the performance of our approach in the object regions by considering only those pixels which belong to a particular object. Here we show the performance on chair, sofa and bed. <ref type="table">Table 2</ref> shows comparison of our approach with Wang et al. <ref type="bibr" target="#b45">[46]</ref> and Eigen and Fergus <ref type="bibr" target="#b8">[9]</ref>. We achieve performance around 1-4% better than previous approaches on all statistics for all the objects.</p><p>Ablative Analysis: We analyze how different sets of convolutional layers influence the performance of our approach. <ref type="table" target="#tab_4">Table 3</ref> shows some of our analysis. We chose a combination of layers from low, mid, and high parts of the VGG network. Clearly from the experiments, we need a combination of different low, mid, high layers to capture rich information present in the image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Pose Estimation</head><p>We evaluated the approach described in Section 3 to estimate the pose of a given object. We trained the pose network using CAD models from Princeton ModelNet <ref type="bibr" target="#b46">[47]</ref> as    <ref type="figure">Figure 5</ref>. Pose prediction on val set. We plot the fraction of instances with predicted pose angular error less than δ θ as a function of δ θ . Similar to <ref type="bibr" target="#b14">[15]</ref> we consider only those objects which have valid depth pixels for more than 50%.</p><p>training data, and used 1260 models for chair, 526 for sofa, and 196 for bed. For each model, we rendered 144 different views corresponding to 4 elevation and 36 azimuth angles. We designed the network to predict one of the 36 azimuth angles, which we treated as a 36-class classification problem. Note that we trained separate pose networks for the chair, sofa, and bed classes. At test time, we forward propagated the selected region from the image, along with its predicted surface normals, and selected the angle with maximum prediction score. We evaluated our approach using the annotations from Guo and Hoiem <ref type="bibr" target="#b12">[13]</ref> where they manually annotated the NYUD2 dataset with aligned 3D CAD models for the categories of interest. <ref type="figure">Figure 5</ref> shows a quantitative evaluation of our approach on the NYUD2 val set. Using the criteria introduced in Gupta et al <ref type="bibr" target="#b14">[15]</ref>, we plot the fraction of instances with predicted pose angular error less than δ θ as a function of δ θ (higher is better). We compare our approach with Gupta et al <ref type="bibr" target="#b14">[15]</ref> who showed results of pose estimation on the NYUD2 val set for objects with at least 50% valid depth pixels. Note that we trained our skip-network for surface normals using the 381 images of the NYUD2 train set. We clearly out-perform the baseline using RGB-only and RGB-D for chairs and sofas. Our approach underperformed for bed. We believe that the gap in performance is due to symmetry in its shape, and proper data augmentation strategies may improve results. More results showing comparison of our approach with <ref type="bibr" target="#b32">[33]</ref> on NYUD2 test set, and a ablative study using different input predicted surface normals <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b45">46]</ref> are available in supplementary material. <ref type="figure">Figure 6</ref>. For each example, the top row shows CAD models retrieved using fc-7 of Pose Network and the bottom row shows the result of nearest-neighbor retrieval using predicted surface normals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Style Estimation</head><p>We used the style network described in Section 3 to determine the style of objects. To reduce the search space, we use this network to re-rank the top-N output of the CAD models retrieved using the fc-7 feature of the pose network. We evaluate our style network using chairs as chairs span a large variety of styles <ref type="bibr" target="#b0">[1]</ref>. To train the model we handlabeled images in the NYUD2 training set with models having very similar style. To assist with the labeling we used our pose network to retrieve similar CAD models over the NYU training set. For each example we looked at the top-30 retrieved CAD models and manually labeled if a particular CAD model is similar to the input example or not. We used these labels to train our style network using the contrastive loss. <ref type="figure" target="#fig_4">Figure 7</ref> shows qualitative examples of our reranking via the style network. Note that the network is able to boost the ranking of similar examples, e.g., the chairs having wheels in the first and last row have different styles in the initial retrieved examples of the pose network. With the re-ranking, we are able to see chairs with wheels consistently.</p><p>We performed a user study similar to Aubry et al. <ref type="bibr" target="#b0">[1]</ref> to evaluate the retrieved models returned by PoseNet and StyleNet (re-ranking). We asked users to select retrieved models that have similar style as the query images in the top-5 retrievals. While the performance for style retrieval was 45% with PoseNet, StyleNet improved by 2% to 47%. We believe the re-ranking results are promising, particularly given the limited training data used compared to the 60M training pairs used by Bell &amp; Bala <ref type="bibr" target="#b2">[3]</ref> to train a similar Siamese network. Further, Aubry et al. <ref type="bibr" target="#b0">[1]</ref> suggested that finding exact model matches for "chair" and similar categories may be difficult as the variation is large within the category. The approximate model matches returned by the PoseNet retrievals, and improved by StyleNet, appear to be in line with prior work in this space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We have demonstrated a successful feed-forward approach for 3D object recognition in 2D images via 2.5D surface normal prediction. Our skip-network approach for surface normal prediction recovers fine object detail and achieves state of the art on the challenging NYU depth benchmark. We formulated a two-stream pose network that jointly reasons over the 2D image and predicted surface normals, and achieves pose prediction accuracy that is comparable to existing approaches based on RGB-D images. When we apply our pose network to RGB-D image data, we surpass the state of the art for the pose prediction task. Finally, our pose-style network shows promising results in retrieving CAD models matching both the depicted object style and pose. Our accurate surface normal predictions open up the possibility of having reliable 2.5D predictions for most natural images, which may have impact on applications in computer graphics and, ultimately, for the goal of full 3D scene understanding.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Skip-network architecture for surface normal prediction. CNN layer responses are concatenated for each pixel, which are passed through a multi-layer perceptron to predict the surface normal for each pixel.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Networks for predicting pose (left) and style (right). Our pose network is trained on a set of rendered CAD views and extracted surface normal pairs. During prediction, an image and its predicted surface normals are used to predict the object pose. For the style network, we train on hand-aligned natural image and CAD rendered view pairs. We initialize the style network with the network trained for poses. See text for more details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Qualitative results for surface normal estimation. Note the fine details of sofa, chair, table, pillow etc. captured by our approach.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 .</head><label>7</label><figDesc>Style re-ranking. For each example the top row shows the top-5 CAD models obtained using our Style Network and the bottom row shows the original retrievals using the Pose Network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head></head><label></label><figDesc>Mean Median RMSE 11.25 • 22.5 • 30 •</figDesc><table>NYUDv2 test 

Chair 
Wang et al. [46] 
44.7 
35.8 
54.9 
14.2 34.3 44.3 
E-F (AlexNet) [9] 38.2 
32.5 
46.3 
14.4 34.9 46.6 
E-F (VGG-16) [9] 33.4 
26.6 
41.5 
18.3 43.0 55.1 
Ours 
32.0 
24.1 
40.6 
21.2 47.3 58.5 

Sofa 
Wang et al. [46] 
36.0 
27.6 
45.4 
21.6 42.6 53.1 
E-F (AlexNet) [9] 27.0 
21.3 
34.0 
25.5 52.4 63.4 
E-F (VGG-16) [9] 21.6 
16.8 
27.3 
32.5 63.7 76.3 
Ours 
20.9 
15.9 
27.0 
34.8 66.1 77.7 

Bed 
Wang et al. [46] 
28.6 
18.5 
38.7 
34.0 56.4 65.3 
E-F (AlexNet) [9] 23.1 
16.3 
30.8 
36.4 62.0 72.6 
E-F (VGG-16) [9] 19.9 
13.6 
27.1 
43.0 68.2 78.3 
Ours 
19.6 
13.4 
26.9 
43.5 69.3 79.3 

Table 2. NYUv2 surface normal prediction: Local object layout. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="true"><head>Table 3 .</head><label>3</label><figDesc>NYUv2 surface normal prediction: Ablative Analysis.</figDesc><table>Chair&amp; 
Sofa&amp; 
Bed&amp; 

Frac/on&amp;&amp;of&amp;instances&amp; 
Frac/on&amp;&amp;of&amp;instances&amp; 
Frac/on&amp;&amp;of&amp;instances&amp; 

NNN&amp;-&amp;Gupta&amp;et&amp;al.&amp;[15]&amp;&amp; 
Ours&amp;(RGBAD)&amp; 
Ours&amp;(RGB)&amp; 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Wang et al.<ref type="bibr" target="#b45">[46]</ref> used a first-order TGV denoising approach to compute normals from depth data which they used to train their model. We did not use the predicted normals from their approach.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements: This work was partially supported by grants from NSF IIS-1320083 and ONR MURI N000141612007. We thank Saining Xie for discussion on skip-network architectures, David Fouhey for providing code to compute normals from Kinect data, and Saurabh Gupta for help with the pose estimation evaluation setup.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Seeing 3D chairs: Exemplar part-based 2D-3D alignment using a large dataset of CAD models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aubry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maturana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Understanding deep features with computer-generated imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aubry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning visual similarity for product design with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Recognition by components: a theory of human image interpretation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Biederman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pyschological review</title>
		<imprint>
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="115" to="147" />
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Visual perception by computer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">O</forename><surname>Binford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE conference on Systems and Control</title>
		<imprint>
			<date type="published" when="1971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Return of the devil in the details: Delving deep into convolutional nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chatfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Object detection with 2D-3D registration and continuous viewpoint estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">B</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Corbett-Davies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The Manhattan world assumption: Regularities in scene statistics which enable Bayesian inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Coughlan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Data-driven 3D primitives for single image understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">F</forename><surname>Fouhey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Unfolding an indoor origami world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">F</forename><surname>Fouhey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Single image 3D without a single 3D image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">F</forename><surname>Fouhey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hussain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Support surface prediction in indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Predicting complete 3d models of indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<idno>abs/1504.02437</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Aligning 3D models to RGB-D images of cluttered scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Hypercolumns for object segmentation and fine-grained localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Recovering the spatial layout of cluttered rooms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Hedau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Automatic photo popup</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Single-view reconstruction via joint analysis of image and shape collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Caffe: Convolutional architecture for fast feature embedding. CoRR, abs/1408</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">5093</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Video compass</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kosecka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Discriminatively trained dense surface normal estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ladicky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zeisl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Estimating spatial layout of rooms using volumetric reasoning about objects and surfaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Joint embeddings of shapes and images via CNN image purification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Fish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (Proceeding of SIGGRAPH Asia)</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Parsing IKEA objects: Fine pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep convolutional neural fields for depth estimation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Fully convolutional models for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning informative edge maps for indoor scene layout prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mallya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Marr</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1982" />
			<publisher>Freeman and Company</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Feedforward semantic segmentation with zoom-out features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mostajabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yadollahpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shakhnarovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Object recognition in the geometric era: A retrospective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Mundy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Toward Category-Level Object Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">4170</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Semantic pose using deep networks trained on synthetic RGB-D</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Papon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schoeler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning deep object detectors from 3D models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ali</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">What is holding back convnets for detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Pepik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ritschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">GCPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Machine perception of 3-D solids. In PhD. Thesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Roberts</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1965" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>ImageNet large scale visual recognition challenge. IJCV</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Box in the box: Joint 3D layout and object reasoning from single images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Efficient exact inference for 3D indoor scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Indoor segmentation and support inference from rgbd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition. CoRR, abs/1409.1556</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Sliding shapes for 3d object detection in depth images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Estimating image depth using shape collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Render for CNN: Viewpoint Estimation in Images Using CNNs Trained with Rendered 3D Model Views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Designing deep networks for surface normal estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fouhey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">3D ShapeNets: A deep representation for volumetric shape modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Localizing 3D cuboids in single-view images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Holistically-nested edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
