<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Seeing through the Human Reporting Bias: Visual Classifiers from Noisy Human-Centric Labels</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Seeing through the Human Reporting Bias: Visual Classifiers from Noisy Human-Centric Labels</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:45+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>When human annotators are given a choice about what to label in an image, they apply their own subjective judgments on what to ignore and what to mention. We refer to these noisy "human-centric" annotations as exhibiting human reporting bias. Examples of such annotations include image tags and keywords found on photo sharing sites, or in datasets containing image captions. In this paper, we use these noisy annotations for learning visually correct image classifiers. Such annotations do not use consistent vocabulary, and miss a significant amount of the information present in an image; however, we demonstrate that the noise in these annotations exhibits structure and can be modeled. We propose an algorithm to decouple the human reporting bias from the correct visually grounded labels. Our results are highly interpretable for reporting "what's in the image" versus "what's worth saying." We demonstrate the algorithm's efficacy along a variety of metrics and datasets, including MS COCO and Yahoo Flickr 100M. We show significant improvements over traditional algorithms for both image classification and image captioning, doubling the performance of existing methods in some cases.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Visual concept recognition is a fundamental computer vision task with a broad range of applications in science, medicine, and industry. Supervised learning of visual concept classifiers has been highly successful partly due to the use of large-scale, high-quality datasets (e.g., <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b28">29]</ref>). Depending on the complexity of the supported task, these datasets generally include annotations for 100s to 1000s of 'typical' concepts. To support an even broader range of applications, it is necessary to train classifiers for tens or even hundreds of thousands of visual concepts that may not be typical. Since supervised learning methods require exhaustive and clean annotations, one would require high quality datasets with orders of magnitude more annotations to train * Work done during internship at Microsoft Research. The Vespa in (c) is described as "yellow", while the bananas in (d) are not, as being yellow is typical for bananas.</p><p>such methods. However, creating such datasets is expensive. An alternative approach is to relax this requirement of pristinely labeled data. The learning algorithm can be enabled to use readily-available sources of annotated data, such as user-generated image tags or captions from social media services like Flickr or Instagram. Such datasets easily scale to hundreds of millions of photos with hundreds of thousands of distinct tags <ref type="bibr" target="#b48">[49]</ref>.</p><p>Images annotated with human-written tags <ref type="bibr" target="#b48">[49]</ref> or captions <ref type="bibr" target="#b5">[6]</ref> focus on the most important or salient information in an image, as judged implicitly by the annotator. These annotations lack information on minor objects or information that may be deemed unimportant, a phenomenon known as reporting bias <ref type="bibr" target="#b15">[16]</ref>. For example, <ref type="figure" target="#fig_0">Figure 1</ref> illustrates two concepts (bicycle, yellow) that are each present in two images, but only mentioned in one. The bicycle may be considered irrelevant to the overall image in (b); and the bananas in (d) are not described as yellow because humans often omit an object's typical properties when referring to it <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b52">53]</ref>. Following <ref type="bibr" target="#b2">[3]</ref>, we refer to this type of labeling as human-centric annotation.</p><p>Training directly on human-centric annotations does not yield a credible visual concept classifier. Instead, it leads to a classifier that attempts to mimic the reporting bias of the annotators. To separate reporting bias from visual ground truth, we propose to train a model that explicitly factors human-centric label prediction into a visual presence classifier (i.e., "Is this concept visually present in this image?") and a relevance classifier (i.e., "Is this concept worth mentioning in this image, given its visual presence?"). We train all these classifiers jointly and end-to-end as multiple "heads" branching from the same shared convolutional neural network (ConvNet) trunk <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b45">46]</ref>.</p><p>We demonstrate improved performance on several tasks and datasets. Our experiments on the MS COCO Captions dataset <ref type="bibr" target="#b5">[6]</ref> show an improvement in mean average precision (mAP) for the learned visual classifiers when evaluated on both fully labeled data (using annotations from the MS COCO detection benchmark <ref type="bibr" target="#b28">[29]</ref>) and on the human generated caption data. We also show that using such visual predictions improves image caption generation quality. Our results on the Yahoo Flickr 100M dataset <ref type="bibr" target="#b48">[49]</ref> demonstrate the ability of our model to learn from "in the wild" data (noisy Flickr tags) and double the performance of the baseline classification model. Apart from just numerical improvements, our results are interpretable and consistent with research in psychology showing that humans tend not to mention typical attributes <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b52">53]</ref> unless required for unique identification <ref type="bibr" target="#b43">[44]</ref> or distinguishability <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b47">48]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Label noise is ubiquitous in real world data. It can impact the training process of models and decrease their predictive accuracy <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b36">37]</ref>. Since there are vast amounts of cheaply available noisy data, learning good predictors despite the label noise is of great practical value.</p><p>The taxonomy of label noise presented in <ref type="bibr" target="#b13">[14]</ref> differentiates between two broad categories of noise: noise at random and statistically dependent noise. The former does not depend on the data, while the latter does. In practice, one may encounter a combination of both types of noise.  <ref type="figure">Figure 2</ref>: A simple classification model for learning from human-centric annotations. The noisy labels (banana is not annotated as yellow) impede the learning process.</p><p>Human-centric annotations <ref type="bibr" target="#b2">[3]</ref> exhibit noise that is highly structured and shows statistical dependencies on the data <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b54">55]</ref>. It is structured in the sense that certain labels are preferentially omitted as opposed to others. Vision researchers have studied human-centric annotations in various settings, such as missing objects in image descriptions <ref type="bibr" target="#b2">[3]</ref>, scenes <ref type="bibr" target="#b3">[4]</ref>, and attributes <ref type="bibr" target="#b49">[50]</ref> and show that these annotations are noisy <ref type="bibr" target="#b2">[3]</ref>. Much of the work on learning from noisy labels focuses on robust algorithms <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b31">32]</ref>, voting methods <ref type="bibr" target="#b1">[2]</ref>, or statistical queries <ref type="bibr" target="#b21">[22]</ref>. Some of these methods <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b21">22]</ref> require access to clean oracle labels, which may not be readily available.</p><p>Explicitly modeling label noise has received increasing attention in recent years <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b53">54]</ref>. Many of these methods operate under the "noise at random" assumption and treat noise as conditionally independent of the image. <ref type="bibr" target="#b25">[26]</ref> models symmetric label noise (independent of the true label), which is a strong assumption for real world data. <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b46">47]</ref> both model asymmetric label noise that is conditionally independent of the image. Such an assumption ignores the input image (and the objects therein) which directly affects the noisy annotations produced by humans <ref type="bibr" target="#b2">[3]</ref>.</p><p>Recently, Xiao et al. <ref type="bibr" target="#b53">[ 54]</ref> introduced an image conditional noise model that attempts to predict what type of noise corrupts each training sample (no noise, noise at random, and structured label swapping noise). Unlike <ref type="bibr" target="#b53">[54]</ref>, our training algorithm does not require a small amount of cleanly labeled training data to bootstrap parameter estimation. Our model is also specifically designed to handle the noise found in human-centric annotations.</p><p>Bootstrapping <ref type="bibr" target="#b39">[40]</ref>, semi-supervised learning (SSL) <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b56">57]</ref> and Positive Unlabeled (PU) learning <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b33">34]</ref> are other ways of learning from noisy labeled data. However, they require access to clean oracle labels. SSL approaches are often computationally impractical <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b57">58]</ref>o r make strong independence assumptions <ref type="bibr" target="#b12">[13]</ref> that do not hold in human-centric annotations. Our approach, which trains directly on noisy labels, can serve as a starting point for these approaches.</p><p>The work described here is also consistent with research in psycholinguistics on object reference and description. Such work demonstrates that humans store typical or   <ref type="figure">Figure 3</ref>: Our model uses noisy human-centric annotations y for learning visually grounded classifiers without access to the visually correct ground truth z. It uses two classifiers: a visual presence classifier v and a relevance classifier r. The visual presence classifier v predicts whether the visual concept w is visually present in an image. The relevance classifier r models the noise and predicts whether the concept should be mentioned or not. We combine these predictions to get the human-centric prediction h.</p><p>"prototypical" representations of objects and their properties <ref type="bibr" target="#b40">[41]</ref>; and this background knowledge appears to have an effect on object description <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b52">53]</ref>. People tend not to mention attributes that are obvious or typical for an object, preferring to name attributes required for conversational relevance <ref type="bibr" target="#b24">[25]</ref>, unique identification against alternatives <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b50">51]</ref> and distinguishability <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b22">23]</ref>. A similar separation between what is observed and what is mentioned falls out naturally from our proposed model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Our Approach</head><p>Our goal is to train visually grounded image classifiers for a set of visual concepts w ∈W(e.g., banana, yellow, zebra) using images and their human-centric annotations. The conventional approach to this problem, shown in <ref type="figure">Figure 2</ref>, is to naively apply a supervised learning algorithm: train a classifier h w for each concept w, to predict its human-centric label y w ∈{ 0, 1} (not mentioned vs. mentioned) as a conditional probability distribution h w (y w |I), in which I is an image.</p><p>The resulting classifier would attempt to mimic human reporting bias by predicting how a human would label the image I regardless of whether w is visually present or absent. Thus, the predictions from each classifier h w will not be visually grounded and do not meet our goal. How can we build classifiers that predict whether a visual concept w is present in an image and "see through" the noise in humancentric annotations?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Factor decoupling</head><p>We propose to structure each concept output h w (y w |I) in terms of two classifiers v w and r w . The first classifier v w models the conditional probability of the visual presence of the concept w in the image. The second classifier r w models the conditional probability of the relevance of the concept w, conditioned on the image and whether or not w is estimated to be visually present. The human-centric predictor is formed by marginalizing over the concept's visual presence, as described next.</p><p>Let z w ∈{ 0, 1} be a latent (or hidden) variable indicating whether the concept w is visually present in an image. Note that the training data only supplies human-centric labels y w ; the true values of z w are unknown during training. For instance, in <ref type="figure" target="#fig_0">Figure 1</ref>(a), y w =1and z w =1when the bicycle is present and mentioned, while y w =0and z w =1in <ref type="figure" target="#fig_0">Figure 1</ref>(b) when the bicycle is present but not mentioned. We refer to z w as the visual presence label.</p><p>The conditional probability of the human-centric label given an image I, h w (y w |I), can now be computed by marginalizing over the latent visual presence label z w :</p><formula xml:id="formula_0">h w (y w |I)= j∈{0,1} r w (y w |z w = j, I)v w (z w = j|I). (1)</formula><p>An illustration of our model is shown in <ref type="figure">Figure 3</ref>. One important property of this formulation is that it allows the model to assign high confidence to unlabeled visual concepts: for an unmentioned concept (y w =0and z w =1 ), the relevance classifier r w allows the visual presence classifier v w to assign a high probability to the true visual label (z w =1 ) while still making a prediction that matches the human-centric label (y w =0 ). This property enables the model to "see through" human reporting bias.</p><p>To simplify notation, we drop the concept index w from y, z, h, v and r when possible. We denote the probability values of r by:</p><formula xml:id="formula_1">r ij = r(y = i|z = j, I), ∀(i, j) ∈{0, 1} 2 .<label>(2)</label></formula><p>Another important property of the factorization in Equation (1) is that it provides a way to get two different predictions for the concept w in the same image. The model can predict the visual presence of a visual concept; or predict how a human would annotate the image. Depending on the task at hand, one of these predictions may be more appropriate than the other, a point we demonstrate later via experimental results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Model learning and parameterization</head><p>We estimate the model parameters by minimizing the regularized log loss of h w , summed over all concepts w, on the training data annotated with human-centric labels. Since our model includes latent variables z for each concept and image, one approach could be to use Expectation Maximization (EM) <ref type="bibr" target="#b6">[7]</ref>. We choose a direct optimization approach <ref type="bibr" target="#b4">[5]</ref> over EM for simplicity, and thus both the model parameters and latent variable posteriors are updated and inferred online. In each SGD minibatch, the model predicts the conditional distributions r and v, marginalizes over the values of z, and uses the log loss of h to drive better estimates of r and v.</p><p>The conditional distributions r and v are modeled with a ConvNet <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b41">42]</ref>. As illustrated in <ref type="figure">Figure 3</ref>, the Con-vNet trunk is shared between the two distributions (per concept) and then branches near the output into two sets of untied parameters. We jointly train one network for all visual concepts w ∈Wby treating learning as a multi-label classification problem. Further network architecture details are given in Section 4 with experiments.</p><p>The conditional probability distribution r models transition probabilities and thus its underlying joint distributioñ r ij =r(y = i, z = j|I) must be a valid probability distribution. To enforce this constraint, we directly estimate the joint distributionr with a softmax operation on a vector of unnormalized scores.</p><p>For each concept w, we first compute four scores s ij using four linear models parameterized by weights m ij and biases b ij , and then normalize them using the softmax function to get a valid joint distributionr ij :</p><formula xml:id="formula_2">s ij = m T ij φ(I)+b ij ,<label>(3)</label></formula><formula xml:id="formula_3">r ij =exp(s ij )/ i ′ j ′ exp(s i ′ j ′ ).<label>(4)</label></formula><p>For φ(I), we use global image features computed by the shared ConvNet trunk (e.g., fc7 layer activations from VGG16 <ref type="bibr" target="#b45">[46]</ref>). These features capture the global image context, which is helpful in estimating r. Each r ij can then be computed fromr by dividing by the marginalr(z = j|I): <ref type="figure">Figure 3</ref> illustrates our process of computing r ij from the image. Since our operations for estimatingr (and thus r) are differentiable, we can backpropagate their errors to the ConvNet, allowing the full model to be trained end-to-end. </p><formula xml:id="formula_4">r ij =r ij / i ′r i ′ j .<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We evaluate our proposed model on two datasets: Microsoft COCO <ref type="bibr" target="#b28">[29]</ref> and a random subset of the Yahoo Flickr Creative Commons 100M (YFCC100M) dataset <ref type="bibr" target="#b48">[49]</ref>. The YFCC100M dataset includes user-generated image tags, which we take as our source of human-centric annotations. For MS COCO, we take the supplied image captions <ref type="bibr" target="#b5">[6]</ref>as human-centric annotations. We use the MS COCO object detection labels for dataset analysis and algorithm evaluation. These labels allow us to verify the accuracy of trained visual presence classifiers, v w ; these labels are never used for training our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experiments on MS COCO 1k visual concepts</head><p>Our first set of experiments use the MS COCO 1000 visual concepts from <ref type="bibr" target="#b11">[12]</ref>. The visual concepts are the 1000 most common words in the MS COCO captions dataset <ref type="bibr" target="#b5">[6]</ref> and include nouns, verbs, adjectives, and other parts of speech (see <ref type="table" target="#tab_2">Table 1</ref> for a breakdown).</p><p>For training, we generate image labels as 1000dimensional binary vectors indicating which of the 1000 target visual concepts are present in the any of the 5 reference captions for each training image. The training set includes approximately 80k images. For evaluation, we follow <ref type="bibr" target="#b11">[12]</ref> and split the val set into equally sized val and test sets of ∼20k images each; we use the same splits as in <ref type="bibr" target="#b11">[12]</ref>. We report results on this 20k image test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Human reporting bias in image descriptions</head><p>We first analyze the annotation mismatch between the caption labels and the detection labels. We obtain labels for the 73 objects common in both the caption and object detection labels (see Section 4.1.3 for details). We use the notation from Section 3.1, and measure the human reporting bias as r 01 : the probability of an object not being mentioned in the caption ground truth (y =0 ) and being present in the detection ground truth (z =1 ), over all the training images.</p><p>To account for object size as a factor in an object not being mentioned, we split these measurements based on the size of the bounding box (sizes as defined in <ref type="bibr" target="#b28">[29]</ref>). <ref type="figure" target="#fig_2">Figure 4</ref> shows this mismatch for the top 20 objects with the highest r 01 for values. A high r 01 value indicates that there is a large mismatch between the caption ground truth and the detection ground truth -objects that are visually present but not mentioned in the captions. As observed, there is a high degree of human labeling noise in the image descriptions, with the object with highest reporting bias mentioned roughly half as much as it appears.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Evaluating human-centric label prediction</head><p>We can evaluate our model in two ways: as a purely visual classifier (v) or as a predictor of human-centric (h) labels. We start with the latter and evaluate our model's predictions against the human-centric MS COCO captions.</p><p>As a strong baseline, we use the recently proposed MILVC <ref type="bibr" target="#b11">[12]</ref> approach. This method applies a ConvNet (VGG16 <ref type="bibr" target="#b45">[46]</ref> or AlexNet <ref type="bibr" target="#b23">[24]</ref>) 1 in a fully-convolutional way to a large input image to generate a 12 × 12 grid of human-centric label predictions. It then uses a noisy-OR <ref type="bibr" target="#b51">[52]</ref> to compute a single prediction from the 144 intermediate values. During training, the noisy-OR induces a form of multiple instance learning (MIL). Note that the baseline model estimates h labels directly without our decomposi- <ref type="bibr" target="#b0">1</ref> Unless otherwise specified, we use VGG16 for all our experiments. tion into relevance and visual presence factors. As a second baseline (Classif.), we use a vanilla classification model akin to <ref type="figure">Figure 2</ref>, in which an ImageNet <ref type="bibr" target="#b42">[43]</ref> pre-trained ConvNet is fine-tuned to directly predict human-centric labels.</p><p>We also include an additional baseline variant that adds extra parameters to control for the fact that our proposed model requires adding extra parameters. Specifically, we train a "Multiple-fc8" model for each method (MILVC and Classif.) that has the same number of parameters as our model. To train Multiple-fc8, we add four extra randomly initialized fc8 (linear classification) layers, each with their own loss. At test time, we average the predictions of all the fc8 layers to get the final prediction.</p><p>We implement two variants of our model to parallel the MILVC and Classif. baselines. In the first variant (MILVC + Latent), v uses a noisy-OR over a 12 × 12 grid of visual presence predictions, and for r we average pool the 144 fc7 activation vectors to obtain a single 4096 dimensional φ(I) for Equation <ref type="bibr" target="#b2">3</ref>. The second variant (Classif. + Latent) generates a 1 × 1 output for v and fc7, and therefore omits the noisy-OR and average pooling. In both cases, h predictions are obtained following Equation 1, using both r and v. Like the baselines, our model is trained to minimize the log loss (cross-entropy loss) over h.</p><p>To train our model, we set the joint noise distributionr to identity (i.e.,r 11 =r 00 =0 .5) for the first two epochs and then update it for the last two epochs. <ref type="table" target="#tab_2">Table 1</ref> shows mean average precision (mAP) <ref type="bibr" target="#b10">[11]</ref> and precision at human recall (PHR) <ref type="bibr" target="#b5">[6]</ref> on the 20k test set. PHR is a metric proposed in <ref type="bibr" target="#b5">[6]</ref>, and measures precision based on human agreement. Briefly, this metric uses multiple references per image to compute a "human recall" value, an estimate of the probability that a human will use a particular word for an image. Precision is then computed at this "human recall" value to get PHR. <ref type="bibr" target="#b5">[6]</ref> shows that for the task of predicting visual concepts, PHR is a more stable metric than an AP metric, since it accounts for human agreement. We report results for the 1000 visual concepts in aggregate, as well as grouped by their part-of-speech (POS) tags, on the MS COCO test split of 20k images <ref type="table" target="#tab_2">(Table 1)</ref>. Our latent variable model improves classification performance over all the baseline networks and architectures by 3t o4 points for both metrics (mAP and PHR). Interestingly, the Multiple-fc8 model, which has the same number of parameters as our latent model, does not show an improvement, even after extensive tuning of learning hyperparameters. This finding makes the contribution of the proposed model evident; the improvement is not simply due to adding extra parameters. It is worth noting that in <ref type="table" target="#tab_2">Table 1</ref>, h is a better predictor of MS COCO caption labels than v,ash directly models the human-centric labels used for evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Evaluating visual presence prediction</head><p>The decoupling of visual presence (v) predictions and human-centric (h) label predictions allows our model to learn better visual predictors. To demonstrate this, we use the fully-labeled ground truth from the COCO detection annotations to evaluate the visually grounded v label predictions.</p><p>Since the 1000 visual concepts include many finegrained visual categories (e.g., man, woman, child) and synonyms (e.g., bike, bicycle), we manually specify a mapping from the visual concepts to the 80 MS COCO detection categories, e.g., {bike, bicycle}→bicycle. We find that 73 of the 80 detection categories are present in the 1000 visual concepts. We use this mapping only at evaluation time to compute the probability of a detection category as the maximum of the probabilities of its finegrained/synonymous categories. <ref type="table" target="#tab_3">Table 2</ref> shows the mean average precision (mAP) of our method, as well as the baseline on these 73 categories. As expected, using the human-centric model (h) for this task of visual prediction hurts performance (slightly). The performance drop is less dramatic when evaluating on these 73 classes because these classes have less label noise for large sized objects as compared to the 1000 visual concepts. We also train a noise-free reference model using the groundtruth visual labels from the detection dataset (i.e., the true values of the latent z labels). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.4">Importance of conditioning on input images</head><p>A central point of this paper (and also in <ref type="bibr" target="#b2">[3]</ref>) is that humancentric label noise is statistically dependent on image data.</p><p>Here we demonstrate that our model is indeed improved by conditioning the noise (relevance) distribution on the input image, in contrast to previous work <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b46">47]</ref> that estimates noise parameters without conditioning on the image. To better understand the importance of this conditioning, we consider a model akin to <ref type="bibr" target="#b46">[47]</ref>. We estimate the latent distribution r without conditioning on the input image, and compare it to our model that computes r conditioned on the image. <ref type="table" target="#tab_4">Table 3</ref> shows that mAP is significantly improved by conditioning on the image. When not conditioned on the image, only minor gains are achieved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Experiments on Flickr image tagging</head><p>Datasets like MS COCO are curated by searching for images with specific objects <ref type="bibr" target="#b28">[29]</ref>. In contrast, social media websites like Flickr contain much larger collections of images that are annotated with user-generated content such as tags, keywords, and descriptions. The words found in such data exhibit the human-centric annotation properties modeled by our approach. We test our model on this "real world" data by using a random subset of ∼89k images from the YFCC100M dataset <ref type="bibr" target="#b48">[49]</ref>. We ensure that these images have at least 5 and at most 30 human annotated tags that are present in the  <ref type="figure">Figure 5</ref>: Our model modifies visually correct detections to conform to human labeling. We show this modification for a few images of target visual concepts in the MS COCO Captions dataset. We first show the variation between h (y axis) and v values (x axis) for each concept in a 2D histogram. After thresholding at v ≥ 0.8, we pick a representative image from each quantile of h (h increases from left to right). As you move from left to right, the model transitions from predicting that a human would not "speak" the word to predicting that a human would speak it. The human-centric h predictions of concepts depend on the image context, e.g., fence at a soccer game vs. fence between a bear and a human (first row). Our model picks up such signals to not only learn a visually correct fence predictor, but also when a fence should be mentioned.</p><p>WordNet <ref type="bibr" target="#b32">[33]</ref> lexicon. We split this dataset into 75k training images and 14k test images, and consider the top 1000 tags as the set of visual concepts. We train the baseline MILVC <ref type="bibr" target="#b11">[12]</ref> model and our model for 4 epochs following the same hyperparameters used for MS COCO training. <ref type="table" target="#tab_5">Table 4</ref> shows the numerical results of these models evaluated on the test set using the same human annotated tags. As explained in Section 4.1.2, we compare against the MILVC baseline, and a model with the same number of parameters as ours (denoted by Multiple-fc8). Our model has double the performance of the baseline MILVC model and increases mAP by 5.5 points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Interpretability of the noise model</head><p>The relevance classifier r models human labeling noise conditioned on the image. Depending on the image, it can enhance or suppress the visual prediction for each concept. We show such modifications for a few visual concepts in </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Correcting error modes by decoupling</head><p>Modeling latent noise in human-centric annotations allows us to learn clean visual classifiers. In <ref type="figure" target="#fig_3">Figure 6</ref>,w e compare our model's visual presence v predictions with the baseline (MILVC) and show a few error modes that it corrects. Our model is able to correct error modes like misspellings (desert vs. dessert in the first row), localizes objects correctly and out of context (fridge in the second row, net in the first row, etc.) and is better at counting (zebra, banana last row). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Using word detections for caption generation</head><p>We now look at the task of automatic image caption generation and show how our model can help improve the task. We consider a basic Long Short-Term Memory (LSTM) <ref type="bibr" target="#b17">[18]</ref> network to generate captions. We use 1000 cells for the LSTM, and learn a 256 dimensional word embedding for the input words. Following <ref type="bibr" target="#b8">[9]</ref>, our vocabulary consists of words with frequency ≥ 5 in the input captions. The image features (1000 visual concept probabilities) are fed once to the LSTM as its first hidden input. We train this LSTM over all the captions in the MS COCO caption training data for 20 epochs using <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b37">38]</ref>. We use beam size of 1 for decoding. <ref type="table" target="#tab_6">Table 5</ref> shows the evaluation of the automatically generated captions using standard captioning metrics. Using the probabilities from our model shows an improvement for all evaluation metrics. Thus, modeling the human-reporting bias can help downstream applications that require such human-centric predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion</head><p>We have introduced an algorithm that explicitly models reporting bias -the discrepancy between what exists and what people mention -for image labeling. By introducing a latent variable to capture "what is in an image" separate from "what is labeled in an image", we leverage human-centric annotations of images to their full potential, inferring visual concepts present in an image separately from the visual concepts worth mentioning. We demonstrate performance improvements over previous work on several tasks, including image classification and image captioning. Further, the proposed model is highly interpretable, capturing which concepts may be included or excluded based on the context and dependencies across visual concepts. Initial inspection of the model's predictions suggests consistency with psycholinguistic research on object description, with typical properties noticed but not mentioned.</p><p>The algorithm and techniques discussed here pave the way for new deep learning methods that decouple human performance from algorithmic understanding, modeling both jointly in a network that can be trained end-to-end. Future work may explore different methods to incorporate constraints on the latent variables, or to estimate their posteriors (such as with EM). Finally, to fully exploit the enormous amounts of data which exist "in the wild", algorithms that explicitly handle noisy data are essential.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>a) A woman standing next to a bicycle with basket. (b) A city street filled with lots of people walking in the rain. (d) A store display that has a lot of bananas on sale. (c) A yellow Vespa parked in a lot with other cars. Human descriptions capture only some of the visual concepts present in an image. For instance, the bicycle in (a) is described, while the bicycle in (b) is not mentioned.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>We use the MS COCO dataset to display the objects with the highest reporting bias. We use the detection labels for objects and see whether they are mentioned in the caption. The black line shows the probability of an object not being mentioned in the caption. The distribution of sizes for objects that are missed are shown by the color bars. Note that for many categories most unreported objects are small or medium in size (green and blue).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>Our model learns clean visual predictors from noisy labels. Here we show corrected false positives: MILVC incorrectly reports a high probability (h ≥ 0.75) for the concept, while our model correctly reports a low probability (v ≤ 0.3); and corrected false negatives: MILVC incorrectly reports a low probability (h ≤ 0.3) for the concept, while our model correctly reports a high probability (v ≥ 0.75). For example, consider zebra vs. zebras, and banana vs. bananas in the last row, where our model correctly "counts" compared to the baseline. Images are from the MS COCO Captions dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 1 :</head><label>1</label><figDesc>mAP and PHR values on MS COCO captions ground truth (20k test images). We add our latent model to each baseline to make predictions that are visually grounded (v) or conform to human (h) labels. POS tags are as follows: Nouns (NN), Verbs (VB), Adjectives (JJ), Determiners (DT), Pronouns (PRP), Prepositions (IN).</figDesc><table>Mean Average Precision 
Precision at Human Recall 

NN VB JJ DT PRP IN Others All 
NN VB JJ DT PRP IN Others All 
Prob 616 176 119 10 
11 
38 
30 
1000 ← Count 

VGG16 
MILVC [12] 
-
41.6 20.7 23.9 33.4 20.4 22.5 16.3 34.0 
52.7 32.8 40.5 40.3 32.2 33.0 24.6 45.8 
MILVC + Multiple-fc8 
-
41.1 20.9 23.7 33.6 21.1 22.8 16.8 33.8 
51.2 32.6 40.8 41.1 31.7 33.5 27.3 45.0 
MILVC + Latent (Ours) 
v 
42.9 21.7 24.9 33.1 19.6 23.0 16.2 35.1 
53.6 35.4 43.3 41.3 28.0 36.0 24.4 47.2 
MILVC + Latent (Ours) 
h 44.3 22.3 25.8 34.4 21.8 23.6 17.3 36.3 
55.5 36.3 44.7 42.9 32.1 37.3 26.4 48.9 

AlexNet 
MILVC [12] 
-
33.2 16.2 20.1 30.9 16.4 19.9 14.6 27.4 
40.0 26.4 36.0 38.2 24.2 27.5 21.9 35.9 
MILVC + Latent (Ours) 
v 
35.6 17.7 21.9 32.4 16.9 20.7 15.2 29.4 
43.9 28.3 37.5 41.2 29.2 29.9 23.3 39.0 
MILVC + Latent (Ours) 
h 36.5 18.0 22.4 32.9 17.8 21.4 15.6 30.1 
45.1 28.7 38.0 41.2 32.2 31.0 24.0 40.0 

VGG16 
Classif. 
-
34.9 18.1 20.5 32.8 19.2 21.8 16.3 29.0 
42.5 30.4 33.9 40.5 30.4 30.7 23.8 38.2 
Classif. + Multiple-fc8 
-
34.2 17.7 19.9 32.6 19.0 21.5 15.9 28.4 
41.3 27.9 32.3 39.6 29.6 31.2 22.6 36.8 
Classif. + Latent (Ours) 
v 
37.7 19.6 22.0 32.6 20.2 22.0 16.3 31.2 
46.3 32.9 36.8 38.9 32.3 33.1 27.0 41.5 
Classif. + Latent (Ours) 
h 38.7 20.1 22.6 33.8 21.2 23.0 17.5 32.0 
47.8 33.7 37.9 42.5 34.2 34.4 29.0 42.9 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc>Visual classification on 73 classes evaluated using fully labeled data from COCO.</figDesc><table>MILVC [12] vh Using ground truth 

mAP 
63.7 
66.8 66.5 
76.3 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc>We show the importance of conditioning the relevance r on the input image. We measure the classification mAP on MS COCO 1k visual concepts using both the visually grounded (v) and the human-centric (h) predictions.Conditioning on the input image shows improvement over the baseline showing that human reporting bias statistically depends on the input image.</figDesc><table>w/o image 
w/ image 

MILVC [12] vh 
vh 

mAP 
34.0 
34.2 34.3 
35.1 36.3 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="true"><head>Table 4 :</head><label>4</label><figDesc>mAP values on a subset of YFCC100M. We add our latent model over the MILVC baseline to make predictions that are visually grounded (v) or that conform to human-centric (h) labels. POS tags: Nouns (NN), Verbs (VB), Adjectives (JJ), Pronouns (PRP), Prepositions (IN).</figDesc><table>Mean Average Precision 

NN VB JJ PRP IN Others All 

Count → 

Prob 791 10 148 13 23 
15 1000 

VGG16 
MILVC [12] 
-
5.7 9.2 5.2 3.8 8.8 
6.1 
5.7 
MILVC + Multiple-fc8 -
4.6 6.2 3.8 2.7 7.3 
3.1 
4.5 
MILVC + Latent (Ours) v 
9.8 15.1 8.9 8.3 12.4 12.4 
9.8 
MILVC + Latent (Ours) h 11.2 15.4 9.9 8.2 16.3 12.5 11.2 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 5 :</head><label>5</label><figDesc>LSTM captioning results on MS COCOFigure 5. After thresholding at v ≥ 0.8, we pick a representative image from each quantile of h (h increases from left to right). The variation in h values for these high confidence v ≥ 0.8 images (shown in a 2D histogram in each row) indicates that h and v have been decoupled by our model. The images show that our model captures subtle nuances in the ground truth, e.g., mention a hat worn by a cat, do not mention the color of a pumpkin, definitely mention pink sheep, etc. It automatically captures that context is important for certain objects like fence and hat, while certain attributes are worth mentioning to help distinguish objects like the orange pillow. Such connections have been shown in both vision research<ref type="bibr" target="#b2">[3]</ref> and psychology<ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b43">44]</ref>.</figDesc><table>Prob 
BLEU-4 
ROUGE 
CIDEr 

MILVC [12] 
-
27.7 
51.8 
89.7 
MILVC + Latent (Ours) 
h 
29.2 
52.4 
92.8 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments: We thank Jacob Devlin, Lucy Vanderwende, Frank Ferraro, Sean Bell, Abhinav Shrivastava, and Saurabh Gupta for helpful discussions. Devi Parikh and Dhruv Batra for their suggestions and organizing the fun 'snack times'.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning the semantics of words and pictures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Barnard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning with annotation noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Beigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">B</forename><surname>Klebanov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL-IJCNLP</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Understanding and predicting importance in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Daume</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Stratos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">What stands out in a scene? a study of human explicit saliency judgment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">N</forename><surname>Sihite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vision research</title>
		<imprint>
			<biblScope unit="volume">91</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Modeling delayed feedback in display advertising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chapelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Microsoft coco captions: Data collection and evaluation server</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.00325</idno>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Maximum likelihood from incomplete data via the em algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Dempster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Laird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Journal of the royal statistical society. Series B (methodological)</title>
		<imprint>
			<date type="published" when="1977" />
			<biblScope unit="page" from="1" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Feifei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning classifiers from only positive and unlabeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Elkan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Noto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">The PASCAL Visual Object Classes Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">From captions to visual concepts and back</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">N</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Platt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Semi-supervised learning in gigantic image collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Classification in the presence of label noise: a survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Frénay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Verleysen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NNLS</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fukushima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biological cybernetics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="193" to="202" />
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Reporting bias and knowledge extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">V</forename><surname>Durme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automated Knowledge Base Construction (AKBC) 2013: The 3rd Workshop on Knowledge Extraction</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>CIKM 2013, AKBC&apos;13</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Eye and brain: The psychology of seeing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>Gregory</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1966" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep classifiers from image tags in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Izadinia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hertzmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Community-Organized Multimodal Mining: Opportunities for Novel Solutions</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACMM</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Learning visual features from large weakly supervised data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jabri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasilache</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.02251</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Efficient noise-tolerant learning from statistical queries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kearns</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JACM</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Koolen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gatt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Goudbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Krahmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Pragmatics</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="3231" to="3250" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kronfeld</surname></persName>
		</author>
		<title level="m">Conversationally relevant descriptions. Proceedings of the 27th Annual Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Design of robust neural network classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Larsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Nonboe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hintz-Madsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">K</forename><surname>Hansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning to classify texts using positive and unlabeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Building text classifiers using positive and unlabeled examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Typicality and object reference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Van Deemter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Noise tolerance under risk minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Manwani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sastry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cybernetics</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Wordnet: a lexical database for english</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Communications of the ACM</title>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="39" to="41" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Watch and learn: Semi-supervised learning of object detectors from videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning to label aerial images from noisy data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Natarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">K</forename><surname>Ravikumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tewari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS. 2013</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A study of the effect of different types of noise on the precision of supervised learning techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">F</forename><surname>Nettleton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Orriols-Puig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fornells</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial intelligence review</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nlpcaffe</surname></persName>
		</author>
		<ptr target="http://github.com/Russell91/NLPCaffe.8" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Language and thought: Aspects of a cognitive theory of semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>Olson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological review</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6596</idno>
		<title level="m">Training deep neural networks on noisy labels with bootstrapping</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Principles of categorization. Concepts: core readings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rosch</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="189" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning internal representations by error propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">DTIC Document</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="1985" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ImageNet Large Scale Visual Recognition Challenge. IJCV</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Pragmatic versus form-based accounts of referential contrast: Evidence for effects of informativity expectations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Sedivy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of psycholinguistic research</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Constrained semisupervised learning using attributes and comparative attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<idno>ECCV. 2012. 2</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Training convolutional networks with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">On locating objects by their distinguishing features in multisensory images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics and Image Processing</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1973" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Thomee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Friedland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Elizalde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Poland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Borth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.01817</idno>
		<title level="m">The new data and new challenges in multimedia research</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Attribute dominance: What pops out? In ICCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Turakhia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Manual for the tuna corpus: Referring expressions in two domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Van Der Sluis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gatt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Van Deemter</surname></persName>
		</author>
		<idno>AUCS/TR0705</idno>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Multiple instance boosting for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Platt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Stored object knowledge and the production of referring expressions: The case of color typicality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Westerbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Koolen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Maes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Psychology</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Learning from massive noisy labeled data for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Studying relationships between human gaze, description, and computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Samaras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Zelinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Learning with local and global consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Lal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="321" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Encyclopedia of Machine Learning</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Semi-supervised learning using gaussian fields and harmonic functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="912" to="919" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
