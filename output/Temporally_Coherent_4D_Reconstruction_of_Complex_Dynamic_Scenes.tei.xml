<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Temporally coherent 4D reconstruction of complex dynamic scenes</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armin</forename><surname>Mustafa</surname></persName>
							<email>a.mustafa@surrey.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Surrey</orgName>
								<address>
									<settlement>Guildford</settlement>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hansung</forename><surname>Kim</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Surrey</orgName>
								<address>
									<settlement>Guildford</settlement>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Yves</forename><surname>Guillemaut</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Surrey</orgName>
								<address>
									<settlement>Guildford</settlement>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><forename type="middle">Hilton</forename><surname>Cvssp</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Surrey</orgName>
								<address>
									<settlement>Guildford</settlement>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Temporally coherent 4D reconstruction of complex dynamic scenes</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:47+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents an approach for reconstruction of 4D temporally coherent models of complex dynamic scenes. No prior knowledge is required of scene structure or camera calibration allowing reconstruction from multiple moving cameras. Sparse-to-dense temporal correspondence is integrated with joint multi-view segmentation and reconstruction to obtain a complete 4D representation of static and dynamic objects. Temporal coherence is exploited to overcome visual ambiguities resulting in improved reconstruction of complex scenes. Robust joint segmentation and reconstruction of dynamic objects is achieved by introducing a geodesic star convexity constraint. Comparative evaluation is performed on a variety of unstructured indoor and outdoor dynamic scenes with hand-held cameras and multiple people. This demonstrates reconstruction of complete temporally coherent 4D scene models with improved nonrigid object segmentation and shape reconstruction.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Existing reconstruction frameworks for general dynamic scenes commonly operate on a frame-by-frame basis <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b31">32]</ref> or are limited to simple scenes <ref type="bibr" target="#b14">[15]</ref>. Previous work on indoor and outdoor dynamic scene reconstruction has shown that joint segmentation and reconstruction across multiple views gives improved reconstruction <ref type="bibr" target="#b16">[17]</ref>. In this work we build on this concept exploiting temporal coherence of the scene to overcome visual ambiguities inherent in single frame reconstruction and multiple view segmentation methods for general scenes. This is illustrated in <ref type="figure" target="#fig_0">Figure 1</ref> where the resulting 4D scene reconstruction has temporally coherent labels and surface correspondence for each object.</p><p>We present a sparse-to-dense approach to estimate dense temporal correspondence and surface reconstruction for non-rigid objects. Initially sparse 3D feature points are robustly tracked from wide-baseline image correspondence using spatio-temporal information to obtain sparse temporal correspondence and reconstruction. Sparse 3D feature correspondences are used to constrain optical flow estimation to obtain an initial dense temporally consistent model of dynamic regions. The initial model is then refined using a novel optimisation framework using a geodesic star convexity constraint for simultaneous multi-view segmentation and reconstruction of non-rigid shape. The proposed approach overcomes limitations of existing methods allowing an unsupervised temporally coherent 4D reconstruction of complete models for general scenes. The scene is automatically decomposed into a set of spatio-temporally coherent objects as shown in <ref type="figure" target="#fig_0">Figure 1</ref>. The contributions are as follows:</p><p>• Temporally coherent reconstruction of complex dynamic scenes. • A framework for space-time sparse-to-dense segmentation and reconstruction. • Optimisation of dense reconstruction and segmentation using geodesic star convexity. • Robust and computationally efficient reconstruction of dynamic scenes by exploiting temporal coherence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work 2.1. Temporal multi-view reconstruction</head><p>Extensive research has been performed in multi-view reconstruction of dynamic scenes. Most existing approaches process each time frame independently due to the difficulty of simultaneously estimating temporal correspondence for non-rigid objects. Independent per-frame reconstruction can result in errors due to the inherent visual ambiguity caused by occlusion and similar object appearance for general scenes. Quantitative evaluation of state-of-the-art techniques for static object reconstruction from multiple views was presented <ref type="bibr" target="#b38">[39]</ref>. Research investigating spatio-temporal reconstruction across multiple frames <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b17">18]</ref> requires accurate initialisation, is limited to simple scenes and does not produce temporally coherent 4D models. A number of approaches that use temporal information <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b27">28]</ref> either require a large number of closely spaced cameras or bi-layer segmentation <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b24">25]</ref> as a constraint for complete reconstruction. Other approaches for reconstruction of general scenes from multiple handheld wide-baseline cameras <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b40">41]</ref> exploit prior reconstruction of the background scene to allow dynamic foreground segmentation and reconstruction. Recent approaches for spatio-temporal reconstruction of multi-view data either work on indoor studio data <ref type="bibr" target="#b34">[35]</ref> or for dynamic reconstruction of crowd sourced data <ref type="bibr" target="#b23">[24]</ref>. Methods to estimate 3D scene flow have been reported in the literature <ref type="bibr" target="#b30">[31]</ref>. However existing approaches are limited to narrow baseline correspondence for dynamic scenes. Scene flow approaches dependent on optical flow <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b3">4]</ref> require an accurate estimate for most of the pixels which fails in the case of large motion. The approach presented in this paper is for general dynamic indoor or outdoor scenes with large non-rigid motions and no prior knowledge of scene structure. Temporal correspondence and reconstruction are simultaneously estimated to produce a 4D model of the complete scene with both static and dynamic objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Multi-view video segmentation</head><p>In the field of image segmentation, approaches have been proposed to provide impressive temporally consistent video segmentation <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b44">45]</ref>. Hierarchical segmentation based on graphs was proposed in <ref type="bibr" target="#b15">[16]</ref>, directed acyclic graph were used to propose an object followed by segmentation in <ref type="bibr" target="#b44">[45]</ref> and <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b33">34]</ref> used optical flow. All of these methods work only for monocular videos. Recently a number of approaches have been proposed for multi-view foreground object segmentation by exploiting appearance similarity <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b43">44]</ref> . These approaches assume a static background and different colour distributions for the foreground and background which limits applicability for general complex scenes and non-rigid objects. To address this issue we introduce a novel method for spatio-temporal multi-view segmentation of dynamic scenes using shape constraints. Single image segmentation techniques using shape constraints provide good results for complex scene segmentation <ref type="bibr" target="#b18">[19]</ref>(convex and concave shapes), but requires manual interaction. The proposed approach performs multi-view video segmentation by initializing the foreground object model using spatiotemporal information from wide-baseline feature correspondence followed by a multi-layer optimization framework using geodesic star convexity to constrain the segmen-tation. Our multi-view formulation naturally enforces coherent segmentation between views and also resolves ambiguities such as the similarity of background and foreground.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Joint segmentation and reconstruction</head><p>Joint segmentation and reconstruction methods simultaneously estimate multi-view segmentation or matting with reconstruction and have been shown to given improved performance for complex scenes. A number of approaches have been introduced for joint optimization. However, these are either limited to static scenes <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b19">20]</ref> or process each frame independently thereby failing to enforce temporal consistency <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b16">17]</ref>. A joint formulation for multi-view video was proposed for sports data and indoor sequences in <ref type="bibr" target="#b16">[17]</ref> and for challenging outdoor scenes in <ref type="bibr" target="#b31">[32]</ref>. Recent work proposed joint reconstruction and segmentation on monocular video achieving semantic segmentation of static scenes. Other joint segmentation and reconstruction approaches that use temporal information based on patch refinement <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b35">36]</ref> work only for rigid objects. An approach based on optical flow and graph cuts was shown to work well for non-rigid objects in indoor settings but requires silhouettes and is computationally expensive <ref type="bibr" target="#b17">[18]</ref>. Practical application of temporally coherent joint estimation requires approaches that work on non-rigid objects for general scenes in uncontrolled environments.</p><p>The proposed approach overcomes the limitations of previous methods enabling robust wide-baseline spatiotemporal reconstruction and segmentation of general scenes. Temporal correspondence is exploited to overcome visual ambiguities giving improved reconstruction together with temporally coherent 4D scene models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>This work is motivated by the limitations of existing multiple view reconstruction methods which either work independently at each frame resulting in errors due to visual ambiguity and occlusion <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b31">32]</ref>, or commonly require restrictive assumptions on scene complexity and structure <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b17">18]</ref>. We address these issues by introducing temporal coherence in the reconstruction to reduce ambiguity, ensure consistent non-rigid structure initialisation at successive frames and improve reconstruction quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Overview</head><p>A novel automatic multi-object dynamic segmentation and reconstruction method based on the geodesic starconvexity shape constraint is proposed to obtain a 4D model of the scene including both dynamic and static objects. An overview of the framework is presented in <ref type="figure">Figure 2</ref> : Sparse reconstruction: The input to the system is multiple view wide-baseline video with known camera intrinsics. Extrinsic parameters are calibrated automatically <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b22">23]</ref> using sparse wide-baseline feature matching. Segmentation-based feature detection (SFD) <ref type="bibr" target="#b32">[33]</ref> is used to <ref type="bibr">Figure 2</ref>. Temporally consistent scene reconstruction framework obtain a relatively large number of sparse features suitable for wide-baseline matching which are distributed throughout the scene including on dynamic objects such as people. SFD features are matched between views using a SIFT descriptor giving a sparse 3D point-cloud and camera extrinsics for each time instant. The sparse point cloud is clustered in 3D <ref type="bibr" target="#b37">[38]</ref> with each cluster representing a unique foreground object. Objects with insufficient detected features are reconstructed as part of the scene background. Initial dense complete scene reconstruction: Sparse reconstruction at each time instant is clustered in 3D <ref type="bibr" target="#b37">[38]</ref> to obtain an initial coarse object segmentation. Delaunay triangulation <ref type="bibr" target="#b12">[13]</ref> is performed on the set of back projected sparse features for each object in the camera image plane with best visibility. This is propagated to the other views using the sparse feature matching to obtain an initial object reconstruction. This reconstruction is refined using the framework explained in Section 3.3 to obtain segmentation and dense reconstruction of each object. Accurate reconstruction of the background object is often challenging due to the lack of features, repetitive texture, occlusion, textureless regions and relatively narrow baseline for distant objects. Hence we create a rough geometric proxy of the background by computing the minimum oriented bounding box for the sparse 3D point cloud using principal component analysis (PCA) <ref type="bibr" target="#b9">[10]</ref>. The dense reconstruction of the foreground objects and background are combined to obtain a full scene reconstruction at the first time instant. For consecutive time instants only dynamic objects are reconstructed with the segmentation and reconstruction of static objects retained which reduces computational complexity. Temporally coherent reconstruction of dynamic objects: Dynamic object regions are detected at each time instant by sparse temporal correspondence of SFD features at successive frames. Sparse temporal feature correspondence allows propagation of the dense reconstruction for each dynamic object to obtain an initial approximation (Section 3.2). The initial estimate is refined using a joint optimisation of segmentation and reconstruction based on geodesic star convexity (Section 3.3). A single 3D model for each dynamic object is obtained by fusion of the view-dependent depth maps using Poisson surface reconstruction <ref type="bibr" target="#b25">[26]</ref>. Subsequent sections present the novel contributions of this work in identifying the dynamic points, initialisation using space-time information and refinement using geodesic star convexity to obtain a dense reconstruction. The approach is demonstrated to outperform state-of-the-art dynamic scene reconstruction and gives a temporally coherent 4D model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Initial temporally coherent reconstruction</head><p>Once the static scene reconstruction is obtained for the first frame, we perform temporally coherent dynamic scene reconstruction at successive time instants. Dynamic regions are identified using temporal correspondence of sparse 3D features. These points are used to obtain an initial dense model for the dynamic objects using optical flow. The initial coarse reconstruction for each dynamic region is refined in the subsequent optimization step with respect to each camera view. Dynamic scene objects are identified from the temporal correspondence of sparse feature points. Sparse correspondence is then used to propagate an initial model of the moving object for refinement. <ref type="figure" target="#fig_1">Figure 3</ref> presents the sparse reconstruction and temporal correspondence.</p><p>Sparse temporal dynamic feature tracking: Numerous approaches have been proposed to track moving objects in 2D using either features or optical flow. However these methods may fail in the case of occlusion, movement parallel to the view direction, large motions and moving cameras. To overcome these limitations we match the sparse 3D feature points obtained using SFD from multiple widebaseline views at each time instant. The use of sparse 3D features is robust to large non-rigid motion, occlusions and camera movement. SFD <ref type="bibr" target="#b32">[33]</ref> detects sparse features which are stable across wide-baseline views and consecutive time instants for a moving camera and dynamic scene. Sparse 3D feature matches between consecutive time instants are  back-projected to each view. These features are matched temporally using a SIFT descriptor to identify the moving points. Robust matching is achieved by enforcing multiple view consistency for the temporal feature correspondence in each view as illustrated in <ref type="figure">Figure 4</ref>. Each match must satisfy the constraint:</p><formula xml:id="formula_0">d t,v (p) + u t,v+1 (p + d t,v (p)) − u t,v (p)− d t,v+1 (p + u t,v (p)) &lt; ǫ where p is the feature image point in view v at frame t, d t,v (p) is the disparity at frame t from view v to v + 1, u t,v (p)</formula><p>is the temporal correspondence from frames t to t + 1 for view v. The multi-view consistency check ensures that correspondences between any two views remain temporally consistent for successive frames. Matches in the 2D domain are sensitive to camera movement and occlusion, hence we map the set of refined matches into 3D to make the system robust to camera motion. The Frobenius norm is applied on the 3D point gradients in all directions <ref type="bibr" target="#b44">[45]</ref> to obtain the 'net' motion at each sparse point. The 'net' motion between pairs of 3D points for consecutive time instants are ranked, and the top and bottom 5 percentile values removed. Median filtering is then applied to identify the dynamic features. <ref type="figure">Figure 5</ref> shows an example with moving cameras.</p><p>Sparse-to-dense model reconstruction: Dynamic 3D feature points are used to initialize the segmentation and reconstruction of the initial model. This avoids the assumption of static backgrounds and prior scene segmentation commonly used to initialise multiple view reconstruction with a coarse visual-hull approximation <ref type="bibr" target="#b16">[17]</ref>. Temporal coherence also provides a more accurate initialisation to overcome visual ambiguities at individual frames. <ref type="figure">Figure 6</ref> illustrates the use of temporal coherence for reconstruction initialisation and refinement. Dynamic feature correspondence is used to identify the mesh for each dynamic object. This mesh is back projected on each view to obtain the region of interest.</p><p>Optical flow <ref type="bibr" target="#b4">[5]</ref> is performed on the projected mask for each view in the temporal domain using the dynamic feature correspondences over time as initialization. Dense multi-view wide-baseline correspondences from the previous frame are propagated to the current frame using the information from the flow vectors to obtain dense multi-view matches in the current frame. The matches are triangulated in 3D to obtain a refined 3D dense model of the dynamic object for the current frame. For dynamic scenes, a new object may enter the scene or a new part may appear as the object moves. To allow the introduction of new objects and object parts we also use information from the cluster of sparse points for each dynamic object. The cluster corresponding to the dynamic features is identified and static points are removed. This ensures that the set of new points not only contain the dynamic features but also the unprocessed points which represent new parts of the object. These points are added to the refined sparse model of the dynamic object. To handle the new objects we detect new clusters at each time instant and consider them as dynamic regions. Once we have a set of dense 3D points for each dynamic object, Poisson surface reconstruction is performed on the set of sparse points to obtain an initial coarse model of each dynamic region R, which is subsequently refined using the optimization framework (Section 3.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Temporally coherent dense reconstruction</head><p>The initial reconstruction and segmentation from dense temporal feature correspondence is refined using a joint optimization framework. A novel shape constraint is introduced based on geodesic star convexity which has previ- <ref type="figure">Figure 6</ref>. Initial sparse-to-dense model reconstruction workflow ously been shown to give improved performance in interactive image segmentation for structures with fine details (for example a persons fingers or hair) <ref type="bibr" target="#b18">[19]</ref>. In this work the shape constraint is automatically initialised for each view from the initial segmentation. The geodesic star-convexity is integrated as a constraint on the energy minimisation for joint multi-view reconstruction and segmentation <ref type="bibr" target="#b16">[17]</ref>. The shape constraint is based on the geodesic distance with foreground object initialisation (seeds) as star centres to which the object shape is restricted. The union formed by multiple object seeds form a geodesic forest. This allows complex shapes to be segmented. In this work to automatically initialize the segmentation we use the sparse temporal feature correspondence as star centers (seeds) to build a geodesic forest automatically. The region outside the initial coarse reconstruction of all dynamic objects is initialized as the background seed for segmentation as shown in in <ref type="figure" target="#fig_3">Figure 7</ref>. The shape of the dynamic object is restricted by this geodesic distance constraint that depends on the image gradient. Comparison with existing methods for multiview segmentation demonstrates improvements in recovery of fine detail structure as illustrated in <ref type="figure" target="#fig_3">Figure 7</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Optimization based on geodesic star convexity</head><p>The depth of the initial coarse reconstruction estimate is refined for each dynamic object at a per pixel level. Our goal is to assign an accurate depth value from a set of depth values D = d 1 , ..., d |D|−1 , U and assign a layer label from a set of label values L = l 1 , ..., l |L | to each pixel p for the region R of each dynamic object. Each d i is obtained by sampling the optical ray from the camera and U is an unknown depth value to handle occlusions. This is achieved by optimisation of a joint cost function <ref type="bibr" target="#b16">[17]</ref> for label (segmentation) and depth (reconstruction):</p><formula xml:id="formula_1">E(l, d) = λ data E data (d) + λ contrast E contrast (l)+ λ smooth E smooth (l, d) + λ color E color (l)<label>(1)</label></formula><p>where, d is the depth at each pixel, l is the layer label for multiple objects and the cost function terms are defined in section 3.3.2. This is solved subject to a geodesic starconvexity constraint on the labels l. A label l is star convex with respect to center c i , if every point p ∈ l is visible to a star center c i in set C = {c 1 , ..., c n } via l in the image x, where n is the number of star centers <ref type="bibr" target="#b18">[19]</ref>. This is expressed as an energy cost:</p><formula xml:id="formula_2">E ⋆ (l|x, C ) = p∈R q∈Γc,p E ⋆ p,q (l p , l q ) (2) ∀q ∈ Γ c,p , E ⋆ p,q = ∞ if l p = l q 0 otherwise (3)</formula><p>where ∀p ∈ R : p ∈ l ⇔ l p = 1 and Γ c,p is the geodesic path joining p to any star center in set C given by: where P c,p denotes the set of all discrete paths between c and p and L (Γ) is the length of discrete geodesic path as defined in <ref type="bibr" target="#b18">[19]</ref>. In our case we define the temporal sparse feature correspondences as star centers, hence the segmentation will include all the points which are visible to these sparse features via geodesic distances in the region R, thereby employing the shape constraint. Since the star centers are selected automatically, the method is unsupervised. The energy in the Eq. 1 is minimized as follows:</p><formula xml:id="formula_3">Γ c,p = arg min Γ∈Pc,p L (Γ)<label>(4)</label></formula><formula xml:id="formula_4">min (l,d) s.t. E(l, d) lǫS ⋆ (C ) ⇔ min (l,d) E(l, d) + E ⋆ (l|x, C ) (5)</formula><p>where S ⋆ (C ) is the set of all shapes which lie within the geodesic distances wrt to the centers in C . Optimization of eq. 5, subject to each pixel p in the region R being at a geodesic distance from the star centers in the set C , is performed using the α-expansion algorithm for a pixel p by iterating through the set of labels in L × D <ref type="bibr" target="#b6">[7]</ref>. Graph-cut is used to obtain a local optimum <ref type="bibr" target="#b5">[6]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Energy cost function</head><p>For completeness in this section we define each of the terms in eq. 1, these are based on previous terms used for joint optimisation over depth for each pixel introduced in <ref type="bibr" target="#b31">[32]</ref>, with modification of the color matching term to improve robustness and extension to multiple labels. Matching term: The data term for matching between views is specified as a measure of photo-consistency as follows:</p><formula xml:id="formula_5">E data (d) = p∈P e data (p, d p ) = M (p, q) = i∈O k m(p, q), if d p = U M U , if d p = U<label>(6)</label></formula><p>where P is the 4-connected neighbourhood of pixel p, M U is the fixed cost of labelling a pixel unknown and q denotes the projection of the hypothesised point P in an auxiliary camera where P is a 3D point along the optical ray passing through pixel p located at a distance d p from the reference camera. O k is the set of k most photo-consistent pairs with reference camera and m(p, q) is inspired from <ref type="bibr" target="#b21">[22]</ref>. Contrast term: The contrast term is as follows: E contrast (l) = p,q∈N e contrast (p, q, l p , l q ) <ref type="figure">Figure 8</ref>. Comparison of segmentation on benchmark static datasets using geodesic star-convexity. e contrast (p, q, l p , l q ) = 0, if (l p = l q ) 1 1+ǫ (ǫ + exp −C(p,q) ), otherwise (8) Smoothness term: This term is defined as:</p><formula xml:id="formula_7">E smooth (l, d) = (p,q)∈N e smooth (l p , d p , l q , d q ) (9) e smooth (l p , d p , l q , d q ) = min(|d p − d q | , d max ), if l p = l q and d p , d q = U 0,</formula><p>if l p = l q and d p , d q = U d max , otherwise (10) d max is set to 50 times the size of the depth sampling step defined in Section 3.3.1 for all datasets. Color term: This term is computed using the negative log likelihood <ref type="bibr" target="#b5">[6]</ref> of the color models learned from the foreground and background markers. The star centers obtained from the sparse 3D features are foreground markers and for background markers we consider the region outside the projected initial coarse reconstruction for each view. The color models use GMMs with 5 components each for FG/BG mixed with uniform color models <ref type="bibr" target="#b8">[9]</ref> as the markers are sparse.</p><formula xml:id="formula_8">E color (l) = p∈P −logP (I p |l p )<label>(11)</label></formula><p>where P (I p |l p = l i ) denotes the probability at pixel p in the reference image belonging to layer l i .  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results and Performance Evaluation</head><p>The proposed system is tested on publicly available multi-view research datasets of indoor and outdoor scenes: static data for segmentation comparison Couch, Chair and Car <ref type="bibr" target="#b26">[27]</ref>; and dynamic data for full evaluation Dance2 <ref type="bibr" target="#b0">[1]</ref>, Office 1 , Dance1 1 , Odzemok 1 , Magician and Juggler <ref type="bibr" target="#b2">[3]</ref>. Dance1, Dance2 and Office are captured from 8 static cameras, Odzemok from 6 static and 2 moving cameras and Magician and Juggler from 6 moving handheld cameras. More information is available on the website 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Multi-view segmentation evaluation</head><p>Segmentation is evaluated against the state-of-the-art methods for multi-view segmentation Kowdle <ref type="bibr" target="#b26">[27]</ref> and Djelouah <ref type="bibr" target="#b10">[11]</ref> for static scenes and joint segmentation reconstruction per frame Mustafa <ref type="bibr" target="#b31">[32]</ref> and using temporal information Guillemaut <ref type="bibr" target="#b17">[18]</ref> for both static and dynamic scenes. For static multi-view data the segmentation is initialised as detailed in Section 3.1 followed by refinement using the constrained optimisation Section 3.3. For dynamic scenes the full pipeline with temporal coherence is used as detailed in 3. Ground-truth is obtained by manually labelling the foreground for Office, Dance1 and Odzemok dataset, and for other datasets ground-truth is available online. We initialize all approaches by the same proposed initial coarse reconstruction for fair comparison. To evaluate the segmentation we measure completeness as the ratio of intersection to union with ground-truth <ref type="bibr" target="#b26">[27]</ref>. Comparisons are shown in <ref type="table">Table 1</ref> and <ref type="figure">Figure 8</ref> and 9 for static benchmark datasets and in <ref type="table">Table 3</ref> and <ref type="figure" target="#fig_0">Figure 10</ref> and 11 for dynamic scenes. Results for multi-view segmentation of static scenes are more accurate than Djelouah, Mustafa and Guillemaut and comparable to Kowdle with improved segmentation of some detail such as the back of the chair. For dynamic scenes the geodesic star convexity based optimization together with temporal consistency gives improved segmentation of fine detail such as the legs of the table in the Office dataset and limbs of the person in the Juggler, Magician and Dance2 datasets in <ref type="figure" target="#fig_0">Figure 10</ref> and 11. This overcomes limitations of previous multi-view perframe segmentation.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Reconstruction evaluation</head><p>Reconstruction results obtained using the proposed method with parameters defined in <ref type="table" target="#tab_0">Table 2</ref> are compared against Mustafa <ref type="bibr" target="#b31">[32]</ref>, Guillemaut <ref type="bibr" target="#b17">[18]</ref>, and Furukawa <ref type="bibr" target="#b13">[14]</ref> for dynamic sequences. Furukawa <ref type="bibr" target="#b13">[14]</ref> is a per-frame multiview wide-baseline stereo approach which ranks highly on the middlebury benchmark <ref type="bibr" target="#b38">[39]</ref> but does not refine the segmentation. <ref type="figure" target="#fig_0">Figure 12</ref> and 13 present qualitative and quantitative comparison of our method with the state-of-the-art approaches. Comparison of reconstructions demonstrates that the proposed method gives consistently more complete and accurate models. The colour maps highlight the quantitative differences in reconstruction. As far as we are aware no ground-truth data exist for dynamic scene reconstruc-  tion from real multi-view video. In <ref type="figure" target="#fig_0">Figure 13</ref> we present a comparison with the reference mesh available with the Dance2 dataset reconstructed using a visual-hull approach. This comparison demonstrates improved reconstruction of fine detail with the proposed technique.</p><p>In contrast to all previous approaches the proposed method gives temporally coherent 4D model reconstructions with dense surface correspondence over time. The introduction of temporal coherence constrains the reconstruction in regions which are ambiguous on a particular frame such as the right leg of the juggler in <ref type="figure" target="#fig_0">Figure 12</ref> resulting in more complete shape. <ref type="figure" target="#fig_0">Figure 14</ref> shows three complete scene reconstructions with 4D models of multiple objects. The Juggler and Magician sequences are reconstructed from moving hand-held cameras.</p><p>Computation times for the proposed approach vs other methods are presented in <ref type="table" target="#tab_3">Table 4</ref>. The proposed approach to reconstruct temporally coherent 4D models is compa-rable in computation time to per-frame multiple view reconstruction and gives a ∼50% reduction in computation cost compared to previous joint segmentation and reconstruction approaches using a known background. This efficiency is achieved through improved per-frame initialisation based on temporal propagation and the introduction of the geodesic star constraint in joint optimisation. Further results can be found in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>This paper present a framework for temporally coherent 4D model reconstruction of dynamic scenes from a set of wide-baseline moving cameras. The approach gives a complete model of all static and dynamic non-rigid objects in the scene. Temporal coherence for dynamic objects addresses limitations of previous per-frame reconstruction giving improved reconstruction and segmentation together with dense temporal surface correspondence for dynamic objects. A sparse-to-dense approach is introduced to establish temporal correspondence for non-rigid objects using robust sparse feature matching to initialise dense optical flow providing an initial segmentation and reconstruction. Joint refinement of object reconstruction and segmentation is then performed using a multiple view optimisation with a novel geodesic star convexity constraint that gives improved shape estimation and is computationally efficient. Comparison against state-of-the-art techniques for multiple view segmentation and reconstruction demonstrates significant improvement in performance for complex scenes. The approach enables reconstruction of 4D models for complex scenes which has not been demonstrated previously. Limitations: As with previous dynamic scene reconstruction methods the proposed approach has a number of limitations: persistent ambiguities in appearance between objects will degrade the improvement achieved with temporal coherence; scenes with a large number of inter-occluding dynamic objects will degrade performance; the approach requires sufficient wide-baseline views to cover the scene.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Temporally consistent scene reconstruction for Odzemok dataset colour-coded to show the obtained scene segmentation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Sparse temporal dynamic feature tracking algorithm: Results on two datasets; Min and Max is the minimum and maximum movement in the 3D points respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .Figure 5 .</head><label>45</label><figDesc>Spatio-temporal consistency check for 3D tracking Sparse dynamic feature tracking for Juggler dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 7 .</head><label>7</label><figDesc>Geodesic star convexity: A region R with star centers C connected with geodesic distance Γc,p. Segmentation results with and without geodesic star convexity based optimization are shown on the right for the Juggler dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 9 .</head><label>9</label><figDesc>Comparison of segmentation with Kowdle.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 10 .</head><label>10</label><figDesc>Segmentation results for dynamic scenes (Error against ground-truth is highlighted in red).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 11 .</head><label>11</label><figDesc>Segmentation results for dynamic scenes on sequence of frames (Error against ground-truth is highlighted in red).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 12 .</head><label>12</label><figDesc>Reconstruction result mesh comparison</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 2 .</head><label>2</label><figDesc>Parameters used for all datasets: λc represents λcontrast</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>Table 3. Dynamic scene segmentation completeness in %</figDesc><table>Dataset 
Guillemaut 
Mustafa 
Proposed 
Magician 68.0 ± 0.7 88.7 ± 0.5 91.2 ± 0.2 
Juggler 
84.6 ± 0.6 87.9 ± 0.6 93.3 ± 0.2 
Odzemok 90.1 ± 0.3 89.9 ± 0.3 91.8 ± 0.2 
Dance1 
99.2 ± 0.5 99.4 ± 0.2 99.5 ± 0.2 
Office 
99.3 ± 0.4 99.0 ± 0.3 99.4 ± 0.2 
Dance2 
98.6 ± 0.3 99.0 ± 0.2 99.0 ± 0.2 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head></head><label></label><figDesc>Dataset Number of ViewsTable 1. Static segmentation completeness comparison with existing methods on benchmark datasetsFigure 13. Reconstruction result comparison with reference mesh and proposed for Dance2 benchmark datasetFigure 14. Complete scene reconstruction with 4D mesh sequence.</figDesc><table>Kowdle 
Djelouah 
Guillemaut 
Mustafa 
Proposed 
Couch 
11 
99.6 ± 0.1 99.0 ± 0.2 97.0 ± 0.3 98.5 ± 0.2 99.7 ± 0.3 
Chair 
18 
99.2 ± 0.4 98.6 ± 0.3 97.9 ± 0.5 98.0 ± 0.5 99.1 ± 0.3 
Car 
44 
98.0 ± 0.7 97.0 ± 0.8 95.0 ± 0.7 97.6 ± 0.3 98.6 ± 0.4 

Dataset 
Furukawa Guillemaut Mustafa Ours 
Dance1 
326 s 
493 s 
295 s 
254 s 
Magician 
311 s 
608 s 
377 s 
325 s 
Odzemok 
381 s 
598 s 
394 s 
363 s 
Office 
339 s 
533 s 
347 s 
291 s 
Juggler 
394 s 
634 s 
411 s 
378 s 
Dance2 
312 s 
432 s 
323 s 
278 s 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 4 .</head><label>4</label><figDesc>Comparison of computational efficiency for dynamic datasets (time in seconds (s))</figDesc><table></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">http://cvssp.org/data/ 2 http://cvssp.org/projects/4d/4DRecon/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<ptr target="http://4drepository.inrialpes.fr/" />
		<title level="m">Institut national de recherche en informatique et en automatique</title>
		<meeting><address><addrLine>IN-RIA) Rhone Alpes. 6</addrLine></address></meeting>
		<imprint/>
	</monogr>
	<note>4d repository</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Flow fields: Dense correspondence fields for highly accurate large displacement optical flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bailer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Taetz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Stricker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Unstructured video-based rendering: Interactive exploration of casually captured videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ballan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Puwein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. on Graph</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multi-view scene flow estimation: A view centered variational approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Basha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Moses</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kiryati</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1506" to="1513" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Pyramidal implementation of the lucas kanade feature tracker. Intel Corporation, Microprocessor Research Labs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bouguet</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">An experimental comparison of min-cut/max-flow algorithms for energy minimization in vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Boykov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kolmogorov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>PAMI</publisher>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Fast approximate energy minimization via graph cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Boykov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Veksler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zabih</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<publisher>PAMI</publisher>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="1222" to="1239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Automatic 3d object segmentation in multiple views using volumetric graph-cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Vogiatzis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hernndez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="14" to="25" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Semiautomatic segmentation with compact shape prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Veksler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Zavadsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Boykov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="206" to="219" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">On the bounding boxes obtained by principal component analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dimitrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Knauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rote</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multi-view object segmentation in space and time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Djelouah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-S</forename><surname>Franco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Boyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">Le</forename><surname>Clerc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Sparse multi-view consistency for object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Djelouah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-S</forename><surname>Franco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Boyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">Le</forename><surname>Clerc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perez</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>PAMI</publisher>
			<biblScope unit="page" from="1" to="1" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Handbook of discrete and computational geometry. chapter Voronoi Diagrams and Delaunay Triangulations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fortune</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="page" from="377" to="388" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Accurate, dense, and robust multiview stereopsis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Furukawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1362" to="1376" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Space-time isosurface evolution for temporally coherent 3d reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Goldluecke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Magnor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="350" to="355" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Efficient hierarchical graph based video segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Grundmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kwatra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Essa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Joint Multi-Layer Segmentation and Reconstruction for Free-Viewpoint Video Applications. IJCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Guillemaut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hilton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="page" from="73" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Space-time joint multi-layer segmentation and depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Guillemaut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hilton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3DIMPVT</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Geodesic star convexity for interactive image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Gulshan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Joint 3d scene reconstruction and class segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Angst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="97" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Multiple View Geometry in Computer Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
	<note>2 edition</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">A quantitative evaluation of confidence measures for stereo vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mordohai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>PAMI</publisher>
			<biblScope unit="page" from="2121" to="2133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Calibration of nodal and free-moving cameras in dynamic scenes for postproduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Imre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Guillemaut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hilton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3DIMPVT</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="260" to="267" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">3d reconstruction of dynamic textures in crowd sourced data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Dunn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Frahm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">8689</biblScope>
			<biblScope unit="page" from="143" to="158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">3d reconstruction of dynamic scenes with multiple handheld cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="601" to="615" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Poisson surface reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kazhdan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bolitho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hoppe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eurographics Symposium on Geometry Processing</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="61" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Multiple view object cosegmentation using appearance and stereo cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kowdle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Temporally consistent reconstruction from multiple video streams using enhanced belief propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Larsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mordohai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fuchs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Silhouette segmentation in multiple views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Boyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1429" to="1441" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A new multiview spacetime-consistent depth recovery framework for free viewpoint video rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1570" to="1577" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Object scene flow for autonomous vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Menze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">General dynamic scene reconstruction from wide-baseline views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mustafa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guillemaut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hilton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Segmentation based features for wide-baseline multi-view reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mustafa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Imre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hilton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3DV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Coherent motion segmentation in moving camera videos using optical flow orientations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Narayana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hanson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1577" to="1584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Generalized connectivity constraints for spatio-temporal 3d reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oswald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sthmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV 2014</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="32" to="46" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Simultaneous segmentation and 3d reconstruction of monocular image sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ozden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Fast object segmentation in unconstrained video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Papazoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1777" to="1784" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Semantic 3D Object Maps for Everyday Manipulation in Human Living Environments. PhD thesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Rusu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<pubPlace>Germany</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Computer Science department, Technische Universitaet Muenchen</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A comparison and evaluation of multi-view stereo reconstruction algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Diebel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Scharstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Multi-object reconstruction from dynamic scenes: An object-centered approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">M</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="page" from="1575" to="1588" />
		</imprint>
		<respStmt>
			<orgName>CVIU</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Modeling dynamic scenes recorded with freely moving cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Taneja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ballan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="613" to="626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Stereoscopic scene flow computation for 3d motion understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vaudrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">95</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="29" to="51" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Joint 3d scene reconstruction and class segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Silhouette extraction from multiple images of an unknown background</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Quan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Video object segmentation through spatially accurate and temporally dense extraction of primary object regions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Javed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Robust bilayer segmentation and motion/depth estimation with a handheld camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
