<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Thin-Slicing for Pose: Learning to Understand Pose without Explicit Pose Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suha</forename><surname>Kwak</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">WILLOW Project Team -Inria /École Normale Supérieure</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Cho</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">WILLOW Project Team -Inria /École Normale Supérieure</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">WILLOW Project Team -Inria /École Normale Supérieure</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Thin-Slicing for Pose: Learning to Understand Pose without Explicit Pose Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:47+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We address the problem of learning a pose-aware, compact embedding that projects images with similar human poses to be placed close-by in the embedding space. The embedding function is built on a deep convolutional network, and trained with triplet-based rank constraints on real image data. This architecture allows us to learn a robust representation that captures differences in human poses by effectively factoring out variations in clothing, background, and imaging conditions in the wild. For a variety of pose-related tasks, the proposed pose embedding provides a cost-efficient and natural alternative to explicit pose estimation, circumventing challenges of localizing body joints. We demonstrate the efficacy of the embedding on pose-based image retrieval and action recognition problems.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>There can be as much value in the blink of an eye as in months of rational analysis.</p><p>-Malcolm Gladwell</p><p>How much detail do we actually need to figure out in order to understand a scene? While the answer largely depends on the scene and a task at hand, we often make a good decision only with a thin slice of information. In fact, a decision by a few glances is sometimes no worse and even better than hours of pondering <ref type="bibr" target="#b24">[26]</ref>. Given a limited time and resource, this thin-slicing decision becomes crucial in particular. For example, drivers often need to rely on their immediate understanding of situations they pass by, otherwise running someone or themselves into danger. This paper addresses learning a thin-slicing machine for human pose, relieving the need for explicit pose estimation. While a person in a specific pose is one of the most informative objects for scene understanding, the problem of pose estimation, aiming at localizing individual body joints, still remains a challenging open problem <ref type="bibr" target="#b11">[13,</ref><ref type="bibr" target="#b41">43,</ref><ref type="bibr" target="#b48">50,</ref><ref type="bibr" target="#b49">51,</ref><ref type="bibr" target="#b59">61]</ref>. Even with a full body visible, frequent self-occlusion and huge diversity in pose make the problem hard to solve. As Similar Dissimilar <ref type="figure">Figure 1</ref>: The manifold of our pose embedding visualized using t-SNE <ref type="bibr" target="#b51">[53]</ref>. Each point represents a human pose image. To better show correlation between the pose embedding and annotated pose, we color-code pose similarities in annotation between an arbitrary target image (red box) and all the other images. Selected examples of color-coded images are illustrated in the right-hand side. Images similar with the target in annotated pose are colored in yellow, otherwise in blue. As can be seen, yellow images lie closer by the target in general, which shows that a position on the embedding space implicitly represents a human pose. (Best viewed in color.) often formulated a complicated structured inference problem <ref type="bibr" target="#b11">[13,</ref><ref type="bibr" target="#b48">50]</ref>, human pose estimation is computationally heavy in practice as well.</p><p>We argue that solving explicit pose estimation may not be necessary for many problems that only require an estimate of pose similarity. To bypass pose estimation, we learn an efficient pose embedding function based on a convolutional neural network (CNN) <ref type="bibr" target="#b16">[18,</ref><ref type="bibr" target="#b30">32]</ref> with a triplet rank loss <ref type="bibr" target="#b10">[12,</ref><ref type="bibr" target="#b42">44,</ref><ref type="bibr" target="#b54">56,</ref><ref type="bibr" target="#b55">57,</ref><ref type="bibr" target="#b57">59]</ref>. Our pose embedding, trained on real data, provides a pose-aware, compact mapping that projects images with similar human poses in same neigh-borhoods of the embedding space and far from each other otherwise. A position in the embedding space represents the corresponding pose in an implicit way, and helps in capturing pose-related semantics in the images as illustrated in <ref type="figure">Fig. 1</ref>. Even without the need for detailed labeling of pose similarity, the triplet rank loss combined with a convolutional neural network architecture enables us to learn a robust embedding space in an efficient manner.</p><p>The proposed pose embedding provides a practical alternative to explicit pose estimation in a variety of applications. Efficient pose retrieval can be performed in this embedding space to find images or video frames with similar poses in a large database. The embedding can be also used for a pose-based feature for action recognition <ref type="bibr" target="#b27">[29,</ref><ref type="bibr" target="#b28">30,</ref><ref type="bibr" target="#b52">54,</ref><ref type="bibr" target="#b58">60]</ref>, group activities <ref type="bibr" target="#b13">[15,</ref><ref type="bibr" target="#b31">33,</ref><ref type="bibr" target="#b32">34,</ref><ref type="bibr" target="#b40">42]</ref>, and human object interaction analysis <ref type="bibr" target="#b60">[62]</ref>. In experiments, we transfer our learned model to the problems of pose-based image retrieval and action recognition, and demonstrate that the proposed embedding successfully generalizes to diverse poses in cluttered scenes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Our approach to human pose embedding is related to a broad range of topics including pose estimation, pose-based action recognition, and similarity-based embedding. Here we briefly review representative work on those topics.</p><p>Pose estimation. Human pose understanding has been studied extensively during the last decade. Unlike our approach, most previous work has focused on localzing explicit body parts or joints. Many of them rely on pictorial structures of human body as a prior <ref type="bibr" target="#b3">[5,</ref><ref type="bibr" target="#b17">19,</ref><ref type="bibr" target="#b21">23,</ref><ref type="bibr" target="#b56">58,</ref><ref type="bibr" target="#b59">61]</ref>. It is shown in <ref type="bibr" target="#b53">[55]</ref> that the pictorial structure can be learned from data. Multi-modality of pose is captured at larger granularities and estimated jointly with body part locations in <ref type="bibr" target="#b41">[43]</ref>. Following the great success in image classification <ref type="bibr" target="#b30">[32,</ref><ref type="bibr" target="#b43">45,</ref><ref type="bibr" target="#b46">48]</ref>, CNN has recently become popular in pose estimation. A pioneering work on this line of research was done by Toshev and Szegedy <ref type="bibr" target="#b49">[51]</ref>, where body joint coordinates are directly estimated by a regression machine based on CNN. Chen and Yuille <ref type="bibr" target="#b11">[13]</ref> combine a graphical model with CNN, where the unary and pairwise clique potentials of the graphical model are learned by CNNs. Instead of an explicit graphical model, Tompson et al. <ref type="bibr" target="#b48">[50]</ref> learn spatial dependencies among all pairs of body joints through CNNs.</p><p>Action recognition with pose. Naturally, pose-based features have shown to be effective for action recognition in images or videos. Pose is treated as a latent variable for action recognition in still images <ref type="bibr" target="#b58">[60]</ref>, and Bao and Fei-Fei <ref type="bibr" target="#b61">[63]</ref> estimate semi-3D pose for the same purpose. For action recognition in video, estimated poses are often employed directly as descriptors <ref type="bibr" target="#b28">[30,</ref><ref type="bibr" target="#b52">54]</ref>. Cheron et al. <ref type="bibr" target="#b12">[14]</ref> use pose as a location prior for CNN-based features being extracted around body parts.</p><p>Embedding by similarity. Learning embedding by pairwise similarity has been extensively studied. The siamesetype CNN architecture was proposed to learn an embedding space that reflects a semantic distance between data <ref type="bibr" target="#b7">[9]</ref>, and has been applied to face verification <ref type="bibr" target="#b14">[16]</ref> and visual representation learning <ref type="bibr" target="#b1">[3,</ref><ref type="bibr" target="#b18">20]</ref>. For an embedding space that preserves relative similarity among triplets, Athisos et al. use AdaBoost with random embedding functions <ref type="bibr" target="#b4">[6]</ref>, and Chechik et al. propose a bi-linear similarity function with a triplet rank loss <ref type="bibr" target="#b10">[12]</ref>. The triplet rank loss has been recently adopted as a loss function of CNNs for various tasks such as image ranking <ref type="bibr" target="#b54">[56]</ref>, face identification <ref type="bibr" target="#b42">[44]</ref>, visual representation learning <ref type="bibr" target="#b55">[57]</ref>, and joint object categorization and camera pose estimation <ref type="bibr" target="#b57">[59]</ref>. Pose embedding. Low dimensional manifolds of articulated human pose have been employed for person tracking <ref type="bibr" target="#b35">[37,</ref><ref type="bibr" target="#b50">52]</ref> and 3D pose estimation from silhouette <ref type="bibr" target="#b0">[2,</ref><ref type="bibr" target="#b19">21]</ref>. Taylor et al. <ref type="bibr" target="#b47">[49]</ref> learn CNNs with neighborhood component analysis criteria <ref type="bibr" target="#b25">[27]</ref> to match images with similar poses and estimate pose by nearest neighbor regression. As a pose encoding, Pons-Moll et al. <ref type="bibr" target="#b37">[39]</ref> propose posebits, which are binary attributes about geometric relations between body parts and used for 3D pose estimation and retrieval. There is a parallel work about pose embedding <ref type="bibr" target="#b23">[25]</ref> motivated by a similar idea with ours. The most important difference of ours from <ref type="bibr" target="#b23">[25]</ref> is that in testing our method does not require any pose information while <ref type="bibr" target="#b23">[25]</ref> assumes that the pelvis location of a person is known. Also, we show how pose embedding can be adopted for action recognition in image, which is not investigated in <ref type="bibr" target="#b23">[25]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Algorithm overview</head><p>Instead of explicit pose estimation, we learn a function that maps an image of a person into an embedding space. A position of the image on the embedding space represents a corresponding pose in an implicit way such that images will be placed nearby if their poses are similar, otherwise far from each other. We design the embedding function using a CNN, and train it with a triplet rank loss <ref type="bibr" target="#b10">[12,</ref><ref type="bibr" target="#b42">44,</ref><ref type="bibr" target="#b54">56,</ref><ref type="bibr" target="#b55">57,</ref><ref type="bibr" target="#b57">59]</ref>. In this work, two separate embedding functions are learned and evaluated: (1) full-body pose embedding that takes a full body as input, and (2) upper-body pose embedding that considers only an upper part of a body as input. Both embedding functions share the same network architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Network architecture</head><p>Details of the network architecture is described in <ref type="figure" target="#fig_0">Fig. 2</ref>. In particular, the structures and parameters of the five convolution layers (i.e., conv1-5) are transferred directly from those of the VGG-S network <ref type="bibr" target="#b9">[11]</ref>, which is pre-trained for ImageNet classification task <ref type="bibr" target="#b39">[41]</ref>. It is demonstrated in <ref type="bibr" target="#b36">[38]</ref> that representation learned for a large scale image classification could be also useful in other tasks as well. The The network consists of 5 convolution layers (conv) followed by 2 fully-connected layers (fc). In details, Local Response Normalization (LRN) <ref type="bibr" target="#b30">[32]</ref> is applied after conv1, and the output of fc1 is regularized by Dropout <ref type="bibr" target="#b44">[46]</ref>. All the convolution layers and related operations (blue) are transferred directly from those of the VGG-S network <ref type="bibr" target="#b9">[11]</ref>, and their parameters are fine-tuned during training.</p><p>parameters of the convolution layers are not fixed, but finetuned during training with a smaller learning rate. We believe that the transferred representation would speed up network learning and also result in better generalization as our training dataset is not large when compared to the number of parameters.</p><p>On the pre-trained part of the network, we add two fully-connected layers (i.e., fc1-2), which are learned from scratch for adaptation to the pose embedding task. The embedding dimensionality is fixed to 128. Finally, an l 2 normalization layer is added on the top of the network so that the embedding vectors lie on a unit sphere.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Triplet rank loss</head><p>The triplet rank loss considers pose distances among a triplet of images. Let X be a set of human pose images and Y a set of pose annotations of images in X . We define a dissimilarity between two images x i , x j ∈ X as a distance between their corresponding pose annotations y i , y j ∈ Y, which is denoted by D(y i , y j ). Our goal is to learn an embedding function f such that Euclidean distances in the embedding space hold the same order relations with the pose distances:</p><formula xml:id="formula_0">||f (x i ) − f (x + i )|| 2 2 &lt; ||f (x i ) − f (x − i )|| 2 2 , ∀x i , x + i , x − i ∈ X s.t. D(y i , y + i ) &lt; D(y i , y − i ).</formula><p>(1)</p><p>Based on this, our triplet rank loss L is defined as a hinge loss with a safety margin:</p><formula xml:id="formula_1">L = N i=1 ||f (x i ) − f (x + i )|| 2 2 − ||f (x i ) − f (x − i )|| 2 2 + δ + ,<label>(2)</label></formula><p>where N is the number of triplets and δ indicates the margin. Given a set of triplets together with pairwise pose distances, the embedding function is learned by minimizing Eq. (2) through error back-propagation <ref type="bibr" target="#b34">[36,</ref><ref type="bibr" target="#b38">40]</ref> and stochastic gradient decent (SGD) with momentum <ref type="bibr" target="#b45">[47]</ref>. We describe details for learning in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Dataset compilation 4.1. Data sources</head><p>Learning a pose embedding space with the triplet loss requires images of human poses, and pose distances between them. To this end, we collect pose images with body joint annotations, and compute the pose distances using the annotated joint coordinates. Our dataset mainly built on the MPII Human Pose dataset <ref type="bibr" target="#b2">[4]</ref> that contains around 25K images with diverse human poses from YouTube videos. The images have extensive key-point annotations for body joints as well as bounding box annotations for heads. Among them, we selected 19,919 images with full body joint annotations, and included them in our dataset. Note that invisible joints are also counted as valid ones if their positions are annotated, because they would be useful to learn an embedding function robust against partial occlusions. In addition, we collected more images from the H3D <ref type="bibr" target="#b6">[8]</ref> and the VOC2009 person (trainval) <ref type="bibr" target="#b20">[22]</ref> datasets with body joint annotations <ref type="bibr" target="#b5">[7]</ref>. As most of human poses in these datasets are not fully annotated, we selected images that have at least 12 joint annotations.</p><p>In total, our dataset comprises 12,366 images for training (MPII: 10,000, H3D: 843, VOC2009 people: 1,523), and 9,919 images for validation (MPII: 9,919).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Image and pose standarization</head><p>In our CNN architecture, all images need to be standarized into a fixed-size input image (i.e., 224×224×3) that captures a target human body. The best way of obtaining such input would be to align images using a stable center of pose (e.g., pelvis), resize them with respect to a human body scale, and crop the target human body. However, this is problematic in testing because such a precise input is hard to be obtained without explicit and accurate pose estimation. Instead, we assume that the target person is localized in the form of a bounding box by a person detector in testing. To approximate this condition during training, we standarize pose images using bounding boxes of target persons, as described in <ref type="figure" target="#fig_1">Fig. 3</ref>. For full-body embedding, we find a bounding box that most tightly encloses all annotated joints, and crop the smallest square box that covers the bounding box while sharing the center position. For upper-body embedding, the cropped area is horizontally aligned using the center of the annotated head, and rescaled so that its size is proportional to that of the head bounding box 1 . We disregard annotations of ankles and knees for upper-body data. All the cropped images and their annotated joint coordinates are resized to the input size of our embedding network. The pose distance between two images is defined as the mean of Euclidean distances between standarized coordinates of corresponding joints. When the two poses do not have the same number of joint annotations, we compute the distance using only shared joints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Learning pose embedding network</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Triplet sampling</head><p>Instead of using all possible triplets in training, which is impractical due to the combinatorial explosion, we sample a set of triplets that would be useful to learn the embedding function. In particular, given an anchor image x i , we divide the other images into two sets of positive (similar) images P i = {x + i } and negative (dissimilar) images N i = {x − i }, respectively, and assemble a triplet by choosing one from each of P i and N i . Since in our dataset there is no given positive/negative distinction a prior, one straightforward way to do this would be to divide images by thresholding their pose distances from the anchor. However, due to the imbalanced distribution of poses in the dataset, some common poses will have a lot of positive images while others could not have any positive. Furthermore, the pose distance metric may be inconsistent over anchor images since we do not know exact body scales nor 3D positions of body joints. Hence, for each anchor image x i , we consider its p nearest neighbors in pose distance as P i , and other im- ages as N i . Examples of positive and negative images are illustrated in <ref type="figure" target="#fig_2">Fig. 4</ref>. Although the above sampling strategy reduces the number of combinations significantly, still there exist numerous triplets per anchor and some of them would be trivial and ineffective for learning embedding network. Hard negative mining has been commonly used to avoid such trivial triplets and boost learning procedure, especially when training images are annotated by categorical labels <ref type="bibr" target="#b42">[44,</ref><ref type="bibr" target="#b55">57,</ref><ref type="bibr" target="#b57">59]</ref>. In our case, however, it is not straightforward to sample hard negatives from N i since human poses are continuous and changes smoothly between P i and N i . In other words, hard negative images are often too close to positive images in pose distance, so it is not desirable to separate such negatives from positives in the embedding space. Instead of hard negative mining, we reduce the size of N i every epoch by removing negative images farthest from the anchor, while assuming that the farthest negative images would be trivial in general. This assumption is not necessarily true, but we found that it holds in most cases. We believe that our approach could allow the embedding networks to focus more on subtle differences between poses in later stages of training. Also, it practically improved performance when the learned embedding was applied to action recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Learning network with random triplets</head><p>Since many triplets share identical images, it is inefficient to calculate embedding vectors and gradients for each triplet individually. For learning network efficiently, we first assemble a mini-batch with an anchor image and randomly selected positive and negative images, and generate triplets by all combinations of positive and negative images in the <ref type="figure">Figure 5</ref>: The MPII validation images on the manifold of our full-body pose embedding. The 2D manifold is estimated by t-SNE <ref type="bibr" target="#b51">[53]</ref>, and the pose images are deployed by <ref type="bibr" target="#b29">[31]</ref>. The zoomed parts of the manifold show that images depicting similar poses are located in similar positions as desired, even though they are not used when learning the embedding function. More visualization results can be found in our project webpage [1].</p><p>batch. By doing this, individual images in the batch are embedded (forward-propagated) once. In this approach, multiple loss-gradients are computed for each embedding vector since an image participates in multiple triplets, so we accumulate them per embedding vector and perform backpropagation once.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experiments</head><p>In this section, we first describe implementation details, and then validate our concept empirically by qualitative analysis of our pose embedding manifold. We also demonstrate the effectiveness of our approach in three tasks: 1) pose retrieval on the MPII dataset <ref type="bibr" target="#b2">[4]</ref>, 2) action recognition on the VOC2012 Action dataset <ref type="bibr" target="#b20">[22]</ref>, and 3) action recog-nition on the People Playing Musical Instrument (PPMI) dataset <ref type="bibr" target="#b60">[62]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Implementation details</head><p>The number of positive images p per anchor is fixed to 30 for full-body pose embedding, and 15 for upper-body counterpart. The size of a mini-batch is set to 128, which consists of an anchor image, 5 randomly selected positives, and 122 randomly selected negatives. The size of N i is decreased by 3K every epoch until it becomes 1K. During training, images in the batch are translated and scaled individually up to ±10% of the input size, and the entire batch is horizontally flipped with probability 0.5. The learning rate for the fully-connected layers is initialized as 0.01, and multiplied by 0.2 every epoch. The learning rates for the pre-trained part (i.e., conv1-5) are 10 times smaller than that for the fully-connected layers. The momentum and weight decay parameters are set to 0.9 and 0.0005, respectively. Our system is implemented in Torch7 <ref type="bibr" target="#b15">[17]</ref>. We run 60K SGD iterations for learning, which took about three days on a single Nvidia TITAN Black with 6GB RAM in our experiment. Code and trained networks will be released online at [1].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Manifold visualization</head><p>We validate our pose embedding qualitatively by visualizing its low-dimensional manifold of the standarized MPII images. We first illustrate how images depicting similar human poses are distributed on the manifold in <ref type="figure">Fig. 1</ref>. As expected, images resembling each other in pose are also close to each other on the manifold in general. This means that a position in the embedding space approximates a human pose. The manifold is also visualized by deploying the images directly on it, and the result is shown in <ref type="figure">Fig. 5.</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Pose retrieval</head><p>We evaluate our full-body pose embedding network on a pose retrieval task using the MPII validation set. Among 9,919 standarized images in the validation set, the first 1,919 images are used as queries, and the remained 8K images are regarded as a test set on which retrieval is performed. We compare our full-body pose embedding with VGG-S <ref type="bibr" target="#b9">[11]</ref>, which is the base network of our embedding function, and Chen and Yuille's method <ref type="bibr" target="#b11">[13]</ref> that estimates human pose explicitly using CNN. For VGG-S, 4,096-D output of its penultimate fully-connected layer is employed as a descriptor. For Chen and Yuille's, 26 part-locations obtained by its explicit pose estimation is directly used. Note that for Chen and Yuille's, the approximate scale of person is assumed to be known in order to boost both of its speed and accuracy.</p><p>Three performance metrics are defined to evaluate retrieval results quantitatively. We first compute the mean of pose distances between queries and their K nearest neigh- bors. Note that the pose distance is the mean of Euclidean distances between standarized joint coordinates, defined in Section 4.2. We also measure retrieval performance by Hit@K rate, which counts how many queries have at least one correct image among their K nearest neighbors. We consider a retrieved image as correct when it belongs to the 50 nearest neighbors of the query in the pose distance. Finally, we design a modified version of normalized dis-counted cumulative gain (nDCG) <ref type="bibr" target="#b8">[10]</ref>:</p><formula xml:id="formula_2">nDCG K (q) = 1 Z K K i=1 2 ri log 2 (i + 1) ,<label>(3)</label></formula><p>where r i = − log 2 (||y q −y i || 2 +1) is the relevance between the query q and i th retrieval in terms of their true poses y q and y i . The relevance is reduced by the discounting factor 1/ log 2 (i + 1) to place a greater emphasis on one returned at a higher rank. nDCG K considers only top K retrievals, and Z K is for normalization so that the maximum nDCG K becomes 1. A higher nDCG means a better retrieval. Note that the modified nDCG evaluates ranking quality while the Hit@K measures classification accuracy. The quantitative evaluation results are summarized in <ref type="figure">Fig. 6</ref>. Our full-body pose embedding outperforms both of VGG-S <ref type="bibr" target="#b9">[11]</ref> and the pose estimation <ref type="bibr" target="#b11">[13]</ref> in terms of all the three metrics. Also, our approach was three orders of magnitude faster than the pose estimation on the same hardware.</p><p>Selected retrieval examples are visualized in <ref type="figure">Fig. 7</ref>. The results show that VGG-S tends to focus more on object category (cello in <ref type="figure">Fig. 7(e)</ref>) or holistic characteristics of images ( <ref type="figure">Fig. 7(a,b,g)</ref>) than pose. Although a subset of its weights is initialized by VGG-S, our network successfully captures human poses out of diverse background contexts. Our embedding often makes reasonable results even if human bodies are partially occluded <ref type="figure">(Fig. 7(c,e)</ref>), and sometimes its results look better than those by oracle <ref type="figure">(Fig. 7(d,f)</ref>). Our approach could fail when query pose is rarely observed from the training data ( <ref type="figure">Fig. 7(g)</ref>), and the explicit pose estimation performs better than ours in that case. However, it often fails to distinguish front-and back-facing poses <ref type="figure">(Fig. 7(a)</ref>) and is inaccurate when query pose is occluded <ref type="figure">(Fig. 7(c)</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.">VOC2012 Action recognition</head><p>The VOC2012 Action dataset <ref type="bibr" target="#b20">[22]</ref> consists of 10 action classes: jumping, phoning, reading, playing instrument,  computer, and walking. The dataset also contains images of others class that does not belong to any of the above classes. The number of human subjects is 6,278 in the trainval set and 6,283 in the test set. The bounding boxes of people performing actions are provided in both training and testing, and we use the boxes to standarize input images of our embedding network as described in Section 4.2.</p><p>We first evaluate our pose embedding on the trainval set to analyze its characteristics. We extract two pose embedding vectors per image: one for full-body pose, denoted by PosE(full), and the other for upper-body pose, denoted by <ref type="bibr">PosE(upper)</ref>. For classification, SVMs with RBF kernels are trained with the embedding vectors. We compute two separate RBF kernels for PosE(full) and PosE(upper), and also compute a combined one, PosE(full,upper), that simply combines the two kernels by summation. The SVM parameter and kernel bandwidths are chosen by cross-validation. The top part of <ref type="table">Table 1</ref> (1st-3rd rows) summarizes average precision (AP) scores of the classification results on the trainval set. It shows that the combination PosE(full,upper) improves over either <ref type="bibr">PosE(full)</ref> or PosE(upper) alone.</p><p>Our pose embeddings, however, are not strong enough to capture all action features beyond pose, e.g., people often perform different actions with similar poses as illustrated in <ref type="figure" target="#fig_4">Fig. 8</ref>. Such an ambiguity issue can be handled by exploiting context information in nearby objects and background. To this end, we combine our pose embedding vectors with features from ImageNet pre-trained CNNs, VGG-16 and VGG-19 <ref type="bibr" target="#b43">[45]</ref>, as follows. We apply VGG-16 to an original image (not standarized) with multiple scales in a fully convolutional manner, and obtain a 4,096-D activation vector, which is in turn l 2 normalized. In a similar manner, we extract a 4,096-D vector through VGG-19. These two vec-  tors are concatenated to obtain an 8,192-D feature vector per image. We denote it by VGG(img). The same procedure is done for the bounding box region so that another 8,196-D feature vector is calculated. We call it VGG(box). VGG(img,box) denotes the concatenation of VGG(img) and VGG(box), which is a 16,384-D feature vector. We refer mAP Oquab et al. <ref type="bibr" target="#b36">[38]</ref> 70.2 Hoai <ref type="bibr" target="#b26">[28]</ref> 76.3 Simonyan and Zisserman <ref type="bibr" target="#b43">[45]</ref> 84.0 VGG(img,box) 82.2 VGG(img,box) + PosE(full,upper) 84.2 <ref type="table">Table 3</ref>: Comparison with state of the arts in VOC Action.</p><p>readers to <ref type="bibr" target="#b43">[45]</ref> for more details about feature extraction through VGG-16 and VGG-19. For classification, we apply a linear kernel for the VGG features, and combine it with those of PosE by simple summation. The AP scores of the combinations of VGG and PosE is reported in the bottom part (4th-7th rows) of Table 1. Note that VGG(img,box) in the table means our reproduction of <ref type="bibr" target="#b43">[45]</ref>. As can be seen from the result, VGGs are improved by combination with PosE(full,upper), especially for the classes like jumping, running, and walking, where people do not interact with nearby objects thus pose is more discriminative than context. It is also worth noting that VGG(img) + PosE(full,upper) performs on par with VGG(img,box) although the representation power of PosEs is limited by the smaller network size and dimensionality compared to VGG(box). This empirically confirms that our embedding is complementary to VGGs and useful for action recognition. <ref type="table" target="#tab_1">Table 2</ref> reports APs of the combinations of VGGs and PosEs on the test set. The result shows that PosEs enhances the performance of walking class with a large margin, and also substantially improves the performance in running and taking photo classes. Our best performing model is also compared with state of the arts in VOC Action in <ref type="table">Table 3</ref>, where VGG(img,box) + PosE(full,upper) outperforms other methods with a small margin. Note that VGG(img,box), our reproduction of <ref type="bibr" target="#b43">[45]</ref>, does not achieve the same performance reported in <ref type="bibr" target="#b43">[45]</ref> probably due to implementation issues. We believe that our best model, that is VGG(img,box) + PosE(full,upper), will perform better when combined with the original VGG(img,box) features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5.">PPMI action recognition</head><p>The PPMI dataset <ref type="bibr" target="#b60">[62]</ref> contains images of people holding 7 musical instruments: bassoon, erhu, flute, French horn, guitar, saxophone, and violin. In this experiment, we consider a task of classifying whether a person is playing or not playing. Performing this task may involve recognizing a pose of persons holding the target instrument.</p><p>For each instrument and each action category, 100 training images and 100 test images are provided. Since all images in the datasets exhibit only upper-bodies, we apply the upper-body pose embedding only. Although the images are already standarized by an upper-body detector, scales and  <ref type="table">Table 4</ref>: Classification accuracy on the PPMI dataset.</p><p>positions of people in the images vary substantially. For each image, we thus crop multiple patches with different scales from the center of the image, and aggregate embedding vectors of the patches by max-pooling per dimension. We train linear SVMs with the aggregated pose embedding vectors, where the SVM hyper-parameter was estimated by cross-validation on the training dataset. The classification result is quantified and compared in <ref type="table">Table 4</ref>. Our approach, denoted by PosE, clearly outperforms the previous results on this task <ref type="bibr" target="#b22">[24,</ref><ref type="bibr" target="#b33">35,</ref><ref type="bibr" target="#b60">62]</ref> with a substantial margin even without explicitly considering instruments. Our method is also compared with a linear SVM on top of 8,192-D feature vector from VGG-16 and VGG-19, that is denoted by VGG. The feature vector is computed by the same procedure as VGG(img) of Section 6.4. VGG demonstrates a strong representation power on this task, outperforming all the others. By concatenating our pose embedding vector with VGG feature, we achieve better classification accuracies. Along with the previous experiments, this demonstrates again that our pose embedding provides a useful feature for pose-related tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>We have presented a deep embedding network for human pose, which leverages available pose annotations and learns a compact mapping from images to a pose-aware space. We have empirically shown that our pose embedding is useful for pose retrieval and action recognition, and that it further improves action recognition performance when combined with different types of features. We believe this thin-slicing type of approach to pose understanding can be an effective alternative for many challenging pose-related applications, in particular, that require cost-efficient and implicit pose information. In future, we will investigate action recognition in videos, spatio-temporal pose embedding, and more effective embedding with other types of loss functions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Our network architecture for pose embedding.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Visualization of the standarization procedure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Examples of anchor images (left), and their positive (middle) and negative images (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :Figure 7 :</head><label>67</label><figDesc>Quantitative evaluation of pose retrieval on the MPII validation set. Oracle is always 1 in (b) and (c). Qualitative examples of the 1st nearest pose.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 8 :</head><label>8</label><figDesc>Nearest neighbors of VOC Action (trainval) images in the full-body pose embedding space. If the nearest neighbors are from the same action class with the query, they are colored in blue, otherwise in red. Even when our pose embedding captures pose configurations well, the action classes of the nearest neighbors are frequently incorrect since semantically different actions can be performed from similar poses.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc>APs (%) on the test set of VOC2012 Action.</figDesc><table>riding bike, riding horse, running, taking photograph, using 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Since this procedure entails head box annotations, 2366 images from the H3D dataset and VOC2009 person datasets, which do not provide head annotation, are not used for learning upper-body embedding.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments. We thank Greg Mori and Vadim Kantorov for fruitful discussions. This work was supported in part by Google Research Award and the ERC grants Activia and VideoWorld.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">3d human pose from silhouettes by relevance vector regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning to see by moving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">2d human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Pictorial structures revisited: People detection and articulated pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Boostmap: A method for efficient approximate similarity rankings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Athitsos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Alon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kollios</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Detecting people using mutually consistent poselet activations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Poselets: Body part detectors trained using 3d human pose annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Signature verification using a &quot;siamese&quot; time delay neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bromley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sckinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning to rank using gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Burges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Shaked</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Renshaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lazier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Deeds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hullender</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Return of the devil in the details: Delving deep into convolutional nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chatfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Large scale online learning of image similarity through ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chechik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Shalit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1109" to="1135" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Articulated pose estimation by a graphical model with image dependent pairwise relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">P-CNN: Pose-based CNN Features for Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chéron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning context for collective activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shahid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning a similarity metric discriminatively, with application to face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Torch7: A matlab-like environment for machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Farabet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BigLearn, NIPS Workshop</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Handwritten digit recognition with a back-propagation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">L</forename><surname>Cun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Habbard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Jackel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Henderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Human pose estimation using body parts dependent joint regressors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dantone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Leistner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Unsupervised visual representation learning by context prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Inferring 3d body pose from silhouettes using activity manifold learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Elgammal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">The PASCAL Visual Object Classes Challenge 2012 (VOC2012) Results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<ptr target="http://www.pascal-network.org/challenges/VOC/voc2012/workshop/index.html.3" />
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Pictorial structures for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huttenlocher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="55" to="79" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A discriminatively trained, multiscale, deformable part model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Pose embeddings: A deep architecture for learning to match human poses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.00302</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Blink :the power of thinking without thinking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<publisher>Brown and Co</publisher>
			<pubPlace>New York : Little</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Neighbourhood components analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Goldberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roweis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Regularized max pooling for image categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hoai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning actions from the web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ikizler-Cinbis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cinbis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Towards understanding action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zuffi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<ptr target="http://cs.stanford.edu/people/karpathy/cnnembed/.5" />
		<title level="m">SNE visualization of CNN codes</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">ImageNet Classification with Deep Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Multi-agent event detection: Localization and role assignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Beyond actions: Discriminative models for contextual group activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="541" to="551" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Modeling view and posture manifolds for tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning and transferring mid-level image representations using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Posebits for monocular human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning representations by back-propagating errors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
		<idno>1986. 3</idno>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">323</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<title level="m">ImageNet Large Scale Visual Recognition Challenge. IJCV</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="1" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Spatio-temporal relationship match: Video structure comparison for recognition of complex human activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ryoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Aggarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">MODEC: Multimodal decomposable models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sapp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Taskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">FaceNet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">On the importance of initialization and momentum in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Pose-sensitive embedding by nonlinear nca regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Spiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Joint training of a convolutional network and a graphical model for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">DeepPose: Human pose estimation via deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">3d people tracking with gaussian process dynamical models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Visualizing highdimensional data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2008-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">An approach to pose-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Beyond physical connections: Tree models in human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Learning fine-grained image similarity with deep ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations using videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Multiple tree models for occlusion and spatial constraints in human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Learning descriptors for object recognition and 3d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Recognizing human actions from still images with latent poses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Articulated pose estimation with flexible mixtures-of-parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Grouplet: A structured image representation for recognizing human and object interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Action recognition with exemplar based 2.5d graph matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
