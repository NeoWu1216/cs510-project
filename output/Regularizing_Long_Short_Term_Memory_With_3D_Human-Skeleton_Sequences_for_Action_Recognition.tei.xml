<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Regularizing Long Short Term Memory with 3D Human-Skeleton Sequences for Action Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Behrooz</forename><surname>Mahasseni</surname></persName>
							<email>mahasseb@eecs.oregonstate.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Oregon State University Corvallis</orgName>
								<address>
									<postCode>97331</postCode>
									<region>OR</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinisa</forename><surname>Todorovic</surname></persName>
							<email>sinisa@eecs.oregonstate.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Oregon State University Corvallis</orgName>
								<address>
									<postCode>97331</postCode>
									<region>OR</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Regularizing Long Short Term Memory with 3D Human-Skeleton Sequences for Action Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:45+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>This paper argues that large-scale action recognition in video can be greatly improved by providing an additional modality in training data -namely, 3D human-skeleton sequences -aimed at complementing poorly represented or missing features of human actions in the training videos. For recognition, we use Long Short Term Memory (LSTM) grounded via a deep Convolutional Neural Network (CNN) onto the video. Training of LSTM is regularized using the output of another encoder LSTM (eLSTM) grounded on 3D human-skeleton training data. For such regularized training of LSTM, we modify the standard backpropagation through time</head> (BPTT)  <p>in order to address the wellknown issues with gradient descent in constraint optimization. Our evaluation on three benchmark datasets -Sports-1M, HMDB-51, and UCF101 -shows accuracy improvements from 1.7% up to 14.8% relative to the state of the art.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>This paper is about classifying videos of human actions. We focus on domains that present challenges along two axes. First, we consider a large set of action classes (e.g., the Sports-1M dataset with 487 action classes <ref type="bibr" target="#b14">[15]</ref>) which may have very subtle inter-class differences and large intra-class variations (e.g., Sports-1M contains 6 different types of bowling, 7 different types of American football and 23 types of billiards). The actions may be performed by individuals or groups of people (e.g., skateboarding vs. marathon), and may be defined by a particular object of interaction (e.g., bull-riding vs. horseback riding). Second, our videos are captured in uncontrolled environments, where the actions are viewed from various camera viewpoints and distances, and under partial occlusion.</p><p>Recent work uses Convolutional Neural Networks (CNNs) to address the above challenges <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b25">26]</ref>. However, despite the ongoing research efforts to: (a) Increase the amount of training data <ref type="bibr" target="#b14">[15]</ref>, (b) Fuse hand-designed and deep-convolutional features <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b40">41]</ref>, and (c) Combine CNNs with either graphical models <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b38">39]</ref>, or recurrent neural networks <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b38">39]</ref> for capturing complex dynamics of human actions, we observe that their classification accuracy is still markedly below the counterpart performance on large-scale image classification. This motivates us to seek a novel deep architecture, leveraging some of the promising directions in (a)-(c).</p><p>Our key idea is to augment the training set of videos with additional data coming from another modality. This has the potential to facilitate capturing important features of human actions poorly represented in the training videos, or even provide complementary information missing in the videos. Specifically, in training, we use 3D human-skeleton sequences of a few human actions to regularize learning of our deep representation of all action classes. This regularization rests on our hypothesis that since videos and skeleton sequences are about human motions their respective feature representations should be similar. The skeleton sequences, being view-independent and devoid of background clutter, are expected to facilitate capturing important motion patterns of human-body joints in 3D space. This, in turn, is expected to regularize, and thus improve our deep learning from videos.</p><p>It is worth noting that the additional modality that we use in training does not provide examples of most action classes from our video domain. Rather, available 3D humanskeleton sequences form a small-size training dataset that is insufficient for robust deep learning. Nevertheless, in this paper, we show that accounting for this additional modality greatly improves our performance relative to the case when only videos are used in training.</p><p>As illustrated in <ref type="figure" target="#fig_2">Fig. 1</ref>, for action recognition, we use Long Short Term Memory (LSTM) grounded via a deep Convolutional Neural Network (DCNN) onto the video. LSTM is modified to have an additional representational layer at the top, aimed at extracting deep-learned feature <ref type="figure" target="#fig_2">Figure 1</ref>: Our novel deep architecture: the LSTM on the left is trained on videos under weak supervision, and the encoder LSTM (eLSTM) on the right is trained in unsupervised manner on 3D human-skeleton sequences. v t and s t denote the input video frame and skeleton data at time t. r v and r s are the output features of LSTM and eLSTM. y andŷ are the ground-truth and predicted class labels. h t , h 1 t , and h 2 t are the hidden layers in the two respective LSTMs. x t is the output feature of DCNN's F C 7 layer. Euclidean distances between corresponding features r v and r s are jointly used with the prediction loss between y andŷ for a regularized learning of the LSTM on videos.</p><p>r v from the entire video. In training, we regularize learning of LSTM such that its output r v is similar to features r s produced by another encoder LSTM (eLSTM) grounded onto 3D human-skeleton sequences. The sequences record 3D locations of 18 joints of a human body while the person performs certain actions, such as those in the Carnegie-Mellon Mocap dataset <ref type="bibr" target="#b0">[1]</ref> and the HDM05 Mocap dataset <ref type="bibr" target="#b21">[22]</ref>. eLSTM is learned in an unsupervised manner by minimizing the data reconstruction error. Note that a hypothetical supervised learning of an LSTM on skeleton data would not be possible in our setting, since we have access to a small dataset representing only a small fraction (or none) of action classes from the video domain. During test time, we do not use any detections of human joints and their trajectories, but classify a new video only based on raw pixels taken as input to the LSTM+DCNN.</p><p>Our main contribution represents a novel regularization of LSTM learning. Unlike the standard regularization techniques, such as drop out or weight decay <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b39">40]</ref>, we define a set of constraints aimed at reducing the Euclidean distances between top-layer features of LSTM trained on videos and corresponding output features of eLSTM. We use these constraints to regularize and thus extend the standard backpropagation through time (BPTT) algorithm <ref type="bibr" target="#b3">[4]</ref>. BPTT back-propagates a class-prediction loss for updating LSTM parameters via stochastic gradient descent. We additionally back-propagate the above constraints between corresponding features. This requires modifying the standard (unconstrained) gradient descent to an algorithm that accounts for constraints. To this end, we use the hybrid steepest descent <ref type="bibr" target="#b8">[9]</ref>.</p><p>In this paper, we consider several formulations of regularizing LSTM learning corresponding to the cases when ground-truth class labels are available for the skeleton sequences, and when this ground truth is not available.</p><p>We present experimental evaluation on three benchmark datasets, including Sports-1M <ref type="bibr" target="#b14">[15]</ref>, HMDB-51 <ref type="bibr" target="#b18">[19]</ref>, and UCF101 <ref type="bibr" target="#b27">[28]</ref>. We report the performance improvement ranging from 1.7% to 14.8% relative to the state of the art.</p><p>In the following, Sec. 2 reviews related work, Sec. 3 specifies LSTM, Sec. 4 formulates our novel deep architecture, and Sec. 5 presents our results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Closely Related Work</head><p>This section reviews related work on: combining CNN and LSTM, encoder LSTM, using skeleton data for action classification, and multimodal deep learning in vision.</p><p>LSTM+DCNN. Frame-level DCNN features have been used as input to LSTM for action classification <ref type="bibr" target="#b3">[4]</ref>. This architecture has been extended with additional layers for convolutional temporal pooling <ref type="bibr" target="#b38">[39]</ref>. The main advantages include that these architectures are deeply compositional in both space and time, and that they are capable of directly handling variable-length inputs (e.g., video frames) and capturing long-range, complex temporal dynamics. They are learned with backpropagation through time (BPTT).</p><p>Our key difference is in the definition of LSTM output layer. Our output layer includes the standard softmax layer for classification and the additional representational layer for mapping input video sequences to a feature space. It is the construction of this new feature space that allows us to regularize LSTM learning. Another difference is that we replace the standard stochastic gradient descent in BPTT, with an algorithm that accounts for constraints in optimization.</p><p>Encoder LSTM. LSTM has been used to encode an input data sequence to a fixed length vector, which, in turn, is decoded to predict the next unobserved data <ref type="bibr" target="#b28">[29]</ref>. However, this recurrent autoencoder-decoder paradigm has not yet demonstrated competitive performance on action classification in videos. Our main difference is that we use the encoder LSTM to generate a feature manifold for regularizing a supervised learning of another LSTM.</p><p>Action classification using skeleton data has a longtrack record <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b4">5]</ref>. However, at test time, these approaches require either 3D locations of human joints, or detection of human joints in videos. In contrast, at test time, we just use pixels as input, and neither detect human joints nor need their 3D locations.</p><p>Multimodal learning. Recent work uses text data as an additional modality to improve image classification <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b20">21]</ref>. For example, a combination of DCNN and LSTM has been used for multimodal mapping of finer object-level concepts in images to phrases <ref type="bibr" target="#b17">[18]</ref>. Closely related work introduces a semi-supervised embedding in deep architectures <ref type="bibr" target="#b35">[36]</ref>. Due to the fundamental differences in our problem statements, we cannot use these approaches. More importantly, instead of a heuristic combination of classification loss and the embedding loss, we use a welldefined set of constraints to explicitly exploit the information contained in the embedding space.</p><p>Another difference is that object classes of text data are in one-to-one correspondence with object classes appearing in images (or it is assumed that the image and text domains have a large overlap of object classes). In contrast, our 3D skeleton sequences represent only a few action classes from the video domain. Similar to <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b26">27]</ref>, we do not use the other modality at test time. For multimodal training, these approaches use, for example, the Euclidean distance <ref type="bibr" target="#b26">[27]</ref>, modified hinge loss <ref type="bibr" target="#b7">[8]</ref>, parameter transfer and regression loss <ref type="bibr" target="#b6">[7]</ref>, or pairwise ranking loss <ref type="bibr" target="#b17">[18]</ref>. Instead, we minimize the cross entropy loss associated with the softmax layer of LSTM, subject to the feature similarity constraints in our regularization. Importantly, in Sec. 4.3, we provide convergence guarantees for our regularized learning of LSTM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">A Brief Review of LSTM</head><p>A major building block of our novel architecture is LSTM <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b28">29]</ref>, depicted in <ref type="figure" target="#fig_0">Fig. 2</ref>. LSTM is a recurrent neural network as briefly reviewed below.</p><p>The LSTM's recurrent unit memorizes previous input information in a memory cell, c t , indexed by time t. The output is hidden variable h t . Three gates control the updates of c t and h t : 'input' i t , 'output' o t , and 'forget' f t . In (1), we summarize their update equations, where symbol ⊙ denotes the element-wise product, and W are LSTM parameters.</p><formula xml:id="formula_0">i t =σ(W xi x t + W hi h t−1 + W ci c t−1 + b i ), o t =σ(W xo x t + W ho h t−1 + W co c t−1 + b o ), f t =σ(W xf x t + W hf h t−1 + W cf c t−1 + b f ), c t =f t ⊙ c t−1 + i t ⊙ tanh(W xc x t + W hc h t−1 + b c ), h t =o t ⊙ tanh(c t ).<label>(1)</label></formula><p>From (1), the input gate, i t , regulates the updates of c t , based on inputs x t and previous values of h and c. The 'output gate', o t , controls if h t should be updated given c t . The forget gate, f t , resets the memory to its initial value. The LSTM parameters W LSTM = {W xi , W xo , W xf , W ci , W co , W cf , W hi , W ho , W hf } are jointly learned using BPTT. The LSTM unit preserves error derivatives of the deep unfolded network. This has been shown to avoid the wellknown vanishing gradient problem, and allows LSTM to capture long-range dependencies in the input data sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Regularizing LSTM for Action Recognition</head><p>As mentioned in Sec. 1, our novel architecture consists of eLSTM for learning a feature representation of 3D human-skeleton sequences, and a stacked DCNN+LSTM for classifying videos. Below, we describe these two components.</p><p>eLSTM. <ref type="figure">Fig. 3</ref> shows the time-unfolded encoder and decoder parts of eLSTM. The goal of eLSTM is to encode the input skeleton sequence, s = {s t : t = 1, 2, . . . }, consisting of 3D locations of human joints. The sequences may have variable lengths in time. eLSTM observes the entire skeleton sequence s, and encodes it to a feature vector r s . The set of encoded representations {r s } are assumed to form a manifold M s of skeleton sequences. To learn the encoder, a decoder LSTM tries to reconstruct the normalized input 3D data in the reverse order. The reconstruction error is then estimated in terms of the mean-squared error in the normalized 3D coordinates, and used to jointly learn both the encoder and decoder LSTMs. The reversed output reconstruction benefits from low range correlations which makes the optimization problem easier. The input to the encoder at each time step t, is the output of the decoder at time step t−1, i.e. s t−1 . An alternative architecture is to learn an encoder LSTM to predict the next skeleton frame. To avoid over-fitting, we use the standard drop-out regularization for eLSTM <ref type="bibr" target="#b39">[40]</ref>.</p><p>DCNN+LSTM. As shown in <ref type="figure" target="#fig_2">Fig. 1</ref>, for classifying human actions in videos we use a stacked architecture of a frame-level DCNN and a two-layer LSTM. DCNNs have been demonstrated as capable of learning to extract good image descriptors for classification <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b15">16]</ref>. For DCNN, <ref type="figure">Figure 3</ref>: The time-unfolded visualization of the encoder LSTM (left) and decoder LSTM (right) for learning a feature representation from input 3D human-skeleton sequences. The encoder LSTM observes the entire skeleton sequence and encodes it to a fixed-length representation. The decoder LSTM tries to reconstruct 3D locations of human joints in the reverse order of the input sequence. The reconstruction error is then used to jointly learn both the encoder and decoder LSTMs.</p><p>we use the same network architecture initially trained on the ImageNet dataset as in <ref type="bibr" target="#b29">[30]</ref>. The only difference is in the number of output units of the softmax layer at the top of DCNN, since we address different numbers of action classes. Note that we later fine-tune DCNN parameters together with LSTM ones in our regularized learning.</p><p>Our LSTM differs from the model used in <ref type="bibr" target="#b3">[4]</ref> in the top output layer. The output layer of our LSTM extends the standard fully connected softmax layer for classification with an additional representation layer. This representation layer is aimed at mapping input video sequences, v = {v t : t = 1, 2, . . . }, with variable lengths in time, to fixed-length vectors r v . The size of this representation layer is set to be equal to that of the output layer of eLSTM. Thus, the vectors r v and r s have the same size.</p><p>Our goal of learning DCNN+LSTM parameters, Θ, is to minimize the classification loss, L(Θ), subject to constraints g between vectors r v and corresponding vectors r s in manifold M s . Thus, for all training videos v ∈ D v , we formulate the regularized learning of DCNN+LSTM as</p><formula xml:id="formula_1">min Θ L(Θ) s.t. ∀v ∈ D v , g(r v , M s ) ≤ 0,<label>(2)</label></formula><p>where L(Θ) = v∈Dv l(v, Θ) is the cross entropy loss associated with the softmax layer of LSTM, and g is a constraint based on the distance between r v and M s . g can be defined in different ways. We only require that the constraints are differentiable functions. In the following, we define two distinct constraints that give the best results in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Class Independent Regularization</head><p>For class independent regularization of learning DCNN+LSTM parameters, the constraint g = g 1 in <ref type="formula" target="#formula_1">(2)</ref> is specified to ensure that, for every video v ∈ D v , r v is sufficiently similar to the mapped vectors r s ∈ M s . This is achieved by defining an upper-bound for the average distance between r v and all r s ∈ M s as in <ref type="formula" target="#formula_2">(3)</ref> </p><formula xml:id="formula_2">g 1 (r v , M s ) = 1 n rs∈Ms r v − r s 2 2 − α,<label>(3)</label></formula><p>where α &gt; 0 is an input parameter, and n is the number of training skeleton sequences. This type of regularization is suitable for cases when training skeleton sequences do not represent the same action classes as training videos or represent a very small subset of action classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Class Specific Regularization</head><p>For class specific regularization of learning DCNN+LSTM parameters, the constraint g = g 2 in (2) is specified to take into account action class labels of training skeleton sequences, when available. This type of learning regularization is suitable for cases when some action classes represented by training skeleton sequences do "overlap" with certain action classes in training videos. The definition of class equivalence between the two modalities can be easily provided along with ground-truth annotations.</p><p>We expect that r v and r s should be more similar if video v and skeleton sequence s share the same class label, l v = l s , than r v and r s ′ of skeleton sequences s ′ with different class labels, l v = l s ′ . This is defined in (4)</p><formula xml:id="formula_3">g 2 (r v , M s ) = 1 n = rs∈Ms lv=ls r v −r s 2 2 − 1 n = r s ′ ∈Ms lv =l s ′ r v −r s ′ 2 2 .<label>(4)</label></formula><p>where n = and n = are the numbers of training skeleton sequences that have l v = l s and l v = l s . The constraint g 2 defined in (4) ensures that the average Euclidean distance between r v and r s ∈ M s for skeleton sequences s of the same action label is less than the average Euclidean distance between r v and r s ′ for skeleton sequences s ′ of different action labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Hybrid Steepest Descent</head><p>We jointly learn parameters, Θ=W LSTM ∪W DCNN , by modifying the standard backpropagation and applying a stochastic gradient descent algorithm which accounts for the aforementioned constraints. One standard approach to solve a constrained convex optimization problem is to use Lagrange multipliers and fuse the constraints with the original objective into a new unconstrained optimization problem. Unfortunately, as demonstrated in <ref type="bibr" target="#b23">[24]</ref>, gradient descent poorly works with Lagrange multipliers, mainly because they introduce saddle points in the new objective. Therefore, we resort to an alternative approach called hybrid steepest descent <ref type="bibr" target="#b8">[9]</ref> for solving our constrained convex optimization, as described below. <ref type="figure">Fig. 4</ref> shows a simulation of hybrid steepest descent for a simple convex optimization problem. At each iteration, the algorithm checks if the current solution -i.e., the current Θ parameters in our case -satisfies all constraints. If so, Θ is updated according to the gradient of the original objective function -i.e., in our case, the cross entropy loss L(Θ) -without considering the constraints. If any constraint is violated by the current solution -i.e in our case, g(r v , M s ) ≤ 0 constraint is not satisfied for r v computed by DCNN+LSTM with the current Θ parameters -, we update Θ according to the gradient of the violated constraints. The proof of correctness, asymptotic stability, and convergence of this algorithm is presented in <ref type="bibr" target="#b8">[9]</ref>. Note that there is no guarantee that the final value of the parameters Θ satisfies all constraints. In our implementation we keep track of the top 5 best solutions based which have the minimum number of violated constraints.</p><p>We use hybrid steepest descent to modify BPTT for regularized learning of DCNN+LSTM. Note that the updates of the recurrent units only depend on the particular value of the the back-propagated gradient. Once this gradient is computed, we use it in the same way as in the standard BPTT. Thus, as summarized in Alg. 1, our modification of BPTT amounts to alternating the specification of the error gradient at the output layer according to the above rules of hybrid steepest descent. In Alg. 1, we use g &gt; 0 to denote a constraint violation, and η t is the time dependent learning rate.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results</head><p>For evaluation, we use the Sports-1M <ref type="bibr" target="#b14">[15]</ref>, HMDB-51 <ref type="bibr" target="#b18">[19]</ref>, and UCF-101 <ref type="bibr" target="#b27">[28]</ref> datasets. Sports-1M consists of more than 1 million videos from Youtube, annotated with 487 action classes. There are on average 3000 videos for each action class, where the average video length is 5 minutes 36 seconds. Considering the large number of action classes, long duration of the videos, and large variety of camera motions, Sports-1M is currently acknowledged as one of the most challenging benchmarks for action recognition in the wild. HMDB-51 consists of 6849 videos with 51 action labels. Each action class has at least 100 videos with an average video length of 3.1 seconds. UCF-101 consists of 13,320 videos of 101 action classes with average video length of 7.2 seconds. We follow <ref type="bibr" target="#b38">[39]</ref>, and report our average accuracy on the given three dataset splits.</p><p>We use human skeleton sequences of the Carnegie-Mellon Mocap <ref type="bibr" target="#b0">[1]</ref> and HDM05 <ref type="bibr" target="#b21">[22]</ref> datasets to train eLSTM. HDM05 consists of 2337 sequences with 130 action classes performed by 5 subjects. These sequences record the 3D locations of 31 human joints at each frame. Carnegie-Mellon Mocap consists of 2605 sequences with 23 action classes. These sequences record the 3D locations of 41 human joints at each frame. For consistency, we use only 18 human-body joints (head, lower back, upper back, upper neck, right/left clavicle, hand, humerus, radius, femur, tibia, foot) of the skeleton sequences, and resolve name conflicts and duplicates in these two datasets.</p><p>eLSTM: An LSTM with two hidden layers is used for the encoder and decoder. Input and output of the encoder and decoder LSTMs are 54 = 18 × 3 dimensional vectors of 3D human body joint positions. We normalize input data in the range of [0, 1]. We empirically verified that eLSTM with 512 and 1024 hidden units in the first and second recurrent layer of the encoder LSTM, and the same number of hidden units in a reverse order for the decoder LSTM results in the smallest reconstruction error. The model is trained on 16 frame sequences. Since the training is unsupervised, both Carnegie-Mellon Mocap and HDM05 datasets are used in training phase. It takes 16-19 hours to converge for about 3160 minutes of skeleton sequences. For defining g 2 constraints, we use the class labels defined in HDM05.</p><p>DCNN: We use GoogLeNet <ref type="bibr" target="#b29">[30]</ref> trained on ImageNet <ref type="bibr" target="#b24">[25]</ref> as DCNN in our approach. This DCNN is fine-tuned on a set of randomly sampled video frames. The output layer is modified for the fine-tuning based on the number of action classes. On average (150-200) frames are sampled from each video. The second to the last fully connected layer (F C 7 ) is used as the frame descriptor input to LSTM. Note that, later, DCNN parameters are fine-tuned again jointly with LSTM training.</p><p>Our Regularized LSTM (RLSTM): Our RLSTM for action recognition contains two hidden layers of 2048 and 1024 hidden units, respectively. The number of output units for classification is 487 for Sports-1M, 51 for HMDB-51, and 101 for UCF101. The number of output units for representation is 512, which is equal to the number of hidden units in the second recurrent layer of eLSTM. Similar specifications are used in <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b28">29]</ref>. Fixed length sequences of 16 frames are used in training. We find that a random initialization of RLSTM converges to a poor local optimum. To overcome this, we train a non regularized LSTM using one-tenth of the training instances in Sports-1M. We use weights of this learned model to initialize weights of RLSTM. Weights of the representation layer are initialized randomly between [-0.1, 0.1]. Similar to <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b28">29]</ref>, we estimate an average prediction of 16 block frames with a stride of 8 in inference. The linear weighting presented in <ref type="bibr" target="#b38">[39]</ref> is used to combine predictions from each block for the entire video.</p><p>Implementation: We use Caffe <ref type="bibr" target="#b13">[14]</ref> and a modified version of the RNN library in Torch <ref type="bibr" target="#b2">[3]</ref> in our experiments. All experiments are performed on an Intel quad core-i7 CPU and 16GB RAM PC with two Tesla-K80 Nvidia cards.</p><p>Baselines: We conduct a comparison with several baselines in order to evaluate effectiveness of different constraints in our regularized learning of RLSTM. These baselines include the following: 1) DCNN: This is a 'single- frame' based action recognition evaluated in <ref type="bibr" target="#b14">[15]</ref> -a video is represented by a single frame and classification is perform only based on the content in that frame, 2) LSTM: This is a (DCNN+LSTM) learned without any regularization and constraints, similar to the approach of <ref type="bibr" target="#b3">[4]</ref> with only difference in the number of hidden units (due to a different number of classes considered).</p><p>Variations of our approach: Based on the constraints, defined in Sec. 4.1 and 4.2, for regularizing learning of RLSTM, we define the following three variations of our approach: 1) RLSTM-g1: uses class independent constraints g1 to regularize the learning, 2) RLSTM-g2: uses class specific constraints g2 to regularize the learning , 3) RLSTM-g3: uses g1 ∪ g2 to regularize the learning. <ref type="table">Table 1</ref> shows our average classification accuracy on HMDB-51 and UCF101. All variations of our method improved the accuracy of the baseline LSTM (3.1% to 12.2%). RLSTM-g2 achieves a better accuracy compared to RLSTM-g1. This strongly supports the hypothesis that deep-learned features of vides and skeleton sequences should be similar, and that our regularization should improve action recognition. Because the 3D human skeleton datasets and UCF101/HMDB-51 share a few common action classes, RLSTM-g3 which combines the class independent and class specific constraints outperforms RLSTM-g2.</p><p>Comparison of our method with the state of the art deep learning based approches is presented in Tab. 2. We can see that RLSTM-g3 outperforms variations of <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b25">26]</ref> which only use raw frames by 1.7%−22%. Higher accuracy is reported in <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b34">35]</ref> on UCF101 for (raw pixels + optical flow) input. In comparison with their accuracy of 88.6% − 90.3%, we achieve a comparable performance of 86.9% accuracy on UCF101 by using only pixels of video frames.</p><p>Hit@k values are a standard evaluation for large-scale datasets. A test instance is considered correctly labeled if the ground truth label is among the top k predictions. Similar to <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b15">16]</ref> we use Hit@1 and Hit@5 values to report accuracy on Sports-1M. <ref type="table" target="#tab_2">Table 3</ref> shows the classification ac-Method UCF101 HMDB-51 <ref type="bibr" target="#b14">[15]</ref> 65.4 - <ref type="bibr" target="#b28">[29]</ref> 75.8 44.1 <ref type="bibr" target="#b3">[4]</ref> 71.12 - <ref type="bibr" target="#b25">[26]</ref> 72.8 40.5 <ref type="bibr" target="#b40">[41]</ref> 79.34 - <ref type="bibr" target="#b31">[32]</ref> 85.2 -RLSTM-g3 86.9 55.3  curacy of the baselines, different RLSTM models, and the state of the art on Sports-1M. One interesting result is that RLSTM-g 1 outperforms RLSTM-g 2 . We believe that this is because of a poor overlap of the large number of action classes in Sports-1M with the set of action classes in the skeleton data obtained from HDM05 and CMU Mocap. Our best approach improves the classification accuracy on Hit@1 by 2.5%−16.6%. Also the baseline non-regularized LSTM yields better accuracy than the temporal pooling approaches of <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b31">32]</ref>. We believe that this is mainly because the baseline LSTM has access to the longer video frames.</p><p>To verify the effectiveness of hybrid-gradient descent used in our regularized learning, we also train our DCNN+LSTM using AdaGrad <ref type="bibr" target="#b5">[6]</ref> and Adam <ref type="bibr" target="#b16">[17]</ref>. These two alternative algorithms are aimed at minimizing the standard weighted sum of the classification and representation loss. The comparison is shown in <ref type="table">Table 4</ref>, where RLSTM denotes our approach to regularized training with hybridgradient descent, and LSTM(·) denotes our approach to regularized training with AdaGrad or Adam.  <ref type="table">Table 4</ref>: Average classification accuracy of our approach on UCF101 and HMDB-51, when the regularized training is conducted with AdaGrad <ref type="bibr" target="#b5">[6]</ref> or Adam <ref type="bibr" target="#b16">[17]</ref> -denoted as LSTM(adg) and LSTM(adm) -or hybrid-gradient descent -denoted as RLSTM. The regularized learning of RLSTM-g1 and RLSTM-g3 is controlled by the α parameter, specified in (3). <ref type="figure" target="#fig_4">Fig. 5</ref> shows how our accuracy changes for different values of α on HMDB-51. We can see that for small values of α the accuracy is very low. We observe that for these values the learning algorithm does not converge. The algorithm alternates gradient updates with respect to classification error and violated constraints. This is because the optimization problem becomes almost infeasible for small values of α. The accuracy gradually increases for larger values of α, but again decreases when α becomes sufficiently large. The accuracy of RLSTM-g1 reaches that of the standard LSTM for large values of α, and remains nearly the same. This is because sufficiently large values of α yield an unconstrained optimization, i.e., non-regularized learning of LSTM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We have proposed a novel deep architecture for largescale action recognition. The main contribution of our work is to use 3D human-skeleton sequences to regularize the learning of LSTM, which is grounded via DCNN onto the video for action recognition. We have modified the backpropagation through time algorithm in order to account for constraints in our regularized joint learning of LSTM+DCNN. Our experimental results demonstrate that the skeleton sequences could successfully constrain the learning of LSTM+DCNN leading to an improved performance relative to the case when LSTM is trained only on videos. Specifically, on Sports-1M, UCF101, and HMDB-51 our accuracy improves by 2.5% − 16.6%, 1.7 − 21.5%, and 11.2 − 14.8%, respectively.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>LSTM unit<ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b28">29]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :− 1 ,</head><label>41</label><figDesc>Simulation of the hybrid steepest descent algorithm for a simple optimization problem presented in<ref type="bibr" target="#b8">[9]</ref>. In this example Θ = x 1 , x 2 , L(Θ) = −x 1 , and g 2 = 10 8 x 1 + x 2 − 28. The dark hashed lines show the boundary of the feasible set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Algorithm 1 :%</head><label>1</label><figDesc>Regularized Learning of LSTM Input: Training videos Dv and Ms Output: LSTM parameters Θ 1 % note that during training we assume sequences in Dv are of the same length This updates Θ by back-propagating the sum of gradients of the violated constraints.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>until convergence;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Average classification accuracy of RLSTM-g1 and RLSTM-g3 on HMDB-51 for different α values, where α is the input parameter that controls the regularized learning of RLSTM-g1 and RLSTM-g3. Small values of α enforce stronger regularization that the feature outputs of RLSTM and eLSTM are highly similar.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 2 :</head><label>2</label><figDesc>Average classification accuracy of RLSTM-g3 and the state of the art on UCF101 and HMDB-51. Our approach outperform the best result in state of the art by 1.7 − 11.2%</figDesc><table>Method 
Hit@1 Hit@5 
Single-Frame [15] 
59.3 
77.7 
LSTM 
71.3 
89.9 
[15] 
60.9 
80.2 
[39] 
72.1 
90.6 
[32] 
61.1 
85.2 
RLSTM-g1 
73.4 
91.3 
RLSTM-g2 
62.2 
85.3 
RLSTM-g3 
75.9 
91.7 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 :</head><label>3</label><figDesc>Average classification accuracy of regularized LSTM models, baselines, and the state of the art on Sports-1M. Our approach outperform the best result in state of the art by 2.5% and the LSTM baseline by 4.6%.</figDesc><table></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This work was supported in part by grant NSF RI 1302700.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Carnegie-Mellon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mocap</surname></persName>
		</author>
		<ptr target="http://mocap.cs.cmu.edu" />
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Deep learning of invariant spatio-temporal features from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Torch: A modular machine learning software library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Marithoz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Hierarchical recurrent neural network for skeleton based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1110" to="1118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Write a classifier: Zero-shot learning using purely textual descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elhoseiny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Elgammal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2584" to="2591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Devise: A deep visual-semantic embedding model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A hybrid steepest descent method for constrained convex optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gerard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">De</forename><surname>Schutter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Verhaegen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Automatica</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Dynamic manifold warping for view invariant action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Medioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="571" to="578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning ensembles of potential functions for structured prediction with latent variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hajimirsadeghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">3d convolutional neural networks for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>PAMI</publisher>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="221" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<title level="m">Caffe: Convolutional architecture for fast feature embedding</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Unifying visual-semantic embeddings with multimodal neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.2539</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">HMDB: a large video database for human motion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Garrote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Group sparsity and geometry constrained dictionary learning for action recognition from depth maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1809" to="1816" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6632</idno>
		<title level="m">Deep captioning with multimodal recurrent neural networks (m-rnn)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Documentation mocap database hdm05</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Clausen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Eberhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Krger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Weber</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Zero-shot learning by convex combination of semantic embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.5650</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Constrained differential optimization for neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Platt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Barr</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<title level="m">ImageNet Large Scale Visual Recognition Challenge. IJCV</title>
		<imprint>
			<date type="published" when="2015-04" />
			<biblScope unit="page" from="1" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Zeroshot learning through cross-modal transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ganjoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="935" to="943" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Ucf101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.0402</idno>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Unsupervised learning of video representations using lstms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mansimov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.04681</idno>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Convolutional learning of spatio-temporal features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2010</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">C3D: generic features for video analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<idno>abs/1412.0767</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">An approach to posebased action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="915" to="922" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Mining actionlet ensemble for action recognition with depth cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1290" to="1297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Action recognition with trajectory-pooled deep-convolutional descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep learning via semi-supervised embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ratle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mobahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks: Tricks of the Trade</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="639" to="655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Leveraging hierarchical parametric networks for skeletal joints based action segmentation and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="724" to="731" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">View invariant human action recognition using histograms of 3d joints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Aggarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshop</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="20" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Beyond short snippets: Deep networks for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yue-Hei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Toderici</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.2329</idno>
		<title level="m">Recurrent neural network regularization</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Exploiting image-trained cnn architectures for unconstrained video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Luisier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Andrews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.04144</idno>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
