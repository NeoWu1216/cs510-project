<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Discriminative Invariant Kernel Features: A Bells-and-Whistles-Free Approach to Unsupervised Face Recognition and Pose Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipan</forename><forename type="middle">K</forename><surname>Pal</surname></persName>
							<email>dipanp@andrew.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Carnegie Mellon University</orgName>
								<orgName type="institution" key="instit2">Carnegie Mellon University</orgName>
								<orgName type="institution" key="instit3">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Juefei-Xu</surname></persName>
							<email>felixu@cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Carnegie Mellon University</orgName>
								<orgName type="institution" key="instit2">Carnegie Mellon University</orgName>
								<orgName type="institution" key="instit3">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marios</forename><surname>Savvides</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Carnegie Mellon University</orgName>
								<orgName type="institution" key="instit2">Carnegie Mellon University</orgName>
								<orgName type="institution" key="instit3">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Discriminative Invariant Kernel Features: A Bells-and-Whistles-Free Approach to Unsupervised Face Recognition and Pose Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:39+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose an explicitly discriminative and 'simple' approach to generate invariance to nuisance transformations modeled as unitary. In practice, the approach works well to handle non-unitary transformations as well. Our theoretical results extend the reach of a recent theory of invariance to discriminative and kernelized features based on unitary kernels. As a special case, a single common framework can be used to generate subject-specific pose-invariant features for face recognition and vice-versa for pose estimation. We show that our main proposed method (DIKF) can perform well under very challenging large-scale semisynthetic face matching and pose estimation protocols with unaligned faces using no landmarking whatsoever. We additionally benchmark on CMU MPIE and outperform previous work in almost all cases on off-angle face matching while we are on par with the previous state-of-the-art on the LFW unsupervised and image-restricted protocols, without any low-level image descriptors other than raw-pixels.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Over the years there have been many different approaches to the problem of general face recognition and pose estimation. However, there is a lot yet to be achieved before one can consider the unconstrained version of these problems fully solved. Though there are many algorithms in literature that perform extremely well on unconstrained datasets such as the LFW <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b37">38]</ref>, given the complexity of these algorithms, it remains unclear as to what underlying objective each of them aims to achieve in the context of unconstrained face matching. To address this need, we refrain from incorporating 'complex' algorithms and optimization problems in our approach and solely base the core mechanism on fundamental principles of generating invariance.</p><p>Although success on the LFW framework has been very encouraging, a paradigm shift is underway towards the role of such large unconstrained databases. It has been suggested, that the problem of face recognition be divided into subtasks of achieving invariance towards transformations of a face <ref type="bibr" target="#b22">[23]</ref>. Further, strong recent progress on the supervised protocols of the LFW dataset nearing human accuracy have perhaps overshadowed the need to understand the fundamental problems in vision tasks such as recognition.</p><p>In light of these arguments, there is a need for methods that are based on fundamental principles, not in order to beat the current state-of-the-art, but instead to further our understanding of the problem itself. Although, there has been significant work in literature generating implicit invariance to specific individual or a small subset of these transformations at once, an approach which generates explicitly invariant features to any unitary modeled transformation while being explicitly discriminative has not been studied. Further, there is a need for a study investigating generation of invariant features to multiple common transformations of faces in a controlled setting. Studies on largescale controlled datasets have the advantage of being more informative regarding the algorithmic shortcomings.</p><p>Contributions. In this paper, we present an approach to obtain explicitly discriminative features that are invariant to multiple transformations that can be locally modeled as unitary. The approach does not involve solving a heavy optimization problem. It is based, instead, on a different manifestation of group invariance and on learning discriminative filters through a simple closed form solution. We focus part of our study to be more exploratory towards understanding the challenges due to specific common transformations such as pose, translation, scale and rotation. Even using a simple method with a closed-form solution, working only with raw-pixels, we are able to, to the best of our knowledge, achieve the state-of-the-art on the MPIE database protocol, and match the previous state-of-the-art on the LFW unsupervised and image-restricted label-free protocol. Hence our approach is 'bells and whistles free'. Our main contributions are:</p><p>1. We propose a simple approach to learn discriminative non-linear features that are invariant to unitary transformations. We extend the reach of a recent theory of invariance to discriminative and kernelized features.</p><p>2. Focusing on the challenging transformation of pose, we propose a simple dense-landmark-free approach which results in a framework capable of performing open-set pose-invariant face recognition and simultaneous pose estimation as illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>.</p><p>3. We extend the method to result in a sequential approach of generating invariance to multiple sub-groups of transformations. Using this, we arrive at a completely landmark-free framework (at evaluation) for transformation invariant face recognition and pose estimation.</p><p>Related work. A majority of the recent efforts on unconstrained face verification focus on LFW, and rely greatly on locating accurate and dense facial landmarks and descriptors to extract overcomplete information from the image and/or use 3D modeling in the algorithm <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b37">38]</ref>. Many of these systems are also closed set, whereas our approach is inherently openset. Further they address different types of transformations differently. Automatic dense facial landmarking, in fact already factors out a majority of the transformations such as translation, rotation and scale. Further, LFW provides reasonably aligned images which help to factor out translation, in-plane rotation and scale. This enables the algorithms to escape the need to account for those transformations within the core framework. In practice, real-time landmarking is expensive, thus there arises a need to explore methods than circumvent the requirement for dense and accurate landmarks.</p><p>A slightly different class of algorithms based on deep learning have gained popularity recently, which utilizes a lot of data (high sample complexity) and increases model complexity drastically <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b11">12]</ref>. These methods although widely successful, fail to provide a better understanding of the problem due to complex models and over-complete feature extraction combined with unconstrained testing protocols.</p><p>In light of the current trend in unconstrained face recognition, large-scale databases such as LFW include an uncontrolled amount of certain unspecified types of transformations in each image. However, other transformations such as translation, in-plane rotation and scale are factored out by providing aligned faces. Having no control over the type and amount of other transformations tends to bias the development of face recognition systems where it is not clear why some algorithms work well while others don't. Thus, eluding the underlying problem which is to generate invariance to all intra-class transformations while being discriminative amongst inter-class transformations. In this work, we present results on a semi-synthetic large-scale data with controlled amounts of transformations.</p><p>Normalized invariant dot-products (NDP). Some recent work by Liao et al. <ref type="bibr" target="#b25">[26]</ref>, adopting a perspective similar to ours in this paper, perform competitively on LFW. However, they use 'external training data' to extract features and require accurate facial alignment before extracting more complex feature descriptors (HOG followed by PCA). To have a fair comparison in our exploratory experiments, we compare against their core baseline method of using normalized dot-products followed by mean (NDP-ℓ 1 ) and max (NDP-ℓ ∞ ) pooling. The multilayer extension of our approach on the other hand requires no such alignment for recognition of misaligned faces and we restrict ourselves to work with raw pixels throughout this study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Linear Invariant Random Features</head><p>I-theory <ref type="bibr" target="#b1">[2]</ref> was proposed to generate invariance to transformations motivated by the properties of the visual cortex. Empirically, through experiments on LFW, it has been shown that sufficient invariance can be learned <ref type="bibr" target="#b25">[26]</ref>.</p><p>Sample complexity. One of the main motivations of Itheory of invariance is the problem of reducing sample complexity to learn a concept. It can be argued that humans tend to learn new concepts (e.g. the structure of a novel object) with very few examples. Yet most machine learning and vision algorithms require a lot of data to learn, with the general focus being on performance rather than sample complexity. Given the advances in cheaper computation, sample complexity might not seem important. However, in order to understand the low sample complexity characteristics of human vision, it might be useful to better explore paradigms which try to achieve low sample complexity. I-theory is one such paradigm we now briefly overview.</p><p>Consider a unitary group of transformations G with group elements g with finite cardinality (|G|). One can represent the action (i.e. translation, rotation etc.) of the group element g on an image as gI(x) = I(g −1 x). The orbit of the images I ′ generated by {I ′ | I ′ = g(I) ∀g ∈ G} is unique to every image since it is the set of all variations of an image as defined by G. In order to compare two orbits, a measure which also introduces invariance is the probability distribution P I induced by the group elements of G on I. The heuristic is to directly/indirectly measure a statistic of the probability distribution, thus generating invariance. It can be shown that I ∼ I ′ ⇔ P I = P I ′ , i.e. if two images are equivalent under some g, then their distributions are identical <ref type="bibr" target="#b1">[2]</ref>. To characterize the distribution P I , one can use arbitrary templates t k , k ∈ {1, . . . , K}, each template providing a 1-D projection of P I . In order to achieve discriminability between say n images/orbits, up to precision ǫ, with confidence (1 − δ), one must have K ≥ 2 cǫ 2 log n δ <ref type="bibr" target="#b1">[2]</ref>. Perhaps one of the most interesting observations that I-theory makes is that, since g is unitary, we have:</p><formula xml:id="formula_0">g(I), t = I, g −1 (t)<label>(1)</label></formula><p>Thus, the distribution of the set { gI, t }, ∀g ∈ G is the same as that of { I, g −1 t }, ∀g ∈ G. Hence, it is not necessary to explicitly observe all transformations of a novel image in order to discriminate it. This is the key to reduce sample complexity, and what our approach capitalizes on working with few faces (∼ 3000 for LFW and &lt;350 for other experiments). One can then use multiple 1-D projections of the distribution in order to characterize the novel object. Characterizing this distribution leads to an invariant computed through a histogram µ n (I) = 1 |K| k η( I, g k (t n ) ), ∀n = 1, . . . , N . Here, η can be a non-linear threshold function η : R → R which results in a histogram approximation of the distribution. The distribution of { gI, t }, ∀g ∈ G can also be explained by its set of moments. Liao et al. <ref type="bibr" target="#b25">[26]</ref> found that the first moment approximation of the distribution worked well in practice which translates to mean pooling. Different pooling schemes would capture different aspects (moments) of the 1-D distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Discriminative Invariant Kernel Features</head><p>Discriminative templates to generate invariant features. Section 2 describes how a set of arbitrary templates transformed by an unitary group can be used to invoke invariance to that group through statistics of the invariant distribution under the action of the group. However, the templates t are not discriminative and they may capture redundant information about the distribution. Discriminability between classes may offer better separation between the orbits of different images.</p><p>Discriminative invariant linear features (DILF). We adopt a simple way of generating discrimination between K classes. We seek a filter or template t k s.t. X T t k = u k , where X ∈ R d×K is the pre-whitened data matrix with K classes. u k is a label vector of zeros with 1 at position k for the k-th class. Under the constraint that the template has to be a linear combination of the data, the solution becomes t k = X(X T X) −1 u k . Now, consider a finite unitary group G acting on X, thereby generating {g n (X n )} i.e. X n = g n (X), where every element in X has been acted upon by g n ∈ G 1 . We train N × K separate templates or filters t kn , one for each class and each transformation. Concretely,</p><formula xml:id="formula_1">t kn = X n (X T n X n ) −1 u k .</formula><p>A key issue is that the learned templates need to be the action of a unitary group in order for the distribution under the orbit to be invariant to that group. Specifically, if T = {t k } is the set of learned templates or filters then, it should be possible to express T as {g n (t 0 ) | g n ∈ G}. We find that this is indeed the case. </p><formula xml:id="formula_2">T k = {t kn = X n (X * n X n ) −1 u k | n = 1, . . . , N } is a set of transformed templates under the action of group G.</formula><p>The proof is analogous to that of Theorem 3.2 to be presented later (with a linear kernel). Since all K sets T k can be expressed as being acted on by a group, one can compute an invariant feature of dimension K by either estimating the distribution or by computing moments of the K 1-D distributions { I, t kn }, ∀k <ref type="bibr" target="#b1">[2]</ref>. Hence, the feature extraction can capitalize on storing templates to 'learn' transformations in group G while being explicitly discriminative.</p><p>Although DILF does achieve explicit discriminability, performance can be enhanced by dot-products in a highdimensional space incorporating inherent non-linearities. This directly motivates the use of kernels into DILF thus arriving at Discriminative Invariant Kernel Features (DIKF).</p><p>Discriminative invariant kernel features (DIKF). We now present our central method of extracting explicitly discriminative invariant features. Our goal remains the same as in the previous section, and so does our problem formulation. However, now our approach incorporates highdimensional embeddings in the form of kernels. We emphasize the fact that the method remains inherently simple (dotproducts followed by statistics computed over the invariant distribution) albeit in a much high dimensional space.</p><p>Consider a feature mapping Φ : R d → H, to some highdimensional Hilbert space H, then learning the filter on a data matrix X n (where each column vector is a vectorized image x kn , ∀k = 1, . . . , K with transformation g n ∈ G), the template becomes</p><formula xml:id="formula_3">Φ(t kn ) = Φ(X n ) Φ(X n ) · Φ(X n ) −1 u k<label>(2)</label></formula><p>However, in order to be able to extract invariant features using DIKF, the K sets of filters need to form to a set acted upon by group G. This puts constraints on the kind of kernel Φ can be. In this case, Φ is constrained to be a unitary kernel as per the following definition.</p><formula xml:id="formula_4">Definition 3.1 (Unitary Kernel). We define a kernel k(x, y) = φ(x), φ(y) to be a unitary kernel if, for a unitary group G, the mapping φ(x) : X → H satisfies φ(gx), φ(gy) = φ(x), φ(y) ∀g ∈ G, ∀x, y ∈ X .</formula><p>We now show that for unitary kernels, indeed, DIKF filters form a set of transformed templates in the kernel space under the unitary group. </p><formula xml:id="formula_5">g with |G| = N , if k(x, y) = φ(x), φ(y) i.e.</formula><p>k is a unitary kernel, and {X n | X n = g n (X), g n ∈ G} are a set of pre-whitened matrices acted upon by G, then the set of DIKF filters</p><formula xml:id="formula_6">T k = Φ(t kn ) = Φ(X n ) Φ(X n ) · Φ(X n ) −1 u k | ∀n</formula><p>is a set of transformed templates under a group.</p><p>Proof. Without loss of generality, consider</p><formula xml:id="formula_7">X 1 = {x i |i = 1, . . . , K} and X n = {g n (x i )|i = 1, . . . , K} where g n ∈ G. Thus, Φ(t kn ) = Φ(X n ) Φ(X n ) · Φ(X n ) −1 u k .</formula><p>For an unitary kernel, Φ(g n (x)), Φ(g n (y)) = Φ(x), Φ(y) . Thus, ∃g : H → H s.t. gΦ(x), gΦ(y) = Φ(x), Φ(y) . g is the transformation between x and g(x) in the kernel Hilbert space. The unitary kernel property implies that g is unitary and therefore linear. Further, it forms a group G ′ in the kernel Hilbert space. Now, we have</p><formula xml:id="formula_8">Φ(t kn ) = Φ(Xn) (Φ(Xn) · Φ(Xn)) −1 u k (3) = Φ(gn(X1))(Φ(gn(X1)) · Φ(gn(X1))) −1 u k (4) = Φ(gn(X1))(Φ(X1) · Φ(X1)) −1 u k (5) = Φ(gn(X1))v k = g n (Φ(X1))v k (6) = g n (Φ(X1)v k )<label>(7)</label></formula><p>In Eqn. 4, with a slight abuse of notation, we use the same notation g n (·) and g n (·) as an operator both on a single image x or a matrix of images X. In Eqn. 5, note that the transformation in the kernel Hilbert space is unitary. In Eqn. 6, we put v k = (Φ(X 1 ) · Φ(X 1 )) −1 u k and recall that g n is linear. Thus, every element of the template set</p><formula xml:id="formula_9">Φ(t kn ) = Φ(X n ) Φ(X n ) · Φ(X n ) −1 u k ∈ H k</formula><p>can be written as an unitary transformation of the vector</p><formula xml:id="formula_10">Φ(X 1 )v k with group element g n ∈ G ′ .</formula><p>Preservation of the group transformation property for all sets T k even in the kernel Hilbert space allows for 1-D distributions of the filter responses t kn with a novel image to be invariant to G. One can compute statistics, such as moments, that therefore become invariant to G in the original image space. In this paper, we explore two such moments, the first moment translating to mean pooling and the infinity moment translating to max pooling. Hence, we can use the learned filters to model the transformations of the data instead of needing to explicitly observe transformations of the novel image, thereby reducing sample complexity. It is interesting to note that unitary transformations allow sets of non-linear filters (templates or hyperplanes) to form sets of transformed templates under a group in the kernel Hilbert space as well. This would allow a much broader class of discriminative models to fit into the approach of generating invariance though moment measurement. Nonetheless, in this study, we restrict ourselves to filter based approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Common Framework for Landmark-Free</head><p>Unsupervised Pose-Invariant Face Recognition and Pose Estimation</p><p>Applying DIKF to faces. Although discriminative invariant feature extraction framework can be applied to any kind of data observing transformations modeled by a group, in this work, we focus on faces. Challenging transformations of faces include translation, in-plane rotation and scaling which can be perfectly modeled linearly by some unitary G. However, out-of-plane rotation or pose variation is considered to be much more challenging being nonlinear. Nonetheless, a small enough pose variation can indeed be approximated by some G. It has been previously observed that pose variations can be piece-wise linearly approximated through transformation-dependent submanifold unfolding <ref type="bibr" target="#b28">[29]</ref>. The training set X = {X n | g n ∈ G} for training our templates to generate invariance towards pose variation would involve faces in different poses. In the case of faces, we do this by generating a 3-D model of each face template and then rendering them in different poses using 3D generic elastic model (3DGEM) <ref type="bibr" target="#b13">[14]</ref>. Thus, for K different faces, we can have N different transformations of pose. The training set would thus have subject variation along one axis and pose variation along the other (see <ref type="figure" target="#fig_0">Fig. 1</ref>). Note that this step is part of dataset generation and not actually a part of the algorithm.</p><p>Unsupervised training of templates. It is straightforward to apply DIKF to a supervised setting. For the unsupervised setting, we simply choose random faces (not restricted to be from different subjects), and generate multiple poses to obtain the training set used to learn the templates/filters. In training DILF and DIKF, we simply allow u k to be 1 only for the k-th face, thereby extracting features discriminative between the faces in the training set without using any labels.</p><p>Single framework for pose-invariant face recognition and pose estimation. There are two different kinds of transformations modeled in the face training set X = {X n | g n ∈ G} , i.e. pose variation and subject variation. The transformation across subjects is a much harder transformation to model. Even though there exists in general, an affine transformation between subjects, it is hard to prove it is a group, and thus is an abuse of the theoretical framework. Nonetheless, in practice, we explore the limits to the method. To generate invariance, we 'pool' across subjects for a subject-invariant pose-selective feature for pose estimation and 'pool' across poses to obtain a pose-invariant subject-selective feature for face verification. This is the essence of the common framework. Note that even if we are unable to observe the entire orbit of G, approximate invariance would hold <ref type="bibr" target="#b1">[2]</ref>. If we only pool across pose variation, we only require the two eye-center locations to be aligned across all the faces. This is a much less restrictive and computationally feasible condition than having to estimate a dense set of landmarks in order to perform feature extraction <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b34">35]</ref>. We later drop this requirement and move towards a completely landmark-free system at evaluation. Algorithm 1 formally describes DIKF, where G in our case models pose-variation for face verification. Sequential invariance to multiple transforms. Affine transformations in images span a huge space, and to generate invariance to the entire group would involve dense sam-pling of the orbit of the group (which comprises of subgroups e.g. unitary transforms contain translation and rotation). This would result in sample complexity being exponential in the number of sub-group elements since we are required to sample all combinations. Instead, we consider the factored-out transformations in sequence, e.g. the translation sub-group, followed by the rotation sub-group and so on. This reduces the sample complexity drastically down to being linear in number of sub-group elements. The procedure applies Algorithm 1 in sequence in multiple 'levels', with each level having its training template based off features from the previous level. Thus, each level progressively adds in invariance until all sub-groups being modeled are covered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Pose-Invariant Face Recognition</head><p>We test our proposed DIKF for pose-invariant face recognition and compare against NDP in a recent study <ref type="bibr" target="#b25">[26]</ref> </p><formula xml:id="formula_11">2 .</formula><p>A. Single-level pose-invariant face recognition on a large-scale semi-synthetic mugshot database.</p><p>Our first experiment is exploratory, and deals exclusively with the most important transformation considered in this paper, i.e., pose variation. Although there has been some interesting previous work on testing pose invariance exclusively using NDP, the experiments were carried out in small scale and by using fully synthetic faces with minimal texture variation <ref type="bibr" target="#b1">[2]</ref>. Even though strong results using a similar pooling approach on LFW have been recently established, the existence of a multitude of transformations in LFW make it difficult to establish how effective was the invariance to pose transformation specifically <ref type="bibr" target="#b25">[26]</ref>.</p><p>Dataset generation. We choose to focus exclusively on the transformation of pose, thus we design this experiment to not include most other transformations such as translation, scale and in-plane rotation. We start with 1,000 frontal mugshot images of different subjects and then use 3D-GEM to generate a 3D model of each face and render multiple poses. We render poses varying from −40 • to 40 • (yaw) and −20 • to 20 • (pitch) in steps of 5 • . Thus, in all for each subject we obtain 153 different poses, thereby coming up to a total of 153,000 images 3 .</p><p>Protocol. We evaluate ℓ ∞ -DIKF and ℓ 1 -DIKF and    Results. <ref type="figure" target="#fig_5">Fig. 2</ref> shows the experimental results with receiver operating characteristics (ROC) curves obtained for all methods <ref type="bibr" target="#b3">4</ref> . We notice that ℓ ∞ /ℓ 1 -DIKF performs considerably better on this task than the other methods. Therefore, in future experiments that are more competitive, we focus on DIKF. Also, we find that NDP-ℓ 1 <ref type="bibr" target="#b25">[26]</ref> (which was shown to perform very competitively on LFW after using some pre-processing) and NDP-ℓ ∞ <ref type="bibr" target="#b1">[2]</ref>, are outperformed by all discriminative methods i.e. ℓ ∞ /ℓ 1 -DIKF and ℓ ∞ /ℓ 1 -DILF. We find this trend to be consistent in all our experiments. However, it is interesting to note that even vanilla dot-products followed by pooling (NDP-ℓ ∞ ), perform as well as they do, suggesting that the goal of explicitly factoring out invariance has some merit. The specific choice of moment of the invariant distribution to measure appears to have an effect only in discriminative methods.</p><p>B. Open set (OS) landmark-free face recognition on large-scale semi-synthetic mugshot database using twolevel sequential invariance generation. In this exploratory experiment, we simulate a controlled situation closer to real-world unconstrained face matching.</p><p>Protocol. We train a two-level invariance generation framework (i.e. running Algorithm 1 in sequence) where the face data set at level 1 is designed to generate invariance for only pose variation (we use the same 250 subjects for pose invariance as in the previous experiment), and the one at level 2, generates invariance towards the four sets of transformations (translation, scale, in-plane rotation and all together). At level 2, we construct four distinct template sets for each of the aforementioned variations exclusively, and another one for all three transformations together. Note that to generate invariance at level 2, we need to store the poseinvariant features of the training faces from level 1 for each transformation set. We use each template set at level 2 to generate transformation invariance from the pose-invariant features obtained in level 1. The sequential training follows what is described in Section 4 and tests on the features of the randomly generated unseen test sets. Note that for this experiment, we are completely landmark and alignment-free at evaluation, since we match using the images in <ref type="figure" target="#fig_6">Fig. 3</ref> as is without any additional information. The hope remains that the framework would be invariant towards all other transformations in the template sets while being discriminative to the transformation across subjects.</p><p>Data generation for level 2 training and overall testing. To construct the face data for training the template filter sets for each of the four transformations, we generate a 3D model of 100 training subjects, and render them at each of the 153 different poses as before. However, we now also synthetically add in the 4 sets of additional transformations (translation, scaling, in-plane rotation and all of them together). Further, we fill the background of the face wherever visible with random noise. Thus overall, we obtain 4 data sets of 15,300 images each. To generate our Open Set testing data, we repeat the randomized process to generate another four 15,300-image sets using the four sets of transformations for 100 unseen subjects. This randomly generated dataset (samples shown in <ref type="figure" target="#fig_6">Fig. 3</ref>) has a very challenging protocol without any landmarks or alignment.</p><p>Results. <ref type="figure" target="#fig_8">Fig. 4</ref> shows the ROC curves. Level 2 ℓ ∞ /ℓ 1 -DIKF outperforms the other methods in all cases 6 . Further, it performs surprisingly well given the difficulty in the cases of adding individual transformations to pose variation. Adding in all transformations and following the OS protocol seems to be too difficult to handle well for all methods. Roughly, in a decreasing order of difficulty the transformations are, all transformations, translation, scale followed by in-plane rotation.</p><p>C. MPIE database. Protocol. There has not been a universal evaluation protocol established for the MPIE database. Therefore, we follow the pose-invariant face recognition protocol as adopted  in previous studies <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b0">1]</ref>. The authors use a subset of MPIE from the first session with neutral expression and frontal lighting, covering nine poses (−60 • to 60 • in yaw in steps of 15 • ) of 249 subjects 7 . We, however, generate 153 poses for each of the gallery image offline and compile our face template set. Since MPIE has 249 subjects, for each query image we form a 249 dimensional pose-invariant feature vector. We then match the features to that of the gallery image. In a parallel experiment, we also explore matching all off-angle posed images against the frontal images. This is a much harder protocol than matching each angle in yaw separately.</p><formula xml:id="formula_12">Level-2`1-DIKF Level-1`1-DIKF Level-2`1-DIKF Level-1`1-DIKF Level-1 NDP-`1 Level-1 NDP-`1 (a) (b) (c) (d)</formula><p>Results. The results are shown in <ref type="table" target="#tab_1">Table 1</ref>. We see that ℓ ∞ -DIKF achieves the state-of-the-art in terms of rank-1 identification rate for all poses except 60 • . We attribute this to the fact that we do not generate faces beyond 40 • yaw during training and so we observe graceful degradation for higher yaw angles. As an important distinction, we do not use any form of landmarking, unlike <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b0">1]</ref>, further our method can support open-set matching, unlike <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b14">15]</ref>. <ref type="figure" target="#fig_9">Fig. 5</ref> shows the ROC curves obtained for the parallel experiment of matching all off-angle posed images against the frontal images. We find that ℓ ∞ -DIKF significantly outperforms other methods.</p><p>D. LFW: real-world unconstrained face recognition database. Here, we apply the ℓ ∞ -DIKF methods on a realworld unconstrained face recognition database: the LFW database <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b18">19]</ref>. Protocol. We follow the standard Unsupervised protocol as well the Image-Restricted, Label-Free Outside Data protocol. Note the transformations in LFW probably do not form a group, we only foucs on generating invariance towards pose in this experiment and use only the two eye-center locations for alignment. The DIKF training is carried out on raw pixels of a set of 3,000 randomly chosen label-free face images (which can and have multiple images from the same <ref type="bibr" target="#b6">7</ref> In their work, pitch variation was not explored and to have a fair comparison we restrict ourselves exclusively to variation along the yaw axis. subject). In order to abide by the Unsupervised protocol, we treat each face image as if they are from different subjects, making our algorithm completely agnostic about label information. Our training procedure also satisfies the second protocol mentioned above.</p><p>Results. The results for the Unsupervised protocol is reported in <ref type="table" target="#tab_2">Table 2</ref> in terms of AUC, and the mean accuracy for the Image-Restricted, Label-Free Outside Data protocol, along with many other competing algorithms. <ref type="figure" target="#fig_10">Fig. 6</ref> shows the ROC curves for the LFW evaluation. The performance obtained by the proposed ℓ ∞ -DIKF method is on par with the state-of-the-art. This is somewhat surprising considering we use only rough alignment and work directly on raw pixels unlike some of the other methods we outperform.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Dense-Landmark-Free Pose Estimation</head><p>Protocol. For the experiment on pose estimation, we generate 15 poses (−40 • to 40 • along yaw and −20 • to 20 • along pitch all in steps of 20 • ) for 350 unseen subjects (5,250 images). For this experiment, we utilize only the two eye-center locations for alignment, hence being denselandmark-free. We construct the data set for training templates using images aligned using the eye-center locations from 250 of these subjects and their poses. Recall that the only difference between the approach for face recog-    <ref type="bibr" target="#b39">[40]</ref> 0.9405 MRF-MLBP <ref type="bibr" target="#b3">[4]</ref> 0.8994 MRF-Fusion-CSKDA <ref type="bibr" target="#b4">[5]</ref> (no ROC) 0.9894 Spartans <ref type="bibr" target="#b21">[22]</ref> 0.9428</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image-Restricted, Label-Free Outside Data Protocol</head><p>Approach Results µ ± S E Proposed ℓ∞-DIKF 0.8867±0.0046 Combined b/g samples based <ref type="bibr" target="#b38">[39]</ref>, aligned 0.8683±0.0034 LBP + CSML <ref type="bibr" target="#b26">[27]</ref>, aligned 0.8557±0.0052 CSML + SVM <ref type="bibr" target="#b26">[27]</ref>, aligned 0.8800±0.0037 High-throughput brain-inspired <ref type="bibr" target="#b29">[30]</ref>, aligned 0.8813±0.0058 LARK supervised <ref type="bibr" target="#b32">[33]</ref>, aligned 0.8510±0.0059 DML-eig combined <ref type="bibr" target="#b40">[41]</ref>, funneled, aligned 0.8565±0.0056 SFRD + PMML <ref type="bibr" target="#b8">[9]</ref> 0.8935±0.0050 Pose Adaptive Filter (PAF) <ref type="bibr" target="#b39">[40]</ref> 0.8777±0.0051 Convolutional DBN <ref type="bibr" target="#b19">[20]</ref> (no ROC) 0.8777±0.0062 Sub-SML <ref type="bibr" target="#b6">[7]</ref> 0.8973±0.0038 VMRS <ref type="bibr" target="#b5">[6]</ref> (no ROC) 0.9110±0.0059 DDML <ref type="bibr" target="#b15">[16]</ref> 0.9068±0.0141 LM3L <ref type="bibr" target="#b16">[17]</ref> 0.8957±0.0153 Hybrid on LFW3D <ref type="bibr" target="#b12">[13]</ref> 0.8563±0.0053 Spartans <ref type="bibr" target="#b21">[22]</ref> 0.8969±0.0036 MSBSIF-SIEDA <ref type="bibr" target="#b27">[28]</ref> 0.9463±0.0095 nition and pose estimation is that in Algorithm 1, G models pose variation for face recognition and subject variation for pose estimation (thereby being a common single framework for both tasks). We train on the 250 subjects with their poses and then test on the 1,500 images of the remaining   100 subjects. For each test image, we extract a subjectinvariant pose-specific DIKF. We then match all features across each other and verify using the ground truth pose labels (15 poses). Results. <ref type="figure" target="#fig_12">Fig. 7</ref> presents the ROC curves and statistics obtained for this experiment. We find that ℓ ∞ -DIKF achieves a much better accuracy of 86% using only the two eye-center coordinates under this challenging protocol, thereby demonstrating the efficacy of the approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>This paper presents a 'bells and whistles' free approach to learn or extract discriminative features that are invariant to unitary transformations. The theoretical results allow discriminative and kernelized features based on unitary kernels, to achieve group invariance through moments, thereby allowing for much more complex models to guarantee invariance. It proposes a single framework for face recognition and pose estimation and the practical algorithm used was always restricted to be 'simple', all the while working on raw-pixels. Yet, DIKF outperforms many previous, more complex methods and achieves (MPIE) or is on par with the state-of-the-art (LFW). The sequential generation of invariance is able to handle much more challenging protocols, unlike pooling over the traditional NDP. Although we focus only on face recognition and pose estimation, the results provide more evidence towards the hypothesize that perhaps a careful balance between invariance and selectivity in important for general vision tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>The common single framework for pose-invariant face recognition and subject-invariant pose estimation. Measuring moments (pooling) across a transformation (i.e. subject or pose in this figure) invokes invariance towards that transformation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Theorem 3.1 (DILF filters form a set of transformed templates under a group). Given a group G of unitary transformation elements g with |G| = N and {X n | n = 1, . . . , N } are pre-whitened template matrices, then the set of DILF filters</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Theorem 3.2 (DIKF filters form a set of transformed templates in the kernel space under a group). Given a group G of unitary transformation elements</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Algorithm 1 :</head><label>1</label><figDesc>Extracting DIKF for I invariant to G input : Input image I (vectorized), X = {Xn = gn(X)|n = 1, . . . , N }, {u k |k = 1, . . . , K}, Φ, G with |G| = N output: Invariant feature vector µ ∈ R K 1 Learn and compute correlations with filters; 2 for gn ∈ G do 3 for k = 1, . . . , K do 4 t kn = Xn Φ(Xn) · Φ(Xn) −1 u k f kn = Φ(t kn ) ⊤ Φ(I) 5 Compute first / infinity moment; 6 for k = 1, . . . = f k ∞ (Max Pooling)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 2 :</head><label>2</label><figDesc>Pose-invariant face recognition results on the semi-synthetic large-scale mugshot database (testing on 114,750 images).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 3 :</head><label>3</label><figDesc>Samples from the testing set containing randomized synthetic transformations for one subject. (L-R): Pose, Noisy background and Scale only; Pose, Noisy background and Translation only; Pose, Noisy background and In-plane Rotation only; All transformations together 5benchmark against NDP-ℓ ∞ and NDP-ℓ 1<ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b1">2]</ref> directly on raw-pixels. We train on 250 subjects (38,250 images) and test each method on the remaining 750 subjects (114,750 images), matching all pose-varied images of a subject to each other. Since we do not model our testing subjects during training, the protocol followed is an Open Set (OS) face verification protocol.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 4 :</head><label>4</label><figDesc>ROC curves for the Level-1 and Level-2 Open Set face matching protocol on large-scale mugshots database (using no landmarks). Different combinations of transformations are modeled for invariance (a): Pose, Noisy background and Scale only, (b): Pose, Noisy background and Translation only, (c): Pose, Noisy background and In-plane Rotation only, and (d): All transformations together.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 5 :</head><label>5</label><figDesc>MPIE results on the parallel experiment: All non-frontal images matched against frontal ones.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 6 :</head><label>6</label><figDesc>ROC curves for LFW evaluation under (L) Unsupervised protocol, and (R) under Image-Restricted, Label-Free Outside Data protocol.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 7 :</head><label>7</label><figDesc>Pose Estimation results on a subset of the large-scale mugshots database generated in Section 5.1 (testing on 1,500 images).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 1 :</head><label>1</label><figDesc>Rank-1 ID rate and VR at 1% FAR (Italics) for ℓ∞-DIKF against five previous benchmarks for the MPIE database.</figDesc><table>L60 • 
L45 • 
L30 • 
L15 • 
R15 • 
R30 • 
R45 • 
R60 • 
All 
Pixel [31] 
0.4 (4.3) 
0.4 (4.3) 
8.8 (7.6) 
15.7 (17.3) 
24.9 (20.9) 
9.6 (11.6) 
1.2 (5.7) 
0.8 (5.0) 
5.6 (3.7) 
PCA [31] 
2.4 (3.6) 
1.2 (4.3) 
18.5 (9.6) 
24.5 (20.1) 
31.3 (23.3) 
18.1 (11.6) 
2.0 (7.2) 
2.4 (5.0) 
10.1 (7.9) 
Prabhu et al. [31] 
44.9 (26.4) 
65.0 (37.1) 
86.7 (59.4) 
97.6 (75.5) 
93.2 (71.7) 
83.5 (49.0) 
65.0 (45.0) 
43.1 (29.7) 
-(-) 
Heo et al. [15] 
-(-) 
-(-) 
87 (-) 
96 (-) 
93 (-) 
90 (-) 
-(-) 
-(-) 
-(-) 
Abiantun et al. [1] 
-(-) 
51.00 (-) 
85.94 (-) 
97.18 (-) 
97.18 (-) 
87.95 (-) 
53.41 (-) 
-(-) 
-(-) 

ℓ∞-DIKF 
27.7 (21.7) 
81.9 (64.7) 
92.4 (77.9) 
98.8 (97.6) 
99.6 (98.8) 
94.4 (76.7) 
83.1 (55.8) 
28.1 (21.7) 
75.75 (49.9) 
ℓ1-DIKF 
4.4 (0.8) 
31.3 (31.3) 
61.5 (60.2) 
98.8 (96.8) 
99.6 (99.2) 
64.3 (64.3) 
28.1 (36.9) 
8.0 (11.2) 
49.5 (36.4) 
NDP-ℓ∞ 
2.8 (1.6) 
1.6 (2.4) 
6.4 (6.0) 
84.7 (75.9) 
92.0 (79.5) 
18.5 (21.7) 
4.4 (2.0) 
2.4 (1.2) 
26.6 (23.2) 
NDP-ℓ1 
0.4 (0.4) 
0.4 (0.4) 
0.8 (1.6) 
64.7 (28.5) 
56.6 (55.8) 
3.2 (2.8) 
0.4 (0.1) 
0.4 (0.4) 
15.9 (6.1) 

10 −3 
10 −2 
10 −1 
10 0 

0 

0.1 

0.2 

0.3 

0.4 

0.5 

0.6 

0.7 

0.8 

0.9 

1 

Unsupervised Protocol 

False Accept Rate 

Veri cation Rate 

Proposed`1-DIKF 
SD-MATCHES, 125x125, aligned 
H-XS-40, 81x150, aligned 
GJD-BC-100, 122x225, aligned 
LARK unsupervised, aligned 
Pose Adaptive Filter (PAF) 
MRF-MLBP 
Spartans 

10 −3 
10 −2 
10 −1 
10 0 

0 

0.1 

0.2 

0.3 

0.4 

0.5 

0.6 

0.7 

0.8 

0.9 

1 

Image−Restricted, Label−Free Outside Data Protocol 

False Accept Rate 

Veri cation Rate 

Proposed`1 DIKF 
Combined b/g samples based methods, aligned 
LBP + CSML, aligned 
CSML + SVM, aligned 
High-throughput brain-inspired feature, aligned 
LARK supervised, aligned 
DML-eig combined, funneled &amp; aligned 
SFRD + PMML 
Pose Adaptive Filter (PAF) 
Sub-SML 
DDML 
LM3L 
Hybrid on LFW3D 
Spartans 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc>LFW results for the two protocols<ref type="bibr" target="#b17">[18]</ref>.SD-MATCHES, 125 × 125 [10], aligned 0.5407 H-XS-40, 81 × 150 [10], aligned 0.7547 GJD-BC-100, 122 × 225 [10], aligned</figDesc><table>Unsupervised Protocol 
Approach 
Results AUC 
Proposed ℓ∞-DIKF 
0.9154 
0.7392 
LARK unsupervised [33], aligned 
0.7830 
LHS [34], aligned 
0.8107 
Pose Adaptive Filter (PAF) </table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">This is a slight abuse of notation, wherein g can act both on vectors and column-wise on matrices.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We present more experiments in the supplementary material.<ref type="bibr" target="#b2">3</ref> The dataset that we generate can be termed as a semi-synthetic dataset, since we use real-world face images to render different poses. In this particular experiment, the template training data contains only the transformations of subject and pose. In order to factor out other common transformations, we use the two eye-center locations for locating and aligning the face. In a future experiment, when we include additional transformations, we drop this requirement and DIKF is applied completely landmark-free (at evaluation).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Throughout this work, we use the cosine distance metric to generate the ROC curves.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">NDP fails to extract any discrimination among subjects at level 2 for the OS protocol.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Sparse feature extraction for pose-tolerant face recognition. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Abiantun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Prabhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savvides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Magic materials: a theory of deep hierarchical architectures for learning sensory representations. MIT, CBCL paper</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Anselmi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Leibo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Rosasco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mutch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tacchetti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Class-specific kernel fusion of multiple descriptors for face verification using multiscale binarised statistical image features. Information Forensics and Security</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Arashloo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2100" to="2109" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Efficient processing of mrfs for unconstrained-pose face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Arashloo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE BTAS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Class-specific kernel fusion of multiple descriptors for face verification using multiscale binarised statistical image features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Arashloo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2100" to="2109" />
			<date type="published" when="2014-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Fast high dimensional vector multiplication face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Barkan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Aronowitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Similarity metric learning for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Blessing of dimensionality: High-dimensional feature and its efficient compression for face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Fusing robust face region descriptors via multiple metric learning for face recognition in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Recognition of faces in unconstrained environments: A comparative study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Solar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Verschae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Correa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Recent Advances in Biometric Systems: A Signal Processing Perspective)</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Multi-directional multi-level dual-cross patterns for robust face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
		<idno>abs/1401.5311</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Robust face recognition via multimodal deep face representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<idno>abs/1509.00244</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Effective face frontalization in unconstrained images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Paz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Enbar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Generic elastic models for 2d pose synthesis and face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Heo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note type="report_type">PhD Thesis</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Gender and ethnicity specific generic elastic models from a single 2d image for novel 2d pose face synthesis and recognition. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savvides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2341" to="2350" />
			<date type="published" when="2012-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Discriminative deep metric learning for face verification in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-P</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Large margin multimetric learning for face and kinship verification in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-P</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision (ACCV)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Results on the Labeled Faces in the Wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Labeled faces in the wild: Updates and new reporting procedures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
		<idno>UM-CS-2014-003</idno>
	</analytic>
	<monogr>
		<title level="m">UMass Amherst</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning hierarchical representations for face verification with convolutional deep belief networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Labeled Faces in the Wild: A Database for Studying Face Recognition in Unconstrained Environments. Technical Report of Univ</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007-10" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
			<pubPlace>Massachusetts, Amherst</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Spartans: Singlesample periocular-based alignment-robust recognition technique applied to non-frontal scenarios. Image Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Juefei-Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savvides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="4780" to="4795" />
			<date type="published" when="2015-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Subtasks of unconstrained face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Leibo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications. (VISAPP)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Hierarchical-pep model for real-world face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2015 IEEE Conference on</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="4055" to="4064" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Probabilistic elastic matching for pose variant face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning invariant representations and applications to face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Leibo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Cosine similarity metric learning for face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">V</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision (ACCV)</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Side-information based exponential discriminant analysis for face verification in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ouamane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bengherabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hadid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cheriet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">11th IEEE International Conference and Workshops on</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">02</biblScope>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
	<note>Automatic Face and Gesture Recognition (FG)</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">An extension of multifactor analysis for face recognition based on submanifold learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savvides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on</title>
		<imprint>
			<date type="published" when="2010-06" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Beyond simple features: A large-scale feature search approach to unconstrained face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Automatic Face and Gesture Recognition (FG)</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Unconstrained poseinvariant face recognition using 3d generic elastic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Prabhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savvides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1952" to="1961" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Face verification using the lark representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">J</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Milanfar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1275" to="1286" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Local higher-order statistics (lhs) for texture categorization and facial analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hussain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jurie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Fisher vector faces in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep learning face representation by joint identification-verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and K. Weinberger</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1988" to="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deep learning face representation from predicting 10,000 classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="1891" to="1898" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deepface: Closing the gap to human-level performance in face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on</title>
		<imprint>
			<date type="published" when="2014-06" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1701" to="1708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Similarity scores based on background samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision (ACCV)</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Towards pose robust face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Distance metric learning with eigenvalue optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research (Special Topics on Kernel and Metric Learning)</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">High-fidelity pose and expression normalization for face recognition in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2015 IEEE Conference on</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="787" to="796" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
