<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Online Detection and Classification of Dynamic Hand Gestures with Recurrent 3D Convolutional Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavlo</forename><surname>Molchanov</surname></persName>
							<email>pmolchanov@nvidia.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Yang</surname></persName>
							<email>xiaodongy@nvidia.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shalini</forename><surname>Gupta</surname></persName>
							<email>shalinig@nvidia.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihwan</forename><surname>Kim</surname></persName>
							<email>kihwank@nvidia.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Tyree</surname></persName>
							<email>styree@nvidia.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
							<email>jkautz@nvidia.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nvidia</forename></persName>
						</author>
						<title level="a" type="main">Online Detection and Classification of Dynamic Hand Gestures with Recurrent 3D Convolutional Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:44+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Automatic detection and classification of dynamic hand gestures in real-world systems intended for human computer interaction is challenging as: 1) there is a large diversity in how people perform gestures, making detection and classification difficult; 2) the system must work online in order to avoid noticeable lag between performing a gesture and its classification; in fact, a negative lag (classification before the gesture is finished) is desirable, as feedback to the user can then be truly instantaneous. In this paper, we address these challenges with a recurrent three-dimensional convolutional neural network that performs simultaneous detection and classification of dynamic hand gestures from multi-modal data. We employ connectionist temporal classification to train the network to predict class labels from inprogress gestures in unsegmented input streams. In order to validate our method, we introduce a new challenging multimodal dynamic hand gesture dataset captured with depth, color and stereo-IR sensors. On this challenging dataset, our gesture recognition system achieves an accuracy of 83.8%, outperforms competing state-of-the-art algorithms, and approaches human accuracy of 88.4%. Moreover, our method achieves state-of-the-art performance on SKIG and ChaLearn2014 benchmarks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Hand gestures and gesticulations are a common form of human communication. It is therefore natural for humans to use this form of communication to interact with machines as well. For instance, touch-less human computer interfaces can improve comfort and safety in vehicles. Computer vision systems are useful tools in designing such interfaces. Recent work using deep convolutional neural networks (CNN) with video sequences has significantly advanced the accuracy of dynamic hand gesture <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b24">25]</ref> and action <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b36">37]</ref> recognition. CNNs are also useful for combining multi-modal data inputs <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b24">25]</ref>, a technique which has proved useful for gesture recognition in challeng-ing lighting conditions <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b26">27]</ref>.</p><p>However, real-world systems for dynamic hand gesture recognition present numerous open challenges. First, these systems receive continuous streams of unprocessed visual data, where gestures from known classes must be simultaneously detected and classified. Most prior work, e.g., <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b26">27]</ref>, regards gesture segmentation and classification separately. Two classifiers, a detection classifier to distinguish between gesture and no gesture and a recognition classifier to identify the specific gesture type, are often trained separately and applied in sequence to the input data streams. There are two reasons for this: <ref type="bibr" target="#b0">(1)</ref> to compensate for variability in the duration of the gesture and (2) to reduce noise due to unknown hand motions in the no gesture class. However, this limits the system's accuracy to that of the upstream detection classifier. Additionally, since both problems are highly interdependent, it is advantageous to address them jointly. A similar synergy was shown to be useful for joint face detection and pose estimation <ref type="bibr" target="#b27">[28]</ref>.</p><p>Second, dynamic hand gestures generally contain three temporally overlapping phases: preparation, nucleus, and retraction <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b13">14]</ref>, of which the nucleus is most discriminative. The other two phases can be quite similar for different gesture classes and hence less useful or even detrimental to accurate classification. This motivates designing classifiers which rely primarily on the nucleus phase.</p><p>Finally, humans are acutely perceptive of the response time of user interfaces, with lags greater than 100 ms perceived as annoying <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b19">20]</ref>. This presents the challenge of detecting and classifying gestures immediately upon or before their completion to provide rapid feedback.</p><p>In this paper, we present an algorithm for joint segmentation and classification of dynamic hand gestures from continuous depth, color and stereo-IR data streams. Building on the recent success of CNN classifiers for gesture recognition, we propose a network that employs a recurrent three dimensional (3D)-CNN with connectionist temporal classification (CTC) <ref type="bibr" target="#b9">[10]</ref>. CTC enables gesture classification to be based on the nucleus phase of the gesture without requiring explicit pre-segmentation. Furthermore, our network addresses the challenge of early detection of gestures, resulting in zero or negative lag, which is a crucial element for responsive user interfaces. We present a new multi-modal hand gesture dataset 1 with 25 classes for comparing our algorithm against state-of-the-art methods and human subject performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Many hand-crafted spatio-temporal features for effective video analysis have been introduced in the area of gesture and action recognition <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b38">39]</ref>. They typically capture shape, appearance, and motion cues via image gradients and optical flow. Ohn-Bar and Trivedi <ref type="bibr" target="#b26">[27]</ref> evaluate several global features for automotive gesture recognition. A number of video classification systems successfully employ improved dense trajectories <ref type="bibr" target="#b38">[39]</ref> and Fisher vector <ref type="bibr" target="#b29">[30]</ref> representations, which are widely regarded as state-of-the-art local features and aggregation techniques for video analysis. Features for depth sensors are usually designed according to the specific characteristics of the depth data. For instance, random occupancy patterns <ref type="bibr" target="#b39">[40]</ref> utilize point clouds and super normal vectors <ref type="bibr" target="#b41">[42]</ref> employ surface normals.</p><p>In contrast to hand-crafted features, there is a growing trend toward feature representations learned by deep neutral networks. Neverova et al. <ref type="bibr" target="#b24">[25]</ref> employ CNNs to combine color and depth data from hand regions and upper-body skeletons to recognize sign language gestures. Molchanov et al. <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref> apply a 3D-CNN on the whole video sequence and introduce space-time video augmentation techniques to avoid overfitting. In the context of action recognition, Simonyan and Zisserman <ref type="bibr" target="#b33">[34]</ref> propose separate CNNs for the spatial and temporal streams that are late-fused and that explicitly use optical flow. Tran et al. <ref type="bibr" target="#b36">[37]</ref> employ a 3D-CNN to analyze a series of short video clips and average the network's responses for all clips. Most previous methods either employ pre-segmented video sequences or treat detection and classification as separate problems.</p><p>To the best of our knowledge, none of the previous methods for hand gesture recognition address the problem of early gesture recognition to achieve the zero or negative lag necessary for designing effective gesture interfaces. Early detection techniques have been proposed for classifying facial expressions and articulated body motion <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b31">32]</ref>, as well as for predicting future events based on incoming video streams <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref>. The predicted motions in many of these methods are aided by the appearance of their environments (i.e., road or parking lot)-something we cannot rely on for gesture recognition. Recently, connectionist temporal classification has been shown to be effective for classification of unsegmented handwriting and speech <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>. We demon-1 https://research.nvidia.com/publication/ online-detection-and-classification-dynamichand-gestures-recurrent-3d-convolutional <ref type="figure">Figure 1</ref>: Classification of dynamic gestures with R3DCNN. A gesture video is presented in the form of short clips C t to a 3D-CNN for extracting local spatial-temporal features, f t . These features are input to a recurrent network, which aggregates transitions across several clips. The recurrent network has a hidden state h t−1 , which is computed from the previous clips. The updated hidden state for the current clip, h t , is input into a softmax layer to estimate class-conditional probabilities, s t of the various gestures. During training, CTC is used as the cost function.</p><p>strate the applicability of CTC for gesture recognition from unsegmented video steams.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>In this section, we describe the architecture and training of our algorithm for multi-modal dynamic hand gesture detection and classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Network Architecture</head><p>We propose a recurrent 3D convolutional neural network (R3DCNN) for dynamic hand gesture recognition, illustrated in <ref type="figure">Fig. 1</ref>. The architecture consists of a deep 3D-CNN for spatio-temporal feature extraction, a recurrent layer for global temporal modeling, and a softmax layer for predicting class-conditional gesture probabilities.</p><p>We begin by formalizing the operations performed by the network. We define a video clip as a volume C t ∈ R k×ℓ×c×m of m ≥ 1 sequential frames with c channels of size k ×ℓ pixels ending at time t. Each clip is transformed into a feature representation f t by a 3D-CNN F:</p><formula xml:id="formula_0">F : R k×ℓ×c×m → R q , where f t = F(C t ),</formula><p>by applying spatio-temporal filters to the clip. A recurrent layer computes a hidden state vector h t ∈ R d as a function of the hidden state after the previous clip h t−1 and the feature representation of the current clip f t :</p><formula xml:id="formula_1">h t = R(W in f t + W h h t−1 ),</formula><p>with weight matrices W in ∈ R d×q and W h ∈ R d×d , and truncated rectified linear unit R : R d → R d , R(x) = min(max(0, x), 4) to limit gradient explosion <ref type="bibr" target="#b28">[29]</ref> during training. Finally, a softmax layer transforms the hidden state vector h t into class-conditional probabilities s t of w classes:</p><formula xml:id="formula_2">s t = S(W s h t + b),</formula><p>with weights W s ∈ R w×d , bias b ∈ R w , and a softmax function S :</p><formula xml:id="formula_3">R w → R w [0,1] , where [S(x)] i = e xi / k e x k .</formula><p>We perform classification by splitting the entire video V into T clips of length m and computing the set of classconditional probabilities S = {s 0 , s 1 , ..., s T −1 } for each individual clip. For offline gesture classification, we average the probabilities of all the clips belonging to a presegmented gesture s avg = 1/T s∈S s, and the predicted class isŷ = argmax i ([s avg ] i ), across all gesture classes i. When predicting online with unsegmented streams, we consider only clip-wise probabilities s t .</p><p>We combine multiple modalities by avereging the classconditional probabilities estimated by the modality-specific networks. During online operation, we average probabilities across modalities for the current clip only. As an alternative to the softmax layer, we additionally consider computing the final classification score with a support vector machine (SVM) <ref type="bibr" target="#b5">[6]</ref> classifier operating on features f t or h t extracted by the R3DCNN. We average the features across video clips and normalize by their ℓ 2 -norms to form a single representation for the entire video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Training</head><p>Let X = {V 0 , V 1 , ..., V P−1 } be a mini-batch of training examples in the form of weakly-segmented gesture videos V i . <ref type="bibr" target="#b1">2</ref> Each video consists of T clips, making X a set of N = T ·P clips. Class labels y i are drawn from the alphabet A to form a vector of class labels y with size |y| = P .</p><p>Pre-training the 3D-CNN. We initialize the 3D-CNN with the C3D network <ref type="bibr" target="#b36">[37]</ref> trained on the large-scale Sport-1M <ref type="bibr" target="#b12">[13]</ref> human action recognition dataset. The network has 8 convolutional layers of 3×3×3 filters and 2 fully-connected layers trained on 16-frame clips. We append a softmax prediction layer to the last fully-connected layer and fine-tune by back-propagation with negative log-likelihood to predict gestures classes from individual clips C i . Training the full model. After fine-tuning the 3D-CNN, we train the entire R3DCNN with back-propagationthrough-time (BPTT) <ref type="bibr" target="#b40">[41]</ref>. BPTT is equivalent to unrolling the recurrent layers, transforming them into a multi-layer feed-forward network, applying standard gradient-based back-propagation, and averaging the gradients to consolidate updates to weights duplicated by unrolling.</p><p>We consider two training cost functions: negative loglikelihood for the entire video and connectionist temporal classification (CTC) for online sequences. The negative log-likelihood function for a mini-batch of videos is:</p><formula xml:id="formula_4">L v = − 1 P P −1 i=0 log p(y i |V i ) ,</formula><p>where p(y i |V i ) = [s avg ] yi is the probability of gesture label y i given gesture video V i as predicted by R3DCNN.</p><p>Connectionist temporal classification. CTC is a cost function designed for sequence prediction in unsegmented or weakly segmented input streams <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>. CTC is applied in this work to identify and correctly label the nucleus of the gesture, while assigning the no gesture class to the remaining clips, addressing the alignment of class labels to particular clips in the video. In this work we consider only the CTC forward algorithm. We extend the dictionary of existing gestures with a no gesture class: A ′ = A ∪ {no gesture}. Consequently, the softmax layer outputs a class-conditional probability for this additional no gesture class. Instead of averaging predictions across clips in a pre-segmented gesture, the network computes the probability of observing a particular gesture (or no gesture) k at time t in an input sequence X :</p><formula xml:id="formula_5">p(k, t|X ) = s k t ∀t ∈ [0, N ).</formula><p>We define a path π as a possible mapping of the input sequence X into a sequence of class labels y. The probability of observing path π is p(π|X ) = t s πt t , where π t is the class label predicted at time t in path π.</p><p>Paths are mapped into a sequence of event labels y by operator B as y = B(π), condensing repeated class labels and removing no gesture labels, e.g., <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>, where 1, 2 are actual gesture classes and "−" is no gesture. Under B, many paths π result in the same event sequence y. The probability of observing a particular sequence y given an input sequence X is the sum of the conditional probabilities of all paths π mapping to that sequence, B −1 (y) = {π : B(π) = y}:</p><formula xml:id="formula_6">B([−, 1, 2, −, −]) = B([1, 1, −, 2, −]) =</formula><formula xml:id="formula_7">p(y|X ) = π∈B −1 (y) p(π|X ).</formula><p>Computation of p(y|X ) is simplified by dynamic programing. First, we create an assistant vectorẏ by adding a no gesture label before and after each gesture clip in y, so thatẏ contains |ẏ| = P ′ = 2P + 1 labels. Then, we compute a forward variable α ∈ R N ×P where α t (u) is the combined probability of all mappings of events up to clip t and event u. The transition function for α is:</p><formula xml:id="formula_8">α t (u) = sẏ u t α t−1 (u) + α t−1 (u − 1) + β t−1 (u − 2) , where β t (u) = α t (u), ifẏ u+1 = no gesture andẏ u =ẏ u+2 0, otherwise</formula><p>andẏ u denotes the class label of event u. The forward variable is initialized with α 0 (0) = sẏ 0 0 , the probability of a path beginning withẏ 0 = no gesture, and α 0 (1) = sẏ 1 0 , the probability of a path starting with the first actual evenṫ y 1 . Since a valid path cannot begin with a later event, we initialize α 0 (i) = 0 ∀i &gt; 1. At each time step t &gt; 0, we consider paths in which the event u is currently active (with probability sẏ u t ) and (1) remains unchanged from the previous time t − 1 (α t−1 (u)), (2) changes from no gesture to the next actual gesture or vice versa (α t−1 (u − 1)), or (3) transitions from one actual gesture to the next while skipping no gesture if the two gestures have distinct labels (β t−1 (u − 2)). Finally, any valid path π must end at time N − 1 with the last actual gestureẏ P ′ −1 or with no gesturė</p><formula xml:id="formula_9">y P ′ , hence p(y|X ) = α N −1 (P ′ − 1) + α N −1 (P ′ ).</formula><p>Using this computation for p(y|X ), the CTC loss is:</p><formula xml:id="formula_10">L CT C = − ln(p(y|X )),</formula><p>expressed in the log domain <ref type="bibr" target="#b8">[9]</ref>. While CTC is used as a training cost function only, it affects the architecture of the network by adding the extra no gesture class label. For presegmented video classification, we simply remove the nogesture output and renormalize probabilities by the ℓ 1 -norm after modality fusion.</p><p>Learning rule. To optimize the network parameters W with respect to either of the loss functions we use stochastic gradient descent (SGD) with a momentum term µ = 0.9. We update each parameter of the network θ ∈ W at every back-propagation step i by:</p><formula xml:id="formula_11">θ i = θ i−1 + v i − γλθ i−1 , v i = µv i−1 − λJ δE δθ batch ,</formula><p>where λ is the learning rate, δE δθ batch is the gradient value of the chosen cost function E with respect to the parameter θ averaged over the mini-batch, and γ is the weight decay parameter. To prevent gradient explosion in the recurrent layers during training, we apply a soft gradient clipping operator J (·) <ref type="bibr" target="#b28">[29]</ref> with a threshold of 10.</p><p>Regularization. We apply a number of regularization techniques to reduce overfitting. We train with weight decay (γ = 0.5%) on all weights in the network. We apply drop-out <ref type="bibr" target="#b10">[11]</ref> to the fully-connected layers of the 3D-CNN at a rate of p = 75%, rescaling the remaining activations by a factor of 1/(1 − p). Additionally, we find that dropping feature maps in the convolutional layers improves generalization in pre-trained networks. For this, we randomly set 10% of the feature maps of each convolutional layer to 0 and rescale the activations of the others neurons accordingly.</p><p>Implementation. We train our gesture classifier in Theano <ref type="bibr" target="#b1">[2]</ref> with cuDNN3 on an NVIDIA DIGITS DevBox with four Titan X GPUs.</p><p>We fine-tune the 3D-CNN for 16 epochs with an initial learning rate of λ = 3 · 10 −3 , reduced by a factor of 10 after every 4 epochs. Next, we train the R3DCNN end-to-end for an additional 100 epochs with a constant learning rate of λ = 3·10 −4 . All network parameters without pre-trained initializations are randomly sampled from a zero-mean Gaussian distribution with standard deviation 0.01.</p><p>Each video of a weakly-segmented gesture is stored with 80 frames of 120×160 pixels. We train with frames of size 112×112 generated by random crops. Videos from the test set are evaluated with the central crop of each frame. To increase variability in the training examples, we apply the following data augmentation steps to each video in addition to cropping: random spatial rotation (±15 • ) and scaling (±20%), temporal scaling (±20%), and jittering (±3 frames). The parameters for each augmentation step are drawn from a uniform distribution with a specified range. Since recurrent connections can learn the specific order of gesture videos in the training set, we randomly permute the training gesture videos for each training epoch.</p><p>We use CNNs pre-trained on three-channel RGB images. To apply them to one-channel depth or IR images, we sum the convolutional kernels for the three channels of the first layer to obtain one kernel. Similarly, to employ the pretrained CNN with two-channel inputs (e.g., optical flow), we remove the third channel of each kernel and rescale the first two by a factor of 1.5.</p><p>For the 3D-CNN, we find that splitting a gesture into non-overlapping clips of m = 8 frames yields the best combination of classification accuracy, computational complexity and prediction latency. To work with clips of size m = 8 frames on the C3D network <ref type="bibr" target="#b36">[37]</ref> (originally trained with m = 16 frames), we remove temporal pooling after the last convolutional layer. Since data transfer and inference on a single 8-frame clip takes less than 30ms on an NVIDIA Titan X, we can predict at a faster rate than clips are accumulated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Dataset</head><p>Recently, several public dynamic gesture datasets have been introduced <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b26">27]</ref>. The datasets differ in the complexity of gestures, the number of subjects and gesture classes, and the types of sensors used for data collection. Among them, the Chalearn dataset <ref type="bibr" target="#b4">[5]</ref> provides the largest number of subjects and samples, but its 20 gesture classes, derived from the Italian sign language, are quite different from the set of gestures common for user interfaces. The VIVA challenge dataset <ref type="bibr" target="#b26">[27]</ref> provides driver hand gestures performed by a small number of subjects (8) against a plain background and from a single viewpoint.</p><p>Given the limitations of existing datasets, to validate our proposed gesture recognition algorithm, we acquired a large dataset of 25 gesture types, each intended for humancomputer interfaces and recorded by multiple sensors and viewpoints. We captured continuous data streams, containing a total of 1532 dynamic hand gestures, indoors in a car simulator with both bright and dim artificial lighting <ref type="figure" target="#fig_0">(Fig. 2)</ref>. A total of 20 subjects participated in data collection, some with two recorded sessions and some with partial sessions. Subjects performed gestures with their right hand while observing the simulator's display and controlling the steering wheel with their left hand. An interface on the display prompted subjects to perform each gesture with an audio description and a 5s sample video of the gesture. Gestures were prompted in random order with each type requested 3 times over the course of a full session.</p><p>Gestures <ref type="figure">(Fig. 3)</ref> include moving either the hand or two fingers up, down, left or right; clicking with the index finger; beckoning; opening or shaking the hand; showing the index finger, or two or three fingers; pushing the hand up, down, out or in; rotating two fingers clockwise or counterclockwise; pushing two fingers forward; closing the hand twice; and showing "thumb up" or "OK".</p><p>We used the SoftKinetic DS325 sensor to acquire frontview color and depth videos and a top-mounted DUO 3D sensor to record a pair of stereo-IR streams. In addition, we computed dense optical flow <ref type="bibr" target="#b6">[7]</ref> from the color stream and the IR disparity map from the IR-stereo pair <ref type="bibr" target="#b3">[4]</ref>. We randomly split the data by subject into training (70%) and test (30%) sets, resulting in 1050 training and 482 test videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results</head><p>We analyze the performance of R3DCNN for dynamic gesture recognition and early detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Offline Gesture Recognition</head><p>Modality fusion. We begin by evaluating our proposed R3DCNN classifier for a variety of input modalities contained in our dataset: color (front view), optical flow from color (front view), depth (front view), stereo IR (top view), and IR disparity (top view) (bottom row of <ref type="figure" target="#fig_0">Fig. 2</ref>). We train a separate network for each modality and, when fusing modalities, average their class-conditional probability vectors. <ref type="bibr" target="#b2">3</ref>  <ref type="table" target="#tab_0">Table 1</ref> contains the accuracy for various combinations of sensor modalities. Observe that fusing any pair of sensors improves individual results. In addition, combin- <ref type="bibr" target="#b2">3</ref> Attempts to learn a parameterized fusion layer resulted in overfitting.  ing different modalities of the same sensor (e.g., color and optical flow) also improves the accuracy. The best gesture recognition accuracy (83.8%) is observed for the combination of all modalities.</p><p>Comparisons. We compare our approach to state-of-theart methods: HOG+HOG 2 <ref type="bibr" target="#b26">[27]</ref>, improved dense trajectories (iDT) <ref type="bibr" target="#b38">[39]</ref>, super normal vector (SNV) <ref type="bibr" target="#b41">[42]</ref>, two-stream CNNs <ref type="bibr" target="#b33">[34]</ref>, and convolutional 3D (C3D) <ref type="bibr" target="#b36">[37]</ref>, as well as human labeling accuracy.</p><p>To compute the HOG+HOG 2 <ref type="bibr" target="#b26">[27]</ref> descriptors, we resample the videos to 32 frames and tune the parameters of the SVM classifier via grid search to maximize accuracy. For iDT <ref type="bibr" target="#b38">[39]</ref>, we densely sample and track interest points at multiple spatial scales, and compute HOG, histogram of optical flow (HOF), and motion boundary histogram (MBH) descriptors from each track. We employ Fisher vectors (FV) <ref type="bibr" target="#b29">[30]</ref> to aggregate each type of iDT descriptor using 128 Gaussian components, as well as a spatio-temporal pyramid <ref type="bibr" target="#b16">[17]</ref> of FV to encode the space and time information.</p><p>Among the CNN-based methods, we compare against the two-stream network for video classification <ref type="bibr" target="#b33">[34]</ref>, which <ref type="table" target="#tab_0">0   1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22</ref> 23 24 <ref type="figure">Figure 3</ref>: Twenty-five dynamic hand gesture classes. Some gestures were adopted from existing commercial systems <ref type="bibr" target="#b0">[1]</ref> or popular datasets <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b26">27]</ref>. Each column shows a different gesture class (0−24). The top and bottom rows show the starting and ending depth frames, respectively, of the nucleus phase for each class. (Note that we did not crop the start and end frames in the actual training and evaluation data.) Yellow arrows indicate the motion of each hand gesture. (A more detailed description of each gesture is available in the supplementary video.)</p><p>utilizes the pre-trained VGG-Net <ref type="bibr" target="#b34">[35]</ref>. We fine-tune its spatial stream with the color modality and the temporal stream with optical flow, each from our gesture dataset. We also compare against the C3D <ref type="bibr" target="#b36">[37]</ref> method, which is trained with the Sports-1M <ref type="bibr" target="#b12">[13]</ref> dataset and fine-tuned with the color or depth modalities of our dataset. Lastly, we evaluate human performance by asking six subjects to label each of the 482 gesture videos in the test set after viewing the corresponding front-view SoftKinetic color video. Prior to the experiment, each subject familiarized themselves with all 25 gesture types. Gestures were presented in random order to each subject for labelling. To be consistent with machine classifiers, human subjects viewed each gesture video only once, but were not restricted in the time allowed to decide each label.</p><p>The results of these comparisons are shown in Table 2. Among the individual modalities, the best results are achieved by depth, followed by optical flow and color. This could be because the depth sensor is more robust to indoor lighting change and more easily precludes the noisy background scene, relative to the color sensor. Optical flow explicitly encapsulates motion, which is important to recognize dynamic gestures. Unlike the two-stream network with action classification <ref type="bibr" target="#b33">[34]</ref>, its accuracy for gesture recognition is not improved by combining the spatial and temporal streams. We conjecture that videos for action classification can be associated with certain static objects or scenes, e.g., sports or ceremonies, which is not the case for dynamic hand gestures. Although C3D captures both shape and motion cues in each clip, the temporal relationship between clips is not considered. Our approach achieves the best performances in each individual modality and significantly outperforms other methods with combined modalities, meanwhile it is still below human accuracy (88.4%).</p><p>Design choices. We analyze the individual components of our proposed R3DCNN algorithm <ref type="table" target="#tab_2">(Table 3)</ref>. First, to understand the utility of the 3D-CNN we substitute it with a 2D-CNN initialized with the pre-trained 16-layer VGG-Net <ref type="table">Table 2</ref>: Comparison of our method to the state-of-the-art methods and human predictions with various modalities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Modality Accuracy HOG+HOG 2 <ref type="bibr" target="#b26">[27]</ref> color 24.5% Spatial stream CNN <ref type="bibr" target="#b33">[34]</ref> color 54.6% iDT-HOG <ref type="bibr" target="#b38">[39]</ref> color 59.1% C3D <ref type="bibr" target="#b36">[37]</ref> color  <ref type="bibr" target="#b26">[27]</ref> color + depth 36.9% Two-stream CNNs <ref type="bibr" target="#b33">[34]</ref> color + opt flow 65.6% iDT <ref type="bibr" target="#b38">[39]</ref> color + opt flow 73.4% Ours all 83.8%</p><p>Human color 88.4%  <ref type="bibr" target="#b34">[35]</ref> and train similarly to the 3D-CNN. We also asses the importance of the recurrent network, the CTC cost function and feature map drop-out. Classification accuracies for these experiments are listed in <ref type="table" target="#tab_2">Table 3</ref>. When the recurrent network is absent, i.e., "No RNN" in <ref type="table" target="#tab_2">Table 3</ref>, the CNN is directly connected to the softmax layer, and the network is trained with a negative log-likelihood cost function. When a recurrent layer with d = 256 hidden neurons is present, we train using the negative log-likelihood and CTC cost functions, denoted "RNN" and "CTC" in <ref type="table" target="#tab_2">Table 3</ref>, respectively. We observe consistently superior performance with 3D-CNN versus 2D-CNN for all sensor types and network configurations. This suggests that local motion information extracted by the spatio-temporal kernels of the 3D-CNN is important for dynamic hand gesture recognition. Notice also that adding global temporal modeling via RNN into the classifier generally improves accuracy, and the best accuracy for all sensors is obtained with the CTC cost function, regardless of the type of CNN employed.</p><p>Finally, we evaluate the effect of feature map drop-out, which involves randomly setting entire maps to zero while training. This technique has been shown to provide little or no improvement when training from a CNN from scratch <ref type="bibr" target="#b10">[11]</ref>. However, when a network pre-trained on a larger dataset with more classes is fine -tuned for a smaller domain with fewer training examples and classes, not all of the original feature maps are likely to exhibit strong activations for the the new inputs. This can lead to overfitting during fine-tuning. The accuracies of the various classifier architectures, trained with and without feature map dropout are denoted by "CTC" and "CTC*" in <ref type="table" target="#tab_2">Table 3</ref>, respectively. They show improved accuracy for all modalities and networks with feature map drop-out, with a greater positive effect for the 3D-CNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Recurrent layer as a regularizer for feature extractors.</head><p>Tran et al. <ref type="bibr" target="#b36">[37]</ref> perform video classification with a linear SVM classifier learned on features extracted from the fully connected layers of the C3D network. Features for each individual clip are averaged to form a single representation for the entire video. In <ref type="table" target="#tab_3">Table 4</ref>, we compare the performance of the features extracted from the C3D network fine-tuned on gesture clips with the features from R3DCNN trained with CTC on entire gesture videos. Features extracted from each clip are normalized by the ℓ 2 -norm. Since R3DCNN connects a C3D architecture to a recurrent network, fc layer features in both networks are computed by the same architecture, each with weights fine-tuned for the gesture recog-nition. However, we observe (columns 1-2, <ref type="table" target="#tab_3">Table 4</ref>) that following fc by a recurrent layer and training on full videos (R3DCNN) improves the accuracy of the extracted features. A plausible explanation is that the recurrent layer help the preceding convolutional network to learn more general features. Moreover, features from the recurrent layer when coupled with an SVM classifier, demonstrate a further improvement in performance (column 3, <ref type="table" target="#tab_3">Table 4</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Early Detection in Online Operation</head><p>We now analyze the performance of our method for online gesture detection, including early detection, when applied to unsegmented input streams and trained with the CTC cost function. R3DCNN receives input video streams sequentially as 8-frame clips and outputs class-conditional probabilities after processing each clip. Generally the nucleus of the gesture spans multiple clips, potentially enabling gesture classification before processing all clips.</p><p>Online operation. <ref type="figure" target="#fig_1">Fig. 4</ref> shows ground truth labels and network predictions during continuous online operation on a video sequence collected outside of our previously described dataset. The ground truth in the top row shows the hand-labeled nucleus phase of each gesture. In most cases, both networks-R3DCNN trained with negative log-likelihood ("RNN") and CTC ("CTC"), respectivelypredict the correct class before the gesture ends. However, the network trained with CTC produces significantly fewer false positives. The two networks also behave differently when the same gesture is performed sequentially, e.g., observe that three instances of the same gesture occur at 13−17s and 27−31s. The CTC network yields an individual peak for each repetition, whereas RNN merges them into a single activation.</p><p>Detection. To detect the presence of any one of the 25 gestures relative to no gesture, we compare the highest current class conditional probability output by R3DCNN to a threshold τ ∈ [0,1]. When the detection threshold is exceeded, a classification label is assigned to the most probable class. We evaluate R3DCNN trained with and without CTC on the test set with hand-annotated gesture nuclei. We compute the area under the curve (AUC) <ref type="bibr" target="#b11">[12]</ref> of the receiver operating characteristic (ROC) curve. The ROC plots the true positive detection rate (TPR)-when the network fires during the nucleus of a gesture-versus the false positive rate (FPR)-when the network fires outside of the nucleus-for a range of threshold values. With the depth modality, CTC results in better AUC (0.91) versus without (0.69) due to fewer false positives. With modality fusion the AUC increases to 0.93.</p><p>We also compute the normalized time to detect (NTtD) <ref type="bibr" target="#b11">[12]</ref> at a detection threshold (τ = 0.3) with a TPR=88% and FPR=15%. The distribution of the NTtD values for vari-   ous gesture types is shown in <ref type="figure" target="#fig_3">Fig. 5</ref>. The average NTtD across all classes is 0.56. In general, static gestures require the largest portion of the nucleus to be seen before classification, while dynamic gestures are classified on average within 70% of their completion. Intuitively, the meaning of a static gesture is clear only when the hand is in the final position.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Evaluation on Previously Published Datasets</head><p>Finally, we evaluate our method on two benchmark datasets: SKIG <ref type="bibr" target="#b17">[18]</ref>, and ChaLearn 2014 <ref type="bibr" target="#b4">[5]</ref>. SKIG contains 1080 RGBD hand gesture sequences by 6 subjects collected with a Kinect sensor. There are 10 gesture categories, each performed with 3 hand postures, 3 backgrounds, and 2 illumination conditions. <ref type="table" target="#tab_4">Table 5</ref> shows classification accuracies, including the state-of-the-art result established by the MRNN method <ref type="bibr" target="#b25">[26]</ref>. Our method outperforms existing methods both with and without the optical flow modality.</p><p>The ChaLearn 2014 dataset contains more than 13K RGBD videos of 20 upper-body Italian sign language gestures performed by 20 subjects. A comparison of results is presented in <ref type="table">Table 6</ref>, including Pigou et al. <ref type="bibr" target="#b30">[31]</ref> with state-of-the-art classification accuracy of 97.2% and Jaccard score 0.91. On the classification task, our method (with color, depth and optical flow modalities) outperforms this method with an accuracy of 98.2%. For early detection on  <ref type="table">Table 6</ref>: Results on the Chalearn 2014 dataset. Accuracy is reported on pre-segmented videos. ( * The ideal Jaccard score is computed using ground truth localizations, i.e., the class prediction is propagated for the ground truth gesture duration, representing an upper bound on Jaccard score.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method Modality Accuracy Jaccard</head><p>Neverova et al. <ref type="bibr" target="#b23">[24]</ref> color + depth + skeleton -0.85 Pigou et al. <ref type="bibr" target="#b30">[31]</ref> color </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we proposed a novel recurrent 3D convolutional neural network classifier for dynamic gesture recognition. It supports online gesture classification with zero or negative lag, effective modality fusion, and training with weakly segmented videos. These improvements over the state-of-the-art are demonstrated on a new dataset of dynamic hand gestures and other benchmarks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Environment for data collection. (Top) Driving simulator with main monitor displaying simulated driving scenes and a user interface for prompting gestures, (A) a SoftKinetic depth camera (DS325) recording depth and RGB frames, and (B) a DUO 3D camera capturing stereo IR. Both sensors capture 320×240 pixels at 30 frames per second. (Bottom) Examples of each modality, from left: RGB, optical flow, depth, IR-left, and IR-disparity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>A comparison of the gesture recognition performance of R3DCNN trained with (middle) and without (bottom) CTC. (The no gesture class is not shown for CTC.) The various colors and line types indicate different gesture classes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>NTtD and gesture length for different classes. Static gestures are marked by red bars.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><head>Table 1 :</head><label>1</label><figDesc>Comparison of modalities and their combinations.</figDesc><table>Sensors 
Accuracy 
Combinations 

Depth 
80.3% 
Optical flow 77.8% 
Color 
74.1% 
IR image 
63.5% 
IR disparity 57.8% 

Fusion Accuracy 
66.2% 
79.3% 
81.5% 
82.0% 
82.0% 
82.4% 
82.6% 
83.2% 
83.4% 
83.8% 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Comparison of 2D-CNN and 3D-CNN trained with 
different architectures on depth or color data. (CTC  *  de-
notes training without drop-out of feature maps.) 

Color 
Depth 

2D-CNN 3D-CNN 2D-CNN 3D-CNN 

No RNN 
55.6% 
67.2% 
68.1% 
73.3% 
RNN 
57.9% 
72.0% 
64.7% 
79.5% 
CTC 
65.6% 
74.1% 
69.1% 
80.3% 
CTC  *  
59.5% 
66.5% 
67.0% 
75.6% 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Accuracy of a linear SVM (C = 1) trained on 
features extracted from different networks and layers (final 
fully-connected layer fc and recurrent layer rnn). 

Clip-wise C3D [37] 
R3DCNN 

Modality 
fc 
fc 
rnn 

Color 
69.3% 
73.0% 74.1% 
Depth 
78.8% 
79.9% 80.1% 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="true"><head>Table 5 :</head><label>5</label><figDesc>Results for the SKIG RGBD gesture dataset.</figDesc><table>Method 
Modality 
Accuracy 

Liu &amp; Shao [18] 
color + depth 
88.7% 
Tung &amp; Ngoc [38] color + depth 
96.5% 
Ours 
color + depth 
97.7% 

MRNN [26] 
color + depth + optical flow 
97.8% 
Ours 
color + depth + optical flow 
98.6% 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head></head><label></label><figDesc>the color modality, with threshold τ = 0.3 (AUC= 0.98) we observed: TPR= 94.8%, FPR= 0.8%, and NTtD= 0.41, meaning our method is able to classify gestures within 41% of completion, neglecting inference time.</figDesc><table>+ depth + skeleton 
97.2% 
0.91 

Our, CTC 
color 
97.4% 
0.97  *  
Our, CTC 
depth 
93.6% 
0.92  *  
Our, CTC 
optical flow 
95.0% 
0.94  *  

Our, RNN 
color + depth 
96.6% 
0.96  *  
Our, CTC 
color + depth 
97.5% 
0.97  *  

Our, RNN 
color + depth + optical flow 97.4% 
0.97  *  
Our, CTC 
color + depth + optical flow 98.2% 
0.98  *  

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Weakly-segmented videos contain the preparation, nucleus, and retraction phases and frames from the no gesture class.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title/>
		<ptr target="https://www.bmw.com/.6" />
	</analytic>
	<monogr>
		<title level="j">Bayerische Motoren Werke AG. Gesture control interface in BMW 7 Series</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Theano: a CPU and GPU math expression compiler</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Breuleux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bastien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Python for Scientific Computing Conference</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The information visualizer, an information workspace</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Card</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">G</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Mackinlay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM CHI</title>
		<imprint>
			<date type="published" when="1991" />
			<biblScope unit="page" from="181" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<ptr target="https://duo3d.com/.5" />
		<title level="m">Code Laboratories Inc. Duo3D SDK</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">ChaLearn Looking at People Challenge 2014: dataset and results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Escalera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Baró</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Bautista</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Madadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Reyes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ponce-López</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">J</forename><surname>Escalante</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCVW</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Liblinear: A library for large linear classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Mach. Learn. Research</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Two-frame motion estimation based on polynomial expansion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Farnebäck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Scand. Conf. on Im. Anal</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The visual analysis of human movement: a survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Gavrila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVIU</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="82" to="98" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Supervised sequence labelling with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>Springer</publisher>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">J</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Improving neural networks by preventing co-adaptation of feature detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Max-margin early event detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hoai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>De La</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torre</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Current issues in the study of gesture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The biological foundations of gestures: motor and semiotic aspects</title>
		<imprint>
			<publisher>Lawrence Erlbaum Associates</publisher>
			<date type="published" when="1986" />
			<biblScope unit="page" from="23" to="47" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Gaussian process regression flow for analysis of motion trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Essa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Activity forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Kitani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">D</forename><surname>Ziebart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Bagnell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning realistic human actions from movies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marszalek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rozenfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning discriminative representations from RGB-D video data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Hand gesture recognition with leap motion and kinect devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Marin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Dominio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zanuttigh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Response time in man-computer conversational transactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AFIPS</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="1968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Gesture recognition: a survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Acharya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Systems, Man, and Cybernetics</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Hand gesture recognition with 3D convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPRW</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multi-sensor system for driver&apos;s hand-gesture recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Pulli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Automatic Face and Gesture Recognition</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Moddrop: adaptive multi-modal gesture recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Nebout</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Multiscale deep learning for gesture detection and localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Nebout</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCVW</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Multimodal gesture recognition using multistream recurrent neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Nishida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nakayama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PSIVT</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Hand gesture recognition in real time for automotive interfaces: a multimodal vision-based approach and evaluations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ohn-Bar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Trivedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE ITS</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Synergistic face detection and pose estimation with energy-based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Osadchy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">L</forename><surname>Cun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Mach. Learn. Research</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1197" to="1215" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">On the difficulty of training recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Improving the fisher kernel for large-scale image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sanchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Beyond temporal pooling: recurrence and temporal convolutions for gesture recognition in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pigou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Van Herreweghe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dambre</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Human activity prediction: early recognition of ongoing activities from streaming videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ryoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Dynamic hand gesture recognition: an examplar based approach from motion divergence fields. Image and Vis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computing</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Evaluation of low-level features and their combinations for complex event detection in open source videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tamrakar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Javed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Divakaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sawhney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3D convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Elliptical density shape model for hand gesture recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">T</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">Q</forename><surname>Ngoc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SoICT</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A robust and efficient video representation for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Oneata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Robust 3D action recognition with random occupancy patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Backpropagation through time: what it does and how to do it</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Werbos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1550" to="1560" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Super normal vector for activity recognition using depth sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
