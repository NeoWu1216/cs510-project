<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Identifying Good Training Data for Self-Supervised Free Space Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Harakeh</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Mechanical Engineering</orgName>
								<orgName type="institution">VRL Lab American University of Beirut</orgName>
								<address>
									<settlement>Beirut</settlement>
									<country key="LB">Lebanon</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Asmar</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Mechanical Engineering</orgName>
								<orgName type="institution">VRL Lab American University of Beirut</orgName>
								<address>
									<settlement>Beirut</settlement>
									<country key="LB">Lebanon</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elie</forename><surname>Shammas</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Mechanical Engineering</orgName>
								<orgName type="institution">VRL Lab American University of Beirut</orgName>
								<address>
									<settlement>Beirut</settlement>
									<country key="LB">Lebanon</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Identifying Good Training Data for Self-Supervised Free Space Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:41+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper proposes a novel technique to extract training data from free space in a scene using a stereo camera. The proposed technique exploits the projection of planes in the v-disparity image paired with Bayesian linear regression to reliably identify training image pixels belonging to free space in a scene. Unlike other methods in the literature, the algorithm does not require any prior training, has only one free parameter, and is shown to provide consistent results over a variety of terrains without the need for any manual tuning. The proposed method is compared to two other data extraction methods from the literature. Results of Support Vector classifiers using training data extracted by the proposed technique are superior in terms of quality and consistency of free space estimation. Furthermore, the computation time required by the proposed technique is shown to be smaller and more consistent than that of other training data extraction methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Scene understanding and modeling is a vital condition for the success of any unmanned autonomous system exploration. In its most basic form, this understanding reduces to delineating occupied space from free space and is essential for a system to safely navigate its environment. Furthermore, knowledge of freespace could provide significant insight in difficult scene understanding problems <ref type="bibr" target="#b0">[1]</ref>.</p><p>The problem of estimating free space in structured and static environments is usually solved by exploiting properties of certain well defined structures. Two examples of free space estimation solutions are that of Hedau et al. <ref type="bibr" target="#b1">[2]</ref> and Labayrade et al. <ref type="bibr" target="#b2">[3]</ref>. While the first exploits the box like geometry of furniture to estimate free space in indoor scenes from a single camera image, the second uses the planar geometry of a road and identifiable lane markings to estimate the free space in urban road scenes. In unstructured or unknown environments such as forest areas, the lack of structure of the scene causes methods relying on static scene properties to fail.</p><p>To account for the ever changing properties of free space in unstructured scenes, it is natural to resort to learning based systems, which usually require a training phase in which training data representing free space is used as an input to the learning algorithm. The extraction and classification of training data is usually performed through direct human supervision; unfortunately, this becomes impractical and time consuming as the range of properties to be learned becomes larger. Furthermore, the resulting system cannot extend classification beyond the environments it learned during training, thereby restricting its autonomy.</p><p>Recent free space estimation approaches tackle this problem through self-supervision, where one classifier directly supervises input to a second classifier. The first classifier uses data it is confident about to label parts of the environment as free space; this data is then provided as input to the second classifier that extends the labeling over the whole environment. The proposed system in this paper lies within this framework, allowing long range fully autonomous free space estimation without relying on any rigid assumptions such as a planar ground or bootstrapping methods. The main contribution in this paper is a novel training data extraction method that is fast, accurate, and can be applied in structured and unstructured environments.</p><p>The remainder of this paper is structured as follows. Section 2 provides a brief summary of previous systems employing self supervision. Section 3 explains in details the training data extraction algorithm proposed in the paper. Section 4 briefly presents the second classification stage based on a one-class Support Vector Classifier (ν-SVC), which is a necessary tool to assess the goodness of the training data extraction algorithms. Section 5 presents the results and analysis of the ν-SVC using our proposed method for training data extraction versus two other training data extraction methods. Finally, Section 6 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>The first part of this section presents a sample of previous work on free space estimation with an emphasis on learning-based methods. The second part gives a brief overview of the v-disparity algorithm and v-disparity image filtering, which is used as a first stage in the proposed training data extraction algorithm. Self-Supervised Learning For Free Space Estimation: a variety of sensors and sensor combinations have previously been employed for free space estimation. Sugar et al. <ref type="bibr" target="#b3">[4]</ref> employed a 3-D LIDAR to find the occupancy probability of the environment through a semi-supervised learning approach. The robot is driven by a human operator through a safe trajectory where it collects the remission and spatial features of free space, which are used as training data for a one-class classifier. Dahlkamp et al. <ref type="bibr" target="#b4">[5]</ref> used a 2-D LIDAR to extract training data belonging to free space using the Probabilistic Terrain Analysis (PTA) algorithm proposed in <ref type="bibr" target="#b5">[6]</ref>. The training data is then projected to a monocular camera and used to build a color based classifier. The PTA algorithm requires unknown parameters to be learned offline using human supervision. These two systems are suitable when the properties of the robot's operating environment resemble these of the training environment. The system presented in this paper differs from both methods in that it is totally independent of any human supervision and it does not have free parameters that need to be trained prior to deployment in a given environment.</p><p>Radars have also been successfully employed for selfsupervision in free space estimation. Milella et al. <ref type="bibr" target="#b6">[7]</ref> used the echo in a radar image to identify ground patches and then projected these patches to a monocular camera coordinate frame in order to train a visual classifier. The classification was done through Mahalanobis distance thresholding. The optimal threshold is determined by constructing ROC curves on a training dataset. In their work, the radar produces training patches at a specified distance of 11.4 meters in front of the robot. Unfortunately, in some scenarios distance patches might not posses the same features as closer ones, thereby causing the latter to be classified as obstacles. The system presented in this paper gets around this problem by extracting training patches from all over the field of view of the camera.</p><p>Stereo cameras are also used for self-supervised free space estimation and provide a dense 3-D representation of the scene with additional color information. Milella et al. <ref type="bibr" target="#b7">[8]</ref> utilizes a stereo camera to extract geometric features that are used to classify voxels in a 3-D point cloud belonging to free space. In order to create the ground model, the system needs to be initialized in an area free of obstacles. The requirement for initialization is problematic when the system fails and the human operator cannot intervene to reinitialize it. Our system does not need any special initialization and in fact can be launched inside a heavily cluttered scene.</p><p>Vernaza et al. <ref type="bibr" target="#b8">[9]</ref> also used a stereo sensor in a Markov Random Field framework to classify pixels in the image be-longing to the ground plane. The largest planar region is assumed to be the ground plane, and pixels belonging to it are taken as ground pixels. This training extraction method fails in scenarios where the ground plane is not the largest plane in the image. The novel training data extraction algorithm presented in this paper utilizes the properties of the projection of the ground on the v-disparity image, and is able to extract training pixels even if the ground is not planar. The V-Disparity Algorithm: the v-disparity algorithm was first proposed by Labayrade et al. <ref type="bibr" target="#b2">[3]</ref> for road estimation in urban scenes. It transforms a disparity image to a vdisparity image by forming a 256-bin histogram of disparity values for each row of the disparity image and concatenating them vertically. For example, a 720x1280 disparity image is transformed into a 720x256 v-disparity image. Oblique and horizontal planes in the disparity image are mapped to slanted lines in the v-disparity image. Thus, detecting slanted lines in the v-disparity image is equivalent to the estimation of the ground plane in the disparity image. <ref type="figure" target="#fig_0">Fig.1</ref>-c shows the constructed v-disparity image, where the slanted line projection of the ground plane termed the ground correlation line can be observed. The V-Disparity Image Filtering and Stochastic Model: Harakeh et al. <ref type="bibr" target="#b9">[10]</ref> noted that traditional line detection methods were unreliable when trying to estimate slanted lines in off-road scenes, and proposed a binary filtering algorithm that takes as an input the v-disparity image and provides as an output a filtered v-disparity image containing only slanted lines. This was coupled with a stochastic model, which uses maximum likelihood linear regression with a first order polynomial basis functions to provide point estimates of the parameters of the ground correlation line. The parameters were then used to estimate the mean of a probability distribution that describes the occupancy probability of each pixel. This approach has two drawbacks. First, offline training is required to determine the optimal classification threshold for ground segmentation, which limits the usability of the modeled probability distribution when extending to a new environment. Second, finding point estimates of the line parameters usually results in over fitted solutions that highly depend on the size and position of the (v, d) pairs remaining in filtered v-disparity image. The system presented in this paper handles these problems by directly modeling the occupancy probability distribution through the Bayesian linear regression framework, which is described in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Bayesian Linear Regression For Training Data Extraction</head><p>This section describes the training data extraction algorithm that we are proposing. First, for each new image the v-disparity image is extracted, then is filtered by using the </p><formula xml:id="formula_0">b) Stereo disparity image. c) v-disparity image. d) Filtered v-disparity image. e)</formula><p>Training pixels in green are extracted using our Bayesian linear regression framework, and are used as input to a One-Class Support Vector Classifier. f) Results of the Support Vector Classifier with the green area as the estimated free-space in the image. binary filtering algorithm <ref type="bibr" target="#b9">[10]</ref>, resulting in an image containing only pixels belonging to the ground correlation line ( <ref type="figure" target="#fig_0">Fig.1-d)</ref>. The remaining (v, d) pairs in the image are used as training data input to Bayesian linear regression to learn the predictive probability distribution P(d|v), where v is the input variable and d the target variable. Although the predictive distribution from Bayesian linear regression is usually used to predict new values of d given v, we found that we can use it to compute the probability of a disparity value d in the disparity image to belong to the ground correlation line, which is analogous to the probability of d belonging to the ground plane.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Learning The Predictive Distribution</head><p>The non-planar nature of the ground plane in off-road scenarios leads to a distorted ground line projection in the v-disparity image that might not be straight. To accommodate this case,the disparity d is modeled as a second degree polynomial function of v, which has the form:</p><formula xml:id="formula_1">d = w 0 + w 1 v + w 2 v 2 + ε = w T φ * (v) + ε,<label>(1)</label></formula><p>where φ * (v) are the set of second degree polynomial basis function,</p><formula xml:id="formula_2">[φ 0 (v) φ 1 (v) φ 2 (v)] = [v 0 v 1 v 2 ] and w the param- eter vector w = [w 0 w 1 w 2 ].</formula><p>ε is a zero mean Gaussian random variable with precision β . The conditional distribution of d takes the following form:</p><formula xml:id="formula_3">P(d|v, w, β ) = N (d; w T φ * (v), β −1 ).<label>(2)</label></formula><p>The vectors</p><formula xml:id="formula_4">v = [v 1 ...v N ] and d = [d 1 ...d N ]</formula><p>are now defined as the training data pairs, where v n ,d n are coordinate pairs extracted from the filtered v-disparity image. Target training variables [d 1 ...d N ] are assumed to be IID variables drawn from the conditional distribution in <ref type="bibr" target="#b1">(2)</ref> and as such, their likelihood function has the expression :</p><formula xml:id="formula_5">P(d|v, w, β ) = N ∏ n=1 N (d n ; w T φ * (v n ), β −1 ).<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Bayesian Linear Regression</head><p>At first, it should be noted that throughout this section, the variables v and v will be added to the conditional variables through the independence assumption. To begin with the Bayesian treatment of linear regression, a prior distribution is defined over the model parameter vector w as:</p><formula xml:id="formula_6">P(w|α) = P(w|v, v, α, β ) = N (0, α −1 I),<label>(4)</label></formula><p>For simplicity, the prior is considered to be zero mean and isotropic Gaussian with a single precision parameter α. This assumption reduces the number of unknown parameters in the prior to only α and results in a Gaussian posterior distribution when multiplied with the likelihood function in <ref type="bibr" target="#b2">(3)</ref>. Having set the prior, the posterior distribution of the parameter vector w given the training data can be written using Bayes rule as:</p><formula xml:id="formula_7">P(w|v, v, d, α, β ) = ΓP(d|v, v, α, β , w)P(w|v, v, α, β ),<label>(5)</label></formula><p>where Γ is a normalization coefficient and P(d|v, v, α, β , w) is the likelihood function in <ref type="bibr" target="#b2">(3)</ref>. The posterior distribution is computed by completing the squares in the exponential and then making use of the standard form of the normalization coefficient of the Gaussian, and has the form:</p><formula xml:id="formula_8">P(w|v, v, d, α, β ) = N (w; µ w , Σ w ),<label>(6)</label></formula><p>where µ w is the mean:</p><formula xml:id="formula_9">µ w = β Σ w Φ T d,<label>(7)</label></formula><p>and Σ w is the 3 × 3 covariance matrix:</p><formula xml:id="formula_10">Σ −1 w = αI + β Φ T Φ.<label>(8)</label></formula><p>Here, I is a 3 × 3 identity matrix and Φ is the design matrix, written in terms of the input vector v as:</p><formula xml:id="formula_11">Φ =     1 v 1 v 2 1 . . 1 v n v 2 n     (9)</formula><p>Th predictive distribution is expanded according to the theorem of total probability as:</p><formula xml:id="formula_12">P(d|v, v, d, α, β ) = w P(d|v, v, d, α, β , w) P(w|v, v, d, α, β )dw.<label>(10)</label></formula><p>It is noted that the predictive distribution is the result of a convolution of two Gaussian distributions in <ref type="formula" target="#formula_3">(2)</ref> and <ref type="formula" target="#formula_8">(6)</ref>. Accordingly, the predictive distribution has the following form:</p><formula xml:id="formula_13">P(d|v, v, d, α, β ) = N (d; µ T w φ * (v), Σ p ),<label>(11)</label></formula><p>where the variance Σ p can be written as:</p><formula xml:id="formula_14">Σ p = 1 β + φ * (v) T Σ w φ * (v).<label>(12)</label></formula><p>Although the unknown parameter w has been marginalized, the previous equations require precise knowledge of the precision parameters α and β , which might not be available apriori.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Learning The Precision Parameters</head><p>In a fully Bayesian treatment, the predictive distribution would be expanded using the theorem of total probability over all three unknown parameters α, β , and w. This expansion would have the form:</p><formula xml:id="formula_15">P(d|v, v, d) = α β w P(d|v, v, d, α, β , w) P(w|v, v,d, α, β )<label>(13)</label></formula><formula xml:id="formula_16">P(α, β |v, v, d)dwdβ dα,</formula><p>which has no closed form solution due to the lack of knowledge of the conditional joint PDF P(α, β |v, v, d). An approximation of the fully Bayesian treatment of this hierarchical model is computed by setting the hyperparameters at the highest level of the hierarchy (α and β ) to their most likely values instead of integrating them out <ref type="bibr" target="#b10">[11]</ref>.</p><p>We start by assuming that the conditional joint pdf is sharply peaked around the values of the true hyperparameters α and β . The predictive distribution in this case can be estimated as:</p><formula xml:id="formula_17">P(d|v, v, d) ≃P(d|v, v, d, α, β ) = w P(d|v, v, d, α, β , w)P(w|v, v, d, α, β )dw.</formula><p>To estimate the two hyperparameters, the conditional joint pdf is expanded using Bayes theorem as:</p><formula xml:id="formula_18">P(α, β |v, v, d) ∝ P(d|v, v, α, β )P(α, β |v, v).<label>(14)</label></formula><p>Due to the lack of knowledge of the hyperparametes α and β their joint prior P(α, β |v, v) is assumed to be uniform and thus is relatively flat. Because of the previous assumption, maximizing the conditional joint pdf P(α, β |v, v, d) is equivalent to maximizing P(d|v, v, α, β ) and as such, the true hyper parameters can be estimated as:</p><formula xml:id="formula_19">α = argmax α P(d|v, v, α, β ), β = argmax β P(d|v, v, α, β ),<label>(15)</label></formula><p>The estimates of the hyperparameters require the computation of the likelihood function P(d|v, v, α, β ), which has the form:</p><formula xml:id="formula_20">P(d|v, v, α, β ) = w P(d|v, v, α, β , w)P(w|v, v, α, β )dw,<label>(16)</label></formula><p>Working out the convolution, the evidence function P(d|v, v, α, β ) has the form:</p><formula xml:id="formula_21">P(d|v, v, α, β ) = β 2π N 2 (α) |Σ −1 w | − 1 2 exp −β 2 ||d − Φµ w || 2 + α 2 µ w µ T w .</formula><p>Maximizing the evidence function is the same as maximizing its natural logarithm and as such, the hyper-parameters can be computed by setting the partial derivative of the logarithm of the evidence function with respect to the respective hyper-parameter to zero. The natural logarithm of the evidence function can be written as:</p><formula xml:id="formula_22">ln P(d|v,v, α, β ) = ln α + N 2 ln β − ln |Σ −1 w | 2 − N 2 ln(2π) − β 2 ||d − Φµ w || 2 − α 2 µ w µ T w .<label>(17)</label></formula><p>The derivative equation with respect to α is:</p><formula xml:id="formula_23">∂ ln P(d|v, v, α, β ) ∂ α = 1 α − 1 2 µ w µ T w + ∂ ln |Σ −1 w | ∂ α .<label>(18)</label></formula><p>The determinant of the matrix Σ −1 w can be rewritten in terms of the eigenvalues of the matrix β Φ T Φ as:</p><formula xml:id="formula_24">|Σ −1 w | = ∏ i (λ i + α).</formula><p>Computing the partial derivative we get:</p><formula xml:id="formula_25">∂ ln |Σ −1 w | ∂ α = ∑ i 1 λ i + α .<label>(19)</label></formula><p>Setting the partial derivative in (18) to zero, the hyperparameter α will have the form:</p><formula xml:id="formula_26">α = 1 µ w µ T w ∑ i λ i λ i + α .<label>(20)</label></formula><p>Proceeding with similar analysis with respect to the hyperparameter β we obtain:</p><formula xml:id="formula_27">1 β = 1 N − ∑ i λ i λ i +α N ∑ n=1 [d n − µ T w φ * (v n )] 2 .<label>(21)</label></formula><p>We note that both solutions are implicit solutions of the parameters themselves. To solve for the hyper-parameters, an initial value must be chosen to calculate µ w and the sum ∑ i λ i λ i +α and then compute α and β using (20) and (21) until convergence. Convergence is determined when the difference between the old and new value of the hyperparameters is less than a specified tolerance. Initial value selection and the tolerance are discussed in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Training Pixel Extraction</head><p>After learning the hyperparameters α and β from (20) and (21) respectively, the final form of the predictive distribution becomes:</p><formula xml:id="formula_28">P(d|v, v, d, α, β ) = N (d; µ T w φ * (v), Σ p ),<label>(22)</label></formula><p>with Σ p computed from <ref type="bibr" target="#b11">(12)</ref>. The predictive distribution (22) is usually used to estimate new values of the target variable d given the row coordinate v as the input variable.</p><p>Here, a confidence interval is specified over the PDF, and disparity values in the disparity image belonging to it are labeled as pixels belonging to free space in the image. An example of training pixel labeling is shown in <ref type="figure" target="#fig_0">Fig.1</ref>-e. The algorithm contains only three free parameters which are the initial values of the hyperparameters, the tolerance for convergence, and the confidence interval. Since we are using the confidence interval for selecting training data, the initial value of the hyperparameters has no effect on the selection procedure. The tolerance is set to a very low value of 10 −10 for both hyper parameters. This leaves the confidence interval to be the only real free parameter in the algorithm controlling the amount of data labeled as training pixels. In our implementation, disparity values laying in the 30% confidence interval are chosen as our training data. Increasing the width of the confidence interval yields more but less precise training data and vice-versa.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Second Stage Classification</head><p>This section briefly describes the second stage classification phase, which is necessary to evaluate the goodness of the extracted training data. At first, the image is separated to constant sized blocks in order to extract histograms necessary for the creation of the feature vector. Blocks are labeled as training blocks if at least 10% of their pixels include training pixels. Kim et al. <ref type="bibr" target="#b11">[12]</ref> showed that the quality of free space estimation deteriorates as the size of the blocks increases. In contrast, the computational speed decreases as the size of the blocks increase. We chose 5 × 32 blocks as a compromise between quality and computation speed.</p><p>Each block in the image has associated with it a 20 dimensional feature vector comprised of: From HSV space:</p><p>• An 8 bin histogram of hue.</p><p>• A 5 bin histogram of saturation.</p><p>• The mean value of hue.</p><p>• The mean value of saturation. From RGB space:</p><p>• The mean value of R.</p><p>• The mean value of G.</p><p>• The mean value of B. From XYZ space:</p><p>• The mean height of each block with respect to the stereo sensor. • The difference in height between the highest and lowest point in the block. This feature vector might not be the optimal for free space estimation, but is sufficient for comparing training data extraction methods.</p><p>The training data extraction algorithm can only extract data from one of the two classes, and therefore the classification problem is formulated as a one-class classification problem in which all the training data belongs to the positive class and where the negative class is severely under sampled. The ν-Support Vector Classifier proposed by <ref type="bibr" target="#b12">[13]</ref> is chosen as the second stage classifier since it requires minimal free parameter selection. A standard off-the-shelf implementation of the ν-SVC is used and thus only the selection of free parameters in this implementation will be discussed. The three free parameters in the ν-SVC algorithm are ν, the kernel scale, and the outlier fraction. ν is a pa-rameter that lies between 0 and 1 and controls the fraction of training data to become support vectors. In this implementation, it is set to 1 which results in using all training data as support vectors. The outlier fraction, which determines the percentage of training data to belong to the negative class is set to be 5%. This combination of ν and outlier fraction allows the ν-SVC classifier to be robust to only small amounts of wrong labels in the training data. A large amount of wrongly labeled training data will change the shape of the decision boundary, emphasizing the effect of training data extraction algorithms on the quality of the final pixel classification and allowing an objective comparison between training extraction algorithms. As part of the algorithm, the scale of the Gaussian kernel is selected automatically, using a heuristic procedure based on training data subsampling. Finally, due to the difference in their scale, features are standardized by subtracting their mean value and dividing by their standard deviation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments and Results</head><p>This section presents an analysis of the goodness of training data extracted by the proposed algorithm by comparing it against training data extracted via other techniques in the literature. This is done by inputting each set of data to the ν-SVC and comparing the three corresponding output pixel labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Datasets</head><p>To be able to perform the necessary experiments, three datasets were created with terrains ranging from planar to non-planar ground. Each frame in the dataset is comprised of a stereo pair of 720 × 1280 colored images, their corresponding disparity image, and pixel X,Y ,and Z coordinate with respect to the camera's coordinate frame. The images are captured by Stereo Lab's ZED stereo camera <ref type="bibr" target="#b13">[14]</ref> and the algorithm provided by the camera's SDK was used to generate the disparity image and the point cloud coordinates. The three datasets include: Difficult dataset: 16 images taken with a hand held stereo camera on highly non-planar terrain <ref type="figure">(Fig.3-first row)</ref>. Medium dataset: 120 images taken with the a stereo camera mounted on UGV driven on a slightly non-planar parklike terrain <ref type="figure">(Fig.3-second and third rows)</ref>. Easy dataset: 145 images taken with the a stereo camera mounted on a UGV driven on a highly planar man-made terrain <ref type="figure">(Fig.3-fourth and fifth rows)</ref>.</p><p>It has to be noted that pixels lacking geometric features due to rectification, occlusion, or being located beyond the stereo camera's maximum range are not considered in this evaluation. Furthermore, ground truth is generated manually for every frame of the three datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Baselines</head><p>Two training data extraction algorithms are used for the sake of comparison with the proposed algorithm. The two algorithms are: Bootstrapping: Bootstrapping was used in <ref type="bibr" target="#b7">[8]</ref> and relies on the assumption that the properties of the ground plane will not change much as the UGV moves through the environment. This algorithm is implemented by manually providing the robot with positive labels in the first frame, which it then uses as training data for classification in the second frame. Positively labeled data in the second frame are used as training data in the third and so on. Plane Fitting: this algorithm was used in <ref type="bibr" target="#b8">[9]</ref> and <ref type="bibr" target="#b11">[12]</ref> and relies on plane fitting in the stereo generated point cloud to determine patches belonging to the ground plane. For maximum robustness towards outliers, M-estimator SAmple Consensus (MSAC) algorithm is used for plane fitting. The expected normal vector of the ground plane is required to be provided as an input, and inlier points determined by the algorithm are used as training data input to the ν-SVC algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Analysis Of The Results</head><p>All experiments were done with the feature vector and ν-SVC parameters held constant across all three datasets. Furthermore, the free parameters of the three training data extraction methods are also fixed over all trails. The labels obtained from the ν-SVC using the three algorithms are compared to ground truth labels to compute three performance criteria, which are the recall, precision, and specificity.</p><p>Recall describes the fraction of ground patches retrieved by the classifier, while precision describes fraction of the retrieved patches that are correct. Specificity on the other hand, describes the fraction of correctly identified negative instances, which in our case are the obstacles. The proposed algorithm's aim is two-fold, first to maximize all three performance criteria of the ν-SVC classifier and second, to keep its performance relatively the same over all the three types of terrain. <ref type="table">Table 1</ref> summarizes the mean recall, precision, and specificity of the ν-SVC classifier using training data from the three algorithms over all the frames of each datasets.</p><p>The ν-SVC classifier using Bootstrapping performed the worst of all three having a mean recall of 0.134 over the three datasets and is found to be unusable for reliable free space estimation. The low recall is attributed to the deterioration of the classification as the camera moves away from its initial position due to the change in the properties of the ground. This phenomenon can be clearly seen in <ref type="figure" target="#fig_1">Fig.2-left</ref> where the recall is plotted as function of frames. Better relative performance of the ν-SVC using Bootstrapping on the Difficult dataset is mainly due to the constant color proper- At the early frames of operation (third and fourth rows), it provides good results, while at later frames (first, second and fifth rows), the quality of classification greatly deteriorates. One advantage of bootstrapping is its low computation time due to the low requirements for training data extraction.</p><p>The ν-SVC utilizing plane fitting reaches 0.9646 and 0.9422 recall on the easy and medium dataset respectively. Compared to the ν-SVC using our algorithm, which has a recall of 0.8671 and 0.8114 on the same datasets, the ν-SVC utilizing plane fitting seems to perform better. We attribute the better performance to the much larger amounts of training data provided by plane fitting in cases of planar ground. However, the increase in recall comes at the expense of a decrease in precision and specificity. On the two datasets, the ν-SVC using our algorithm achieves a precision of 0.9604 and 0.9326 respectively vs 0.9340 and 0.8931 for the ν-SVC using plane fitting. The specificity of the ν-SVC using our algorithm was also better, achieving 0.9853 and 0.9781 on the two datasets vs a specificity of 0.9731 and 0.9592 for the ν-SVC using plane fitting. On the Difficult dataset, a deterioration in the quality of classification of the ν-SVC using plane fitting was observed. In highly non planar environments, plane fitting only provides training data from the largest locally planar patch with a normal vector closest to that provided as input for the algorithm <ref type="figure">(Fig.3 third column,  first row)</ref>. This leads to a reduction in recall to a value of 0.5784. As the recall decreases, the precision increases to 0.9959 and the specificity to 0.9992. On the other hand, the ν-SVC using our algorithm is able to provide a recall value of 0.8147, providing an increase of 0.2368 over the recall of the ν-SVC using plane fitting. This high recall is accompanied with high values of precision and specificity, 0.9855 and 0.9725 respectively. This shows that our algorithm is able to provide reliable training data on highly non planar terrain.</p><p>Another important criterion to consider is the computation time of each training extraction algorithm. The proposed algorithm includes v-disparity image generation, filtering and Bayesian linear regression and was implemented in Matlab, as were the other two data extraction algorithms. All the algorithms ran on the same Laptop. The fifth column of <ref type="table">Table 1</ref> shows that as the nature of the scene becomes more non-planar, the computation time of plane fitting increases. Furthermore, <ref type="figure" target="#fig_1">Fig.2</ref>-right shows that the variance of the computation time between frames is very large for plane fitting, which is mainly due to the dependence of its computation time on the density of the point cloud. The pro- <ref type="table">Table 1</ref>: Evaluation of the ν-SVC using the three training data extraction algorithms over the three datasets. The evaluation is based on the average recall, precision, specificity, and computation time ( of the training data extraction algorithm, in seconds) over all the frames of each dataset. As the terrain becomes harsher, our algorithm proves to produce better results.  <ref type="figure">Figure 3</ref>: Examples of the training data extracted using our algorithm and the plane fitting algorithm (first and third columns respectively), and the final classification results obtained from ν-SVC using our algorithm (green), largest fitted plane algorithm (red) and bootstrapping (blue). Bootstrapping does not explicitly extract training data at each frame and thus only results of the final classification are shown.</p><p>posed algorithm shows a more consistent computation time whether across datasets ( <ref type="table">Table 1</ref>, fifth column) or across frames <ref type="figure" target="#fig_1">(Fig.2)</ref>.</p><p>The intuition behind the improved performance provided by the ν-SVC using our proposed algorithm for training data extraction is that in non-planar environments, the ground plane is actually made up of many small oblique and horizontal planes, which are all projected to slanted lines in the v-disparity image. Using the v-disparity filtering algorithm to extract these lines is conceptually equivalent to fitting planes to the whole scene in one shot. This allows us to extract training data over the whole scene even in highly non-planar scenarios <ref type="figure">(Fig.3-first column, first row)</ref> and results in the computational time of our algorithm to remain approximately the same whether the terrain is planar or non-planar. Finally, selecting training data by using the confidence interval allows picking only high confidence pixels for training, increasing the final classification's precision and specificity. Such examples can be seen in the final row of <ref type="figure">Fig.3</ref>, where the training data provided by our algorithm results in better classification results. Plane fitting can be seen to provide a large amount of wrongly labeled training pixels resulting in a deterioration in the quality of the final classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We presented a novel method to extract training data for free space classification. The proposed algorithm does not require any prior training, has only one free parameter, and is shown to provide consistent results over a variety of terrains, without requiring any terrain-specific tuning. ν-SVC using our algorithm for training data selection is shown to provide comparable results in planar terrain and much better results in highly non-planar ones over other methods in literature. Finally, our algorithm requires less computation time to extract data from environments of varying typologies. The time to extract the data is both low and consistent regardless of the environment being planar or not.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Acknowledgments</head><p>This work was supported by the University Research Board (URB) at the American University of Beirut and by the Lebanese National Council for Scientific Research (LNCSR) .</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Flowchart of our system. a) Stereo Left Image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Plots of recall values of the ν-SVC and computational time (in seconds) of the three training data extraction methods per frame of the datasets.. ties of the ground in this dataset. The results of the ν-SVC using Bootstrapping are shown in the fifth column of Fig.3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>ν-SVC using Our Training Extraction Algorithm</figDesc><table>Dataset 
Recall 
Precision Specificity Time 
Easy 
0.8671 
0.9604 
0.9853 
0.0547 
Medium 
0.8514 
0.9326 
0.9781 
0.0592 
Difficult 
0.8147 
0.9855 
0.9725 
0.0598 
ν-SVC using plane fitting 
Dataset 
Recall 
Precision Specificity Time 
Easy 
0.9646 
0.9340 
0.9731 
0.1681 
Medium 
0.9422 
0.8931 
0.9592 
0.4833 
Difficult 
0.5784 
0.9960 
0.9957 
1.1138 
ν-SVC using Bootstrapping 
Dataset 
Recall 
Precision Specificity Time 
Easy 
0.0326 
0.9787 
0.9995 
NA 
Medium 
0.0869 
0.9853 
0.9963 
NA 
Difficult 
0.2838 
0.9959 
0.9992 
NA 
</table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tree trunks as landmarks for outdoor vision slam</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Asmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Zelek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Abdallah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition Workshop, 2006. CVPRW&apos;06. Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="196" to="196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Recovering free space of indoor scenes from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Hedau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2807" to="2814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Real time obstacle detection in stereovision on non flat road geometry through&quot; v-disparity&quot; representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Labayrade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Aubert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-P</forename><surname>Tarel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Vehicle Symposium</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2002" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="646" to="651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Traversability analysis for mobile robots in outdoor environments: A semi-supervised learning approach based on 3d-lidar data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Suger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Steder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Burgard</surname></persName>
		</author>
		<ptr target="http://ais.informatik.uni-freiburg.de/publications/papers/suger15icra.pdf2" />
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Int. Conf. on Robotics &amp; Automation (ICRA)</title>
		<meeting>of the IEEE Int. Conf. on Robotics &amp; Automation (ICRA)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Self-supervised monocular road detection in desert terrain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Dahlkamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kaehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Stavens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">R</forename><surname>Bradski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics: science and systems</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Probabilistic terrain analysis for high-speed desert driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Montemerlo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics: Science and Systems</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="16" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Visual ground segmentation by radar supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Reina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Underwood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Douillard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Robotics and Autonomous Systems</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="696" to="706" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A multi-baseline stereo system for scene segmentation in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Reina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Foglia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Technologies for Practical Robot Applications (TePRA), 2013 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Online, selfsupervised terrain classification via discriminatively trained submodular markov random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vernaza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Taskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA 2008. IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note>Robotics and Automation</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Ground segmentation and occupancy grid generation using probability fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Harakeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Asmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shammas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Robots and Systems (IROS), 2015 IEEE/RSJ International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Pattern recognition and machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Traversability classification for ugv navigation: A comparison of patch and superpixel representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Robots and Systems</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Sv estimation of a distributions support</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Williamson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Labs</surname></persName>
		</author>
		<ptr target="https://www.stereolabs.com/.6" />
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
