<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">WarpNet: Weakly Supervised Matching for Single-view Reconstruction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">NEC Labs America</orgName>
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">NEC Labs America</orgName>
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manmohan</forename><surname>Chandraker</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">NEC Labs America</orgName>
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">WarpNet: Weakly Supervised Matching for Single-view Reconstruction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:48+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present an approach to matching images of objects in fine-grained datasets without using part annotations, with an application to the challenging problem of weakly supervised single-view reconstruction. This is in contrast to prior works that require part annotations, since matching objects across class and pose variations is challenging with appearance features alone. We overcome this challenge through a novel deep learning architecture, WarpNet, that aligns an object in one image with a different object in another. We exploit the structure of the fine-grained dataset to create artificial data for training this network in an unsupervised-discriminative learning approach. The output of the network acts as a spatial prior that allows generalization at test time to match real images across variations in appearance, viewpoint and articulation. On the CUB-200-2011 dataset of bird categories, we improve the AP over an appearance-only network by 13.6%. We further demonstrate that our WarpNet matches, together with the structure of fine-grained datasets, allow single-view reconstructions with quality comparable to using annotated point correspondences.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Reconstructing an object from a single image is a significant challenge, that can be tackled by matching keypoints to other instances in a fine-grained dataset. However, such datasets exhibit large intra-class shape variations or interclass appearance variations, which cannot be handled by traditional features such as SIFT <ref type="bibr" target="#b19">[20]</ref>. Recently, methods have been proposed to match instances across categories, relying on supervision in the form of part (keypoint) annotations <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b28">29]</ref> or 3D CAD models <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b7">8]</ref> to augment appearance information with shape priors. Such annotations are laborintensive, thus, too sparse for reconstruction and not scalable. Further, it can be quite difficult to obtain human-labeled annotations for parts that are not nameable. In contrast, this paper presents a framework to match images of objects with some degree of non-rigidity and articulation, across category and pose variations, without requiring supervised annotations. We then present an approach to the challenging novel problem of unsupervised single-view object reconstruction.</p><p>We postulate that the structure of fine-grained datasets,  <ref type="figure">Figure 1</ref>: Given a single image of an object, we propose a novel deep learning framework for obtaining keypoint matches to other objects in a fine-grained dataset, without using any part annotations. The output of our network is used as spatial prior for matching across variations in appearance, pose and articulation (bottom), which is not possible with appearance features alone (top). Our match quality is high enough to be propagated across images to be used for single-view reconstruction without using any manually annotated keypoints (right).</p><p>combined with the power of convolutional neural networks (CNNs), allows matching instances of different categories without supervised annotation. Fine-grained datasets for objects such as birds can be analyzed along two dimensions -appearance and shape. Instances within the same category that are imaged in different poses can be matched by appearance similarity, while instances with similar pose or viewpoint from different categories can be matched through similarity in global shape. Instances with both appearance and shape variations may then be matched by propagation <ref type="figure" target="#fig_0">(Fig. 3)</ref>. In Section 3, we demonstrate a practical realization of this intuition by introducing a deep learning architecture, WarpNet, that learns to warp points on one object into corresponding ones on another (from a possibly different category or pose), without requiring supervised annotations.</p><p>WarpNet is a Siamese network that accepts two images as input (Section 3.2). To overcome the absence of annotated keypoints, our training presents an image and a warped version related by a known thin-plate spline (TPS) transformation, which yields artificial correspondences. We assume the object bounding box and foreground segmentation are known, which can be obtained through state-of-the-art segmentation <ref type="bibr" target="#b9">[10]</ref> or co-segmentation methods <ref type="bibr" target="#b16">[17]</ref>. We experi-  <ref type="figure">Figure 2</ref>: Overview of our framework. (a) Lacking part annotations, we exploit the fine-grained dataset to create artificial correspondences.</p><p>(b) These are used to train our novel deep learning architecture that learns to warp one object into another. (c) The output of the network is used as a spatial prior to match across appearance and shape variations. (d) Our high-quality matches can be propagated across the dataset. We use the WarpNet output and the structure of fine-grained categories to perform single-view reconstruction without part annotations. ment using both ground truth and co-segmentation outputs. In Section 3.1, we exploit neighborhood relationships within the dataset through the pose graph of Krause et al. <ref type="bibr" target="#b16">[17]</ref> to compute exemplar TPS transformations between silhouettes, from which our artificial transformations are sampled. A point transformer layer inspired by <ref type="bibr" target="#b13">[14]</ref> is used to compute the warp that aligns keypoints without supervision, which provides a spatial prior for matching (Section 4). We show that WarpNet generalizes well to match real images with distinct shapes and appearances at test time. In particular, it achieves matching accuracy over 13.6% higher than a baseline ILSVRC CNN <ref type="bibr" target="#b8">[9]</ref>.</p><p>Establishing matches between a given instance and other objects in the dataset opens the door to a novel problemweakly supervised reconstruction in fine-grained datasets. Several sub-problems must be solved to achieve this goal, such as match propagation and image subset selection. Prior works such as <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b28">29]</ref> approach these sub-problems, but the absence of supervised annotations poses new challenges. In Section 4.2, we suggest ways to overcome them through the use of matches from our WarpNet, the pose graph and heuristics that exploit the structure of fine-grained datasets. We demonstrate reconstructions that are nearly as good as those obtained using supervised annotations and better than those from appearance-only CNNs or unsupervised baselines such as deformable spatial pyramids <ref type="bibr" target="#b15">[16]</ref>.</p><p>To summarize, our key contributions are:</p><p>• A novel deep learning architecture, WarpNet, that predicts a warp for establishing correspondences between two input images across category and pose variations. • A novel exemplar-driven mechanism to train WarpNet without requiring supervised keypoint annotations. • An approach to unsupervised single-view object reconstruction that exploits the structure of the fine-grained dataset to yield reconstructions of birds nearly on par with the method that uses supervised part annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Supervised matching Several recent approaches use deep learning to learn a similarity metric between image patches  in a supervised manner <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b32">33]</ref>. These works focus on matching images of the same instance (for example, the Statue of Liberty <ref type="bibr" target="#b30">[31]</ref>) from various viewpoints, while we match deformable objects of different instances exhibiting a wide variety of appearances. Our task requires semantic understanding of object shape, beyond just local appearance. A CNN framework to predict dense optical flow on general scenes is proposed by <ref type="bibr" target="#b11">[12]</ref>, but in a supervised manner. Matching or keypoint localization may be improved by augmenting appearance similarity with spatial priors. Supervised methods often use a dataset with labeled parts to obtain a non-parametric prior on keypoint locations <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b27">28]</ref>. These priors may be learned from data <ref type="bibr" target="#b25">[26]</ref>, but require supervised part annotations during training. Such annotation is laborious and consequently available only for a few nameable parts, which might be too sparse for reconstruction. Unsupervised matching Also related to our approach are methods that use unsupervised spatial priors for dense matching <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b26">27]</ref>. Unlike our work, these methods are purely geometric and do not learn category-specific semantic properties. Recently, <ref type="bibr" target="#b33">[34]</ref> proposes an unsupervised approach for dense alignment of image sets. But while their focus is global consistency, our emphasis is on pairwise matching through the WarpNet framework (for which they use flow). Thus, our contribution is complementary and may be used by their framework. We evaluate quantitatively on deformable bird categories, while they use rigid categories on PASCAL.</p><p>Single-view reconstruction A new challenge in computer vision is to reconstruct a target object from a single image, using an image collection of similar objects. The seminal work of <ref type="bibr" target="#b28">[29]</ref> demonstrates the possibility of a solution, but relies on ground truth part annotations to establish correspondences. The subsequent works of <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b6">7]</ref> take a step further in using part annotations only during training. In contrast, we do not require part annotations at either train or test time.</p><p>CNNs for learning transformations Similar to the recent work of <ref type="bibr" target="#b1">[2]</ref>, we use a Siamese network to predict transformations. The key difference is that predicting the ego-motion transformation in <ref type="bibr" target="#b1">[2]</ref> is a pretext for feature learning, while we directly use the predicted transformation as well as its appearance features for matching. Further, they require ground truth transformation parameters in order to train their network, while we use the structure of the fine-grained dataset to generate artificial correspondences and implicitly optimize the parameters. Finally, rigid transformations in <ref type="bibr" target="#b1">[2]</ref> are discretized in bins and the task is posed as classification, while our network outputs continuous thin-plate spline transformation parameters with a matching objective.</p><p>Our architecture is inspired by the recent spatial transformer network of Jaderberg et al. <ref type="bibr" target="#b13">[14]</ref>, which introduces a deep learning module to predict a spatial transformation. This acts as an attention mechanism driven by a classification objective. We extend the idea further to predict a warping function that aligns two object instances in an unsupervised manner. Our approach is in line with the recent work of <ref type="bibr" target="#b0">[1]</ref>, which demonstrates that CNNs can be trained without supervised labels by treating an image patch and its transformed versions as a "surrogate" class. However similar to <ref type="bibr" target="#b1">[2]</ref>, the unsupervised training objective of classifying the surrogate class is geared towards learning good features, while we show that the output of our network trained by an artificial dataset actually generalizes to matching real image pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Learning without Part Annotations</head><p>We present a deep learning framework, WarpNet, that learns the correspondence from one image to another without requiring part annotations. Given two images I 1 and I 2 , our network outputs a function that takes points in I 1 to points in I 2 . We parameterize this function as a thin-plate spline (TPS) transformation since it can capture shape deformations well <ref type="bibr" target="#b4">[5]</ref>. Inspired by Dosovitskiy et al. <ref type="bibr" target="#b0">[1]</ref>, we generate artificial correspondences by applying known transformations to an image. However, our approach is distinct in using the structure afforded by fine-grained datasets and dealing with non-rigidity and articulations. Our network generalizes well to instances of different categories at test time and we use its output as a spatial prior in computing a match between two objects. <ref type="figure">Figure 2</ref> gives an overview of our approach. We discuss each step in detail below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Generating Unsupervised Correspondences</head><p>Since we do not have annotated point correspondences, we create artificial ones by applying random spatial and chromatic transformations to images. The key requirement is that the spatial transformations applied are complex enough to learn meaningful correspondences, while producing transformed images that are reflective of actual image pairs to match at test time. For instance, affine transformations are not expressive enough to capture non-rigid deformations and articulations in birds. Instead, we use TPS transformations and exploit the fine-grained dataset to generate exemplar warps that span a realistic range of transformations.</p><p>We use the pose graph of Krause et al. <ref type="bibr" target="#b16">[17]</ref>, whose edge weights are determined by the cosine distance of the fourthlayer of a pre-trained ILSVRC CNN, which captures abstract concepts such as class-independent shape. We compute shape context TPS warps <ref type="bibr" target="#b4">[5]</ref> between the silhouettes of images that are within 3 nearest-neighbors apart on the pose graph. We sort the TPS warps using the mean of their bending and affine energy, retaining only those between the 50th and 90th percentiles to avoid warps that are too trivial or too drastic. We create m transformed versions of every image by sampling from this set of TPS warps. We sample n points uniformly on the foreground, which we use as correspondences. <ref type="figure" target="#fig_1">Figure 4</ref> shows the effect of transformations sampled from the exemplar-TPS warps. The images on the left are the originals and the ones on the right are transformed versions. Notice how the transformation induces changes in shape and articulations around the head and the tail, which validates the utility of our exemplar TPS warps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">WarpNet Architecture</head><p>Our proposed WarpNet is a Siamese network <ref type="bibr" target="#b10">[11]</ref> that takes two images related by an exemplar TPS transformation, I 1 and I 2 , along with the corresponding n keypoint locations,</p><formula xml:id="formula_0">5$ Conv5layers$ Conv5layers$ …$ θ$ Feature$extrac@on$ @ed$weights$ Point$transformer$ p 2 $ T θ (p 1 )$ p 1 $</formula><p>L25loss$ TPS$ <ref type="figure">Figure 5</ref>: WarpNet architecture. Visual features are extracted from two input images using a Siamese CNN. They are combined to predict a deformed grid that parameterizes a TPS transformation. The network objective is to minimize the distance between corresponding points p1 and p2 of the image pair after applying the predicted transformation to p2.</p><p>as inputs during training (at test time, the input consists only of two images from possibly different categories and poses that must be matched). The main objective of WarpNet is to compute a function that warps points p 2 in I 2 to image coordinates in I 1 , such that after warping the L2 distance to the corresponding points p 1 in I 1 is minimized. <ref type="figure">Figure 5</ref> illustrates the architecture of WarpNet.</p><p>First, the input images are passed through convolution layers with tied weights. The extracted features are then combined by element-wise subtraction of the feature maps. We subtract rather than concatenate the feature maps along the channels, since concatenation significantly increases the number of parameters in the network making it unstable to train. The combined feature maps are passed through a point transformer, similar to <ref type="bibr" target="#b13">[14]</ref>, which regresses on the (x, y) coordinates of a deformed K × K grid. The output grid, normalized to a range of [−1, 1] × [−1, 1], acts as the control points for computing a grid-based TPS transformation from I 2 to I 1 . This involves solving a system of linear equations, handled by the TPS layer. Please see the supplementary materials for details. The predicted TPS transformation is applied to the keypoints of I 2 generating the transformed version T θ (p 2 ), which finally gets sent to the L2 loss layer along with p 1 . Since every step consist of linear operations, the whole network can be trained with backpropagation.</p><p>We implicitly train the warp parameters in terms of distance between corresponding points rather than direct supervision against the TPS warp coefficients. This provides a natural distance between warps, where we can train the network without knowing the exact transformation parameters used. <ref type="figure">Figure 6</ref> illustrates the output of the trained network given two real images as input, denoted source and target. Despite the fact that the network has never seen objects of different instances, it is able to compute warps between the two objects. Note that WarpNet accounts for variations in shape (fat to skinny, small to large birds), articulation (such as the orientation of the head or the tail) and appearance.</p><p>Network$Output$ Warped$Source$ Target$ Source$ <ref type="figure">Figure 6</ref>: Visualizations of the network output. WarpNet takes two images, source and target, as inputs and produces a 10x10 deformed lattice (last column) that defines a TPS warp from target to source. The third column shows the warped source image according to the network output. Notice how the network accounts for articulations at the tail and the head as well as differences in shape of the birds. WarpNet is trained in an unsupervised manner and none of these images were seen by the network during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Matches$with$classifica@on$CNN$</head><p>Matches$with$WarpNet$ </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Matching and Reconstruction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Matching with WarpNet</head><p>Given two images I i and I j , a match for a point u i in I i is the most similar point v j in I j using the similarity score consisting of an appearance term and a spatial term:</p><formula xml:id="formula_1">s(u i ,v j )=exp ✓ −d f (u i ,v j ) σ f ◆ + λ exp ✓ −d w (u i ,v j ) σ w ◆ ,<label>(1)</label></formula><formula xml:id="formula_2">where d f (u, v)</formula><p>is the L2 distance of appearance features extracted at u i and v j , while d w is a symmetric spatial prior:</p><formula xml:id="formula_3">d w (u, v)= 1 2 (||x u i − T θij (x v j )|| + ||x v j − T θji (x u i )||). (2)</formula><p>We use WarpNet to compute T θ·,· in both directions. The matches are then ranked by the ratio-test strategy <ref type="bibr" target="#b19">[20]</ref>, which allows discarding points in I i that are similar to many other points in I j . Since the keypoints are extracted densely on the foreground, we compute the similarity score ratio between the first and second nearest neighbors that are at least 10 pixels away. <ref type="figure" target="#fig_2">Figure 7</ref> shows a few qualitative matching results comparing the baseline CNN and WarpNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Single-View Object Reconstruction</head><p>Obtaining good matches is a critical first step towards 3D reconstruction. While single-view 3D reconstruction methods in the past have relied on expensive supervised inputs such as part annotations or CAD models, our matching enables a first approach towards a challenging new problem, namely, part annotation free single-view reconstruction. We discuss initial approaches to variants of existing supervised methods or structure from motion (SFM) pipelines that may be used to solve this problem without requiring annotations.</p><p>Propagating correspondences In the CUB-200-2011 dataset, there are only 60 images for each category. Moreover, birds are often imaged from preferred viewpoints, but it is critical for reconstruction to obtain matches across a well-distributed set of viewpoints. On the other hand, deformations may be very high even within a category (open wings as opposed to closed), which makes straightforward matching within a category challenging. Inspired by the work of Carreira et al. <ref type="bibr" target="#b6">[7]</ref>, we use a shortest path method to propagate matches across objects of similar shapes in the dataset, in order to obtain a denser set of tracks. However, note that we lack the initial set of point annotations as well as the camera poses obtained through part annotations in <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b28">29]</ref>, who also manually select a subset of keypoints to eliminate articulations. Instead, we determine unsupervised matches purely through our WarpNet and rely on the pose graph to determine nearest neighbors for propagation.</p><p>Choosing a subset for reconstruction A key problem we encounter is the choice of images for reconstruction. In previous works on reconstruction within PASCAL VOC <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b28">29]</ref>, it has been possible to use the entire dataset since it contains less than 1000 images for birds. In contrast, CUB-200-2011 contains nearly 12000 images, which poses computational challenges and requires greater vigilance against outliers. Moreover, annotations in <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b28">29]</ref> preclude the need for algorithmic considerations on baseline or shape variations in choosing the image set. For instance, to reconstruct a sitting bird imaged from a frontal view, we must propagate matches to side views of sitting birds in other categories to ensure a good baseline, while avoiding images of flying birds.</p><p>Given a collection of images, several heuristics have been proposed for selecting the right subset or order for multiview rigid-body reconstruction <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref>. However, those are not directly applicable for single-view reconstruction of deformable objects. Instead, we propose three heuristics that utilize the structure of fine-grained bird datasets:</p><p>• Use images from categories that share a keyword (for example, all "warblers", or all "sparrows"). • Use images from categories that are related by an ornithological taxonomy, as defined by <ref type="bibr" target="#b22">[23]</ref>. • Use images from the five nearest neighbor subcategories on a similarity tree of bird species <ref type="bibr" target="#b5">[6]</ref>.</p><p>The above heuristics perform comparably and address the same goal -introduction of matched keypoints from more than one subcategory to ensure good viewpoint coverage.</p><p>Reconstruction Given an image of a target object from one particular class, we consider images from several other categories using one of the above heuristics. We compute pairwise matches at 85% precision threshold between all pairs of images whose distance on the pose graph is less than 4. We ignore pairs that have less than 50 surviving matches. We then set up a virtual view network <ref type="bibr" target="#b6">[7]</ref> to propagate matches across all the selected images. We use scores from (1), bounded between [0, 1], as weights on the graphs connecting the keypoints. After propagation, we discard as spurious any propagated matches with shortest path distance more than 0.4 and remove all images that have less than 30 matches with the target object. We then create the measurement matrix of tracked keypoints of the target object. We only consider keypoints visible in at least 10% of the images as stable enough for reconstruction. We finally send the observation matrix to the rigid factorization method of <ref type="bibr" target="#b20">[21]</ref>, which robustly handles missing data, to obtain 3D shape. <ref type="bibr" target="#b0">1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>We perform experiments on the CUB-200-2011 dataset which contains 11788 images of 200 bird categories, with 15 parts annotated <ref type="bibr" target="#b29">[30]</ref>. We reconstruct without part annotation, assuming objects are localized within a bounding box. We quantitatively evaluate our matches using and extending the part annotations. Next, we evaluate the effectiveness of WarpNet as a spatial prior and analyze the choice of transformations for creating the artificial training dataset. Finally, we demonstrate the efficacy of our framework with several examples of unsupervised single-view reconstruction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Experimental Details</head><p>We create the pose graph of <ref type="bibr" target="#b16">[17]</ref> using the conv4 feature of AlexNet trained on ILSVRC2012 <ref type="bibr" target="#b17">[18]</ref>. For creating the artificial dataset, we only use the training data (∼6000 images) and create m =9copies of each image using our exemplar-TPS. We resize all images to 224 × 224. This results in approximately 120k image pairs, each with n = 100 point correspondences. Following <ref type="bibr" target="#b0">[1]</ref>, we apply spatial and chromatic data augmentation on-the-fly during training.</p><p>We use the VGG-M architecture of <ref type="bibr" target="#b8">[9]</ref> until the pool5 layer as the feature extraction component of WarpNet. The point transformer consists of C512-C256-F1024-D-Op using the notation of <ref type="bibr" target="#b1">[2]</ref>. Both convolutional layers use 3x3 kernel, stride 1 with no padding, with ReLU non-linearity. The output layer is a regressor on the grid coordinates, with grid size K = 10. The feature extraction weights are initialized with weights pre-trained on the ILSVRC classification task, following prior state-of-the-art for correspondence <ref type="bibr" target="#b18">[19]</ref>.</p><p>For matching and reconstruction, images are resized with aspect ratio intact and the smallest side 224 pixels. We uniformly sample points on the foreground with a stride of 8 as keypoints for matching. For all experiments we use L2normalized conv4 features extracted at the keypoints using the hole algorithm <ref type="bibr" target="#b9">[10]</ref> for computing the appearance term in <ref type="bibr" target="#b0">(1)</ref>. Hyperparameters used for matching are σ f =1 .75, σ w = 18, λ =0.3, tuned using the artificial dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Match Evaluation</head><p>We compare our approach with ILSVRC pre-trained VGG-M conv4 <ref type="bibr" target="#b8">[9]</ref>, SIFT at radius 8 <ref type="bibr" target="#b19">[20]</ref> and matches from the deformable spatial pyramid (DSP) <ref type="bibr" target="#b15">[16]</ref>. Only the appearance term in (1) is used for computing matches with VGG-M conv4 and SIFT. For computing the matches with DSP, we mask out the background prior to extracting SIFT features following <ref type="bibr" target="#b6">[7]</ref> and only keep matches of the keypoints. For this experiment, the set of keypoints to match includes the locations of annotated parts.</p><p>In order to evaluate WarpNet as a stand-alone learned spatial prior, we compare WarpNet with DSP by replacing the SIFT features in DSP with VGG features. We call this method VGG+DSP. We further evaluate WarpNet against the original DSP by using WarpNet as a spatial prior for SIFT matches, where the unary term d f in <ref type="formula" target="#formula_1">(1)</ref> is computed with SIFT features. We call this method SIFT+WarpNet.</p><p>As discussed in Section 3.1, the only supervision required in training WarpNet is the segmentation mask to mine exemplar-TPS transformations. We also evaluate the robustness of WarpNet using co-segmentation outputs of <ref type="bibr" target="#b16">[17]</ref>, called VGG+coseg.</p><p>Test set We evaluate on 5000 image pairs that are within 3 nearest neighbors apart on the pose graph, comprising more than 50k ground truth matches. <ref type="bibr" target="#b1">2</ref> Due to the unsupervised nature of the pose graph, these pairs exhibit significant articulation, viewpoint and appearance variations (see <ref type="bibr">Figures 1,</ref><ref type="bibr" target="#b5">6)</ref>. We remove severely occluded pairs with less than 7 parts visible in both images and pairs whose TPS warp computed from part annotations have very high bending energy. None of the test images were used to train WarpNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation metrics</head><p>We evaluate the accuracy of matches with the percentage of correct keypoints (PCK) metric <ref type="bibr" target="#b31">[32]</ref>, where a match is considered correct if the predicted point is within α * L of the ground-truth correspondence. Following <ref type="bibr" target="#b1">[2]</ref>, we chose L to be the mean diagonal length of the two images. We also compute the precision-recall (PR) curve adopting the procedure of <ref type="bibr" target="#b21">[22]</ref>. A match is considered a true positive within a radius α =0.05, otherwise it is a false positive. In this setup, a recall of 1 is obtained only if all 2 Please see supplementary materials for results on a test set with 1nearest neighbors, where we observe similar trends but with higher PCKs. the matches retrieved are correct, that is, 100% α-PCK. We compute PR curves using the ratio-test values described in Section 4.1 for ranking the matches and report AP. For DSP, we use its matching cost for ranking instead of the ratios, since second closest matches are not available.</p><p>Results <ref type="figure" target="#fig_3">Figure 8</ref>(a) shows the obtained PR curves. Warp-Net achieves an AP of 53.4%, an 13.6% increase over matches using just the appearance feature of VGG-M conv4. WarpNet achieves a much higher recall due to its spatial prior, learned without using any part annotations. As a side note, conv4 features of WarpNet alone achieve very similar performance to the VGG-M conv4. In all cases, WarpNet outperforms DSP as a spatial prior and changing SIFT to VGG features yields around 5% improvement in the final recall. WarpNet-coseg still outperforms the baseline VGG-M by 10.8%, showing our approach is applicable even without ground truth segmentations. <ref type="figure" target="#fig_4">Figure 9</ref>(a) shows the PCK as a function of α, where WarpNet consistently outperforms other methods. We observe that VGG-M conv4 and DSP perform similarly, showing that while DSP obtains low recall at high precision, its overall match quality is similar to CNN features, an observation in line with <ref type="bibr" target="#b6">[7]</ref>. Since only high precision matches are useful for reconstruction where outliers need to be avoided, we show the same curves thresholded at 85% precision in <ref type="figure" target="#fig_4">Figure 9</ref>(b) for VGG-M and our method. Note that some methods in black have zero recall at this precision. The growing gap between WarpNet and VGG-M conv4 as α increases suggests that, unlike WarpNet, appearance features alone make grossly wrong matches (see <ref type="figure" target="#fig_2">Figures 1 and 7)</ref>.</p><p>Expanding the set of part annotations A caveat of the CUB-200-2011 for our task is that part annotations are sparse and concentrated on semantically distinct parts such as eyes and beaks around the head region, with only four points on the bird body that are often not all visible. To investigate matching performance more densely, we carefully expand  the ground-truth matches using the annotated parts. This process is illustrated in <ref type="figure" target="#fig_5">Figure 10</ref>. Given a pair of images I 1 and I 2 , we Delaunay triangulate each image independently using the parts visible in both as vertices. For a point u within a triangle in I 1 , we consider points in I 2 that are within the same triangle as possible candidates (shown as pink dots in <ref type="figure" target="#fig_5">Figure 10</ref>), find the point that is closest to u in terms of barycentric coordinates and accept this as a new pseudo ground-truth match if the distance is less than 0.1. <ref type="figure" target="#fig_3">Figure 8</ref>(b) shows the PR curve obtained using the pseudoground truth matches (in addition to the annotated parts). We see the same trends as <ref type="figure" target="#fig_3">Figure 8</ref>(a), but with a wider gap between the baselines and our method. This is reasonable given that bird bodies usually consist of flat or repeated textures that are challenging to match with local appearances alone, highlighting the efficacy of WarpNet's spatial prior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Choice of Transformations</head><p>We now analyze the choice of exemplar TPS transformations for creating the artificial dataset. We train another WarpNet under the same settings, but on an artificial dataset created using only affine spatial transformations, which we refer to as AffineNet. Note that AffineNet's output is still a TPS transformation, thus, it has the same capacity as the original WarpNet. <ref type="figure" target="#fig_6">Figure 11</ref> in comparison to WarpNet and VGG-M conv4. Warp-Net outperforms AffineNet in all aspects. While AffineNet has a higher final recall (that is PCK of all matches) than VGG-M conv4, its recall at high precision is slightly lower than that of VGG-M conv4. This is highlighted in <ref type="figure" target="#fig_6">Figure  11</ref>(b), which shows PCK of matches at 85% precision over α, where AffineNet performs on par with VGG-M conv4. This indicates that the warps predicted by AffineNet are helpful in a general sense, but not precise enough to improve the recall at high precision. This experiment shows that using exemplar-TPS transformations for creating the artificial dataset is critical for training a useful WarpNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Single-view Object Reconstruction</head><p>We compare our method with three other matching methods. One is a supervised matching approach similar to <ref type="bibr" target="#b6">[7]</ref>, where the network predicted TPS warp T θ in (2) is replaced by the supervised TPS warp computed using the annotated keypoints. We call this approach supervised and it is an upper-bound to our method since ground-truth part annotations are used for reconstruction. We also perform reconstructions with VGG-M conv4 features alone and DSP. We do not include the mirrored image as another viewpoint of the target object, since bilateral symmetry does not hold for articulated objects. For post-processing we use the xysnapping method proposed in <ref type="bibr" target="#b6">[7]</ref>, which only uses the zcomponent from the reconstructed shape, while fixing the x, y coordinates. We do not resample the target objects multiple times prior to factorization since it did not seem to make a difference. <ref type="figure" target="#fig_7">Figure 12</ref> shows reconstructions for various types of birds using the four methods from three viewpoints: camera view, 45 • azimuth and 45 • elevation. The colors indicate depth values (yellow is close, blue is far), with range fixed across all methods. WarpNet produces reconstructions that are most consistent with the supervised approach. Reconstructions from VGG-M and DSP are noisy due to errors in matching and often produce extreme outlier points that had to be clipped for ease of visualization. Articulated parts such as tails and wings are particularly challenging to match, where VGG-M and DSP often fail to recover consistent depths. A weakness of our method is that the TPS prior may sometimes hallucinate birds of similar pose even with wide baseline. This may be avoided by better choice of images for reconstruction. Please see supplementary material for more results, qualitative matches and reconstruction videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions and Future Work</head><p>We introduce a framework for matching and reconstruction in fine-grained datasets that avoids the expense and scalability challenges of part annotations. The core of our approach is a novel deep learning architecture that predicts a function to warp one object into another. We show that our network can be trained without supervised part annotations by exploiting the structure of fine-grained datasets and use its output as a spatial prior for accurate matching. Our approach achieves significant improvements over prior state-of-the-art without using part annotations and we show reconstructions of similar quality as supervised methods. Key challenges for future work are to determine optimal subsets of images for reconstruction and a good order for adding images that allows incremental reconstruction with bundle adjustment.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Intuition for matching in fine-grained datasets without supervised point annotations. Matching within a category exploits appearance similarity, while matching instances across related categories is possible through global shape similarity. By propagation, one may match across variations in both appearance and shape.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Sample exemplar-TPS warped images used for training our WarpNet. Left: original images, right: artificial versions made by applying exemplar TPS warp + chromatic transformation. Notice changes in shape and articulations at the head and the tail.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 7 :</head><label>7</label><figDesc>Sample matches obtained by ILSVRC trained CNN versus WarpNet, where WarpNet's relative robustness to variations in appearance, pose and articulation may be noted.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 8 :</head><label>8</label><figDesc>Precision-Recall curves for matching points between neighboring images on the pose graph. We evaluate points with (a) human-annotated correspondences and (b) expanded pseudoground-truth correspondences.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 9 :</head><label>9</label><figDesc>PCK (higher the better) over varying definition of correctness α. (a) Mean PCK of all retrieved matches regardless of ratio score. (b) Mean PCK with matches thresholded at 85% precision, which are the matches used for reconstruction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 10 :</head><label>10</label><figDesc>Illustration of the pseudo-gt correspondences. We triangulate each image using the annotated keypoints (colored points). The match for the big red dot in the left image is found by looking at points within the same triangle (small pink dots) in the right image and picking the closest point in terms of barycentric coordinates.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 11 :</head><label>11</label><figDesc>Comparing results for WarpNet trained on artificial data created using affine-spatial transformations with (a) PR curves and (b) PCK over α. WarpNet trained with exemplar-TPS is more effective in terms of recall and precision.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 12 :</head><label>12</label><figDesc>Sample reconstructions showing 3 views for each method: The camera viewpoint followed by the 45 • azimuth in counterclockwise direction (top right) and 45 • elevation (bottom right). Colors show the depth where yellow is closer and blue is farther. The supervised method uses the spatial prior computed from annotated part correspondences, which can be seen as an upper bound. No part correspondences were used for the last three methods. WarpNet consistently obtains reconstructions most similar to the supervised method.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">A rigid factorization suffices to produce good reconstructions since the dataset is large enough, but non-rigid methods alternately could be used.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments This work was part of A. Kanazawa's internship at NEC Labs America, in Cupertino. A. Kanazawa and D. Jacobs were also supported by the National Science Foundation under Grant No. 1526234.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Discriminative unsupervised feature learning with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning to see by moving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Dense object reconstruction with semantic priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Y</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR. IEEE</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Localizing parts of faces using a consensus of exemplars. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">N</forename><surname>Belhumeur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Kriegman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2930" to="2940" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Shape matching and object recognition using shape contexts. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Puzicha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="509" to="522" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">How do you tell a blackbird from a crow?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">N</forename><surname>Belhumeur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Computer Vision (ICCV)</title>
		<meeting>Int. Conf. Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2013-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Virtual view networks for object reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tulsiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR. IEEE</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">What shape are dolphins? building 3D morphable models from 2D images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">J</forename><surname>Cashman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Fitzgibbon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="232" to="244" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Return of the devil in the details: Delving deep into convolutional nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chatfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Semantic image segmentation with deep convolutional nets and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning a similarity metric discriminatively, with application to face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">L L</forename><surname>Cun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="539" to="546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Häusser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hazırbaş</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Van Der Smagt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<title level="m">Flownet: Learning optical flow with convolutional networks. ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Matchnet: Unifying feature and metric learning for patchbased matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Categoryspecific object reconstruction from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tulsiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deformable spatial pyramid matching for fast dense correspondences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Fine-grained recognition without part annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="5546" to="5555" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Do convnets learn correspondence?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Estimating 3D shape from degenerate sequences with missing data. Computer Vision and Image Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Costeira</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="261" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A performance evaluation of local descriptors. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1615" to="1630" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">The birds of north america online</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Ornithology</surname></persName>
		</author>
		<ptr target="http://bna.birds.cornell.edu/BNA/" />
		<editor>p. rodewald</editor>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Modeling the world from Internet photo collections. IJCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Skeletal graphs for efficient structure from motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Computer Vision and Pattern Recognition</title>
		<meeting>Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Joint training of a convolutional network and a graphical model for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1799" to="1807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A method for visualizing pedestrian traffic flow using SIFT feature point tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tsuduki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fujiyoshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PSIVT</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="25" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Viewpoints and keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tulsiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Regognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Reconstructing PASCAL VOC</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vicente</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>De Agapito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Batista</surname></persName>
		</author>
		<editor>CVPR. IEEE</editor>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">The Caltech-UCSD Birds-200-2011 Dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<idno>CNS-TR-2011-001</idno>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Picking the best DAISY</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Winder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="178" to="185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Articulated pose estimation with flexible mixtures-of-parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1385" to="1392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning to compare image patches via convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Flowweb: Joint image set alignment by weaving consistent, pixel-wise correspondences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
