<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Random Features for Sparse Signal Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jen-Hao</forename><forename type="middle">Rick</forename><surname>Chang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aswin</forename><forename type="middle">C</forename><surname>Sankaranarayanan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">V K</forename><surname>Vijaya Kumar</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Random Features for Sparse Signal Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:44+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Random features is an approach for kernel-based inference on large datasets. In this paper, we derive performance guarantees for random features on signals, like images, that enjoy sparse representations and show that the number of random features required to achieve a desired approximation of the kernel similarity matrix can be significantly smaller for sparse signals. Based on this, we propose a scheme termed compressive random features that first obtains low-dimensional projections of a dataset and, subsequently, derives random features on the low-dimensional projections. This scheme provides significant improvements in signal dimensionality, computational time, and storage costs over traditional random features while enjoying similar theoretical guarantees for achieving inference performance. We support our claims by providing empirical results across many datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Random features <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b45">46]</ref> is an approach to perform kernel-based inference on very large datasets. In the traditional kernel approach, we need to construct the kernelsimilarity matrix whose storage and computational time are quadratic in the size of the dataset; the quadratic dependence on the size of the dataset makes the approach infeasible for Big Data scenarios. Random features addresses this problem by explicitly constructing finite-dimensional random features from the data such that inner products between the random features approximate the kernel functions. Inference with random features achieves comparable performance as those of the kernel-based ones while enjoying the scalability of linear inference methods. Recently, it also achieves performance comparable to a convolutional neural network on datasets like the ImageNet <ref type="bibr" target="#b12">[13]</ref>.</p><p>We show that for signals enjoying sparse representations (either canonically or in a transform basis), the performance guarantees of random features can be significantly strengthened. Specifically, we prove that the dimension of random * This work was supported by the ARO Grant W911NF-15-1-0126. features required to approximate a stationary kernel function <ref type="bibr" target="#b41">[42]</ref> dominantly depends on the signal sparsity instead of the ambient dimension. For images, whose ambient dimension is often far greater than their sparsity, our analysis greatly improves the theoretical bounds of random features.</p><p>We next show that both computational and storage costs of random features applied to sparse signals can be significantly improved by first performing a dimensionality reduction using random projection and subsequently, applying random features to the dimensionality-reduced signals. There are several advantages to this scheme. First, we show that the theoretical guarantees in approximating the original kernel function are similar to that of random features applied on sparse signals. This means that the additional dimensionality reduction step does not hinder our ability to approximate kernel functions. Second, the dimensionality reduction can be performed optically with compressive cameras <ref type="bibr" target="#b18">[19]</ref>. In regimes where sensing is costly, (for example, short-wave infrared and midwave infrared), the use of compressive cameras enables sensing with low-resolution sensors <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b31">32]</ref> with associated savings in cost of the camera. Third, in the context of compressive imaging, inference tasks such as classification and detection are often simpler than recovery <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b40">41]</ref> and hence, we can expect to use high compression ratios in the dimensionality reduction step. In our experiments, we are able to achieve 10 − 30× compression with little loss in classification accuracy.</p><p>Contributions. In this paper, we propose a scheme called compressive random features that applies random features on compressive measurements of signals that enjoy sparse representations either canonically or in a transform basis (see <ref type="figure" target="#fig_0">Figure 1</ref>). Our contributions are three-fold:</p><p>• We prove that the number of random features required to accurately approximate the kernel function depends predominantly on the sparsity of the signals.</p><p>• We show that random features applied to dimensionalityreduced signals or equivalently, compressive measurements, are able to approximate isometric kernel functions of the original uncompressed data and provide analytical guarantees that bound the loss in performance. <ref type="figure" target="#fig_0">Figure 1</ref>: Overview of typical kernel methods, random features, and our compressive random features. N is the number of training samples, M is the dimension of the random features, d is the dimension of the original uncompressed data, and m is the dimension of the compressive measurements. In testing computations of typical and compressive random features schemes, we include the cost to construct random features and to apply classifiers for one test input.</p><p>• We also observe that our proposed scheme for compressive inference offers comparable classification performance, across many datasets, to similar approaches applied directly on the original data while providing reduced computational time and storage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Notations. We use d as the dimension of original uncompressed signals, m as the dimension of compressive measurements, M as the dimension of random features, and N as the number of training samples. We use lowercase boldface letters to denote vectors and uppercase letters to denote matrices. We say a signal is k-sparse if it has at most k nonzero entries. All norms in this paper are ℓ 2 -norm, denoted by · . The element-wise complex conjugate of a vector x is written as x. We define the diameter and the radius of a set X as diam(X ) = max</p><p>x,y∈X</p><p>x − y ,</p><formula xml:id="formula_0">radius(X ) = max x∈X x .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Random feature method</head><p>A hallmark of kernel-based inference is the development of kernel trick, which utilizes kernel function to efficiently evaluate similarity in infinitely high dimensional spaces; thereby, kernel-based inference is capable of approximating any decision boundary or function provided we have sufficient training samples <ref type="bibr" target="#b41">[42]</ref>. Despite this attractive ability, kernel methods are prohibitive for large datasets because of their high storage and time complexities during both the training and testing phases. Specifically, with N training samples, kernel trick usually requires computing and storing a kernel matrix whose size is N × N . Testing a single input requires evaluating kernel function between the input and a large portion of training samples <ref type="bibr" target="#b25">[26]</ref>.</p><p>The goal of random features <ref type="bibr" target="#b37">[38]</ref> is to achieve a scalable implementation of kernel methods. Given a stationary kernel function K(x, y) = f (x − y) := f (δ), its Fourier transform, p(ω), has only nonnegative entries <ref type="bibr" target="#b39">[40]</ref> due to the positive definiteness of the kernel and hence, can be treated as a probabilistic density function. The inverse Fourier transform of the kernel function is given as <ref type="bibr" target="#b0">(1)</ref> where d is the dimension of the data, and φ ω (x) := e jω ⊤ x . The sample mean 1 M M i=1 φ ωi (x)φ ωi (y) is thus an unbiased estimator of K(x, y) when {ω i } are i.i.d. samples from p. Since f (δ) and p(ω) are real, we can reduce φ ω (x) and define a real-valued random feature generating function, Φ :</p><formula xml:id="formula_1">K(x, y) = R d p(ω)e jω ⊤ (x−y) dω = Ep[φω(x)φω(y)],</formula><formula xml:id="formula_2">R d → R M , as Φ(x) = 2 M cos(ω ⊤ 1 x + b), · · · , cos(ω ⊤ M x + b) ⊤ ,<label>(2)</label></formula><p>where ω i is drawn from the distribution p and b is drawn uniformly from [0, 2π]. For the commonly used Gaus-</p><formula xml:id="formula_3">sian kernel K(x, y) = exp(− x − y 2 /(2σ 2 )), p(ω) = N (0, σ −2 I d ), where I d is the d × d identity matrix.</formula><p>Rahimi and Recht <ref type="bibr" target="#b37">[38]</ref> showed that the inner product of random features uniformly converges to K(x, y) in probability.</p><p>In particular, when training samples are from a compact set X ⊂ R d , in order to have P sup x,y∈X | Φ(x), Φ(y) − K(x, y)|&gt; ǫ less than a constant q, the dimension of random features</p><formula xml:id="formula_4">M = O d ǫ 2 log σp diam(X ) qǫ ,<label>(3)</label></formula><p>where σ 2 p := E p (ω ⊤ ω) is the second moment of p(ω).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Compressive sensing and compressive inference</head><p>Compressive sensing <ref type="bibr" target="#b2">[3]</ref> aims to sense a highdimensional signal from a low-dimensional measurements. Specifically, any d-dimensional, k-sparse signal can be exactly recovered from its m-compressive measurements, provided m = O(k log d k ). One of the main results in CS is the restricted isometry property (RIP) <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref> which suggests that distances between sparse signals are approximately preserved by certain measurement matrices, including random projections and partial Fourier matrices <ref type="bibr" target="#b38">[39]</ref>.</p><formula xml:id="formula_5">A m × d matrix P satisfies RIP (of order 2k) if for all k-sparse signals x, y ∈ R d , we have (1 − δ) x − y 2 ≤ Px − Py 2 ≤ (1 + δ) x − y 2 with some δ ∈ (0, 1)</formula><p>. This means that all pairwise distances between k-sparse signals are approximately preserved after projected by P. Sub-Gaussian random matrices and random orthoprojectors are known to satisfy RIP with high probability <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>. To generate a m × d random orthoprojector, we first i.i.d. sample its entries from a zero-mean Gaussian or Bernoulli distribution. Then we run the Gram-Schmidt process row-wise (assuming its rows are linearly independent) and multiply the result by d/m.</p><p>The approximate preservation of distances enables inference directly in the compressive domain. This ideatermed compressive inference -has resulted in many theoretical and practical algorithms for estimation <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b40">41]</ref> and classification <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b35">36]</ref> without the need of an intermediate reconstruction step. This saves the computation required to recover the signals and thus, lowers the computational and memory requirements of the inference algorithm.</p><p>The goal of this paper is to provide theoretical guarantees for applying random features onto compressive measurements. We can, therefore, perform non-linear inference on compressive inference without sacrificing its benefitslow time and storage complexities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Random features for sparse signals</head><p>The theoretical guarantee of random features provided in <ref type="bibr" target="#b37">[38]</ref> is for generic datasets and does not exploit any model on the data. If we know that our signals enjoys sparse representations, either canonically or in some transform basis, can we tighten the bound required for approximating a kernel function? We address this question in this section.</p><p>The following theorem characterizes the performance of random features approximating stationary kernel functions for signals that enjoy sparse representations. Theorem 1. (Fourier random feature with k-sparse data) Let X be a compact set of k-sparse vectors in R d . Let the random features for a stationary kernel function, Φ, be defined as in <ref type="bibr" target="#b1">(2)</ref> </p><formula xml:id="formula_6">with σ 2 p = E p [ω ⊤ ω]</formula><p>being the second moment of the Fourier transform of the kernel function. Given ǫ &gt; 0 and q ∈ (0, 1], there exists a constant c 1 &gt; 0, such that, when</p><formula xml:id="formula_7">M = c1 k ǫ 2 log σp diam(X ) qǫ d k ,<label>(4)</label></formula><p>the probability P sup</p><formula xml:id="formula_8">x,y∈X Φ(x), Φ(y) − K(x, y) &gt; ǫ ≤ q.</formula><p>The proof for Theorem 1 is provided in the appendix. As can be seen from the theorem, the dimension of random features depends predominantly on the sparsity of the signal, k, instead of its ambient dimensionality, d. Thereby, for sparse signals, the bound (4) greatly improves the original one shown in <ref type="formula" target="#formula_4">(3)</ref>. We note that the factor k log d k commonly appears in theoretical results of compressive sensing, e.g., for constructing m×d random sub-Gaussian matrices satisfying RIP <ref type="bibr" target="#b1">[2]</ref> and for ensuring stable recovery of sparse signals <ref type="bibr" target="#b16">[17]</ref>. Since the approximation of kernel function with random features can be considered as constructing a lowdimensional embedding of the reproducing kernel Hilbert space associated with the kernel function <ref type="bibr" target="#b4">[5]</ref>, it is not surprising that k log d k appears in our results. The following corollary extends the above theorem to signals which are not canonically sparse but are sparse in some transform basis. Examples of rotationally invariant stationary kernel functions include those depending only on the ℓ 2 -norm of the signal, like the Gaussian kernel and the B-spline kernel <ref type="bibr" target="#b41">[42]</ref>. Since images are often sparse in wavelet bases, this corollary allows us to apply random features on images with far-fewer features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Compressive random features</head><p>We now consider the application of random features to compressive measurements. We term this scheme compressive random features. By performing inference directly with compressive random features, we bypass the computationally-expensive reconstruction step. For images, which are originally dense but sparse after transformation, our scheme effectively reduces computational and storage costs and enjoys the low signal-acquisition cost provided by compressive cameras. These benefits make our scheme compelling in scenarios like Internet-of-things, where device cost, computation, and storage are of utmost concern.</p><p>Can we compute random features directly on the compressive measurements of sparse signals (either canonically or in a transform basis) without deteriorating its ability to approximate kernel functions? The following theorem addresses this question. </p><p>the probability P sup</p><p>x,y∈X</p><formula xml:id="formula_10">(| Φ(Px), Φ(Py) − K(x, y)|) &gt; ǫ ≤ q.</formula><p>The proof is provided in the appendix. Comparing to the bound in Theorem 1, we can see that the effect of dimensionality reduction before constructing random features does not significantly impede its ability to approximate isometric kernel functions. By centering the data, we can reduce radius(X ) to diam(X ). Thereby, the required M only increases by an order of 1 ǫ 2 log d k , but in return we gain the advantages of reduced device cost, computation, and storage (see <ref type="figure" target="#fig_0">Figure 1</ref>). In the context of compressive inference, this theorem provides a guarantee for applying random features directly on compressive measurements.</p><p>From our experiments in Section 5, we observe that it is possible to achieve a high compression ratio (m ≪ d) and still obtain comparable accuracies as those of the typical random features. The reason may be that Theorem 2 requires all pairwise kernel function values are approximated; nevertheless, in classification scenarios, we are allowed to heavily compress the data as long as data points belonging to different classes do not collapse onto each other <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b44">45]</ref>. This enables us to use high compression ratios. We leave the analysis as a future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Analysis of storage and time complexity</head><p>We now analyze the storage and time complexity for compressive random features. As can be seen from  <ref type="figure" target="#fig_4">(N dm+M N m)</ref>, which is smaller than the cost to construct typical random features, O(M N d), when m is small. We note that the accelerated random feature construction techniques <ref type="bibr" target="#b28">[29]</ref> are also applicable to our compressive random features scheme.</p><p>Testing time with our scheme are as follows. It takes O(M m) to construct compressive features and O(M ) to perform the inner product. To store a linear SVM, we only need to store the M +1 coefficients of the separating hyperplane. Instead, a typical kernel SVM requires the storage of all non-zero dual variables and their corresponding training samples. With large datasets, the number of non-zero dual variables usually grows linearly with N <ref type="bibr" target="#b25">[26]</ref>. To test an image, typical kernel methods require O(N d) to evaluate the kernel function between the image and training samples. This makes kernel methods costly during the testing phase as well. In summary, with compressive random features, we can achieve nonlinear-class classification performance with improved storage and time complexity compared to both original kernel methods and typical random features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>We conducted experiments on 5 datasets to examine the classification performance of linear SVMs using our compressive random features. We compared the performance against six methods, whose legends are as follows:</p><p>• Original linear: Linear SVM trained directly with original uncompressed data.</p><p>• Original kernel: Typical kernel-based SVM trained directly with original uncompressed data.</p><p>• Compressive linear: Linear SVM trained directly on compressive measurements, a technique commonly used in prior work <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b35">36]</ref>.</p><p>• Compressive kernel: Typical kernel-based SVM trained directly with compressive measurements.</p><p>• Typical random features: Linear SVM trained with random features applied to the original data.</p><p>• Compressive random features: Linear SVM trained with our compressive random features.</p><p>Among the last three methods, the compressive kernel approach is expected to achieve highest accuracies, since its kernel function is computed exactly. Further, in spite of undergoing both dimensionality reduction and kernel function   approximation, the proposed compressive random features is expected to achieve accuracy that is comparable to typical random features, especially when the dimension of compressive measurements, m, is large enough. We also expect to achieve accuracies comparable to the kernel-based SVM when the dimension of random features, M , is large enough so to precisely approximate the kernel function.</p><p>In all experiments, the SVMs are directly trained with pixel values or their compressive measurements (although our scheme also supports sparse features, like features learned by convolutional neural networks). Due to memory issues, in some instances, we downsampled images. We use the Gaussian kernel function K(x, y) = exp(− x − y 2 /(2σ 2 )) in all experiments, with σ kept the same across different methods. We used C-SVM in LIBLINEAR <ref type="bibr" target="#b20">[21]</ref> with C = 1. Finally, all results are averages over 20 trials.</p><p>We briefly introduce the 5 datasets used for validation.</p><p>• MNIST <ref type="bibr" target="#b29">[30]</ref> contains 60, 000 training images and 10, 000 test images. The 28 × 28 gray-scale images contain digits from 0 to 9. We set σ = 10. The results are shown in <ref type="figure" target="#fig_1">Figure 2</ref>.</p><p>• 8 scene categories dataset <ref type="bibr" target="#b34">[35]</ref> contains 256×256 RGB images of 8 different scenes, like mountain views, streets, highways, and coast, . . . , etc. There are 2688 images, and we randomly split them into 2150 training images and 538 test images. We resized images into 32 × 32. We set σ = 8. The results are shown in <ref type="figure" target="#fig_7">Figure 3</ref>.</p><p>• INRIA person dataset <ref type="bibr" target="#b13">[14]</ref> contains 128 × 64 RGB images. Each positive image contains a standing person, and the negative images do not. There are 8506 training images and 2482 test images. We resized the images to 32×16. We set σ = 5. The results are shown in <ref type="figure" target="#fig_8">Figure 4</ref>.</p><p>• CIFAR-10 [27] contains 32 × 32 RGB images of 10 different objects, like airplanes and horses. Each class has 5000 training images and 1000 test images. We set σ = 18. The results are shown in <ref type="figure" target="#fig_10">Figure 5</ref>.</p><p>• Street view house numbers dataset <ref type="bibr" target="#b33">[34]</ref> contains 32 × 32 RGB images. It contains images with different digits taken from house numbers in Google Street View images. It has 73257 training images and 26032 test images. We set σ = 13. The results are shown in <ref type="figure" target="#fig_11">Figure 6</ref>.</p><p>Observations. Across all experiments, compressive random features has a performance that is comparable to typical random features and outperforms compressive linear SVMs even under high compression ratio ( m d =0.07). Further, as shown in <ref type="figure" target="#fig_1">Figure 2d</ref>, working with dimensionalityreduced compressive measurements effectively reduces the   time to construct random features. In some datasets, we observe that when the dimension of random feature M is small, compressive linear SVMs are able to achieve better accuracies than both compressive random features and typical random features. This could be due to poor approximations of the kernel similarities at small values of M . As expected, when using larger values of M , both random feature methods achieve higher accuracies. Looking at the results of CIFAR-10 and the street view house numbers datasets, all these methods still have room for improvement compared with state-of-the-art methods like convolutional neural networks (CNN). This gap in performance can be attributed, in part, to our reliance on pixelvalues as the underlying features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion and discussion</head><p>We propose compressive random features, a framework for kernel-based inference on compressive measurements that enjoys low acquisition, computation, and storage costs, along with theoretical guarantees on its ability to approximate kernel similarities. In the context of compressive inference, we introduced a novel method to perform scalable nonlinear inference. Thereby, for many applications, our scheme provides an effective solution that provides a tradeoff between inference performance and design considera-tions like cost, computation, and storage. Finally, we note that even though we focused on sparse signals and stationary kernel functions, we conjecture that similar results can be derived for low-dimensional smooth manifolds and for dot-product kernels.</p><p>Comparison to the Nyström method. The Nyström method <ref type="bibr" target="#b17">[18]</ref> is another popular method for large-scale kernel-based inference. By first obtaining a low-rank approximation of the kernel matrix, the Nyström method obtains eigen-decomposition of the low-rank matrix and generates features using the eigenvectors. Since this process involves learning from data, the Nyström method can achieve better performance by exploiting structures specific to the dataset. However, the dependency on training data also makes the Nyström method less flexible than random features, whose feature construction function can be designed independent to the overall training-testing process. In this context, we can view the results in this paper as a potential approach to incorporate more knowledge of the signal into random features without having to perform learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Proofs</head><p>As discussed in Section 2.2, random orthoprojectors satisfy RIP with high probability. We state the following theorem which will be utilized to prove our theorem.   </p><p>The proof of the theorem is a simple extension of the results in <ref type="bibr" target="#b1">[2]</ref>.</p><p>Minimizing the right hand side w.r.t. r results in r = ( αz β ) ≤ 3 for all z ≥ 1. Setting an upper bound for the right hand side and solving for M will prove the theorem.</p><p>Proof of Corollary 1. We use the following property of multi-variant Fourier transform:</p><formula xml:id="formula_12">F (f (Bx)) = 1 det(B) (Ff )(B −T ω).</formula><p>Since B is a orthonormal basis and f is rotational invariant, we have</p><formula xml:id="formula_13">p(ω) = F (f (x)) = F (f (Bx)) = (F f )(B −T ω) = p(Bω).</formula><p>Therefore, p is rotational invariant. Let α = B ⊤ x for all x ∈ X . We have ω ⊤ x = ω ⊤ Bα = (B ⊤ ω) ⊤ α := z ⊤ α.</p><p>Since p is rotational invariant and ω ∼ p, z ∼ p. So Theorem 1 can be applied to α, which is k-sparse.</p><p>The sketch to prove Theorem 2 is as follows. In order to uniformly bound the approximation of kernel function with compressive features, we simply need to uniformly bound the errors caused by compressive sensing and random feature approximation separately.</p><p>Proof of Theorem 2. Let f ( x−y ) := K(x, y), ∀x, y. By triangular inequality, we have P sup </p><formula xml:id="formula_14">+ f ( Px − Py ) − f ( x − y ) &gt; ǫ ≤ P sup x,y∈X Φ(Px), Φ(Py) − f ( Px − Py ) &gt; ǫ 2 + P sup x,y∈X f ( Px − Py ) − f ( x − y ) &gt; ǫ 2<label>(8)</label></formula><p>Let D P be the diameter of PX and D = 2 radius(X ). Using the result in <ref type="bibr" target="#b37">[38]</ref>, we can bound the first term:</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Corollary 1 .</head><label>1</label><figDesc>Suppose a stationary kernel function is also rotationally invariant, i.e, f (Bδ) = f (δ) for any orthonormal basis B. Let X be a compact set in R d . Given an orthonormal basis Ψ, if for all x ∈ X , Ψx is k-sparse, then Theorem 1 holds on X .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Theorem 2 .</head><label>2</label><figDesc>(Compressive random feature) Let X be a compact set of k-sparse vectors in R d . Let P : R d → R m be a random orthoprojector constructed as described in Section 2.2. Let Φ : R m → R M be the random features of an isometric kernel function, defined asK(x, y) = f ( x − y ), with σ 2 p = E p [ω ⊤ ω]being the second moment of its Fourier transform. Given ǫ &gt; 0 and q ∈ (0, 1], there exist constants c 1 , c 2 &gt; 0, such that, when m = c</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Figure 1, if we use compressive cameras, we can get compressive measurements without actual computation, and the storage costs are O(N m). Since m = O(k log d), the saving of storage is large when k ≪ d. For compressive random features, it costs O(M N m) to construct and O(M N ) to store; in contrast, typical kernel methods require O(N 2 d) computation and O(N 2 ) storage for a kernel matrix. In the absence of compressive cameras where we obtain random sketches by computations, the total computational cost to construct compressive features is O</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>( d )</head><label>d</label><figDesc>Random feature construction time (in seconds). For compressive random features, the random projection time is included. The experiments were conducted on Intel Xeon CPU E5-2620 2.0GHz with 16GB memory. Note that in our experiments, typical and compressive random features have similar SVM training and testing time.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 2 :</head><label>2</label><figDesc>MNIST results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>) m = 400 ( m d = 0.13).(b) Accuracy ( %) under various settings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 3</head><label>3</label><figDesc>) m = 200 ( m d = 0.13).(b) Accuracy ( %) under various settings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 4 :</head><label>4</label><figDesc>INRIA person dataset results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>) m = 400 ( m d = 0.13).(b) Accuracy ( %) under various settings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 5 :</head><label>5</label><figDesc>CIFAR) m = 400 ( m d = 0.13).(b) Accuracy ( %) under various settings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 6 :</head><label>6</label><figDesc>Street view house numbers dataset results.Theorem 3. Let P : X → R m be a m × d random orthoprojector constructed as described in Section 2.2. If X is the set of k-sparse vectors in R d and m ∈ log(2)+k log(12/δ)+k log(ed/k) δ 2 /16−δ 3 /48, d , then for δ ∈ (0, 1) we have P ∀x ∈ X , (1 − δ) x 2 ≤ Px 2 ≤ (1 + δ)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>Px), Φ(Py) − f ( Px − Py )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>1 z+2 .</head><label>z+2</label><figDesc>After substituting r, the right hand side becomes , and we haveP sup ∆∈M |f (∆)|&gt; ǫThe last inequality holds because 2z</figDesc><table>α 

2 

z+2 β 

z 
z+2 

2z 

−z 

z+2 + z 

2 
z+2 

≤ 
edσp diam(X ) 
zǫ 

2z 
z+2 

exp 
−M ǫ 2 
4(z + 2) 
2z 

−z 

z+2 + z 

2 
z+2 

≤ 3 
2edσp diam(X ) 
zǫ 

2z 
z+2 

exp 
−M ǫ 2 
4(z + 2) 
. 

−z 

z+2 + z 

2 
z+2 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">r 2 := 2αr −z + βr 2 .</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Proof of Theorem 1. The difference of two k-sparse vectors is at most 2k-sparse. Let z := 2k. For any x, y ∈ X , x−y belongs to one of the d z z-dimensional subspaces, M 1 , . . . , M ( d z ) . Each M j , j=1, . . . , d z , is compact and has diameter at most twice diam(X ). Thus, we can construct a ǫ-net in each of M j , j=1, . . . , d z , and {x−y|∀x, y ∈ X } ⊆ ∪ ( d z ) j=1 M j . Each net will have at most T = (2 diam(X )/r) z balls of radius r <ref type="bibr" target="#b19">[20,</ref><ref type="bibr">Chapter 5]</ref>. Denote ∆ as (x, y) and M as {∆|∀x, y∈X }. Let ∆ i,j , i = 1, . . . T be the i-th center in the ǫ-net of M j . Define f : M → R, f (∆) := Φ(x), Φ(y) − K(x, y) and let L f be the Lipschitz constant of f . By limiting L f and making sure that all f (∆ i,j ) are small, we can provide a bound to the overall approximation error:</p><p>Let</p><p>as <ref type="bibr" target="#b37">[38]</ref>, we use Markov's inequality and get</p><p>Using a union bound and Hoeffding's inequality, we have</p><p>The last inequality holds because by the construction of ran-</p><p>The second term can be bounded as follows. Since f is differentiable and continuous and X is compact, the derivative of f attains its maximum and minimum in X . Therefore, the kernel function is Lipschitz continuous in X . Let L be the Lipschitz constant. By Theorem 3, we have ∀x, y ∈ X , | Px−Py − x−y | ≤ δ x−y ≤ δD with probability at least 1 − 2 exp(−c 1 δ 2 m). By the definition of Lipschitz continuous we have |f ( Px − Py ) − f ( x−y )| ≤ δDL. Thus, by setting δ = ǫ 2DL , we bound the second term using <ref type="bibr" target="#b5">(6)</ref>. Therefore, we have the following result:</p><p>When m is large enough, the second term is less than the first term, the right hand side of (9) is less than </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Compressive classification and the rare eclipse problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Bandeira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Mixon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1404.3203</idno>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A simple proof of the restricted isometry property for random matrices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Baraniuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Davenport</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Devore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wakin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Constructive Approximation</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Compressive sensing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">G</forename><surname>Baraniuk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Proc. Mag</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="118" to="121" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The cramér-rao bound for estimating a sparse parameter vector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ben-Haim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">C</forename><surname>Eldar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Proc</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Reproducing kernel Hilbert spaces in probability and statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Berlinet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Thomas-Agnan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Compressed learning: Universal sparse dimensionality reduction and learning in the measurement domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Calderbank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jafarpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schapire</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Decoding by linear programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Candes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Info. Theory</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="4203" to="4215" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The restricted isometry property and its implications for compressed sensing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Candès</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comptes Rendus Mathematique</title>
		<imprint>
			<biblScope unit="volume">346</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="589" to="592" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Stable signal recovery from incomplete and inaccurate measurements</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Candes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Romberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comm. Pure and Applied Mathematics</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1207" to="1223" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Near-optimal signal recovery from random projections: Universal encoding strategies?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Candes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Info. Theory</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5406" to="5425" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Compressive sensing for background subtraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Cevher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Duarte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">G</forename><surname>Baraniuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">FPA-CS: Focal plane array-based compressive imaging in short-wave infrared</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Asif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Veeraraghavan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Scalable kernel methods via doubly stochastic gradients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-F</forename><forename type="middle">F</forename><surname>Balcan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Signal processing with compressive measurements</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Davenport</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">T</forename><surname>Boufounos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">B</forename><surname>Wakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">G</forename><surname>Baraniuk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Selected Topics in Signal Processing</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The smashed filter for compressive classification and target recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Davenport</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Duarte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">B</forename><surname>Wakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">N</forename><surname>Laska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Takhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">F</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">G</forename><surname>Baraniuk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Electronic Imaging</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Lower bounds for sparse recovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Indyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Woodruff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM-SIAM Symp. Discrete Algorithms (SODA)</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">On the Nyström method for approximating a gram matrix for improved kernel-based learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Drineas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Mahoney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2153" to="2175" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Single-pixel imaging via compressive sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Duarte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Davenport</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Takhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">N</forename><surname>Laska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">E</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">G</forename><surname>Baraniuk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Proc. Mag</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">83</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Compressed sensing: theory and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">C</forename><surname>Eldar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kutyniok</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Liblinear: A library for large linear classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R.-E</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1871" to="1874" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Metric learning by collapsing classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Globerson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">T</forename><surname>Roweis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Compressive sampling for signal classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Haupt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Castro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nowak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fudge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Asilomar Conf. Signals, Systems and Computers</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1430" to="1434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">NuMax: A convex approach for learning near-isometric linear embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hegde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Baraniuk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Proc</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">22</biblScope>
			<biblScope unit="page" from="6109" to="6121" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Random projections for manifold learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hegde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Baraniuk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Random feature maps for dot product kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Karnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Reconstruction-free action inference from compressive imagers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Turaga</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>PAMI</publisher>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Fastfood-approximating kernel expansions in loglinear time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sarlós</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Reconstruction-free inference on compressive measurements</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lohit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Turaga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sankaranarayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Recent results of medium wave infrared compressive sensing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mahalanobis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shilling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Muise</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied optics</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">34</biblScope>
			<biblScope unit="page" from="8060" to="8070" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Compressed least-squares regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Maillard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Reading digits in natural images with unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop on Deep Learning and Unsupervised Feature Learning</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Modeling the shape of the scene: A holistic representation of the spatial envelope</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="145" to="175" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Random projections for support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Boutsidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Magdon-Ismail</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Drineas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Fast and scalable polynomial kernels via explicit feature maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pagh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Intl. Conf. Knowledge Discovery and Data Mining (SIGKDD)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Random features for large-scale kernel machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rahimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">On sparse reconstruction from fourier and gaussian measurements</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rudelson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vershynin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comm. Pure and Applied Mathematics</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1025" to="1045" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Fourier analysis on groups</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Rudin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990" />
			<publisher>John Wiley &amp; Sons</publisher>
			<biblScope unit="volume">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Compressive acquisition of linear dynamical systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">K</forename><surname>Turaga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">G</forename><surname>Baraniuk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Imaging Sciences</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Learning with kernels: support vector machines, regularization, optimization, and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Scholkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<publisher>MIT press</publisher>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Efficient additive kernels via explicit feature maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>PAMI</publisher>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="480" to="492" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Distance metric learning for large margin nearest neighbor classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="207" to="244" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Distance metric learning with application to clustering with side-information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Random laplace feature maps for semigroup kernels on histograms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sindhwani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Avron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mahoney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
