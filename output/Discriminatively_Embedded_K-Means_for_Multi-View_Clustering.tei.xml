<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Discriminatively Embedded K-Means for Multi-view Clustering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinglin</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Automation</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Han</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Automation</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiping</forename><surname>Nie</surname></persName>
							<email>feipingnie@gmail.com</email>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Center for OPTIMAL</orgName>
								<orgName type="institution">Northwestern Polytechnical University Xi&apos;an</orgName>
								<address>
									<addrLine>xujignlinlove, junweihan2010</addrLine>
									<postCode>710072</postCode>
									<country key="CN">P. R. China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Discriminatively Embedded K-Means for Multi-view Clustering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:39+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In real world applications, more and more data, for example, image/video data, are high dimensional and represented by multiple views which describe different perspectives of the data. Efficiently clustering such data is a challenge. To address this problem, this paper proposes a novel multi-view clustering method called Discriminatively Embedded K-Means (DEKM), which embeds the synchronous learning of multiple discriminative subspaces into multiview K-Means clustering to construct a unified framework, and adaptively control the intercoordinations between these subspaces simultaneously. In this framework, we firstly design a weighted multi-view Linear Discriminant Analysis (LDA), and then develop an unsupervised optimization scheme to alternatively learn the common clustering indicator, multiple discriminative subspaces and weights for heterogeneous features with convergence. Comprehensive evaluations on three benchmark datasets and comparisons with several state-of-the-art multi-view clustering algorithms demonstrate the superiority of the proposed work.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>As a fundamental technique in machine learning, pattern recognition and computer vision fields, clustering is to assign data of similar patterns into the same cluster and reflect the intrinsic structure of the data. In past decades, a variety of classical clustering algorithms such as K-Means Clustering <ref type="bibr" target="#b14">[15]</ref> and Spectral Clustering <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref> have been invented.</p><p>In recent years, due to the rapid development of information technology, we are often confronted with data represented by heterogeneous features. These features are generated by using various feature construction ways. One good example is image/video data. A large number of different visual descriptors, such as SIFT <ref type="bibr" target="#b19">[20]</ref>, HOG <ref type="bibr" target="#b6">[7]</ref>, LBP <ref type="bibr" target="#b21">[22]</ref>, GIST <ref type="bibr" target="#b22">[23]</ref>, CMT <ref type="bibr" target="#b29">[30]</ref> and CENT <ref type="bibr" target="#b28">[29]</ref>, have been proposed to characterize the rich content of image/video data from different perspectives. Each type of features may capture the specific information about the visual data. To cluster these data, one challenge is how to integrate the strengths of various heterogeneous features by exploring the rich information among them, which certainly can lead to more accurate and robust clustering performance than by using each individual type of features.</p><p>Nowadays, the data is often represented by very high dimensional features, which renders another challenge for the clustering. A number of earlier efforts have been devoted to addressing these two challenges. Focusing on one challenge that data is very high dimensional, many dimensionality reduction-based clustering methods <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b15">16]</ref> have been developed, which mostly concern simultaneous subspace selection by LDA and clustering. These methods generally are more appropriate for single-view data clustering. Although they may be extended to multi-view data clustering task by simply concatenating different views as input or integrating each view of clustering results to the final results, these extended methods still cannot achieve the satisfactory performance due to the lack of intercoordination and complementation between different views during clustering.</p><p>Focusing on another challenge that data is represented by multi-view, a school of unsupervised multi-view clustering methods have been presented. Although these methods can achieve interactions among heterogeneous features, there still exist some problems regarding heavy computational complexity or curse of dimensionality. Most of these methods can be roughly classified into two categories: Multi-View K-Means Clustering (MVKM) and Multi-View Spectral Clustering (MVSC). Many MVSC approaches essentially extend the Spectral Clustering from single view to multiple views and are mainly based on similarity graphs or matrices. Although this kind of multi-view clustering algorithms <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b4">5]</ref> can achieve encouraging performance, they still have two main drawbacks. On the one hand, the construction of the similarity graph for high dimensional data is a heavy work because many factors must be considered, such as the choice of similarity function and the type of similarity graph. This heavy work may greatly affect the final clustering performance. On the other hand, MVSC algorithms generally need to build proper similarity graph for each view. The more the number of different views, the more complex constructing similarity graphs will be. Thus, MVSC algorithms cannot effectively tackle high-dimensional multi-view data clustering.</p><p>Different from MVSC algorithms, MVKM approaches are more superior to deal with high-dimensional data because they do not need to construct a similarity graph for each view. This kind of methods is originally derived from the G-orthogonal non-negative matrix factorization (NMF) which is equivalent to relaxed K-Means clustering (RKM) <ref type="bibr" target="#b8">[9]</ref>. Recently, Cai et al. <ref type="bibr" target="#b2">[3]</ref> proposed the robust multi-view K-Means clustering (RMVKM) by using ℓ 2,1 -norm <ref type="bibr" target="#b10">[11]</ref> to replace the ℓ 2 -norm and learning individual weight for each view. However, RMVKM was performed in the original feature space without any discriminative subspace learning mechanism that may render curse of dimensionality when dealing with multi-view and high dimensional data. In addition, although the work in <ref type="bibr" target="#b30">[31]</ref> also extended the model from <ref type="bibr" target="#b9">[10]</ref> to the multi-view case, they sum the scatter matrices and produce a separate cluster assignment for each view, which is quite different from the proposed method.</p><p>According to above mentioned analysis, both directly extending single-view to multi-view and existing multi-view algorithms are far from thoroughly addressing the multiview clustering issue. In this paper, we propose a novel unsupervised multi-view scheme aiming to address above two challenges. The proposed method DEKM embeds the synchronous learning of multiple discriminative subspaces into multi-view K-Means clustering to construct a unified framework, and adaptively control the intercoordinations between different views simultaneously.</p><p>The highlights of DEKM method are in two aspects. Firstly, learning multiple discriminative subspaces is fulfilled synchronously. Under this unified and embedded framework, DEKM realizes the intercoordination of these subspaces and further makes them complement each other. Secondly, DEKM develops an intertwined and iterative optimization instead of just applying existing methods in an iterative manner, which not only maintains the relative independency on different discriminative subspaces, but also keeps the consistency of clustering results of multiple views. This multi-view extension is the first work among the earliest efforts to sum the clustering objectives via a weighted way. These are quite different from several recent works. Comprehensive evaluations on several benchmark image datasets and comparisons with some state-of-the-art multi-view clustering approaches demonstrate the efficiency and superiority of DEKM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">The proposed framework</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Formulation</head><p>According to <ref type="bibr" target="#b16">[17]</ref>, the trace ratio LDA for single-view was defined as follows:</p><formula xml:id="formula_0">W = arg max W T W=Im T r(W T S B W) T r(W T S W W)<label>(1)</label></formula><p>where W ∈ R d×m denotes the projection matrix which is a set of orthogonal and normalized vectors. It enables to reduce the dimensionality from d to m. S B and S W denote the between-class scatter matrix and the within-class scatter matrix, respectively. Suppose that X ∈ R d×N is the data matrix with N samples and d-dimension after centralization and G ∈ R N ×C is the clustering indicator matrix where each row of G denotes the clustering indicator vector for each sample, and C is the number of clusters. G ic = 1(i = 1, ..., N ; c = 1, ..., C) if the i-th sample belongs to the c-th class and G ic = 0 otherwise. Using G, S B and S W can be rewritten as:</p><formula xml:id="formula_1">S B = XG(G T G) −1 G T X T S W = XX T −XG(G T G) −1 G T X T<label>(2)</label></formula><p>Because of S T = S B + S W , (1) is equivalent to the following problem:</p><formula xml:id="formula_2">W = arg max W T W=Im T r(W T S B W) T r(W T S T W)<label>(3)</label></formula><p>We know that (3), as a supervised method, can seek a discriminative subspace to separate different classes maximally. Recently, the combination of dimensionality reduction and clustering has become a hot issue <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b15">16]</ref>. However, those methods are only designed for single-view issue. In this paper, we firstly design a weighted multi-view LDA and then develop an unsupervised optimization scheme to solve this multi-view framework.</p><p>Given M types of heterogeneous features, k = 1, 2, ..., M , we suppose X k ∈ R d k ×N as the data matrix for the k-th view. Referring to the definition of trace ratio LDA, we propose that, for two d k ×d k positive semi-definite matrices S k B and S k T , the weighted multi-view trace ratio LDA can be defined as finding M differen projection matrices W k | M k=1 respectively:</p><formula xml:id="formula_3">W k | M k=1 = arg max W T k W k =Im k | M k=1 M k=1 (α k ) γ T r(W T k S k B W k ) T r(W T k S k T W k )<label>(4)</label></formula><p>where W k denotes the projection matrix which reduces the dimensionality from d k to m k in the k-th view. α k is the weight for each view and γ is the parameter to control the weights distribution. S k B and S k T denote the S B and S T in the k-th view, respectively:</p><formula xml:id="formula_4">S k B = X k G(G T G) −1 G T X T k , S k T = X k X T k (5)</formula><p>It is apparent that the weighted multi-view LDA, i.e. (4), is still supervised. However, in the real applications, labeling data is very expensive. Without any label information, we know neither projection matrices W k | M k=1 nor clustering indicator matrix G of (4), which is adverse for doing high-dimensional clustering. Thus, we propose an unsupervised optimization scheme to solve the following weighted multi-view LDA:</p><formula xml:id="formula_5">max W k | M k=1 , α k | M k=1 ,G M k=1 (α k ) γ T r(W T k X k G(G T G) −1 G T X T k W k ) T r(W T k S k T W k ) −1 s.t.W T k W k = I m k | M k=1 , G ∈ Ind, M k=1 α k = 1, α k ≥ 0 (6)</formula><p>where Ind is a set of clustering indicator matrices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Optimization</head><p>The key difficulty of solving <ref type="formula">(6)</ref> is that (6) has become an unsupervised complex matter. In other words, the numerator of (6),</p><formula xml:id="formula_6">X k G(G T G) −1 G T X T k , actually S k B , is closely re- lated to G. However, W k | M k=1 , α k | M k=1</formula><p>and G are unknown. To simultaneously obtain these variables in a better way, we offer the Theorem 1 to transform (6) into a more tractable framework (7) which is the proposed method DEKM. Actually, W k | M k=1 are not decoupled in <ref type="formula" target="#formula_7">(7)</ref> since G is also a variable to be optimized. Theorem 1. Solving (6) is equivalent to solving the following objective function:</p><formula xml:id="formula_7">min W k | M k=1 , α k | M k=1 ,G M k=1 (α k ) γ ||W T k X k − F k G T || 2 F T r(W T k S k T W k ) s.t.W T k W k = I m k | M k=1 , G ∈ Ind, M k=1 α k = 1, α k ≥ 0<label>(7)</label></formula><p>Proof. Obviously, using the properties of matrix trace, <ref type="bibr" target="#b6">(7)</ref> can be rewritten as the following formula:</p><formula xml:id="formula_8">min W k | M k=1 , α k | M k=1 ,G M k=1 (α k ) γ T r (W T k X k −F k G T ) T (W T k X k −F k G T ) T r(W T k S k T W k ) = min W k | M k=1 , α k | M k=1 ,G M k=1 (α k ) γ T r(X T k W k W T k X k )−2T r(F T k W T k X k G) +T r(F k G T GF T k ) T r(W T k S k T W k )<label>(8)</label></formula><p>Due to solving the minimum, we get its derivative with respect to F k . Ignoring irrelevant terms and using the rules of matrix derivative, we can obtain:</p><formula xml:id="formula_9">F k = W T k X k G(G T G) −1<label>(9)</label></formula><p>Excitingly, F k ∈ R m k ×C is the cluster centroid in discriminative subspace for the k-th view. Substituting (9) into (8), there is:</p><formula xml:id="formula_10">min W k | M k=1 , α k | M k=1 ,G M k=1 (α k ) γ 1− T r(W T k X k G(G T G) −1 G T X T k W k ) T r(W T k S k T W k ) ⇔ max W k | M k=1 , α k | M k=1 ,G M k=1 (α k ) γ T r(W T k X k G(G T G) −1 G T X T k W k ) T r(W T k S k T W k ) −1<label>(10)</label></formula><p>Therefore, solving <ref type="formula">(6)</ref> is equivalent to solving <ref type="formula" target="#formula_7">(7)</ref>.</p><p>Further, we decompose <ref type="formula" target="#formula_7">(7)</ref> into three subproblems and solve them via alternate iteration method.</p><p>Step1:</p><formula xml:id="formula_11">Solving G when W k | M k=1 , F k | M k=1 and α k | M k=1 are fixed.</formula><p>Obtaining G via a weighted multi-view K-Means clustering is an unsupervised learning stage. The clustering indicator matrix G is unknown and we search the optimal solution of G among multiple low-dimensional discriminative subspaces.</p><p>We separate X k and G into independent vectors respectively. Then <ref type="formula" target="#formula_7">(7)</ref> can be replaced by the following problem:</p><formula xml:id="formula_12">min G M k=1 (α k ) γ ||W T k X k −F k G T || 2 F = min G N i=1 M k=1 (α k ) γ ||W T k x i k −F k g i || 2 2 s.t.G ∈ Ind, g i ∈ G, g ic ∈ {0, 1}, C c=1 g ic = 1<label>(11)</label></formula><p>where x i k is the i-th column of X k , which corresponds to the i-th sample in the k-th view and g i is the i-th row of G, which denotes the clustering indicator vector for the i-th sample. Assigning G into (11) one by one is equivalent to tackling the following problem for the i-th sample:</p><formula xml:id="formula_13">c * = arg min c M k=1 (α k ) γ ||W T k x i k −F k e c || 2 2<label>(12)</label></formula><p>where e c is the c-th row of identity matrix I C and c * means that the c * -th element of g i is 1 and others are 0. There are only C kinds of candidate clustering indicator vectors, so we can easily find out the solution of (12).</p><p>Step2: Solving W k | M k=1 and F k | M k=1 when G and α k | M k=1 are fixed.</p><p>Calculating W k | M k=1 and F k | M k=1 via a weighted multiview LDA is a supervised learning stage. Moreover, the discriminative subspace W k for each view is closely related to the clustering indicator matrix G and its weight α k .</p><p>From <ref type="formula" target="#formula_9">(9)</ref>, we know that F k is a function of W k and G. When G and α k | M k=1 are fixed, substituting (9) into <ref type="formula" target="#formula_7">(7)</ref> and omitting constant terms, the objective function becomes:</p><formula xml:id="formula_14">min W k | M k=1 M k=1 T r(W T kS k W W k ) T r(W T k S k T W k ) , s.t.W T k W k = I m k | M k=1 (13) whereS k W = (α k ) γ [X k X T k − X k G(G T G) −1 G T X T k ]</formula><p>denotes the weighted within-class scatter matrix for the k-th view. Thus, solving (13) equals to solving the following formula:</p><formula xml:id="formula_15">max W k | M k=1 M k=1 T r(W T kS k B W k ) T r(W T k S k T W k ) , s.t.W T k W k = I m k | M k=1 (14) whereS k B = (α k ) γ X k G(G T G) −1 G T X T</formula><p>k denotes the weighted between-class scatter matrix for the k-th view. <ref type="bibr" target="#b13">(14)</ref> jointly optimizes M distinct discriminative subspaces in parallel. The solution W k for each view is solved by a trace ratio LDA when G and α k | M k=1 are fixed. Step3: Solving α k | M k=1 when W k | M k=1 and G are fixed. Learning the non-negative normalized weight α k for each view assigns the more discriminative image feature with higher weight. To derive the solution of α k | M k=1 , we rewrite (7) as:</p><formula xml:id="formula_16">min α k | M k=1 M k=1 (α k ) γ H k , s.t. M k=1 α k = 1, α k ≥ 0<label>(15)</label></formula><p>where</p><formula xml:id="formula_17">H k = W T k X k −F k G T 2 F T r(W T k S k T W k )<label>(16)</label></formula><p>Thus, the Lagrange function of (15) is:</p><formula xml:id="formula_18">M k=1 (α k ) γ H k − λ( M k=1 α k − 1)<label>(17)</label></formula><p>where λ is the Lagrange multiplier. In order to get the optimal solution, we set the derivative of (17) with respect to α k to zero and then substitute the result into the constraint M k=1 α k = 1. There is:</p><formula xml:id="formula_19">α k = (γH k ) 1 1−γ M v=1 (γH v ) 1 1−γ<label>(18)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 The algorithm of DEKM method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input:</head><p>Data for M views {X k |k = 1, 2, ..., M }, X k ∈ R d k ×N . The number of clusters C. The reduced dimension m k for each view and the parameter γ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Output:</head><p>The projection matrix W k , cluster centroid matrix F k and weight α k for the k-th view. The common clustering indicator matrix G. Initialization:</p><p>Set t = 0. Initialize G ∈ Ind. Initialize W k by W T k W k = Im k and initialize the weight α k = 1/M for the k-th view.</p><p>While not converge do 1: Calculate G by :</p><formula xml:id="formula_20">c * = arg min c M k=1 (α k ) γ ||W T k x i k −F k ec|| 2 2 2: Calculate F k by F k = W T k X k G(G T G) −1 and update W k | M k=1 by max W k | M k=1 M k=1 T r(W T kS k B W k ) T r(W T k S k T W k ) 3: Update α k | M k=1 by: α k = (γH k ) 1 1−γ M v=1 (γHv ) 1 1−γ End While, return W k | M k=1 , G and α k | M k=1</formula><p>To sum up, in Algorithm 1, we can obtain G via Step1, which is equivalent to the Discriminative K-Means including the interrelations among multi-view features. Updating W k | M k=1 via Step2 is the dimensionality reduction for each view. Updating α k | M k=1 via Step3 fulfills the learning of multiple weights simultaneously. Then we repeat this process iteratively until the objective function value becomes converged.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Convergence analysis</head><p>As mentioned above, DEKM is a unified and embedded multi-view framework solved by an unsupervised optimization scheme. It is obvious that when we transform (6) into (7), it can be divided into three subproblems. Here we show the following proof to verify the convergence of Discriminatively Embedded K-Means (DEKM) algorithm. Theorem 2. In each iteration, no matter the objective function value of <ref type="bibr" target="#b5">(6)</ref> or that of its variant <ref type="bibr" target="#b6">(7)</ref>, which all decrease until the algorithm converges.</p><p>Proof. Supposing after the t-th iteration, we have obtained W </p><formula xml:id="formula_21">W (t+1) k = arg max W k (α (t) k ) γ · · · · · · T r[W (t)T k X k G (t) (G (t)T G (t) ) −1 G (t)T X T k W (t) k ] T r(W (t)T k S k T W (t) k ) −1 = arg min W k (α (t) k ) γ · · · · · · T r[W (t)T k (S k T −X k G (t) (G (t)T G (t) ) −1 G (t)T X T k )W (t) k ] T r(W (t)T k S k T W (t) k )<label>(19)</label></formula><p>Referring to the way of argumentation for <ref type="bibr" target="#b5">[6]</ref>, through rewriting <ref type="bibr" target="#b18">(19)</ref> we have:</p><formula xml:id="formula_22">T r W (t+1)T kS k(t) W W (t+1) k T r W (t+1)T k S k T W (t+1) k ≤ T r W (t)T kS k(t) W W (t) k T r W (t)T k S k T W (t) k (20) wherẽ S k(t) W = (α (t) k ) γ [S k T − X k G (t) (G (t)T G (t) ) −1 G (t)T X T k ] = (α (t) k ) γ S k(t) W</formula><p>and it denotes the weighted within-class scatter matrix for the k-th view at the t-th iteration.</p><p>In the same way, we fix W k | M k=1 and α k | M k=1 as W</p><formula xml:id="formula_23">(t) k | M k=1</formula><p>and α (t) k | M k=1 respectively, and solve for G (t+1) . According to (6), we can obtain:</p><formula xml:id="formula_24">G (t+1) = arg max G M k=1 (α (t) k ) γ · · · · · · T r[W (t)T k X k G (t) (G (t)T G (t) ) −1 G (t)T X T k W (t) k ] T r(W (t)T k S k T W (t) k ) −1 = arg min G M k=1 (α (t) k ) γ · · · · · · T r[W (t)T k (S k T −X k G (t) (G (t)T G (t) ) −1 G (t)T X T k )W (t) k ] T r(W (t)T k S k T W (t) k )<label>(21)</label></formula><p>By rewriting <ref type="bibr" target="#b20">(21)</ref>, there is:</p><formula xml:id="formula_25">M k=1 T r W (t)T kS k(t+1) W W (t) k T r W (t)T k S k T W (t) k ≤ M k=1 T r W (t)T kS k(t) W W (t) k T r W (t)T k S k T W (t) k (22) wherẽ S k(t+1) W = (α (t) k ) γ S k T −X k G (t+1) (G (t+1)T G (t+1) ) −1 G (t+1)T X T k = (α (t) k ) γ S k(t+1) W</formula><p>and it is the weighted within-class scatter matrix for the k-th view at the t+1-th iteration. Similarly, we fix W k | M k=1 and G as W (t) k | M k=1 and G (t) respectively, and solve for α (t+1) k | M k=1 . According to <ref type="bibr" target="#b5">(6)</ref>, for each view, α (t+1) k can be calculated by:</p><formula xml:id="formula_26">α (t+1) k = arg max α k (α (t) k ) γ · · · · · · T r[W (t)T k X k G (t) (G (t)T G (t) ) −1 G (t)T X T k W (t) k ] T r(W (t)T k S k T W (t) k ) −1 = arg min α k (α (t) k ) γ · · · · · · T r[W (t)T k (S k T −X k G (t) (G (t)T G (t) ) −1 G (t)T X T k )W (t) k ] T r(W (t)T k S k T W (t) k )<label>(23)</label></formula><p>Thus, <ref type="bibr" target="#b22">(23)</ref> can be further rewritten as follows:</p><formula xml:id="formula_27">(α (t+1) k ) γ T r W (t)T k S k(t) W W (t) k T r W (t)T k S k T W (t) k ≤ (α (t) k ) γ T r W (t)T k S k(t) W W (t) k T r W (t)T k S k T W (t) k<label>(24)</label></formula><p>Integrating <ref type="formula" target="#formula_1">(20)</ref>, <ref type="bibr" target="#b21">(22)</ref> and <ref type="formula" target="#formula_1">(24)</ref>, we arrive at:</p><formula xml:id="formula_28">M k=1 T r W (t+1)T k (α (t+1) k ) γ S k(t+1) W W (t+1) k T r W (t+1)T k S k T W (t+1) k ≤ M k=1 T r W (t)T k (α (t+1) k ) γ S k(t+1) W W (t) k T r W (t)T k S k T W (t) k ≤ M k=1 T r W (t)T k (α (t+1) k ) γ S k(t) W W (t) k T r W (t)T k S k T W (t) k ≤ M k=1 T r W (t)T k (α (t) k ) γ S k(t) W W (t) k T r W (t)T k S k T W (t) k<label>(25)</label></formula><p>Thus, <ref type="bibr" target="#b24">(25)</ref> proves that (6) and its variant <ref type="formula" target="#formula_7">(7)</ref> are lower bounded and their objective function value decreases after each iteration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we evaluate the performance of DEK-M on three benchmark datasets in terms of two standard clustering evaluation metrics, namely Accuracy (ACC) <ref type="bibr" target="#b1">[2]</ref> and Normalized Mutual Information (NMI) <ref type="bibr" target="#b1">[2]</ref>. Before do anything, we need to centralize the data and normalize all values in the range of [−1, 1].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>In our experiments, by following <ref type="bibr" target="#b2">[3]</ref>, three benchmark image datasets including Caltech101 <ref type="bibr" target="#b12">[13]</ref>, MSRC <ref type="bibr" target="#b27">[28]</ref> and Handwritten [1] were adopted for evaluations. <ref type="figure" target="#fig_1">Figure 1</ref> shows some image examples from above three datasets. Table 1 summarizes the information of each dataset including the number of images and classes, heterogeneous features and the dimensionality of each type of feature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Toy example</head><p>In this section, we conducted a toy experiment to verify the effectiveness of DEKM. For simplicity, we worked on the two-view case given by <ref type="bibr" target="#b18">[19]</ref>. We show the projection directions (green solid lines) with different numbers of iterations Initialization, Iteration = 3, Iteration = 5 and Iteration = 7 in <ref type="figure" target="#fig_5">Figure 2</ref>. Performing our method with γ &gt; 1 on this synthetic data, ACC is 0.9950 and N-MI is 0.9590. It is observed that DEKM can exactly obtain projection directions which separate different clusters maximally and achieve better and stable accuracy with few iteration steps.</p><p>In contrast, if we performed LDA on each individual view, the results are decreased significantly. It further demonstrates that DEKM method has no trivial solution when γ &gt; 1 and incorporates multiple views effectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Performance evaluation</head><p>Comparison methods. Firstly, we compared the performance of DEKM with Embedded K-Means clustering <ref type="bibr" target="#b25">[26]</ref> (EKM) for single-view to simply explain the advantage of multi-view. Secondly, to emphasize the importance of intercoordination among multiple views, we compared the results of DEKM with AEKM which concatenates all views together directly and then performs EKM clustering. Thirdly, we compared DEKM with some baseline methods naive Multi-view K-Means clustering (NMVKM), its robust version LMVKM (NMVKM with ℓ 2,1 -norm) and R-MVKM <ref type="bibr" target="#b2">[3]</ref> to demonstrate the significant advantage of discriminative subspace learning. Finally, when we ignore the weight of each view, DEKM can degenerate to a simple version DEKM (SDEKM), which verifies the necessity of the weight learning.</p><p>Comparison results. From comparison results shown in <ref type="table" target="#tab_1">Tables 2 and 3</ref>, we have the following observations.</p><p>In <ref type="table" target="#tab_1">Table 2</ref>, DEKM performs significantly better than EK-M. It is straightforward to demonstrate the superiority of multi-view. In <ref type="table" target="#tab_2">Table 3</ref>, compared with AEKM without any mutual information among multiple views, it is clear that DEKM can boost the clustering performance due to the intercoordinations of different views. In addition, DEKM outperforms other methods (NMVKM, LMVKM, RMVK-M and SDEKM). On the one hand, compared with NMVK-M, LMVKM and RMVKM, DEKM simultaneously obtains multiple discriminative subspaces which has great effects on the performance of algorithms. On the other hand, unlike SDEKM, DEKM adaptively learns the weight for each view to better integrate heterogeneous image features and then improve the performance of clustering.</p><p>Furthermore, we tested the convergence speed of DEKM on three datasets which is shown in <ref type="figure" target="#fig_7">Figure 3</ref>. It is observed that DEKM algorithm can converge with few iteration steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Evaluation of key components of the proposed method</head><p>There are three key components in DEKM algorithm: the initialization of G, the dimensionality of embedded subspace m k for the k-th view and the parameter γ.</p><p>Initialization. According to <ref type="bibr" target="#b2">[3]</ref>, it can be seen that NMVKM and RMVKM always simply use general random method to initialize G. However, random initialization greatly affects the results of clustering. Like in <ref type="bibr" target="#b2">[3]</ref>, the ACC of NMVKM is 0.7002 ± 0.085, and the ACC of RMVKM is 0.8142 ± 0.087. We can see that the precision level, i.e. 8.5% or 8.7%, is not high, such that it is hard to always remain high performance. In addition, unstable initialization is difficult to control parameters and obtain ideal results. Thus, we initialize G in a new way to substantially reduce the dependence of clustering result on initialization and conveniently tune parameters.</p><p>We first sort the rows of identity matrix I C randomly and get the matrixĨ C , and then we use direct product of vector 1 and matrixĨ C to produce the initial G:</p><formula xml:id="formula_29">G = 1 ⊗Ĩ C s.t.I C ∈ R C×C ,Ĩ C ∈ R C×C , 1 ∈ R f loor(N/C)×1 (26)</formula><p>where 1 denotes a column vector with all elements being 1. Sometimes the number of samples N cannot be divisible by the number of clusters C, so we need to extra select r = N − C × f loor(N/C) rows fromĨ C randomly to fill the indivisible part. In other words, this new initialization makes the number of samples for each label equally as far as possible. Note that we do not care whether these labels are correct or not as long as the numbers of different kinds of labels are equal. As the mapping relationships between the labels of different clusters are nearly invariable, we can obtain more stable initialization.</p><p>Dimension of views. In above discussions, we assume that the total scatter matrix is always invertible. However, in     real applications, high-dimensional complex data may lead total scatter matrix to be singular. If so, we can adopt P-CA as a preprocessing step to ensure the total scatter matrix invertible.</p><p>In this paper, dimensionality m k is an important parameter because the curse of dimensionality may occur if m k is large and otherwise there exists overlap of different clusters. We determined m k heuristically by grid search and choosed the one with the best clustering accuracy. <ref type="table" target="#tab_1">Tables 2  and 3</ref> show that DEKM outperforms other methods greatly, when we find suitable parameter m k . For example, we have performed the test on Caltech101-7 dataset whose total dimensionality of all views reaches up to 2659. Through DEKM algorithm, we not only reduce the total dimensionality of all views from 2659 to 1194, but also learn multiple discriminative subspaces to significantly improve the performance of clustering.</p><p>Parameter γ. In DEKM method, we use one parameter γ to control the distribution of weights for different views. According to <ref type="bibr" target="#b17">(18)</ref> and the characteristic of the function <ref type="bibr" target="#b0">1</ref> 1−γ , two extreme cases are produced. When γ → ∞, DEKM can get equal weights 1 M . When γ → 1 + , suppos- EKM(view1) 0.5048 ± 0.00 0.4365 ± 0.00 0.3243 ± 0.00 0.1666 ± 0.00 0.6340 ± 0.00 0.6253 ± 0.00 EKM(view2) 0.6286 ± 0.00 0.5436 ± 0.00 0.5578 ± 0.00 0.4080 ± 0.00 0.7680 ± 0.00 0.7313 ± 0.00 EKM(view3) 0.5048 ± 0.00 0.4734 ± 0.00 0.3738 ± 0.00 0.2948 ± 0.00 0.5745 ± 0.00 0.5361 ± 0.00 EKM(view4) 0.4238 ± 0.00 0.3277 ± 0.00 0.6961 ± 0.00 0.6276 ± 0.00 0.4280 ± 0.00 0.4995 ± 0.00 EKM(view5) 0.6714 ± 0.00 0.6275 ± 0.00 0.7007 ± 0.00 0.6235 ± 0.00 0.6455 ± 0.00 0.5462 ± 0.00 EKM(view6) 0.5476 ± 0.00 0.5527 ± 0.00 0.6667 ± 0.00 0.5635 ± 0.00 0.6975 ± 0.00 0.6429 ± 0.00 DEKM 0.9238 ± 0.00 0.8649 ± 0.00 0.8503 ± 0.00 0.8231 ± 0.00 0.9530 ± 0.00 0.9098 ± 0.00 NMVKM 0.7810 ± 0.00 0.7122 ± 0.00 0.7143 ± 0.00 0.7337 ± 0.00 0.7810 ± 0.00 0.7661 ± 0.00 LMVKM 0.7762 ± 0.00 0.7190 ± 0.00 0.7664 ± 0.00 0.7208 ± 0.00 0.8030 ± 0.00 0.7853 ± 0.00 RMVKM 0.9048 ± 0.00 0.8463 ± 0.00 0.7846 ± 0.00 0.7145 ± 0.00 0.9125 ± 0.00 0.8539 ± 0.00 AEKM 0.7810 ± 0.00 0.7293 ± 0.00 0.7302 ± 0.00 0.7299 ± 0.00 0.8950 ± 0.00 0.8152 ± 0.00 SDEKM 0.8810 ± 0.00 0.8002 ± 0.00 0.8254 ± 0.00 0.7465 ± 0.00 0.9355 ± 0.00 0.8753 ± 0.00 DEKM 0.9238 ± 0.00 0.8649 ± 0.00 0.8503 ± 0.00 0.8231 ± 0.00 0.9530 ± 0.00 0.9098 ± 0.00    ing H p = min{H k |k = 1, ..., p, ..., M }, we substitute H p into (18) and solve its weight:</p><formula xml:id="formula_30">lim γ→1 + α p = lim γ→1 + 1 1+ v =p (H v /H p ) 1 1−γ = 1<label>(27)</label></formula><p>It can be seen that DEKM assigns 1 to the weight of the view whose H p value is the smallest and assign 0 to the weights of other views. Using such kind of strategy,we not only assure DEK-M has no trivial solution when γ &gt; 1, but also reduce the parameters of the model greatly. In our experiments, we searched log 10 γ in the range from 0 to 1 with incremental step 0.1 to obtain the best parameter γ. In <ref type="figure" target="#fig_9">Figure 4</ref>, we show that γ dominates the performance of DEKM algorithm on three datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we have proposed an unsupervised clustering framework which embeds multiple discriminative subspaces learning into multi-view K-Means clustering to construct an unified framework, and adaptively control the intercoordinations between multiple views via the weight learning. Besides, our optimization scheme efficiently solved the proposed objective function with global optimality and convergence. Comprehensive evaluations on widely used image benchmarks have demonstrated DEKM is effective for clustering high-dimensional multi-view data.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>M k=1 , G (t) and α (t) k | M k=1 . In the t + 1-th iteration, we firstly fix G and α k | M k=1 as G (t) and α (t) k | M k=1 respectively, and then solve W (t+1) k for each view. Thus, when G (t) and α (t) k | M k=1 are fixed, according to (6), W (t+1) k can be solved by the following equation:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>Some example images from (a) Caltech101, (b) MSRC and (c) Handwritten numerals data sets.(b) Iteration=3</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 2 .</head><label>2</label><figDesc>Projection directions of synthetic data with different iterations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 3 .</head><label>3</label><figDesc>The convergence curve of DEKM on Handwritten, M-SRCv1 and Caltech101-7 dataset, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 4 .</head><label>4</label><figDesc>The influence of parameter γ on Handwritten, MSRCv1 and Caltech101-7 datasets, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><head>Table 1 .</head><label>1</label><figDesc>Descriptions of testing datasets.</figDesc><table>View 
MSRCv1 
Caltech101-7 
Handwritten 

1 
CMT (48) 
CMT (48) 
FAC (216) 
2 
HOG (100) 
HOG (100) 
PIX (240) 
3 
LBP (256) 
LBP (256) 
ZER (47) 
4 
SIFT (210) 
SIFT (441) 
MOR (6) 
5 
GIST (512) 
GIST (512) 
KAR (64) 
6 
CENT (1302) CENT (1302) FOU (76) 
Images 210 
441 
2000 
Classes 7 
7 
10 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 .</head><label>2</label><figDesc>Comparison of DEKM and EKM on MSRCv1, Caltech101-7 and Handwritten datasets.</figDesc><table>Method 
MSRCv1 
Caltech101-7 
Handwritten 
ACC 
NMI 
ACC 
NMI 
ACC 
NMI 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 .</head><label>3</label><figDesc>Clustering Performances of the compared methods on MSRCv1, Caltech101-7 and Handwritten datasets.</figDesc><table>Method 
MSRCv1 
Caltech101-7 
Handwritten 
ACC 
NMI 
ACC 
NMI 
ACC 
NMI 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Uci machine learning repository</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Asuncion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Newman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Document clustering using locality preserving indexing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TKDE</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1624" to="1637" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multi-view k-means clustering on big data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Heterogeneous image feature integration via multi-modal spectral clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Kamangar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1977" to="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Diversityinduced multi-view subspace clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="586" to="594" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A convex formulation for semi-supervised multi-label feature selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Spectral clustering with two views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">R</forename><surname>De Sa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICM-L workshop on learning with multiple views</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="20" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Nonnegative lagrangian relaxation of k-means and spectral clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">D</forename><surname>Simon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECML</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="530" to="538" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Adaptive dimension reduction using discriminant analysis and k-means clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">R1-pca: rotational invariant l 1-norm principal component analysis for robust subspace factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Subspace clustering of high dimensional data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Domeniconi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Papadopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gunopulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SDM</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVIU</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="59" to="70" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multiple kernel learning based multi-view spectral clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3774" to="3779" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Algorithm as 136: A kmeans clustering algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Hartigan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Statistics</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="100" to="108" />
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Discriminative embedded clustering: A framework for grouping high-dimensional data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>TNNLS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Trace ratio problem revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TNN</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="729" to="735" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A co-training approach for multiview spectral clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Daumé</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Co-regularized multi-view spectral clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Daume</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Distinctive image features from scale-invariant keypoints. IJCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Multiple non-redundant spectral clustering views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G</forename><surname>Dy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="831" to="838" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Multiresolution gray-scale and rotation invariant texture classification with local binary patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ojala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pietikainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Maenpaa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="971" to="987" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Modeling the shape of the scene: A holistic representation of the spatial envelope</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="145" to="175" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Normalized cuts and image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<publisher>TPAMI</publisher>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="888" to="905" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A tutorial on spectral clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Von</forename><surname>Luxburg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics and Computing</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="395" to="416" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Unsupervised feature selection via unified trace ratio formulation and k-means clustering (track)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECMLPKDD</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Multi-feature spectral clustering with minimax optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="4106" to="4113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Locus: Learning object classes with unsupervised segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jojic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Where am i: Place instance and category recognition using spatial pact</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Color texture moments for content-based image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A subspace cotraining framework for multi-view clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-L</forename><surname>Dugelay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PR</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="73" to="82" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Spectral clustering and transductive learning with multiple views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Burges</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1159" to="1166" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
