<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Fashion Style in 128 Floats: Joint Ranking and Classification using Weak Data for Feature Extraction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edgar</forename><surname>Simo-Serra</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Waseda University</orgName>
								<address>
									<settlement>Tokyo</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroshi</forename><surname>Ishikawa</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Waseda University</orgName>
								<address>
									<settlement>Tokyo</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Fashion Style in 128 Floats: Joint Ranking and Classification using Weak Data for Feature Extraction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:40+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a novel approach for learning features from weakly-supervised data by joint ranking and classification. In order to exploit data with weak labels, we jointly train a feature extraction network with a ranking loss and a classification network with a cross-entropy loss. We obtain highquality compact discriminative features with few parameters, learned on relatively small datasets without additional annotations. This enables us to tackle tasks with specialized images not very similar to the more generic ones in existing fully-supervised datasets. We show that the resulting features in combination with a linear classifier surpass the state-of-the-art on the Hipster Wars dataset despite using features only 0.3% of the size. Our proposed features significantly outperform those obtained from networks trained on ImageNet, despite being 32 times smaller (128 singleprecision floats), trained on noisy and weakly-labeled data, and using only 1.5% of the number of parameters. 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>With the emergence of large-scale datasets and the appearance of deep networks with millions of parameters, researchers have started to replace hand-crafted global image features such as GIST <ref type="bibr" target="#b20">[21]</ref> with those obtained from intermediate representations of deep networks trained for classification on large datasets <ref type="bibr" target="#b43">[44]</ref>. Although this has led to a great improvement over the previous generation of features, these networks are learned in a fully-supervised manner on large amounts of data with very costly and time-consuming annotation. Features learned on one dataset can be used on another, but naturally not all datasets are equal <ref type="bibr" target="#b31">[32]</ref>, and thus features taken from networks trained on ImageNet <ref type="bibr" target="#b6">[7]</ref> will not work as well on datasets with very different visual characteristics, such as the scene classification dataset Places <ref type="bibr" target="#b48">[49]</ref>, and vice versa. While unsupervised feature learning exists as an alternative to supervised learning, the <ref type="bibr" target="#b0">1</ref>   <ref type="figure">Figure 1</ref>: Overview of the proposed feature learning approach. We train a feature extraction on weakly annotated data by jointly training a feature extractor network with a classification network. For training, an anchor image (center) is provided in conjunction with a similar image (right) and a dissimilar image (left) according to a metric provided on the weak noisy annotations. The classification loss l C serves to learn useful feature maps while the ranking loss l R on the triplet of Feature CNN encourages them to learn a discriminative feature representation.</p><p>lack of guidance to what to learn given by explicit labels makes it a much more complex task <ref type="bibr" target="#b18">[19]</ref>.</p><p>However, images obtained from the Internet usually have associated metadata which, although often inaccurate, can be used as weak labels. In this paper, we study how to exploit data with weak labels in order to obtain high-quality compact discriminative features without additional annotations. With such features, we tackle tasks in which the images are more specific and not very similar to those of existing fully-supervised datasets such as ImageNet or Places.</p><p>In this work, we focus on the domain of fashion images, which have only recently become the focus of research <ref type="bibr" target="#b39">[40]</ref>. These images have several characteristics that make them Mean Hipster Wars <ref type="bibr">(1,893 images)</ref> Mean Fashion144k <ref type="bibr">(277,527 images)</ref> Places <ref type="bibr">(2,469,371 images)</ref> Mean ImageNet <ref type="bibr">(1,331,167 images)</ref> Mean <ref type="figure">Figure 2</ref>: We show example images and the mean image from the Hipster Wars <ref type="bibr" target="#b15">[16]</ref>, Fashion144k <ref type="bibr" target="#b26">[27]</ref>, Places <ref type="bibr" target="#b48">[49]</ref>, and ImageNet <ref type="bibr" target="#b6">[7]</ref> datasets. In both fashion-related datasets we can make out a human silhouette, although it is significantly more diffuse in the Fashion144k dataset due to the much larger pose variation. In the Places dataset mean image we can see a gradient where the top of the image is clearer, likely corresponding to the sky in many images. While in the ImageNet mean we see a much more uniform image with a slightly clearer area in the center of the image. Unlike the other datasets, Fashion144k only has weak labels. With our approach we are able to exploit the Fashion144k for training to evaluate on the Hipster Wars dataset, outperforming fully supervised approaches that use larger datasets such as ImageNet or Places.</p><p>very challenging to tackle with computer vision. On one hand, they have small local details such as accessories that only depend on a very small part of the image, making segmentation very challenging <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b42">43]</ref>. On the other hand, we still have to consider more global properties such as the fashion style <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b33">34]</ref>, which depend jointly on the various items in the image. These difficulties, along with the fact that the images generally have a 3:4 aspect ratio and tend to have much brighter colors, cause the features taken from networks trained on ImageNet or Places to generalize poorly to fashion-oriented tasks. While no large fullyannotated fashion dataset exists, there are many datasets, such as the Paperdoll <ref type="bibr" target="#b40">[41]</ref> and Fashion144k <ref type="bibr" target="#b26">[27]</ref> datasets, that have a large amount of weak labels exploitable for learning. In this work, we take advantage of these noisy labels to learn compact discriminative features with deep networks that can then be used in other challenging fashionrelated tasks, such as style classification <ref type="bibr" target="#b15">[16]</ref>, in which they greatly outperform the state-of-the-art and other pretrained CNN baselines, while having 1.5% the number of parameters and being the size of a SIFT descriptor <ref type="bibr" target="#b19">[20]</ref>: 128 floats and 32× smaller than the best competing approach.</p><p>Instead of training networks for classification and using an intermediate-layer representation as a feature vector, we propose performing joint classification and ranking as shown in <ref type="figure">Fig. 1</ref>. The ranking acts as a soft constraint on the intermediate layer and encourages the model to learn more representative features guided by the classifier. We perform ranking by considering three images simultaneously: we first pick an anchor image and then pick an image that is similar to the anchor and one that is different. We establish a similarity metric by exploiting the weak user-provided labels, which are also used as a classification target. By simultaneously considering both ranking and classification, we are able to outperform approaches that use either ranking or classification alone. Our approach allows us to efficiently make use of weak labels to learn features on datasets closer to the target application, as shown in <ref type="figure">Fig. 2</ref>. In summary, our novel approach for feature learning:</p><p>• Exploits large amounts of weakly-labeled data commonly found on the Internet. • Learns a compact discriminative representation with few parameters on relatively small datasets. • Allows for efficient comparisons by Euclidean distances. We complement our in-depth quantitative analysis with visualizations for qualitative analysis. In addition to our feature extraction network learning approach we present a novel visualization approach for comparing image similarity between two images by exploiting the change in the extracted features when partially occluded.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Fashion: Interest in fashion has been growing in the computer vision community. Some of the more traditional problems have been semantic segmentation of garments <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b42">43]</ref>, image retrieval <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b14">15]</ref>, and classification of garments and styles <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b41">42]</ref>. Attempting to directly predict more esoteric measurements, such as popularity <ref type="bibr" target="#b37">[38]</ref> or fashionability <ref type="bibr" target="#b26">[27]</ref>, have also been recently studied. As defining absolute metrics is rather complicated in such a subjective domain as fashion, exploiting relative attributes <ref type="bibr" target="#b16">[17]</ref> and learning image similarity <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b33">34]</ref> have also been proposed. Many of these approaches rely on datasets created by crawling the Internet and have large amounts of exploitable weak labels <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b38">39]</ref>. Yamaguchi et al. <ref type="bibr" target="#b38">[39]</ref> used these tags to provide priors for semantic segmentation. Our method is complementary to these approaches and can provide features for greater performance.</p><p>Deep Learning: There has been a flurry of new research focusing on exploiting deep learning in computer vision. Much of it focuses on improving classification results on large datasets such as ImageNet <ref type="bibr" target="#b6">[7]</ref> or Places <ref type="bibr" target="#b48">[49]</ref>. This has led from initial models such as Alexnet <ref type="bibr" target="#b17">[18]</ref> to more sophisticated ones such as the VGG models <ref type="bibr" target="#b28">[29]</ref> with up to 19 layers; or the Googlenet models <ref type="bibr" target="#b30">[31]</ref>, that jointly use convolutions of different sizes in each layer. A more structured analysis of different networks was presented by Chatfield et al. <ref type="bibr" target="#b3">[4]</ref>, where they also analyzed using bottleneck layers to provide features of different sizes. All these approaches rely on fully supervised datasets. Xiao et al. <ref type="bibr" target="#b36">[37]</ref> extended learning to partially noisy labels. However, they still require roughly 50% of the labels to be correct and need to learn another network to correct the noisy labels, while only noisy labels suffice for our approach.</p><p>Deep Similarity: Instead of learning classification networks, it is possible to directly learn similarity using deep neural networks. A popular approach consists of Siamese networks <ref type="bibr" target="#b2">[3]</ref>, in which a pair of inputs is used simultaneously to train a neural network model. The loss encourages similar inputs to have similar network outputs and dissimilar inputs to have dissimilar network outputs. This method has been recently applied with great success to local feature descriptors <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b44">45]</ref> and also for obtaining better representations of product images <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b33">34]</ref>. It has also been extended to triplet images for ranking <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b35">36]</ref> and has been very successfully applied to face recognition <ref type="bibr" target="#b24">[25]</ref> in particular. We build upon this concept of image triplets for our ranking loss and show that by combining the ranking with classification results can be significantly improved.</p><p>Weak Data: We can identify two major sources of weak labels when using deep networks: text <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b13">14]</ref> and image tags <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref>. Text can generally be found accompanying images and thus can be directly exploited as a form of weak label. Frome et al. <ref type="bibr" target="#b8">[9]</ref> use text accompanying images to train more semantically meaningful classifiers, while Karpathy and Fei-Fei <ref type="bibr" target="#b13">[14]</ref> use them as a form of weak annotation of the objects that lie in the image to perform localization. More recently, it has been seen that detectors seem to emerge when training deep networks for classification <ref type="bibr" target="#b47">[48]</ref>. This has been utilized to learn models for semantic segmentation <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref>. However, as far as we know, we are the first to propose leveraging user-provided tags to learn discriminative features for a specific domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>We present a method for learning discriminative features from weakly-labeled data by jointly training both a feature extraction network and a classification network. A ranking loss on triplets of images is applied on the feature extraction network whose output is then fed into the classification network, where a classification loss is employed. After training, the feature extraction network can be used to provide discriminative features for other algorithms without a need for the classification network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Joint Ranking and Classification</head><p>We formulate the problem as a joint ranking and classification problem. A ranking loss on triplets of images is applied on a feature extraction network, while a classification loss is employed on a classification network that uses the output of the feature extraction network. For training, we assume that we have a set of images with associated weak labels with large amounts of noise.</p><p>For ranking, we take three images as input simultaneously, as shown in <ref type="figure">Fig. 1</ref>. One image (center) is the anchor or reference image, to which the second image (right) is similar and the last image (left) is dissimilar. We assume that we have a similarity metric r(·, ·) between the weak labels of a pair of images. We consider two thresholds τ s and τ d that, given this metric, determine when two images are similar and dissimilar, respectively. Thus, two images I 1 and I 2 , with labels y 1 and y 2 respectively, will be similar when r(y 1 , y 2 ) &lt; τ s and dissimilar when r(y 1 , y 2 ) &gt; τ d . We will define each image triplet as T = (I − , I, I + ) where r(y − , y) &gt; τ d and r(y, y + ) &lt; τ s .</p><p>Suppose we have a set T of possible noisy tags, or attributes of an image like red-sweater or even just red. A label l = (l t ) t∈T for an image assigns l t ∈ {0, 1} to each tag t. If a tag applies to an image (e.g., if the tag is red-sweater and a red sweater appears in the image), the label for the image assigns 1 to the tag. Note that we assume these tags to be noisy and not exact. Let |l| be the number of tags that label l assigns 1. We propose using the similarity function between labels a and b defined as "intersection over union":</p><formula xml:id="formula_0">r(a, b) = |a ∧ b| |a ∨ b| ,<label>(1)</label></formula><p>where ∧ and ∨ operate on the labels as tag-wise minimum and maximum, respectively. Given an image triplet T , we use the feature extraction network to obtain a triplet of features</p><formula xml:id="formula_1">T f = (f − , f , f + ),</formula><p>where f is a feature vector of each image. We then compute a distance between two feature vectors and apply a ranking loss that encourages the distance d + between the anchor and the similar image to be smaller than the distance d − between the anchor and the dissimilar image.</p><p>For comparing features, we consider the Euclidean distance · 2 . In contrast with <ref type="bibr" target="#b24">[25]</ref>, we normalize the distances instead of normalizing the feature pairs to have unitary norm. This changes the hard constraint into a soft constraint, which is essential for being able to learn using a classification loss and ranking loss simultaneously. We normalize the pair of distances (d − , d + ) obtained from the feature triplet using the softmax operator:</p><formula xml:id="formula_2">d − = exp( f − − f 2 ) exp( f − − f 2 ) + exp( f + − f 2 )<label>(2)</label></formula><formula xml:id="formula_3">d + = exp( f + − f 2 ) exp( f − − f 2 ) + exp( f + − f 2 )</formula><p>.</p><p>(</p><p>With distances now normalized to the [0, 1] range, we define a ranking loss l R that maximizes the dissimilar distance d − and minimizes the similar distance d + :</p><formula xml:id="formula_5">l R (d + , d − ) = 0.5 (d + ) 2 + (1 − d − ) 2 = (d + ) 2 , (4) which is 0 only when f + − f 2 = 0 and f − − f 2 &gt; 0.</formula><p>In contrast to <ref type="bibr" target="#b35">[36]</ref>, the loss is normalized; because of that, we do not need to use large amounts of weight decay in order to ensure that the output of the network does not tend to infinity. In fact, we find that the implicit regularization provided by dropout and batch normalization are sufficient and do not rely on weight decay at all. While the ranking loss l R by itself should be sufficient to learn discriminative features, we found in practice that it is critical to complement it with a classification loss. We do this by employing a separate classification network that uses the features of the dissimilar image f − and outputs a pre-</p><formula xml:id="formula_6">diction value X − = (X t − ) t∈T , X t − = (X t −,0 , X t −,1 ) ∈ R 2</formula><p>for each binary value on each tag. We do not use the anchor image features f nor the similar image features f + , as they form a subset of the training images, unlike the dissimilar images, which are chosen randomly. With y − as the noisy target label for the input image, we use multi-label crossentropy loss for classification:</p><formula xml:id="formula_7">l C (X − , y − ) = 1 |T | t∈T l × (X t − , y t − ) ,<label>(5)</label></formula><formula xml:id="formula_8">l × (x, y) = −x y + log (exp(x 0 ) + exp(x 1 )) . (6)</formula><p>Finally, we combine both losses to obtain the model loss:</p><formula xml:id="formula_9">l(d + , d − , X − , y − ) = l R (d + , d − ) + αl C (X − , y − ) ,<label>(7)</label></formula><p>where α is a weight to balance the different loss functions. The classification loss l C affects both the feature extraction network and the classification network, while the ranking loss l R only affects the feature extraction network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Feature Extraction Network</head><p>We follow the approach of <ref type="bibr" target="#b28">[29]</ref> of using 3x3 kernels for the convolutional filters to keep the number of weights down for the network and allow increasing the number of layers. One pixel padding is used to keep the input size and output size of the convolutional layers constant. In order to allow efficient learning the entire network from scratch, we rely on Batch Normalization layers <ref type="bibr" target="#b12">[13]</ref>. Dropout <ref type="bibr" target="#b29">[30]</ref> is used to prevent overfitting throughout the architecture.</p><p>A full overview of the architecture can be seen in Table 1. We note two important differences with commonly used networks: firstly, it uses a 3:4 aspect ratio for the input images as they are dominant in the fashion community; and secondly, it has a very small number of parameters compared to widely-used models. This is due to using only a </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Classification Network</head><p>Due to the noisy nature of the weak labels, the objective of the classification network is to aid the learning of the feature extraction network and not high classification performance. It consists of a batch normalization layer, followed by a rectified linear unit layer, a linear layer with 128 hidden units, and finally another linear layer which outputs the set of predictions X for classification. This network is kept small to encourage the propagation of gradients into the feature extraction network and hasten the learning. The initial batch normalization and rectified linear unit layers help partially isolate the classification network from the feature extraction network. When learning with 123 tags, the classification network has a total of only 48,502 parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Joint Learning</head><p>Both networks are trained jointly using backpropagation <ref type="bibr" target="#b23">[24]</ref>. Instead of using stochastic gradient descent which is dependent on setting a learning-rate hyperparameter, we utilize the ADADELTA algorithm <ref type="bibr" target="#b45">[46]</ref>, which adaptively sets the learning rate each iteration. No image cropping, momentum, nor weight decay is used. The only image preprocessing consists of subtracting the mean from each color channel and dividing by the standard deviation.</p><p>Initialization is critical for learning both networks: even with batch normalization, we were unable to train both networks jointly from scratch. We overcome this issue by first training the feature extraction network with an additional fully-connected layer for classification (Eq. <ref type="formula" target="#formula_7">(5)</ref>). Once the optimization has converged, the additional classification layer is removed from the feature extraction network and the classification network is added with random weights. Finally, both networks are trained jointly.</p><p>Since it is impossible to precompute all the possible values of the similarity metric r(·, ·) for large datasets, we use a simple sampling approach for the triplet of images when using the ranking loss. We initially choose a random anchor image I. We then randomly sample an image I r and check to see if r(I, I r ) &gt; τ s or r(I, I r ) &lt; τ d . In the first case, I r is added as the similar image I + and in the latter case it is added as the dissimilar image I − to the image triplet. This is done until the image triplet is completely formed. If it is not formed in a set number of iterations, a new anchor image is chosen and the procedure is restarted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results</head><p>We implement our approach using the Torch7 framework <ref type="bibr" target="#b5">[6]</ref>. We train our model on the Fashion144k dataset <ref type="bibr" target="#b26">[27]</ref>, and evaluate on the Hipster Wars dataset <ref type="bibr" target="#b15">[16]</ref>. We compare our results against publicly available pretrained CNNs, and the state-of-the-art style descriptor <ref type="bibr" target="#b40">[41]</ref> baselines. Our approach outperforms all baselines while being more efficient to compute and compact. We also perform additional experiments for the prediction of fashionability and see that we outperform all other approaches in the accuracy metric. In all cases, our joint classification and ranking approach outperforms using either classification or ranking losses alone, and using a Siamese architecture (described in the supplemental material).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Cleaning the Dataset</head><p>We train on the Fashion144k dataset <ref type="bibr" target="#b26">[27]</ref>. Since the images have been obtained from chictopia.com without any sort of filtering, a large amount of images are not representative of what we wish to learn: a descriptor for fashion style. Thus, we would like to clean the data, i.e., take only the images we consider suitable for training and not others. An example of images which we wish to classify are shown in <ref type="figure">Fig. 3</ref>. As it is unreasonable to do the cleaning manually, we train a classifier after a minor amount (6,000 images) of annotation that can be done in a couple of hours. We will show that this gives a significant increase in performance.</p><p>We annotate the images based on whether or not they contain a fully-visible person centered in the image (the <ref type="figure">Figure 3</ref>: Examples of clean and dirty images from the Fashion144k dataset <ref type="bibr" target="#b26">[27]</ref> are shown in the top and bottom rows, respectively. While there is much diversity, the clean images show figures more or less centered with the whole body visible, whereas dirty images have strong filters, show close-ups of objects, and/or are severly cropped.</p><p>supplemental material contains more details). We use a 1:1 train-to-test split to finetune the VGG 16 Layers Model <ref type="bibr" target="#b28">[29]</ref> pretrained on ImageNet for the binary classification task of whether or not an image is suitable for training. We are able to obtain 94.23% accuracy on the 3,000 test images.</p><p>As weak annotations, we use the "color" tags provided by the Fashion144k dataset which consist of color-garment tags such as red-sweater or blue-boots, the set of which we denote by T 1 and has 3,180 unique tags. We split the tags into colors and garments, resulting in a total of 123 unique weakly-annotated tags T 2 . These tags are the only ones used when performing classification. However, for the weak metric (Eq. (1)), we consider the set formed by the union of the "color" tag set and the split tag set:</p><formula xml:id="formula_10">T = T 1 ∪ T 2 .</formula><p>We build a clean version of the Fashion144k dataset by first filtering out entries for which less than three tags in T 2 are assigned 1, to reduce the noise. We additionally filter images using our trained classifier on whether or not they are suitable. This results in 80,554 training images and 8,948 validation images with a 9:1 train to validation split.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Training the Model</head><p>We train with a batchsize of 32 for classification and 16 when jointly training for classification and ranking, due to the higher memory usage. We use a similarity threshold of τ s = 0.75 and dissimilarity threshold of τ d = 0.01. Examples of triplets of images T used for learning can be seen in <ref type="figure" target="#fig_0">Fig. 4</ref>. When jointly training for classification and ranking, we set the classification loss weight to α = 0.01 such that the losses are of comparable magnitude. We initially train the feature extraction network with a classification loss. We then use the best performing model evaluated on the validation set to initialize the weights for the feature extraction network when using other losses. In particular, we consider joint classification and ranking loss, only ranking loss, and Siamese loss. We also compare to using the non-cleaned dataset which we denote as "dirty".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Hipster Wars Dataset</head><p>We evaluate on the Hipster Wars dataset <ref type="bibr" target="#b15">[16]</ref>, which consists of similar images to the Fashion144k dataset <ref type="bibr" target="#b26">[27]</ref> used to train, but from different sources. The dataset is made up of pictures of upright people in the center of the image; each corresponds to one of five styles: hipster, bohemian, goth, preppy, and pinup. The task is to perform 5-way clothing style classification. We compare against the state-of-theart which is based on a 39,168-dimensional style descriptor <ref type="bibr" target="#b40">[41]</ref> that is built by first estimating the 2D pose, which is trained in a supervised manner, and then extracting features for 32 × 32 pixel patches around all the pose keypoints. We also consider publicly available standard pretrained networks on ImageNet and Places. All approaches except ours use fully-supervised noise-free data for training.</p><p>We evaluate the features by combining them with a linear SVM <ref type="bibr" target="#b7">[8]</ref> with L 2 regularization and L 2 loss to predict the style. We perform 5-fold cross validation to set the regularization parameter and evaluate 100 times using random splits with a 9:1 train to test ratio as done in <ref type="bibr" target="#b15">[16]</ref>. We consider the top δ = 0.5 images from each style for classification. Each of the dimensions of the features are normalized independently using the training set such that the mean is 0 and the standard deviation is 1, except for approaches that directly learn embeddings, e.g., Ranking, Joint, and Siamese models. We report accuracy, precision, recall, and intersection over union (iou) in <ref type="table">Table 2</ref>. We can see that all our models outperform all the other approaches. By cleaning the data and improving the loss objective from classification to Siamese, Siamese to Ranking, and finally Ranking to Joint Classification and Ranking, we are able to improve performance.</p><p>We also consider two other scenarios: no training and fine-tuning. The results for not training and directly using <ref type="table">Table 2</ref>: Comparison with the state-of-the-art on the Hipster Wars dataset. We evaluate as in <ref type="bibr" target="#b15">[16]</ref> by computing the mean of 100 random splits with a 9:1 train to test ratio. For all the models we additionally display the number of parameters, and the dimension of the features. Dirty refers to training on a non-cleaned version of the Fashion 144k dataset. Our compact features significantly outperform the others.  <ref type="bibr" target="#b15">[16]</ref> and instead compare with the results from the confusion matrix they provide in their paper.</p><p>‡ Not directly comparable but in the order of hundreds of thousands. feature distances is shown in <ref type="table" target="#tab_3">Table 3</ref>. Our approach clearly outperforms other approaches, with a 20% increase in performance with respect to 4096-dimensional features and 50% increase with respect to similar size 128-dimensional features. If we use a single split instead of using 100 random splits and fine-tune the deep networks, we get the results shown in <ref type="table" target="#tab_4">Table 4</ref>. Fine-tuning the deep network significantly improves the performance; however, the best performing network is still within 1% of our approach, despite using a 32× larger internal feature representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Predicting Fashionability</head><p>We also evaluate on the much more complicated task of fashionability prediction on the Fashion144k dataset <ref type="bibr" target="#b26">[27]</ref>. This consists of rating how fashionable a person in an image is on a scale of 1 to 10. As this is the dataset used for training, although with a different objective, we use only  the images not used in the training set for evaluation. We use 8,000 images for training and 948 images for testing. As with the Hipster Wars dataset, we evaluate the features using a linear SVM with L 2 regularization and L 2 loss and set the regularization parameter with 5-fold cross validation. We compare against deep network baselines in <ref type="table" target="#tab_5">Table 5</ref>. We can see our approach is able to significantly outperform the 128-dimensional feature network. However, it is outperformed by some of the 4096-dimensional features from deep networks. This is likely due to the fact that the fashionability score, while correlated with the style of the outfit, is greatly affected by non-visible factors such as social connections <ref type="bibr" target="#b37">[38]</ref>, and thus larger features are beneficial. Despite this, our approach outperforms similar networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Visualizing the Style Descriptor</head><p>We follow a similar approach to <ref type="bibr" target="#b46">[47]</ref> to visualize how the input image is related to our descriptor. Instead of focusing on a single neuron, however, we consider the entire style descriptor output by displaying both the norm and projecting it onto PCA basis. We do this by sliding a 48 × 48 bounding box around the input image with the mean color of the input  <ref type="figure">Figure 6</ref>: We analyze how the style of two image matches by using our descriptor. Blue indicates that the area is dissimilar between the images using the middle anchor image as a reference, while red represents similar areas in the image. The distance between the descriptors are shown above each visualization map. Our approach is capable of finding similarities between the fashion styles in the images. We compare with a fine-tuned VGG_CNN_M_128 network which can't identify the clothing changes in the image.</p><p>image and calculating the style descriptor. We compare the resulting descriptors with the original image descriptor and visualize the change. In this way, we can localize the parts of the image that have the greatest effect on the descriptor, i.e., the parts that our feature extraction network focuses on.</p><p>We show examples on the Fashionista dataset <ref type="bibr" target="#b39">[40]</ref> in <ref type="figure" target="#fig_1">Fig. 5</ref> and see that our style descriptor reacts strongly to different parts of the body for different individuals.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Matching Styles</head><p>Instead of considering only a single image, we can consider pairs of images and match parts of the image using the style descriptor. This is done in a similar way as visualizing a single style descriptor, that is, by sliding a 48 × 48 pixel bounding box around the image. The difference is that we consider two feature descriptors f 1 = f (I 1 ) and f 2 = f (I 2 ) corresponding to two different images I 1 and I 2 simultaneously. We employ the difference between both feature vectors to evaluate how well this vector matches the change of the style descriptor f (·) given an image partially occluded at pixel location (u, v) with a bounding box mask B(u, v) by using the dot product: <ref type="bibr" target="#b7">(8)</ref> where I M (u, v) is the output map at pixel (u, v), and • is the Hadamard product or element-wise matrix multiplication.</p><formula xml:id="formula_11">I M (u, v) = (f (I 1 • B(u, v)) − f 1 ) · (f 2 − f 1 ) ,</formula><p>We show results in <ref type="figure">Fig. 6</ref> where we can see that our descriptor is effectively capturing the notion of outfit similarity in a reasonable way. Note that this concept of local outfit similarity was learned automatically from noisy userprovided tags without any pixel level annotations. On the other hand, the fine-tuned VGG_CNN_M_128 model gives similar maps regardless of the image compared to: it is overfitting to the Hipster Wars dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7.">Exploring the Fashion Style Space</head><p>Finally, we perform a qualitative analysis of the resulting style descriptors obtained by visualizing the fashion style space using t-SNE <ref type="bibr" target="#b32">[33]</ref>. The style descriptors can be compared efficiently by using Euclidean distances. We visualize the Hipster Wars "Pinup" class in <ref type="figure" target="#fig_2">Fig. 7</ref>. Our features display a remarkable robustness to background changes and focus on the outfit. They are also able to capture subtleties such as the transition from pink dresses without patterns to floral patterns, and group navy dresses with white spots regardless of the background and the wearer's ethnicity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>We have presented a novel approach to weaklysupervised learning of features consisting of joint ranking and classification. This allows learning compact 128dimensional features for more specific types of images that may be very costly and complicated to annotate. Our method allows us to learn discriminative features that are able to outperform both the previous state of the art and the best-performing model trained on ImageNet while being the size of a SIFT descriptor. The proposed joint ranking and classification approach consistently improves results over using either classification or ranking loss alone. We complement our model with a simple approach to automatically clean the data. In addition to a quantitative analysis, we present a new approach both to visualize the individual descriptor activations and to find similarities between two style images. Our analysis of the resulting descriptor shows it is robust to backgrounds and is able to capture finegrained details such as flower patterns on pink dresses.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 4 :</head><label>4</label><figDesc>Example of triplets of images used for training the model when using a ranking loss on the cleaned version of the Fashion144k dataset. For each triplet the anchor image I is displayed in the center with the dissimilar image I − on the left and the similar image I + on the right.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 5 :</head><label>5</label><figDesc>We analyze the relationship between the image and the style descriptor by moving an occluding box around the image and display the change in the norm of the descriptor and the change of the first three components on PCA basis computed on all the vectors extracted on the image. The positive and negative values are encoded in blue and red respectively. The norm is normalized so that the minimum value is white and the maximum value is blue, while the PCA representations are normalized by dividing by the maximum absolute value. Large descriptor changes correspond to the location of the individual and the PCA modes refer to the location of different garments. We compare with a fine-tuned VGG_CNN_M_128 network and see that our approach focuses on the figure and not the background.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 7 :</head><label>7</label><figDesc>Visualization of the fashion style space of the Pinup class from the Hipster Wars<ref type="bibr" target="#b15">[16]</ref> dataset using t-SNE<ref type="bibr" target="#b32">[33]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>Models available at http://hi.cs.waseda.ac.jp/ esimo/research/stylenet/</figDesc><table>Feature 
CNN 

Feature 
CNN 

Feature 
CNN 

Classifier 

Shared 
parameters 

Dissimilar 
Similar 

I 
I 
I 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 1 :</head><label>1</label><figDesc>Feature extraction network architecture. All convolutional layers have 1 × 1 padding and all layers besides the max pooling layer have a 1 × 1 stride, while the max pooling layers have a 4 × 4 stride.</figDesc><table>type kernel size output size 
params 

convolution 
3 × 3 
384x256x64 
1,792 
convolution 
3 × 3 
384x256x64 
36,928 
dropout (25%) 
384x256x64 

max pooling 
4 × 4 
96x64x64 
batch normalization 
96x64x64 
128 
convolution 
3 × 3 
96x64x128 
73,856 
convolution 
3 × 3 
96x64x128 
147,584 
dropout (25%) 
96x64x128 

max pooling 
4 × 4 
24x16x128 
batch normalization 
24x16x128 
256 
convolution 
3 × 3 
24x16x256 
295,168 
convolution 
3 × 3 
24x16x256 
590,080 
dropout (25%) 
24x16x256 

max pooling 
4 × 4 
6x4x256 
batch normalization 
6x4x256 
512 
convolution 
3 × 3 
6x4x128 
32,896 
fully-connected 
128 
393,344 

TOTAL 
128 
1,572,544 

single fully-connected layer with 128 hidden neurons and 
decreasing the number of filters before the fully-connected 
layer. This allows the model to have high performance 
while having only 1,572,544 parameters. In comparison, 
the VGG 16 layer network [29] has 134,260,544 parame-
ters when considering only feature extraction. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head></head><label></label><figDesc>feature params dim. acc. pre. rec. iou We were unable to reproduce the results of</figDesc><table>Ours Joint 1.6M 
128 75.9 75.4 76.5 61.5 
Ours Ranking 1.6M 
128 74.5 74.2 74.5 59.6 
Ours Siamese 1.6M 
128 73.3 72.9 74.0 58.2 
Ours Classification 1.6M 
128 73.5 71.7 74.1 57.3 
Ours Joint Dirty 1.6M 
128 72.9 72.1 73.1 57.0 

Kiapour et al. [16]  † 
 ‡ 39,168 70.6 70.6 70.4 54.6 

VGG_CNN_M [4] 
99M 4096 71.9 72.9 70.9 56.2 
VGG 16 Layers [29] 134M 4096 70.1 70.5 69.7 54.8 

VGG_CNN_M_1024 [4] 

86M 1024 70.4 71.1 69.5 54.2 

VGG_CNN_M_128 [4] 

82M 
128 63.5 62.8 63.5 46.3 
VGG 16 Places [49] 134M 4096 57.4 57.6 59.4 41.5 

 † </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="true"><head>Table 3 :</head><label>3</label><figDesc>Comparison with deep networks using feature distances on the Hipsters Wars dataset. For each image in the dataset, we sort all the remaining images by distance and consider a top-n match if one of the n nearest images is of the same class. No training is done at all.</figDesc><table>feature dim. top-1 top-2 top-3 

Ours Joint 
128 
63.5 
79.9 
86.3 

VGG_CNN_M [4] 4096 
53.2 
71.7 
81.3 
VGG 16 Layers [29] 4096 
53.2 
71.5 
80.4 
VGG_CNN_M_128 [4] 
128 
44.6 
64.0 
76.2 
VGG 16 Places [49] 4096 
40.1 
61.0 
72.0 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="true"><head>Table 4 :</head><label>4</label><figDesc>Comparison against fine-tuned deep networks on the Hipster Wars dataset. For the fine-tuned networks, the numerator is the fine-tuned result and the denominator is the result of using a logistic regression without fine-tuning.</figDesc><table>feature dim. 
acc. 
pre. 
rec. 
iou 

Ours Joint 128 
68.4 
66.1 
67.9 
51.0 

VGG_CNN_M 4096 68.4 /64.6 67.3 /64.2 68.8 /63.0 51.8 /46.8 
VGG 16 Layers 4096 63.8 /63.3 62.6 /62.6 63.5 /61.9 46.5 /45.4 
VGG_CNN_M_128 128 62.6 /57.2 60.4 /55.1 62.1 /56.9 44.5 /39.0 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 5 :</head><label>5</label><figDesc>Results on the Fashion144k dataset for the task of predicting how fashionable a person is on a scale of 1 to 10. We compare against strong CNN baselines and all variations of our model. We also consider our model architecture with random weights to observe the effect of learning, and average the results for 10 random initializations. The model with random weights performs almost the same as the VGG_CNN_M_128 model that has been pretrained on ImageNet. Our model outperforms all models in accuracy. feature dim. acc. pre. rec. iou</figDesc><table>Ours Joint 
128 17.0 14.7 15.2 7.1 
Ours Classification 
128 14.6 12.7 14.5 6.3 
Ours Siamese 
128 13.9 11.9 24.2 5.8 
Ours Random 
128 13.0 10.8 11.5 4.9 

VGG 16 Layers [29] 4096 16.6 15.1 15.7 8.0 
VGG 16 Places [49] 4096 15.8 14.0 14.7 7.3 
VGG_CNN_M [4] 4096 13.2 11.8 11.5 6.0 
VGG_CNN_M_128 [4] 
128 13.2 10.8 11.7 4.8 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements: This work was partially supported by JSPS KAKENHI #26108003 as well as JST CREST.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning visual similarity for product design with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bala</surname></persName>
		</author>
		<idno>2015. 3</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. on Graphics (SIGGRAPH)</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Apparel classifcation with style</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bossard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dantone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Leistner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wengert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Quack</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Signature verification using a &quot;siamese&quot; time delay neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bromley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Säckinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Return of the devil in the details: Delving deep into convolutional nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chatfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Describing clothing by semantic attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Girod</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Torch7: A Matlab-like environment for machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Farabet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BigLearn, NIPS Workshop</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Feifei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">LIBLINEAR: A library for large linear classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R.-E</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1871" to="1874" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Devise: A deep visual-semantic embedding model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">MatchNet: Unifying feature and metric learning for patchbased matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep metric learning using triplet network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hoffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ailon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Cross-domain image retrieval with a dual attribute-aware ranking network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep visual-semantic alignments for generating image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Where to buy it: Matching street clothing photos in online shops</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kiapour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Hipster wars: Discovering elements of fashion styles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Kiapour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yamaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Whittlesearch: Interactive image search with relative attribute feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kovashka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="185" to="210" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Modeling the shape of the scene: A holistic representation of the spatial envelope</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="145" to="175" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Is object localization for free? -Weakly-supervised learning with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Weakly-and semi-supervised learning of a deep convolutional network for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning representations by back-propagating errors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Phibin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A High Performance CRF Model for Clothes Parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Simo-Serra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Neuroaesthetics in Fashion: Modeling the Perception of Fashionability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Simo-Serra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Discriminative Learning of Deep Convolutional Feature Point Descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Simo-Serra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Trulls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ferraz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Unbiased look at dataset bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Visualizing data using tsne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning visual clothing style with heterogeneous dyadic co-occurrences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kovacs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Runway to realway: Visual analysis of fashion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vittayakorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yamaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning fine-grained image similarity with deep ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning from massive noisy labeled data for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Chic or social: Visual popularity analysis in online fashion networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yamaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">E</forename><surname>Ortiz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACMMM</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Paper doll parsing: Retrieving similar styles to parse clothing items</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yamaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Kiapour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Parsing clothing in fashion photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yamaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Kiapour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">E</forename><surname>Ortiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Retrieving similar styles to parse clothing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yamaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Kiapour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">E</forename><surname>Ortiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Mix and match: Joint model for clothing and attribute recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yamaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Okatani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sudo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murasaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Taniguchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Clothing co-parsing by joint image segmentation and labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">How transferable are features in deep neural networks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lipson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning to compare image patches via convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">ADADELTA: an adaptive learning rate method. CoRR, abs/1212</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">5701</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Object detectors emerge in deep scene cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">À</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Learning deep features for scene recognition using places database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
