<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Regularity-Driven Building Facade Matching between Aerial and Street Views</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Wolff</surname></persName>
							<email>wolff@psu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Electrical Engineering and Computer Science</orgName>
								<orgName type="institution">The Pennsylvania State University University Park</orgName>
								<address>
									<region>PA. 16802</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">T</forename><surname>Collins</surname></persName>
							<email>rcollins@cse.psu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Electrical Engineering and Computer Science</orgName>
								<orgName type="institution">The Pennsylvania State University University Park</orgName>
								<address>
									<region>PA. 16802</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanxi</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electrical Engineering and Computer Science</orgName>
								<orgName type="institution">The Pennsylvania State University University Park</orgName>
								<address>
									<region>PA. 16802</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Regularity-Driven Building Facade Matching between Aerial and Street Views</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:45+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present an approach for detecting and matching building facades between aerial view and street-view images. We exploit the regularity of urban scene facades as captured by their lattice structures and deduced from median-tiles' shape context, color, texture and spatial similarities. Our experimental results demonstrate effective matching of oblique and partially-occluded facades between aerial and ground views. Quantitative comparisons for automated urban scene facade matching from three cities show superior performance of our method over baseline SIFT, Root-SIFT and the more sophisticated Scale-Selective Self-Similarity and Binary Coherent Edge descriptors. We also illustrate regularity-based applications of occlusion removal from street views and higher-resolution texture-replacement in aerial views.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>With the increasing availability of Google maps and other online mapping tools, geolocating consumer images has become a popular yet challenging task. As a step in this direction, we are interested in matching aerial view facades, such as those automatically detected by the method in <ref type="bibr" target="#b18">[19]</ref>, with a set of street-view facades to identify the same buildings. This is a challenging problem due to large differences in viewpoint and lighting <ref type="figure">(Figure 1</ref>), temporal disparities between aerial and street-level image collection, and perspective deformations at the street-view level due to the camera's close proximity to each building. Occlusions also complicate the problem -lower levels of a building may be blocked by other buildings in aerial views, and streetview images can be occluded by trees, street-lights, cars, and pedestrians, as well as by other buildings.</p><p>Urban facade feature-level matching is inherently ambiguous due to pattern regularities. Even though many existing works in computer vision and computer graphics have exploited such regularities computationally (see Section 2), <ref type="bibr">Figure 1</ref>. Aerial-view (top-left) and street-view (top-right) images from the same facade of an NYC building (image data provided by Google). Our facade matching pipeline finds corresponding facades in spite of drastic variations in viewpoint and lighting. Our method is regularity-driven, using features induced from the automatically detected lattices. Bottom row shows additional matched street-views of this facade. Note that a facade can be matched correctly even when the detected street-view lattice does not overlap with the aerial view lattice. matching between aerial and street views of the same facade poses technical challenges beyond generic image patch matching and even beyond ground-level-only wide-baseline facade matching. Furthermore, very little work (e.g. <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b29">30]</ref>) has explored a regularity-driven approach for urban scene segmentation and matching at the facade level.</p><p>We propose to use a lattice and its associated median tiles (motifs) as the basis for matching widely differing aerial and street-level facade views. Using a lattice tile/motif as a novel, regularity-based descriptor for facades immediately distinguishes this work from all local descriptor-based methods, since regularity is not a local property <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref>. We formulate the facade matching problem as a joint regularity optimization problem, seeking well-defined features that reoccur across both facades to serve as match indicators. Match costs based on edge shape contexts, L*a*b color features, and Gabor filter responses are used to find the best one-to-one matching of sampled patches between two roughly aligned motifs, yielding an effective cost function for matching widely disparate facade views ( <ref type="figure">Figure 1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>It is well known that generic local features such as HOG <ref type="bibr" target="#b7">[8]</ref> or SIFT <ref type="bibr" target="#b22">[23]</ref> are difficult to match across extreme changes in illumination, viewing angle and image resolution. More robust patch matching features have been proposed, based on feature descriptor normalization to reduce descriptor variance, e.g. Root-SIFT <ref type="bibr" target="#b1">[2]</ref> and edge contrast normalization <ref type="bibr" target="#b35">[36]</ref>, or by methodically trying combinations of feature transforms and binning layouts while learning parameters to maximize matching performance <ref type="bibr" target="#b32">[33]</ref>. However, even with the use of robust generic patch descriptors, matching architectural facades is inherently difficult due to an ambiguity in finding the correct correspondence among self-similar patches <ref type="bibr" target="#b27">[28]</ref>. These correspondence ambiguities lead in turn to difficulties in estimating planar homographies, fundamental matrices, camera locations, and other quantities computed in a typical structure from motion pipeline <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b16">17]</ref>.</p><p>Approaches to wide-baseline facade matching in the literature can be broken roughly into three strategies. The first strategy is to correct for the differences in viewing angle, allowing view-dependent matching using traditional local features to proceed. This is commonly achieved by applying an orthorectification preprocessing step that transforms an arbitrary perspective view of a planar facade into a frontal view where repetition of pattern elements occurs along the horizontal and vertical image axes <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b33">34]</ref>. This can be done by discovering vertical and horizontal vanishing points and solving for the camera rotation that unwarps the view <ref type="bibr" target="#b33">[34]</ref>. The vanishing line of a planar surface can also be estimated from change of scale of repeated pattern elements in the image <ref type="bibr" target="#b6">[7]</ref>, allowing affine rectification, while rotation and reflection among the elements introduces further constraints that allow solving for a true frontal view (up to similarity transform) <ref type="bibr" target="#b25">[26]</ref>.</p><p>More generally, the authors of <ref type="bibr" target="#b34">[35]</ref> note that repeated patterns form low-rank textures and present an algorithm called TILT that performs automatic orthorectification of intensity patterns in user-defined regions. Orthorectification greatly simplifies subsequent translation and reflection symmetry analysis <ref type="bibr" target="#b33">[34]</ref>, allows the use of more discriminative local features such as upright SIFT <ref type="bibr" target="#b2">[3]</ref>, and reduces the degrees of freedom needed to align two facade views <ref type="bibr" target="#b16">[17]</ref>.</p><p>An alternative to orthorectification is to warp one view into approximate alignment with another oblique view, prior to matching. In <ref type="bibr" target="#b30">[31]</ref>, ground based multi-view stereo is used to produce texture-mapped depth maps that are then re-rendered based on known camera pose information to synthesize the approximate appearance of the building as seen in the target aerial view. The work of <ref type="bibr" target="#b0">[1]</ref> aligns a dominant plane between two oblique aerial views by introducing into the patch matching process an explicit search over affine transformations that simulate the range of patch distortions expected due to viewpoint changes. A recent paper by <ref type="bibr" target="#b17">[18]</ref> uses range data and camera parameters from Google street views to warp the dominant building surface plane to appear approximately like a 45% aerial view in order to collect a cross-view patch dataset for deep learning.</p><p>A second broad strategy for wide-baseline facade matching is to form feature descriptors specialized for describing self-similar symmetric patterns. A Scale-Selective Self-Similarity (S 4 ) descriptor is developed in <ref type="bibr" target="#b3">[4]</ref> to capture local self-similarity of a patch to its surrounding region, computed at an intrinsic scale proportional to the spatial wavelength of repetition of the pattern. The similarity descriptor for an image patch is formed as a binned log-polar representation of its local autocorrelation surface, computed at the intrinsic scale. Computed over a grid of patches, these descriptors are clustered to detect and segment facades, and to form a set of visual words for naive Bayes matching of facades. The work of <ref type="bibr" target="#b11">[12]</ref> densely scores local horizontal and vertical reflection symmetries and local 2n-fold rotational symmetries at all locations and scales in an image. Being based on local symmetry rather than photometry, the resulting descriptors can match facades across large changes of image appearance (e.g. day vs night, drawing vs photo, and modern vs historical view).</p><p>The third strategy for facade matching is to explicitly treat the facade as a near-regular texture and to isolate and match unique tiles representing the underlying translated pattern element. One-dimensional frieze patterns and twodimensional wallpaper patterns are generated when a fundamental pattern element is shifted by integer multiples of one (frieze) or two (wallpaper) generator vectors to form a lattice. However, any translational offset of the lattice defines an equally good partition of the facade pattern into repeating elements, thus there is an inherent ambiguity in determining a unique tile for matching.</p><p>Recent work by Ceylan et al. <ref type="bibr" target="#b5">[6]</ref> requires a user to outline the fundamental repeating element of a pattern, while our application requires an automated solution. In <ref type="bibr" target="#b8">[9]</ref>, unique tiles are defined by finding the lattice offset such that the Fourier transform of the repeated pattern has phase co- lattice is represented by the median tile of its translational symmetry. c) A cost matrix is computed from all potential point correspondences for each street-to-aerial pair. Each motif pair will have a cost matrix for each of the four feature costs (shape context, color information gain, texture, spatial smoothness). d) A match cost for each street to aerial facade pair is computed as the sum of its optimal point correspondence set costs. Positive/negative matches are determined by a threshold, learned by maximizing precision/recall on a separate training set.</p><p>efficients of zero at its fundamental frequencies in the horizontal and vertical directions. Extracted mean tiles are then matched based on similarity of their grayscale patterns and of the largest two peaks in their RGB color histograms. The work of <ref type="bibr" target="#b19">[20]</ref> defines a motif of a repeated pattern as a tile that locally exhibits the same rotation and reflection symmetries that characterize the entire periodic pattern. This idea is used in <ref type="bibr" target="#b29">[30]</ref> to match facades based on normalized cross-correlation of their respective motifs.</p><p>Our proposed approach in this paper is also based on extracting the motif of a lattice to use as a descriptor for facade matching. However, unlike <ref type="bibr" target="#b8">[9]</ref> and <ref type="bibr" target="#b29">[30]</ref>, our matching is based on filtering out candidates using a progressively more discriminative pipeline of features, starting with coarse lattice-structure (geometric) filtering, followed by filtering based on illumination/shadow insensitive color distributions, and finishing with filtering based on features that capture the spatial layout of motif pattern edges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Regularity-based Matching Approach</head><p>We propose a regularity-based matching pipeline to identify corresponding facades across aerial and street-level views <ref type="figure" target="#fig_0">(Figure 2</ref>). High resolution aerial views are first processed by the method in <ref type="bibr" target="#b18">[19]</ref> to extract a set of near-regular building facades. Lattices are extracted for each aerial facade and for a set of candidate street-view images that are potential matches, using the translational symmetry detection algorithm developed in <ref type="bibr" target="#b24">[25]</ref>. To reduce computational complexity when searching for corresponding street-view images for a detected aerial facade, approximate camera pose information available with both aerial and street-view images is used. Specifically, by backprojecting viewing rays into a UTM ground coordinate system to estimate the approximate ground location for the aerial facade, we select one hundred street-view camera locations that are in close proximity to the estimated aerial facade location. Each street view location yields eight camera shot directions, giving a total of 800 candidate images which are further pruned by the orthogonality between the estimated normal vectors of the corresponding lattice and that of the ground plane.</p><p>These lattices facilitate ortho-rectification of the aerial and street-view facades and provide a basis for extracting motifs summarizing their appearance. Each lattice partitions an image region into tiles, which are brought into alignment and then fused by computing the pixel-wise median <ref type="bibr" target="#b19">[20]</ref>. This median tile or motif summarizes the scene facade in terms of regularity and appearance. However, different views of the same facade will still result in orthorectified tiles with slightly different appearances due to projective distortion and differences in scene illumination. One way to look at our method is to consider each computed median tile to be a sample from the entire facade distribution generated under different geometry and lighting conditions. The median tile, as a representative of that distribution, allows us to compare local distributions generated from aerial and street-view samples to identify whether they belong to the same whole-facade distribution.</p><p>The main technical contribution of our work is to define a matching cost function to compare a street-view motif to an aerial-view motif based on similarity of color, texture and edge-based context features. The remainder of this section describes in detail this cost function, the features that comprise it, and the sample-based matching procedure that produces a final motif-pair matching score. <ref type="figure">Figure 3</ref>. Each motif's edge image is sampled, and patches around each sample are described by shape context, color and texture features. The first two features are illustrated here. (a) Each sampled point is described by its own log-polar histogram and shape context scores. For each attempted match, a cost matrix is formed from the SSD scores between all pairs of possible point correspondences between the two motifs. (b) The color cost term is computed from the information gain score between the two equalized LAB color-spaces. A 32x32 patch at each of the sampled points is used to obtain each distribution. The motif shown is from the NYC dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Motif Cost Function</head><p>We characterize a motif by randomly sampling at most 400 points from its high-gradient (edge) pixels. Given two motifs extracted from two facades, we compute their similarity in the form of a pairwise, point-to-point cost function formed as a weighted combination of four terms: (1) local shape context <ref type="bibr" target="#b4">[5]</ref>, (2) color, (3) texture, and (4) location proximity:</p><formula xml:id="formula_0">C i,j = W EÊi,j + W DDi,j + W TTi,j + W LLi,j<label>(1)</label></formula><p>whereÊ i,j is the edge-based shape context cost function for matching sampled pixels i and j, andD,T , andL are the corresponding color similarity, texture similarity, and location proximity cost functions, respectively. Since any offset in the translational lattice yields a valid motif tile, we first roughly align each street-view facade motif with the aerial motif before comparison by circularly shifting it to the offset that yields the maximum normalized cross correlation (NCC) score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Shape Context</head><p>Spatial edge layout is a useful measure for discriminating between different window shapes/sizes, as well as weakly discriminating between buildings with different surface textures, e.g. uniform texture vs. brick texture. Each sampled edge point is characterized by a local shape context <ref type="bibr" target="#b4">[5]</ref>, using a normalized log-polar histogram, as shown in <ref type="figure">Figure 3a</ref>. The normalized cost of matching two sampled points, i and j, is given bŷ</p><formula xml:id="formula_1">E i,j = K (h i (k) − h j (k)) 2 h i (k) + h j (k)<label>(2)</label></formula><p>where k is a bin belonging to a log-polar histogram, h.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Color</head><p>We characterize the color appearance of a building by the color distribution of the motif of the repeated facade pattern. Color distribution of the motif is measured in CIELab color space to account for potential differences in lighting or the presence of shadows. Work done in <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b28">29]</ref> shows that the CIELab color space is effective at detecting/segmenting despite shadows, since the presence of a shadow will linearly shift each of the three CIELab color space dimensions by a proportional amount depending on the strength of the shadow. We describe the overall texture of a motif by its L, a*, and b* distributions, f L (x), f a * (x), and f b * (x) respectively. When comparing two motifs, we first shift the L space distribution of the street-view motif so that its mean value matches the mean of the aerial-view motif. We then shift the a* and b* distributions by the L space shift, ∆L, multiplied by a corresponding proportionality constant, γ, effectively obtaining a shadow-invariant color space. The shifting process is described by the equation *</p><formula xml:id="formula_2">f d (x) = f (x − γ d ∆L)<label>(3)</label></formula><p>where d is the color dimension, either a* or b*. In our experiments we set γ a =.135 and γ b =.435, learned from a training set of street-to-aerial facade matches separate from the ones used for evaluating the PR curves.</p><p>To compare color distributions, our approach uses information gain, also known as the Kullback-Leibler divergence, D KL <ref type="bibr" target="#b15">[16]</ref>. Information gain effectively measures the overall difference between two distributions by measuring the loss of information that occurs when one probability distribution is used to approximate another. In our case, we use information gain to measure how well the aerial-view patch describes the street-view patch, as given by</p><formula xml:id="formula_3">D KL (f A d , * f S d ) = x f A d (x) ln f A d (x) * f S d (x)<label>(4)</label></formula><p>where * f S d is the shadow-corrected street-view motif distribution, and f A d is the aerial-view motif distribution. Two identical distributions result in a score of 0. We normalize D KL byD</p><formula xml:id="formula_4">KL = 1 − exp(−D KL )<label>(5)</label></formula><p>To obtain the cost associated with the color similarity, we apply the Kullback-Leibler divergence to a 32x32 image patch at pixels i and j for each of the color spaces, as shown by <ref type="figure">Figure 3b</ref>. The costD i,j is the average divergence over the three CIELab color dimensions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Texture</head><p>Gabor filter bank responses have been shown to be effective descriptors for many datasets <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b26">27]</ref>. While urban facade datasets are not as sparse as previously tested datasets, texture features can be useful discriminators for building facades. When comparing two motifs, we apply four 1wavelength Gabor filters to each motif at 0 • , 45 • , 90 • , and 135 • . The texture costT i,j for each pair of sampled points associated with matching two motifs is the sum of each filter response's SSD (sum squared difference).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.4">Location Proximity</head><p>Due to the rigid structure of building facades, relative locations of corresponding motif pixels are expected to vary smoothly, e.g. according to affine deformations due to viewpoint. Therefore, we include an additional location change cost in order to bias the overall solution by this smoothness constraint. The cost is given byL i,j , which is the relative distance between the two matched points as a ratio to the maximum possible distance (diagonal of motif).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Matching by Cost Minimization</head><p>Optimal correspondences between the sets of sampled points from two motifs are solved by minimizing the cost function of Equation 1 over all 1-1 point correspondences, solved as a bipartite matching problem using the Hungarian algorithm <ref type="bibr" target="#b14">[15]</ref>. Weights for the four component cost matrices are W E = W D = W T = 0.3167 and W L = 0.05. This gives edge-based, color-based and texture-based appearance features equal weighting, while spatial similarity has a lower weight that adds a slight smoothing bias. An <ref type="figure">Figure 5</ref>. Evaluation of street-view facades matched to 120 different aerial-view facades (40 NYC, 10 SF, 70 Rome). We show the Precision-Recall curves for our proposed method against (1) a baseline approach using SIFT descriptor matching in orthorectified views, (2) Root-SIFT <ref type="bibr" target="#b1">[2]</ref>, a renormalization of SIFT that outperforms SIFT for retrieving building facades across large view changes, (3) Binary Coherent Edge Descriptors <ref type="bibr" target="#b35">[36]</ref>, a generic patch matching descriptor applied to our extracted motifs, and (4) S 4 <ref type="bibr" target="#b3">[4]</ref>, a sophisticated symmetry-based feature designed for facade matching between aerial and ground-level views. The far right panel shows results of our method using different combinations of the 3 major feature spaces (shape context, color, texture) used in our motif matching cost function. overall matching score for the pair of motifs is given by the sum of the costs returned by the Hungarian algorithm with the optimal point-match set. All potential pairs of motif matches are ranked based on their matching scores, from which positive/negative matches are then determined. <ref type="figure" target="#fig_1">Figure 4</ref> shows qualitative matching results for an aerial facade. Given an aerial facade and its automatically detected lattice, samples of some of the candidate street-view images are shown.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results</head><p>A quantitative evaluation of our method is carried out on a set of 120 aerial facades. Each facade is visible in 10-15 street images, giving us over 1000 total potential facade matches. We have hand labeled all street-view facades corresponding to each aerial facade in the dataset. These labeled facades are treated as the ground truth during our evaluation. A true positive match from a street-view facade to an aerial facade occurs when their motifs achieve the highest ranking matching score and they are from the same scene facade. Such a motif-based match can occur even in cases where the two detected facade lattices do not have any spatial overlap. This type of match is still 1-to-1 (albeit not pixel to pixel) since only the best-scoring lattice/motif pair is chosen, one from an aerial view image and one from a street-view image, and thus the Precision-Recall curve is well-defined. <ref type="figure">Figure 5</ref> shows a quantitative evaluation based on 120 aerial facade examples. Four different sets of precisionrecall curves are shown. The first three show our method compared with other matching methods on 40 NYC, 10 SF, and 70 Rome facades, respectively. To make this comparison fair, street view and aerial view facades were first orthorectified using their detected lattices before computing SIFT descriptors, since it is known that SIFT features are not able to match well across large, oblique viewpoint changes. Even with that help, SIFT and Root-SIFT match-ing are not as effective at matching facades as our proposed method, or the other sophisticated methods. Finally, we compare the average results of different combinations of our cost function feature spaces across all three cities. Although color alone is not an effective tool for discriminating between different facades, it still adds improvement when used in conjunction with other features. In <ref type="figure">Figure 6</ref>, a 3D cost space is shown for the shape, color, and texture feature costs computed when matching to a particular NYC aerial facade. Blue/red stars are used to indicate whether a street facade is a ground truth match/non-match to the reference aerial facade. The decision made by our matching process is depicted by a green or red dashed line for a match or nonmatch, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Applications</head><p>In this section we show two potential applications for regularity-based matching by using the 2D lattice information for image enhancement in both aerial and street-level views. The first application removes foreground objects that occlude an architectural facade of interest (inpainting) and the second replaces low-resolution facade texture with a higher-resolution version (superresolution). Both inpainting <ref type="bibr" target="#b13">[14]</ref> and superresolution <ref type="bibr" target="#b24">[25]</ref> of a repeated facade pattern have been addressed previously, but those works synthesize a virtual new texture assuming a perfectly repeating pattern, whereas our approach copies actually observed pattern data from a different unoccluded or higher-resolution view. Inpainting work such as <ref type="bibr" target="#b31">[32]</ref>, also copies information from other views, but the region to be inpainted is chosen by a user. Our approach automatically detects the region of occlusion by analyzing the facade pattern.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Removal of Street-Level Occlusion</head><p>From our set of matched street-view lattices, a central lattice is built that collects and associates patches from each facade across all images in which that facade is vis- <ref type="figure">Figure 6</ref>. NYC 3D Feature-Cost Space for the three major features used in our proposed method. Blue star = ground truth positive, red star = ground truth negative. Green dashed line = selected as a match by our matching process, red dashed line = selected as non-match. a, h show facades that are matched with low cost to the aerial facade. b, g are examples of facades with similar appearances according to our feature space but are considered false positives, while c is an image that our method does not match well due to significant affine deformations and changes in the window reflection colors. Quantitative results shown in <ref type="figure">Figure 5</ref> ible. Cross-view matching is performed by correlating each lattice patch set over the patch sets of other images while maintaining the alignments of the two patch sets. The correlation offset location with the highest score is selected as the best matched lattice alignment. That is,</p><formula xml:id="formula_5">loc Q = arg max i,j NCC i,j (P, Q)<label>(6)</label></formula><p>where Q is the set of lattice patches currently being considered, P is the current central lattice patch set, i,j is the offset of Q with respect to the origin of P, and NCC computes the mean normalized cross correlation score between two lattice patch sets at an offset of i,j (correlation scores between one or more null patches are not included in the mean score). We leverage the initial aerial view facade by restricting the offset location from causing the central lattice to exceed the aerial-view lattice patch set dimensions. At the end of this process, the central lattice patch set contains patch samples from all matched facades, in their appropriate relative positions with respect to each facade. Note that multiple sample patches may be available for the same relative facade location, when that location is visible in multiple matched street views. After building a central lattice that contains all visible street-view patches, we are able to automatically remove both major and minor occlusions from a given viewpoint <ref type="figure" target="#fig_2">(Figure 7)</ref>. Minor occlusions are defined as objects that are small and thus minimally affect the occluded lattice patch. Examples include street lamps, sign poles, or electrical wires. These types of occlusions can be automatically detected by comparing the difference to the median patch of this patch to its corresponding median differences from other viewing angles. Patches with minor occlusions are considered those with difference energies several standard deviations above the mean difference energy.</p><p>Major occlusions occur when an object obstructs a large portion of the building from some views, affecting the perceived regularity. We can detect these by finding patches in the central lattice patch set that are present in some images, but not present in others even though they fall within that image's field of view.</p><p>To correct/replace occluded patches, a mapping of the coordinates from one patch to another, F, is defined by determining the projective transformation between the four corner locations of the occluded patch and the corner locations of a corresponding matched patch. The pixels of the occluded patch are replaced using the mapping F p o (i, j) = p m (F{i, j})</p><p>where p o is a pixel in the obstructed patch, p m is a pixel in a matched patch. We select the image for the patch replacement as the image in closest proximity to the image containing the occlusion in order to minimize perspective distortion. <ref type="figure">Figure 8</ref>. Six aerial lattice patches (from NYC dataset) replaced with corresponding street-view lattice patches after automatically adjusting for lighting differences in CIELab space 5.2. Aerial-Level Image Enhancement through Texture Replacement</p><p>As explained in Section 5.1, a central lattice patch set containing all aligned patches detected from street-level views is constructed and can be used to replace pattern tiles that are occluded. Since the central lattice patch set is also aligned with the original aerial image facade lattice, it is also possible to perform texture replacement of patches in the aerial view with patches extracted from the set of street views. Since street views are often of significantly higher resolution than the aerial imagery, this type of texture replacement can be used to generate higher resolution aerial views, as shown in <ref type="figure">Figure 8</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We have addressed the scientific problem of aerial to street-view facade matching. This application poses technical challenges beyond generic image patch matching and even beyond ground-level-only, wide-baseline facade matching. Our results have shown that regularity is an effective tool in extracting discriminative facade features that can be used for matching under challenging viewpoint and lighting changes. By analyzing facade lattice structures, we show that color, shape, and edge-based features combine to form an effective cost function for differentiating between buildings when used within a framework that performs pairwise matching of sample patches summarizing the motif tile of the repeated facade pattern. We also have shown two example applications facilitated by multi-view facade matching and alignment: removal of occlusion from street-level views, and image enhancement of facade texture in aerial views.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Acknowledgement</head><p>This work is supported in part by NSF grants IIS-1218729, IIS-1144938 (REU), and IIS-1248076 (CRE-ATIV). The Google urban scene data set for research is highly appreciated.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Flowchart showing the overall process of the proposed method on the NYC dataset. a) Lattices are extracted from a street-view (bottom) database and the aerial facade (top) in question. Detected lattices are pruned based on their estimated world-coordinate normal vector to keep only vertical facades. b) Each</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>Sample images selected from positive/negative matching results as determined by our matching pipeline (green/red borders, respectively) on the NYC dataset. Matched facades within the positive images are colored blue. Sample motifs from the aerial and matched street view facades are also shown.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 7 .</head><label>7</label><figDesc>Occlusion removal is performed on this NYC streetview by replacing missing/obstructed lattice patches with available patches from another viewpoint. This is facilitated by construction of a central lattice patch set that brings into alignment corresponding patches from all of the matched street views.</figDesc></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Ultra-wide baseline aerial imagery matching in urban environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Altwaijry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2013-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Three things everyone should know to improve object retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arandjelovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Handling urban location recognition as a 2D homothetic problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Baatz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Köser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Grzeszczuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="266" to="279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Ultra-wide baseline facade matching for geo-localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Sawhney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV Workshop on Visual Analysis and Geo-localization of Large-Scale Imagery</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Shape matching and object recognition using shape contexts. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Puzicha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="509" to="522" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Coupled structure-from-motion and 3D symmetry detection for urban facades</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pauly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Planar affine rectification from change of scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision (ACCV)</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="347" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition, 2005. CVPR 2005. IEEE Computer Society Conference on</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="886" to="893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Image matching and retrieval by repetitive patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doubek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Perdoch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2010-08" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Shadow removal method for real-time extraction of moving objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fukui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yamamoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Iwahori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Woodham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Knowledge-Based Intelligent Information and Engineering Systems</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1021" to="1028" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Single-image shadow detection and removal using paired regions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2033" to="2040" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Image matching using local symmetry features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Hauagge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="206" to="213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Correcting for duplicate scene structure in sparse 3D reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Heinly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Dunn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Frahm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="780" to="795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Analysis of building textures for reconstructing partially occluded facades</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Korah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rasmussen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="359" to="372" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The Hungarian method for the assignment problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">W</forename><surname>Kuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Naval Research Logistics Quarterly</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="83" to="97" />
			<date type="published" when="1955" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">On information and sufficiency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kullback</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Leibler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Mathematical Statistics</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="79" to="86" />
			<date type="published" when="1951" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Epipolar geometry estimation for urban scenes with repetitive structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kushnir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Shimshoni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Pattern Analysis and Machine Intelligence (PAMI)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="2381" to="2395" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning deep representations for ground-to-aerial geolocalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Local regularity-driven city-scale facade detection from aerial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014-06" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A computational model for periodic pattern perception based on frieze and wallpaper groups</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tsin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Pattern Analysis and Machine Intelligence (PAMI)</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="354" to="371" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Computational symmetry in computer vision and computer graphics: A survey. Foundations and Trends in Computer Graphics and Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hel-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1" to="199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The promise and perils of nearregular texture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tsin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page">159</biblScope>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Distinctive image features from scaleinvariant keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Texture features for browsing and retrieval of image data. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">S</forename><surname>Manjunath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="837" to="842" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Translation-symmetry-based perceptual grouping with applications to urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Brocklehurst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision (ACCV)</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Rectification, and segmentation of coplanar repeated patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pritts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="2973" to="2980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Texture classification using rotation-and scale-invariant gabor texture features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Riaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rehman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Qamar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="607" to="610" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>IEEE</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Structure from motion for scenes with large duplicate structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">N</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Steedly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="3137" to="3144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Cast shadow segmentation using invariant color features. Computer Vision and Image Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Salvador</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cavallaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ebrahimi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">95</biblScope>
			<biblScope unit="page" from="238" to="259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Detecting and matching repeated patterns for automatic geo-tagging in urban environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lublinerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Dellaert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2008-06" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Accurate geo-registration by ground-to-aerial image matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Furukawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hernandez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="525" to="532" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Get out of my picture! Internet-based inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Whyte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning local image descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A J</forename><surname>Winder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Detecting large repetitive structures with salient boundaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Frahm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<idno>Septem- ber 2010. 2</idno>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<biblScope unit="page" from="142" to="155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">TILT: transform invariant low-rank textures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ganesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="24" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Binary coherent edge descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
