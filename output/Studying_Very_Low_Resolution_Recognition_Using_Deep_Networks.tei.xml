<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Studying Very Low Resolution Recognition Using Deep Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Beckman Institute</orgName>
								<orgName type="institution" key="instit2">University of Illinois at Urbana-Champaign</orgName>
								<address>
									<postCode>61801</postCode>
									<settlement>Urbana</settlement>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Chang</surname></persName>
							<email>chang87@illinois.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Beckman Institute</orgName>
								<orgName type="institution" key="instit2">University of Illinois at Urbana-Champaign</orgName>
								<address>
									<postCode>61801</postCode>
									<settlement>Urbana</settlement>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingzhen</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Beckman Institute</orgName>
								<orgName type="institution" key="instit2">University of Illinois at Urbana-Champaign</orgName>
								<address>
									<postCode>61801</postCode>
									<settlement>Urbana</settlement>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liu</surname></persName>
							<email>dingliu2@illinois.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Beckman Institute</orgName>
								<orgName type="institution" key="instit2">University of Illinois at Urbana-Champaign</orgName>
								<address>
									<postCode>61801</postCode>
									<settlement>Urbana</settlement>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
							<email>t-huang1@illinois.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Beckman Institute</orgName>
								<orgName type="institution" key="instit2">University of Illinois at Urbana-Champaign</orgName>
								<address>
									<postCode>61801</postCode>
									<settlement>Urbana</settlement>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Studying Very Low Resolution Recognition Using Deep Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:46+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Visual recognition research often assumes a sufficient resolution of the region of interest (ROI</head><p>). That is usually violated in practice, inspiring us to explore the Very Low Resolution Recognition (VLRR) problem. Typically, the ROI in a VLRR problem can be smaller than 16 × 16 pixels, and is challenging to be recognized even by human experts. We attempt to solve the VLRR problem using deep learning methods. Taking advantage of techniques primarily in super resolution, domain adaptation and robust regression, we formulate a dedicated deep learning method and demonstrate how these techniques are incorporated step by step. Any extra complexity, when introduced, is fully justified by both analysis and simulation results. The resulting Robust Partially Coupled Networks achieves feature enhancement and recognition simultaneously. It allows for both the flexibility to combat the LR-HR domain mismatch, and the robustness to outliers. Finally, the effectiveness of the proposed models is evaluated on three different VLRR tasks, including face identification, digit recognition and font recognition, all of which obtain very impressive performances.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>While object recognition research has witnessed substantial achievements so far, it is often assumed that the region of interest (ROI) is large enough and contains sufficient information for recognition. However, this assumption usually does not hold in practice. One typical example is face recognition from video surveillance <ref type="bibr" target="#b37">[39]</ref>. Due to the prohibitive costs of installing high-definition cameras all around, most surveillance systems have to rely on cameras of very limited definitions. Besides, wide-angle cameras are normally used in a way that the viewing area is maximized. In turn, the face region in the scene can be extremely small and low-quality. In a text recognition system <ref type="bibr" target="#b14">[15]</ref>, cheap and versatile cameras make it possible to quickly scan documents, but their low definitions also present challenges for * Zhangyang Wang and Thomas Huang's research works are supported in part by US Army Research Office grant W911NF-15-1-0317. robust character segmentation and recognition. While similar problems are ubiquitous for recognition in the wild, a principled approach to solve them is highly desirable.</p><p>Regrettably, the general Very Low Resolution Recognition (VLRR) problem has been largely overlooked, except for certain existing efforts in face recognition <ref type="bibr" target="#b37">[39,</ref><ref type="bibr" target="#b24">26]</ref>. Empirical studies <ref type="bibr" target="#b20">[22]</ref> in face recognition proved that a minimum face resolution between 32×32 and 64×64 is required for stand-alone recognition algorithms. An even lower resolution results in a much degraded recognition performance <ref type="bibr" target="#b20">[22,</ref><ref type="bibr" target="#b11">12]</ref> for conventional recognition models. The severe information loss from HR to LR makes it unlikely to extract or recover sufficient recognizable features directly from LR subjects <ref type="bibr" target="#b37">[39]</ref>. Typically, the ROI in a VLRR problem can be smaller than 16 × 16 pixels, and is even difficult (but still possible) to be recognized by human viewers.</p><p>In this paper, we make the first attempt to solve the VLRR problem using deep learning methods <ref type="bibr" target="#b17">[18]</ref>. Starting from the simplest baseline, we perform a step-by-step model evolution, gradually obtaining more sophisticated and powerful models. Any extra complexity, when introduced, is fully justified by both analysis and simulation results. The final outcome, named Robust Partially Coupled Networks, achieves feature enhancement and recognition simultaneously. It is equipped with both the flexibility to combat the cross-resolution domain mismatch, and the robustness to outliers. The proposed models are applied to resolving real VLRR problems on three different tasks, including face identification, digit recognition and font recognition, all of which obtain very impressive performances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Problem Definition</head><p>In real-world settings, VLRR directly recognizes visual subjects from low-resolution (LR) images, without any predefined high-resolution (HR) image. Here, we introduce HR images as "auxiliary variables" to our model training, by assuming each training image has both LR and HR versions available. As verified by our following experiments, HR images help discover more discriminative features which are prone to be overlooked from LR images. In testing, there are only LR images available, and the VLRR model is applied without any need of HR images.</p><p>There is indeed no absolute boundary between LR and HR images. However, the literature in object and scene recognition have observed a significant performance drop, when the image resolution is decreased below 32 × 32 pixels (see <ref type="figure" target="#fig_1">Fig. 2</ref> of <ref type="bibr" target="#b27">[29]</ref>). In <ref type="bibr" target="#b37">[39]</ref>, the authors also reported a degradation of the recognition performance, when face regions became smaller than 16 × 16 pixels.</p><p>We follow <ref type="bibr" target="#b27">[29,</ref><ref type="bibr" target="#b37">39]</ref> to choose the LR image resolution to be no more than 16 × 16, and HR no less than 32 × 32 1 . In this paper, we treat the original training images as HR images, unless otherwise specified. To generate LR images for training (and also testing), we downsample the original images by a factor of s, and then upscale them back to the original resolution, by nearest neighbor (NN) interpolation. The upscaling operation is intended to ensure sufficiently large spatial supports for hierarchal convolutions, as well as to facilitate feature transfer. Since NN interpolation does not bring in any new information, the upscaled images are treated as our default LR images hereinafter.</p><p>In summary, the problem is defined as: learning a VLRR model from training images which have both LR and HR versions, and applying the model on LR testing images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Model I: Basic Single Network</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Motivation</head><p>Deep convolutional neural networks (CNNs) <ref type="bibr" target="#b17">[18]</ref> have recently shown an explosive popularity, partially due to its prevailing success in image recognition tasks, on various subjects such as faces <ref type="bibr" target="#b26">[28]</ref>, digits <ref type="bibr" target="#b21">[23]</ref>, texts <ref type="bibr" target="#b15">[16]</ref> and fonts <ref type="bibr" target="#b31">[33]</ref>. However, all the models assume reasonable resolutions of ROIs <ref type="bibr" target="#b17">[18]</ref>. Popular datasets, such as LFW <ref type="bibr" target="#b12">[13]</ref> and ImageNet <ref type="bibr" target="#b17">[18]</ref>, mostly come with moderate to high image resolutions (typically around several hundred pixels per dimension). It remains unexplored whether the performances of conventional CNNs are still reasonable for VLRR tasks. We will thus start from investigating the basic single CNN for VLRR, as the simplest baseline. <ref type="figure" target="#fig_0">Fig. 1</ref> illustrates the our Model I (basic CNN) that is similar to the popular ImageNet structure <ref type="bibr" target="#b17">[18]</ref>, with 3 convolutional layers and 2 fully-connected layers. Since LR images in VLRR problems do not have too much (low-and high-level) information to be extracted hierarchically, we do not refer to deeper architectures. For the i-th (i = 1, 2, 3) convolutional layer, we assume it of n i channels, filter sizes of f i , and the stride of 1. Note that a LR image (less than 16 × 16) in VLRR problems contains little self-similarity to be utilized, and is often corrupted. Therefore as verified in our experiments, applying large convolutional filter sizes to extract patches (as in <ref type="bibr" target="#b7">[8]</ref>, etc.) brings little benefits. The two fully-connected layers have the dimensionality of m 4 and m 5 , respectively. The network is trained from end to end, as suggested by <ref type="bibr" target="#b17">[18]</ref> and many others.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Technical Approach</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Simulation</head><p>We adopt the popular CIFAR-10 and CIFAR-100 datasets <ref type="bibr" target="#b16">[17]</ref>, as our simulation subjects. The CIFAR-10 dataset consists of 60,000 32 × 32 color images, in 10 classes, with 6000 images per class. For each class, there are 5,000 images for training and 1,000 for testing. The CIFAR-100 dataset is just like the CIFAR-10 and of the same total volume, except it has 100 classes containing 600 images each. In all experiments, we convert images to gray scale for model simplicity. The original (HR) images are first downscaled by s = 4 into 8 × 8. They are then upscaled back to 32 × 32 by NN interpolation, as the LR images. For each LR image, we subtract its mean and normalize its magnitude, which are later put back to the recovered HR image. Small additive Gaussian noise (σ = 0.05) is added as the default data augmentation in training.</p><p>We implement our models using the cuda-convnet package <ref type="bibr" target="#b17">[18]</ref>. ReLU is adopted as the neuron. The batch size is 128. The learning rate starts from 0.1 and is divided by 10 when the training curve reaches a plateau. Dropout is also applied. We also compare the VLRR models trained on LR inputs with those trained on original HR images. The size of the last fully-connected layer is always chosen to be the number of classes, e.g., m 5 = 10 for CIFAR-10 and 100 for CIFAR-100. m 4 is fixed to be 1024 by default. We then vary other network configurations to examine how the baseline performances are affected, which are outlined in <ref type="table" target="#tab_0">Table  1</ref>. The following important insights are readily concluded:</p><p>• The performances of VLRR models with LR input images are largely degraded compared to those obtained on HR images.</p><p>• In VLRR experiments, larger filter sizes are hardly beneficial, because VLRR subject cannot afford as much spatial resolution (and detail) loss as HR images. In addition, increasing the size of m 4 marginally rises the performance, while bringing in significant complexity. We also tried to repeat Conv. Layer 3 for deeper architectures, ending up with the observation that increasing depth does not contribute visibly to VLRR, since LR images do not have rich visual semantic information. The above findings demonstrate the distinct characteristics of VLRR, that VLRR problems do not benefit as much from larger filter sizes, more filter channels or deeper models, as conventional visual recognition problems do. Therefore, we adopt the following moderate configuration as default: n 1 = 64, f 1 = 5; n 2 = 64, f 2 = 3; n 3 = 32, f 3 = 1; m 4 = 1024.</p><p>3. Model II: Single Network with Super-Resolution Pre-training 3.1. Motivation <ref type="table" target="#tab_0">Table 1</ref> reminds that directly classifying LR visual objects are unreliable and prone to overfitting, since their visual features are scarce and highly degraded. On the other hand, it is noteworthy that although HR images are not available in real testing, it could still be utilized in training as auxiliary information to obtained enhanced features.</p><p>Classical paradigms first apply a super-resolution (SR) algorithm <ref type="bibr" target="#b34">[36]</ref> to a LR image and then classify its SR result <ref type="bibr" target="#b37">[39]</ref>. Recently, the SR performance has been noticeably improved, with the aid of deep network models <ref type="bibr" target="#b30">[32,</ref><ref type="bibr" target="#b33">35]</ref>. However, the recovered HR images still suffer from not only inevitable oversmoothness and detail loss, but also various artifacts introduced by the reconstruction process <ref type="bibr" target="#b1">[2]</ref>, that further undermine the subsequent recognition performance. The authors in <ref type="bibr" target="#b37">[39]</ref> incorporated discriminative constraints to learn the relationship between the HR and LR face images for recognition. In <ref type="bibr" target="#b11">[12]</ref>, class-specific facial features were included in SR as a prior. Such "close the loop" ap-proaches perform consistently better than traditional twostage (SR then recognition) pipelines. Model II is a "decomposed" version of Model I into two sub-networks: the super resolution (SR) sub-network for unsupervised pre-training, and the recognition sub-network for supervised fine-tuning. As outlined in <ref type="figure" target="#fig_1">Fig. 2</ref>, the SR sub-network consists of four convolutional layers, that takes LR images as inputs and HR as outputs. It is trained in an unsupervised way to predict the nonlinear mapping between LR and HR images. Compared to the convolutional parts in <ref type="figure" target="#fig_0">Fig. 1</ref>, it is easy to notice the newly added Conv. 4 Layer, that has only one channel. It has been revealed in <ref type="bibr" target="#b7">[8]</ref> that such a layer acts as a linear filter that averages its input feature maps to produce the final reconstruction. After the SR sub-network is trained, the Conv. 4 Layer is DISCARDED, and the two fully-connected layers as well as a softmax classifier are added on its top. The entire network, which now has exactly the same topology as Model I, are jointly tuned in a supervised manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Technical Approach</head><p>Earlier multi-layer feature learning methods, such as the Auto-Encoder and the Denoising Auto-Encoder, perform a pre-training of weights using unlabeled data alone for reconstruction, followed by supervised fine-tuning <ref type="bibr" target="#b3">[4]</ref>. The previous work <ref type="bibr" target="#b8">[9]</ref> showed that unsupervised pre-training appeared to play a regularization role predominantly. The Model II shares the similar idea and targets the unsupervised part at SR, instead of reconstruction. The final supervised fine-tuning correlate those unsupervised features with class labels. The resulting network aims to achieve resolution enhancement and recognition simultaneously.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Simulation</head><p>We follow the default network configuration and the same experiment setting as in Section 2.2. The LR and HR images are first utilized to train a SR subnetwork, with a upscaling factor of 4. CIFAR-10 witnesses a drop of 2.32% in the classification error rate (ending up to be 25.15%), while on CIFAR-100 the error rate is reduced to 46.50%, with a decrease of 3.45%. Remark: Since SR is a one-to-many problem, any one of the "many" is a plausible solution, and it is impossible to ensure that the added details are authentic to the original HR image. In contrast, object recognition tries to identify the one from "many". Then, one may have a bit concern in the reasoning why the SR pre-training could possibly improve the visual recognition performance? Our explanation is, while not guaranteed to be faithful, the hallucinated details from SR pre-training help discover subtle but discriminative features, that benefit recognition. They are otherwise prone to be overlooked from LR images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Model III: Single Network with Pre-training</head><p>and LR-HR Feature Transfer</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Motivation</head><p>Though limited improvements are obtained by introducing SR pre-training, a large performance gap of around 10% on CIFAR-10/100 still exists, between models trained with LR and HR inputs. Why is there such a performance gap? One hypothesis is that the SR pre-training process does not bring in enough discriminative ability for recognition purposes <ref type="bibr">[21]</ref>. The hypothesis is further evidenced, when we look at the visualizations of filters learned from either HR or LR inputs. While there are clearly visible edges and curves in most HR filters, LR filters learn much fewer recognizable patterns, and suffer from more "dead" filters. Our idea is to discover more discriminative features, by blending HR images in the training data. A Domain Adaptation Viewpoint. Our goal is equivalent to learning more transferable features from HR to LR, which resembles the domain adaption scenarios. HR images constitute the source domain, and LR images are the target domain. It was hypothesized <ref type="bibr" target="#b2">[3]</ref> that the intermediate representations could be shared across variants of the input distribution. The model in <ref type="bibr" target="#b9">[10]</ref> learned a cross-domain feature extraction, using unlabeled data from all domains. A classifier is trained on the transformed labeled data from the source domain, and tested on the target domain. A Data Augmentation Viewpoint. Since in Model II, HR images were already in the training set, it may look questionable that why HR inputs may still help SR, if no new information would be added. Our explanation is: blending HR images with the original LR training data could be treated as a problem-specific data augmentation approach for SR pre-training. Common data augmentations would corrupt or modify the training images in many ways. Similarly, the introduction of HR images is alternatively viewed, as to augment the original LR images, by transforming them into HR versions. The difference is that it enhances the training data, rather than degrading them. <ref type="figure">Figure 3</ref>. The super resolution (SR) sub-network enhanced by LR-HR feature transfer, as the pre-training part of Model III. Note it does not contain the fully-connected recognition sub-network, which is the same as in <ref type="figure" target="#fig_0">Fig. 1 (fc4, fc5 and softmax)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Technical Approach</head><p>The super resolution (SR) sub-network of Model III, as shown in <ref type="figure">Fig. 3</ref>, is related to the domain adaption techniques from <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b29">31]</ref>. In the context of domain adaption, we treat LR and HR images to be from two related but non-overlapping domains. The SR sub-network could be viewed as as two fully-coupled channels. One LR-HR channel reconstructs the HR domain samples from the LR domain samples, which is essentially equivalent to the SR sub-network of Model II. The other HR-HR channel reconstructs the HR domain samples from themselves, who is supposed to learn more discriminative features than the former one. By enforcing the two channels to use fully-shared hidden layers, the model aims to learn cross-domain, transferable features. With the shared filters from the HR-HR channel, the LR-HR channel is also expected to obtain enhanced features for the SR pre-training task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Simulation</head><p>We first pre-train the unsupervised SR model enhanced by feature transfer in <ref type="figure">Fig. 3</ref>, followed by supervised tuning. All experiment settings follow previous ones, except that we find when training such a "hybrid" model, choosing smaller learning rates helps a faster and more steady convergence. In practice, we start with the learning rate of 0.01 and do not anneal it during training. On CIFAR-10, the error rate is reduced to 21.72%, while on CIFAR-100 the error rate goes down to 43.03%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Model IV: Partially Coupled Networks</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Motivation</head><p>From Model I to Model III, we have obtained up to 10% accuracy gains in our CIFAR-10/100 simulation experiments. Is there any further room for improvements?</p><p>We take a step back and re-think about the key prerequisite in Model III: we utilize both LR and HR data to learn a fully shared feature representation across domains. Since each LR-HR pair indicates the same scene or object, it is reasonable to assume that there exists a hidden space that are approximately shared by LR and HR features <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b36">38]</ref>. However, the assumption that the LR and HR data should have strictly equal feature representations in the coupled subspace, appears to be overly strong.</p><p>As explored previously <ref type="bibr" target="#b28">[30]</ref>, the mappings between LR and HR spaces are complex, spatial-variant and nonlinear. Taking face recognition for example, <ref type="bibr" target="#b20">[22]</ref> empirically showed that the similarity measures in the LR and HR spaces were not identical, so that the direct matching of a LR probe image within the HR gallery performed poorly. The domain mismatch between LR and HR images is further enlarged, due to the noise, blur, and other corruptions when generating LR images.</p><p>We make a bold hypothesis that LR and HR image features do not fully overlap even after highly sophisticated transforms. Therefore, we attempt to "relax" Model III a bit, in order for certain flexibility to cope with feature variances, due to the mismatches of LR and HR domains.  <ref type="figure" target="#fig_1">Fig. 2</ref>. The major difference between PCSRN and the SR subnetwork in Model III <ref type="figure">(Fig. 3)</ref>, lies in that the former are "loosely" coupled by only sharing k i convolutional filters in each layer (i = 1, 2, 3). They tend to capture the commonality feature patterns across both LR and HR domains 2 . Based on our above hypothesis, PCSRN also allows each single channel to learn domain-specific features by m i − k i unshared filters (i = 1, 2, 3), that complement the shared features and handle the mismatches. <ref type="figure">Fig. 3</ref> could also be viewed as a special case of <ref type="figure" target="#fig_2">Fig. 4</ref> when m i = k i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Technical Approach</head><p>After PCSRN is pre-trained, we follow the same routine to replace Conv. Layer 4 by two fully-connected layers, followed by a softmax classifier independently on each top. Each channel (including the shared filters) has exactly the same topology as Model II or III. The whole two-channel Model IV is fine-tuned in a supervised manner. Either channel classifies its own input, while the two parts still interact by taking advantage of shared filters. In testing, we "decouple" the model and use only the left LR-HR channel in <ref type="figure" target="#fig_2">Fig.  4</ref>, including all the shared filters.</p><p>The "partially coupled" architecture manifests our important hypothesis, that the domain mismatches in VLRR problems are only reduced but do not vanish even after <ref type="bibr" target="#b1">2</ref> These shared features are usually the basic correspondences that mostly persists under resolution changes, such as pixel intensity histogram, strong edges, and specific structural layouts <ref type="bibr" target="#b20">[22]</ref>, that make up the foundation for cross-resolution recognition <ref type="figure" target="#fig_2">Figure 4</ref>. The Partially Coupled SR Networks (PCSRN), as the pre-training part of Model IV. Note it does not contain the fullyconnected recognition sub-networks. For each channel, its recognition sub-network is the same as in <ref type="figure" target="#fig_0">Fig. 1 (fc4, fc5 and softmax)</ref>. The two channels share ki convolutional filters in the i-th layer. learned transforms. It gives rise to important extra flexibility to transfer knowledge across domains. Despite looking like a reckless idea, its benefits will be validated next. <ref type="figure" target="#fig_2">Fig. 4</ref> outlines the general idea, but it is still unclear whether and how much performance gains could be obtained from the more complicated architecture. In addition, how many of the filters should be coupled? To answer them, we conduct a quantitative research on the CIFAR-10/100 cases. We fix the numbers of filters n 1 , n 2 and n 3 , and vary the numbers of shared filters k 1 , k 2 and k 3 . We define the "coupled ratio" for layer i: <ref type="figure" target="#fig_0">1, 2, 3)</ref>. Note when c i = 0 (i = 1, 2, 3), the model splits into two uncorrected parts and is equivalent to Model II. On the contrary, c i = 1 (i = 1, 2, 3) leads to the fully-coupled Model III.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Simulation</head><formula xml:id="formula_0">c i = k i /n i (i =</formula><p>To identify the best configuration, we perform a course grid search on each c i (i = 1, 2, 3), from 0 to 1 with a step size of 0.25. Even so, a "brutal-force" search leads to training models with at least 5 3 = 125 configurations. To avoid such an exhaustive effort, we develop a strategy by referring to the common belief in domain adaptation <ref type="bibr" target="#b9">[10]</ref>, that the discrepancy between two domains is expected to be gradually reduced with layers goes deeper <ref type="bibr" target="#b29">[31]</ref>. Hence we always give priority to increasing c i of deeper layers (starting from Conv. Layer 3, then 2 and 1). Once the error rate tends to rise, we roll back the last change, and instead increase the coupled ratio of its immediate lower layer. As summarized in <ref type="table" target="#tab_1">Table 2</ref>, the results strongly support to our previous "non-overlapping" hypothesis. In fact, all partially coupled models that we tried in <ref type="table" target="#tab_1">Table 2</ref> (0 &lt;c i &lt;1) obtain better performances than Model II (c i = 0), while most of them are superior to Model III (c i = 1) as well. Moreover, it is observed that higher layers are more sensitive to overregularization (i.e., being overly coupled ) than lower layers. For example, simply increasing c 3 from 0.75 to 1.00, when c 1 and c 2 are both fixed at 05, raise the error rate on CIFAT-10 from 19.91% to 20.72%. As a result, we choose c 1 = 0.50, c 2 = 0.75 and c 3 = 0.75, as our default configuration hereinafter. The resulting model gradually narrows the gap between the two domains with increasing number of layers, but still allows for certain flexibility of each channel for domain-specific features. We have verified that the partially coupled architecture could lead to additional gains compared to the fully-coupled model. However, it remains unresolved how to adaptively choose the best c i for specific cases, instead of ad-hoc trials. One potential solution is to learn a mapping function that regularizes the intrinsic relationship between the two domain-specific representations, such as in <ref type="bibr" target="#b28">[30]</ref>. Beyond VLRR, the "partially coupled" architecture could be potentially applied to many other cross-domain recognition problems. We leave it as future work.</p><p>6. Model V: Robust Partially Coupled Networks <ref type="bibr" target="#b5">6</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>.1. Motivation</head><p>So far, with the help of the SR pre-training as well as the flexible "partially-coupled" domain adaptation, we obtain satisfactory results on the CIFAR-10/100 LR data, which are artificially downsampled from clear HR groundtruth.</p><p>However, typical real data from low-resolution sources, such as video surveillance, are usually accompanied with sensor noise and impulsive outliers. Even in high-resolution cases, there are outlier factors that may destroy the manifold structure badly. One such example could be found in face recognition: since faces are neither perfectly convex nor Lambertian <ref type="bibr" target="#b20">[22]</ref>, face images taken under directional illumination often suffer from self-shadowing, specularity, or saturations in brightness. The pose and expression changes also introduce more spatially localized artifacts, that makes a fraction of the data grossly corrupted.</p><p>We adopt Mean-Square Error (MSE) as the loss function in SR pre-training, who could be rather sensitive to outliers. One may argue that MSE works widely well in most unsupervised deep learning models, e.g., Burger et al. <ref type="bibr" target="#b4">[5]</ref> showed that the plain multi-layer perceptrons can produce decent results and handle different types of noise. In fact, it has been observed that the learning ability of deep systems could be weakened by these outliers, which are pervasive in real-world data <ref type="bibr" target="#b6">[7]</ref>. A few robust learning methods have been proposed to immunize the harmful influences caused by outliers. A correntropy-induced loss function is proposed to combat outliers <ref type="bibr" target="#b22">[24]</ref>. The authors of <ref type="bibr" target="#b35">[37]</ref> concatenated another specific denoising CNN to handle the complex and signal-dependent outliers, at the cost of the growing parameter volume due to the merge. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Technical Approach</head><p>We replace the MSE loss of Model IV, with the convex and continuous huber loss function <ref type="bibr" target="#b13">[14]</ref>, in SR pretraining. We name it Robust Partially Coupled Networks, as our Model V. The huber loss is widely used in robust regression for its lower sensitivity to outliers. It comprises two parts, corresponding to the ℓ 2 and ℓ 1 losses <ref type="bibr" target="#b0">[1]</ref>, formally defined as:</p><formula xml:id="formula_1">H c (x, y) = { 1 2 (x − y) 2 if |x − y| &lt; c c|x − y| − c 2 2 if |x − y| ≥ c<label>(1)</label></formula><p>The cutting edge parameter c intuitively specifies the maximum value of error, after which the ℓ 1 penalty has to be applied. We are aware that more robust losses could be designed, and choose huber loss here because it is easy to implement and calculate.</p><p>In VLRR, we do find that the outliers possess a much stronger negative impact (we will verify it in simulation) than in conventional recognition, since LR images may not supply a sufficiently representative feature distribution to sketch the correct data manifold. The use of huber loss greatly alleviates the problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Simulation</head><p>We adopt the "clean" CIFAR-10/100 data, while creating "corrupted" versions by adding salt-and-pepper impulse noise on 5%, 10% and 20% of randomly selected pixels before downsampling. We then test Model VI and V on both datasets, whose only difference lies in the choice of MSE or huber loss in SR pre-training. Dropout is applied to both models. We fix the empirical value c = 1.345 in (1) as suggested by <ref type="bibr" target="#b13">[14]</ref> and many others. During pre-training, we observe that huber loss appears to accelerate the model convergence a bit. As in <ref type="table" target="#tab_2">Table 3</ref>, the performances of huber loss is almost identical (marginally worse) than MSE in the outlier-free cases, but becomes superior in the presence of outliers, and finally gains a remarkable margin of up to 3% over MSE with 15% outliers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.">Review and Comparison</head><p>We compare all our five models in <ref type="table" target="#tab_3">Table 4</ref>, and evaluate whether they bring in extra complexity <ref type="bibr" target="#b2">3</ref> . The general methodology to design and evolve models, is to focus on adapting the model both to the specific task (SR pretraining, etc.), and to the specific data domains (LR-HR feature transfer, partially coupled, huber loss, etc.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Solving Real VLRR Problems</head><p>We apply our proposed models to solve three VLRR applications. All model configurations, parameters and training details are similar to the CIFAR-10/100 simulation cases, unless otherwise specified.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.">VLRR Face Identification</head><p>Dataset Although many face image datasets, such as LFW <ref type="bibr" target="#b12">[13]</ref>, SCface <ref type="bibr" target="#b10">[11]</ref>, and CMU-PIE <ref type="bibr" target="#b25">[27]</ref>, have been developed, most of them were collected in controlled indoor environments with limited variability. A new UCCS dataset was recently proposed in <ref type="bibr" target="#b23">[25]</ref> as a larger and more challenging benchmark for unconstrained face recognition. The authors of <ref type="bibr" target="#b23">[25]</ref> reported a baseline face identification result of around 78% top-1 accuracy on a 180-subject subset of original-resolution images, by extracting 73 face attributes <ref type="bibr" target="#b18">[19]</ref> and using SVM for classification. We normalize the cropped face regions to 80 × 80 as HR, and downsample them by a factor s = 5 for LR images of 16 × 16. We perform evaluations on a 180-subject subset, where each subject has 25 or more images. We use 4,500 images for training, and the remaining 935 for testing. Performance Considering the training data is not sufficient (it is a common defect for unconstrained face datasets), we perform layer-by-layer greedy unsupervised training <ref type="bibr" target="#b3">[4]</ref> for all models <ref type="bibr" target="#b3">4</ref> . As in <ref type="figure" target="#fig_3">Fig. 5 (a)</ref>, The performances consistently improve with our model evolves, finally reaching 40.97%@top-1 and 22.35%@top-5, with a significant margin of around 10%@top-1 over the simplest baseline. Model V correctly classifies 552 out of 935 testing samples in top-1 results, and 726 in top-5. Model II reaches a 4.60% improvement over Model I, while Model III further improves by 2.57%. That identifies the key role of hallucinated features in this task, by both SR pre-training and LR-HR transfer. Besides, with more free model parameters, the partially-coupled architecture contributes to a performance rise of 1.71%@top-1. Finally, the utility of huber loss brings an extra gain of 1.06%@top-1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.">VLRR Digit Recognition</head><p>Dataset SVHN <ref type="bibr" target="#b21">[23]</ref> is a real-world massive digit image dataset with highly complex scenes. Note the median of all digit height is 28 pixels, while more than 10,000 images have their original heights less than 10 pixels <ref type="bibr" target="#b21">[23]</ref>. That validates SVHN as a proper research object for VLRR. We utilize 604,388 (including the extra set) images for training and 26,032 for testing, covering 10 classes. All images are resized to 32 × 32 as HR, and LR images are obtained by downsampling with s = 4 (8 × 8). For training, we have both HR and LR images available, while the testing set sees only LR images. Note that the preprocessing of SVHN introduces "distracting digits" to the sides of some digits of interest <ref type="bibr" target="#b21">[23]</ref>. Those strong outliers could largely confuse models, considering the highly noise-sensitive nature of very low resolution images <ref type="bibr" target="#b37">[39]</ref>.</p><p>Performance We calculate the top-1 and top-5 error rates for all five models. As in <ref type="figure" target="#fig_3">Fig. 5</ref> (b), Model still achieves the lowest error rates, with impressive records of 43.02%@to-1 and 29.18%@top-5. Note that the human top-1 accuracy, for original SVHN images of heights less than 25 pixels, is only around 82.0% <ref type="bibr" target="#b21">[23]</ref>. Also, it is to our special attention that Model V obtains a 4.05%@top-1 margin over Model IV, showing the great benefits of the robust huber loss, especially in the existence of strong outliers (distracting digits). There is another performance gap of 4.03%@top-1 between Model II and III, which proves that the LR-HR transfer enhances the feature learning. In addition, the fullycoupled and partially-coupled architecture demonstrate a 2.67%@top-1 difference. <ref type="figure">Figure 6</ref>. Examples (normalized) from the VFR testing set <ref type="bibr" target="#b5">[6]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.">VLRR Font Recognition</head><p>Dataset The VFR dataset used in <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b32">34,</ref><ref type="bibr" target="#b31">33]</ref> includes 2,383 font classes. For each class, 1,000 synthetic text images were rendered for training, with a normalized height of 105. A testing set was collected with 325 real-world images of 93 classes, and of various ROI (text) dimensions. Further statistics show that approximately 1/6 of the testing images (52) fall into VLRR cases (e.g, character heights are less than 16 pixels), and a majority of those "VLRR images" fail to be recognized even with the latest model <ref type="bibr" target="#b31">[33]</ref>, due to the very heavy detail loss and compression artifacts after normalization. We aim to overcome it by explicitly taking resolution-enhancement into account during training, formulating a VLRR problem. All training images are downsampled by a random factor s between <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b14">15]</ref> to obtain LR images with heights between <ref type="bibr" target="#b6">[7,</ref><ref type="bibr">21]</ref> pixels, with the intention to make the learnt model robust to a wide range of very low resolutions in testing.</p><p>Performance With the identical training and testing sets, we fulfills the same task with an emphasis on improving recognition ability of VLRR subjects. We follow the data augmentations in <ref type="bibr" target="#b31">[33]</ref> to train all models. While SR pretraining, LR-HR transfer and partially-coupled networks each still lead to visible gains, it is interesting to notice that Model V hardly benefits from the huber loss. That might be in part due to the very small sample size; but more importantly, the training set contains all rendered synthetic data without any outliers, which makes the robust loss unnecessary in unsupervised training. The proposed model reaches the error rates of 36.31%@top-1 and 16.31%@top-5. That outperforms the best accuracy numbers reported on VFR before, 38.15%@top-1 and 20.62%@top-5 <ref type="bibr" target="#b31">[33]</ref>. A closer inspection reveals that our model correctly classify 33 out of 52 "VLRR images" as per top-5 results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Conclusion</head><p>In this paper, we study the challenging VLRR problem using the tool of deep networks. Starting from the simplest CNN baseline, we gradually evolve our models, with each step well motivated and justified. In addition to the large learning capacity, the final model also benefits from the SR pre-training, the domain adaptation with partial flexibility, and the robust loss. The effectiveness of the proposed models is evaluated on three different VLRR tasks, with outstanding performances obtained.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>The basic architecture of conventional convolutional neural network (CNN).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>The super resolution (SR) sub-network, as the pretraining part of Model II. Note it does not contain the fullyconnected recognition sub-network, which is the same as inFig. 1 (fc4, fc5 and softmax). The new Conv. 4 Layer has only one channel, and is discarded after pre-training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 depicts</head><label>4</label><figDesc>Partially Coupled SR Networks (PCSRN), as the pre-training part of Model IV. PCSRN is built with two unsupervised channels in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>The top-1 and top-5 error rates of all five models, on three real VLRR applications.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 .</head><label>1</label><figDesc></figDesc><table>The top-1 classification error rates (%) of various net-
work configurations, with either LR or HR training images, on the 
CIFAR-10 and CIFAR-100 datasets. 

n 1 
n 2 
n 3 
f 1 
f 2 
f 3 
CIFAR-10 
CIFAR-100 
LR 
HR 
LR 
HR 
5 
3 
1 
64 
64 
32 
27.47 14.41 
45.03 
38.37 
7 
3 
1 
64 
64 
32 
28.86 14.24 
49.82 
37.07 
5 
5 
1 
64 
64 
32 
28.82 14.06 
50.01 
36.69 
5 
3 
3 
64 
64 
32 
29.13 13.78 
49.95 
35.02 
5 
3 
1 
32 
32 
32 
30.03 17.91 
50.81 
42.23 
5 
3 
1 
64 
32 
32 
27.87 15.45 
48.91 
40.27 
5 
3 
1 
64 
64 
64 
28.08 14.32 
46.67 
37.61 

• While in HR experiments, adding more filter channels 
generally helps, it may hurt VLRR performances. That 
is likely to be the result of model overfitting, since LR 
subjects contain scarce visual features. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 2 .</head><label>2</label><figDesc>The top-1 classification error rates (%) with different configurations of coupled filters, on the CIFAR-10 and CIFAR-100 datasets.(n1 = 64, n2 = 64, n3 = 32)</figDesc><table>k 1 
k 2 
k 3 
c 1 
c 2 
c 3 
CIFAR-10 CIFAR-100 
0 
0 
0 
0 
0 
0 
25.15 
46.50 
0 
0 
16 
0 
0 
0.50 
21.81 
43.48 
0 
0 
32 
0 
0 
1.00 
22.85 
44.03 
0 
32 
16 
0 
0.50 0.50 
21.43 
43.27 
0 
64 
16 
0 
1.00 0.50 
21.87 
44.09 
32 
32 
16 
0.50 0.50 0.50 
21.04 
43.21 
32 
32 
24 
0.50 0.50 0.75 
19.91 
41.52 
32 
32 
32 
0.50 0.50 1.00 
20.72 
41.90 
32 
48 
24 
0.50 0.75 0.75 
18.77 
41.64 
32 
64 
24 
0.50 1.00 0.75 
19.52 
42.99 
48 
48 
24 
0.75 0.75 0.75 
19.06 
42.15 
64 
64 
32 
1.00 1.00 1.00 
21.72 
43.03 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 .</head><label>3</label><figDesc>The top-1 classification error rates (%), with MSE or huber loss functions in SR pre-training, on the clear and corrupted CIFAR-10/100 datasets.</figDesc><table>CIFAR-10 CIFAR-100 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="true"><head>Table 4 .</head><label>4</label><figDesc>Review and Comparison of All Models. ( * : the error gains are in the presence of 15% outliers.)</figDesc><table>Improvement Over 
Add 
Error Rate Reduction (%) 
The Last Model 
Complexity? 
CIFAR-10 
CIFAR-100 
Model II 
exploit HR training data and perform SR pre-training 
No 
2.32 
3.45 
Model III 
learn transferable feature from HR data to enhance the SR pre-training 
No 
3.43 
3.47 
Model IV 
introduce extra flexibility and avoid over-regularization by the partially-coupled structure 
Yes 
2.95 
4.86 
Model V 
adopt the huber loss in pre-training to be robust to outliers 
No 
2.81  *  
3.31  *  

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Note that our developed models can apparently be applied, even when the standard of LR changes.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Here we refer to the parameter complexity. For example, the training data doubles from Model II to III, but the amount of free model parameters keeps unchanged. Therefore, the parameter complexity is the same, but the training time definitely increases.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">For Model I, the pre-training is only to reconstruct the LR image itself. For Model II to V, the pre-training is still SR-type, but is performed layerby-layer instead of end-to-end.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Convex optimization with sparsity-inducing norms. Optimization for Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jenatton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Obozinski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Limits on super-resolution and how to break them</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Learning deep architectures for ai. Foundations and trends R ⃝ in Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Greedy layer-wise training of deep networks. NIPS</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Popovici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Image denoising: Can plain neural networks compete with bm3d? In CVPR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">C</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Schuler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harmeling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Large-scale visual font recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">X</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR. IEEE</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Efficient and robust deep learning with correntropy-induced loss function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Principe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computing and Applications</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning a deep convolutional network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The difficulty of training deep architectures and the effect of unsupervised pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Manzagol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Domain adaptation for large-scale sentiment classification: A deep learning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Scface-surveillance cameras face database. Multimedia Tools and Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Grgic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Delac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Grgic</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Simultaneous super-resolution and feature extraction for recognition of low-resolution faces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Hennings-Yeomans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">V</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR. IEEE</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Labeled faces in the wild: A database for studying face recognition in unconstrained environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Robust estimation of a location parameter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Huber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Mathematical Statistics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="1964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Text recognition of low-resolution document images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">Y</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rinker</surname></persName>
		</author>
		<editor>ICDAR. IEEE</editor>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep features for text spotting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Attribute and simile classifiers for face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">N</forename><surname>Belhumeur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Nayar</surname></persName>
		</author>
		<editor>ICCV. IEEE</editor>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Fundamental limits of reconstruction-based superresolution algorithms under local translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang ; Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV. IEEE</title>
		<imprint>
			<publisher>PAMI</publisher>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
	<note>Coupled space learning of image style transformation</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A meta-analysis of face recognition covariates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">M</forename><surname>Lui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bolme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Draper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Beveridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Givens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Phillips</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BTAS</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Reading digits in natural images with unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS workshop on deep learning and unsupervised feature learning</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Robust deep network with maximum correntropy criterion for seizure detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BioMed research international</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Large scale unconstrained open set face database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sapkota</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Boult</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE BTAS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Synthesis-based robust low resolution face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shekhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">The cmu pose, illumination, and expression (pie) database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bsat</surname></persName>
		</author>
		<editor>FG. IEEE</editor>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep learning face representation from predicting 10,000 classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">80 million tiny images: A large data set for nonparametric object and scene recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Semi-coupled dictionary learning with applications in image super-resolution and photo-sketch synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">Y</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR. IEEE</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Deeply coupled auto-encoder networks for cross-view classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1402.2031</idno>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep networks for image super-resolution with sparse prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deepfont: Identify your font from an image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Real-world font recognition using deep network and domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.00028</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Self-tuned deep super resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPRW</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning super-resolution jointly from external and internal examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep convolutional neural network for image deconvolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Image superresolution via sparse representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Very low resolution face recognition problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">W</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Yuen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
