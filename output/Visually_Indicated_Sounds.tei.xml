<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Visually Indicated Sounds Predicted Sound</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Owens</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Mcdermott</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><forename type="middle">H</forename><surname>Adelson</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<address>
									<addrLine>1 MIT 2 U.C. Berkeley</addrLine>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Visually Indicated Sounds Predicted Sound</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>(Amplitude 1/2 )</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:48+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Time (seconds)</head><p>Input video <ref type="figure">Figure 1</ref>: We train a model to synthesize plausible impact sounds from silent videos, a task that requires implicit knowledge of material properties and physical interactions. In each video, someone probes the scene with a drumstick, hitting and scratching different objects. We show frames from two videos and below them the predicted audio tracks. The locations of these sampled frames are indicated by the dotted lines on the audio track. The predicted audio tracks show seven seconds of sound, corresponding to multiple hits in the videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Objects make distinctive sounds when they are hit or scratched. These sounds reveal aspects of an object's material properties, as well as the actions that produced them. In this paper, we propose the task of predicting what sound an object makes when struck as a way of studying physical interactions within a visual scene. We present an algorithm that synthesizes sound from silent videos of people hitting and scratching objects with a drumstick. This algorithm uses a recurrent neural network to predict sound features from videos and then produces a waveform from these features with an example-based synthesis procedure. We show that the sounds predicted by our model are realistic enough to fool participants in a "real or fake" psychophysical experiment, and that they convey significant information about material properties and physical interactions.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>From the clink of a ceramic mug placed onto a saucer, to the squelch of a shoe pressed into mud, our days are filled with visual experiences accompanied by predictable sounds. On many occasions, these sounds are not just statistically associated with the content of the images -the way, for example, that the sounds of unseen seagulls are associated with a view of a beach -but instead are directly caused by the physical interaction being depicted: you see what is making the sound.</p><p>We call these events visually indicated sounds, and we propose the task of predicting sound from videos as a way to study physical interactions within a visual scene <ref type="figure">(Figure 1)</ref>. To accurately predict a video's held-out soundtrack, an algorithm has to know about the physical properties of what it is seeing and the actions that are being performed. This task implicitly requires material recognition, but unlike traditional work on this problem <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b35">36]</ref>, we never explicitly tell the algorithm about materials. Instead, it learns about them by identifying statistical regularities in the raw audiovisual signal.</p><p>We take inspiration from the way infants explore the physical properties of a scene by poking and prodding at the objects in front of them <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b2">3]</ref>, a process that may help them learn an intuitive theory of physics <ref type="bibr" target="#b2">[3]</ref>. Recent work suggests that the sounds objects make in response to these interactions may play a role in this process <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b40">41]</ref>.</p><p>We introduce a dataset that mimics this exploration process, containing hundreds of videos of people hitting, scratching, and prodding objects with a drumstick. To synthesize sound from these videos, we present an algorithm that uses a recurrent neural network to map videos to audio features. It then converts these audio features to a waveform, either by matching them to exemplars in a database and transferring their corresponding sounds, or by parametrically inverting the features. We evaluate the quality of our predicted sounds using a psychophysical study, and we also analyze what our method learned about actions and materials through the task of learning to predict sound.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Our work closely relates to research in sound and material perception, and to representation learning.</p><p>Foley The idea of adding sound effects to silent movies goes back at least to the 1920s, when Jack Foley and collaborators discovered that they could create convincing sound effects by crumpling paper, snapping lettuce, and shaking cellophane in their studio 1 , a method now known as Foley. Our algorithm performs a kind of automatic Foley, synthesizing plausible sound effects without a human in the loop.</p><p>Sound and materials In the classic mathematical work of <ref type="bibr" target="#b24">[25]</ref>, Kac showed that the shape of a drum could be partially recovered from the sound it makes. Material properties, such as stiffness and density <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b13">14]</ref>, can likewise be determined from impact sounds. Recent work has used these principles to estimate material properties by measuring tiny vibrations in rods and cloth <ref type="bibr" target="#b7">[8]</ref>, and similar methods have been used to recover sound from high-speed video of a vibrating membrane <ref type="bibr" target="#b8">[9]</ref>. Rather than using a camera as an instrument for measuring vibrations, we infer a plausible sound for an action by recognizing what kind of sound this action would normally make in the visually observed scene.</p><p>Impact sounds have been used in other work to recognize objects and materials. Arnab et al. <ref type="bibr" target="#b1">[2]</ref> recently presented a semantic segmentation model that incorporates audio from impact sounds, and showed that audio information could <ref type="bibr" target="#b0">1</ref> To our delight, Foley artists really do knock two coconuts together to fake the sound of horses galloping <ref type="bibr" target="#b5">[6]</ref>. help recognize objects and materials that were ambiguous from visual cues alone. Other work recognizes objects using audio produced by robotic interaction <ref type="bibr" target="#b38">[39]</ref>.</p><p>Sound synthesis Our technical approach resembles speech synthesis methods that use neural networks to predict sound features from pre-tokenized text features and then generate a waveform from those features <ref type="bibr" target="#b27">[28]</ref>. There are also methods, such as the FoleyAutomatic system, for synthesizing impact sounds from physical simulations <ref type="bibr" target="#b42">[43]</ref>. Work in psychology has studied low-dimensional representations for impact sounds <ref type="bibr" target="#b6">[7]</ref>, and recent work in neuroimaging has shown that silent videos of impact events activate the auditory cortex <ref type="bibr" target="#b18">[19]</ref>.</p><p>Learning visual representations from natural signals Previous work has explored the idea of learning visual representations by predicting one aspect of a raw sensory signal from another. For example, <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b20">21]</ref> learned image representations by predicting the spatial relationship between image patches, and <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b21">22]</ref> by predicting the egocentric motion between video frames. Several methods have also used temporal proximity as a supervisory signal <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b43">44]</ref>. Unlike in these approaches, we learn to predict one sensory modality (sound) from another (vision). There has also been work that trains neural networks from multiple modalities. For example, <ref type="bibr" target="#b31">[32]</ref> learned a joint model of audio and video. However, while they study speech using an autoencoder, we focus on material interaction, and we use a recurrent neural network to predict sound features from video.</p><p>A central goal of other methods has been to use a proxy signal (e.g., temporal proximity) to learn a generically useful representation of the world. In our case, we predict a signal -sound -known to be a useful representation for many tasks <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b34">35]</ref>, and we show that the output (i.e. the predicted sound itself, rather than some internal representation in the model) is predictive of material and action classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The Greatest Hits dataset</head><p>In order to study visually indicated sounds, we collected a dataset containing videos of humans (the authors) probing environments with a drumstick -hitting, scratching, and poking different objects in the scene ( <ref type="figure" target="#fig_0">Figure 2</ref>). We chose to use a drumstick so that we would have a consistent way of generating the sounds. Moreover, since the drumstick does not occlude much of a scene, we can also observe what happens to the object after it is struck. This motion, which we call a reaction, can be important for inferring material properties -a soft cushion, for example, will deform more than a firm one, and the sound it produces will vary with it. Similarly, individual pieces of gravel will scatter when they are hit, and their sound varies with this motion <ref type="figure" target="#fig_0">(Figure 2</ref>, right).</p><p>Unlike traditional object-or scene-centric datasets, such as ImageNet <ref type="bibr" target="#b9">[10]</ref> or Places <ref type="bibr" target="#b45">[46]</ref>, where the focus of the image is a full scene, our dataset contains close-up views of a small number of objects. These images reflect the viewpoint of an observer who is focused on the interaction taking place (similar to an egocentric viewpoint). They contain enough detail to see fine-grained texture and the reaction that occurs after the interaction. In some cases, only part of an object is visible, and neither its identity nor other high-level aspects of the scene are easily discernible. Our dataset is also related to robotic manipulation datasets <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b14">15]</ref>. While one advantage of using a robot is that its actions are highly consistent, having a human collect the data allows us to rapidly (and inexpensively) capture a large number of physical interactions in real-world scenes.</p><p>We captured 977 videos from indoor (64%) and outdoor scenes (36%). The outdoor scenes often contain materials that scatter and deform, such as grass and leaves, while the indoor scenes contain a variety of hard and soft materials, such as metal, plastic, cloth, and plastic bags. Each video, on average, contains 48 actions (approximately 69% hits and 31% scratches) and lasts 35 seconds. We recorded sound using a shotgun microphone attached to the top of the camera and used a wind cover for outdoor scenes. We used a separate audio recorder, without auto-gain, and we applied a denoising algorithm <ref type="bibr" target="#b19">[20]</ref> to each recording.</p><p>Detecting impact onsets We detected amplitude peaks in the denoised audio, which largely correspond to the onset of impact sounds. We thresholded the amplitude gradient to find an initial set of peaks, then merged nearby peaks with the mean-shift algorithm <ref type="bibr" target="#b12">[13]</ref>, treating the amplitude as a density and finding the nearest mode for each peak. Finally, we used non-maximal suppression to ensure that onsets were at least 0.25 seconds apart. This is a simple onsetdetection method that most often corresponds to drumstick impacts when the impacts are short and contain a single peak 2 . In many of our experiments, we use short video clips that are centered on these amplitude peaks.</p><p>Semantic annotations We also collected annotations for a sample of impacts (approximately 62%) using online workers from Amazon Mechanical Turk. These include material labels, action labels (hit vs. scratch), reaction labels, and the pixel location of each impact site. To reduce the number of erroneous labels, we manually removed annotations for material categories that we could not find in the scene. During material labeling, workers chose from finergrained categories. We then merged similar, frequently confused categories (please see the supplementary material for details). Note that these annotations are used only for analysis: we train our models on raw audio and video. Examples of several material and action classes are shown in <ref type="figure" target="#fig_0">Figure 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Sound representation</head><p>Following work in sound synthesis <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b29">30]</ref>, we compute our sound features by decomposing the waveform into subband envelopes -a simple representation obtained by filtering the waveform and applying a nonlinearity. We apply a bank of 40 band-pass filters spaced on an equivalent rectangular bandwidth (ERB) scale <ref type="bibr" target="#b15">[16]</ref> (plus a low-and high-pass filter) and take the Hilbert envelope of the responses. We then downsample these envelopes to 90Hz (approximately 3 samples per frame) and compress them. More specifically, we compute envelope s n (t) from a wave-form w(t) and a filter f n by taking:</p><formula xml:id="formula_0">s n = D(|(w * f n ) + jH(w * f n )|) c ,<label>(1)</label></formula><p>where H is the Hilbert transform, D denotes downsampling, and the compression constant c = 0.3. In the supplementary material, we study how performance varies with the number of frequency channels. The resulting representation is known as a cochleagram. In <ref type="figure" target="#fig_1">Figure 3</ref>(a), we visualize the mean cochleagram for a selection of material and reaction classes. This reveals, for example, that cloth sounds tend to have more low-frequency energy than those of rock.</p><p>How well do impact sounds capture material properties in general? To measure this empirically, we trained a linear SVM to predict material class for the sounds in our database, using the subband envelopes as our feature vectors. We resampled our training set so that each class contained an equal number of impacts (260 per class). The resulting material classifier has 45.8% (chance = 5.9%) classaveraged accuracy (i.e., the mean of per-class accuracy values), and its confusion matrix is shown in <ref type="figure" target="#fig_1">Figure 3</ref>(b). These results suggest that impact sounds convey significant information about materials, and thus if an algorithm could learn to accurately predict these sounds from images, it would have implicit knowledge of material categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Predicting visually indicated sounds</head><p>We formulate our task as a regression problem -one where the goal is to map a sequence of video frames to a sequence of audio features. We solve this problem using a recurrent neural network that takes color and motion information as input and predicts the subband envelopes of an audio waveform. Finally, we generate a waveform from these sound features. Our neural network and synthesis procedure are shown in <ref type="figure">Figure 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Regressing sound features</head><p>Given a sequence of input images I 1 , I 2 , ..., I N , we would like to estimate a corresponding sequence of sound features s 1 , s 2 , ..., s T , where s t ∈ R 42 . These sound features correspond to blocks of the cochleagram shown in <ref type="figure">Figure 4</ref>. We solve this regression problem using a recurrent neural network (RNN) that takes image features computed with a convolutional neural network (CNN) as input.</p><p>Image representation We found it helpful to represent motion information explicitly in our model using a twostream approach <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b37">38]</ref>. While two-stream models often use optical flow, it is challenging to obtain accurate flow estimates due to the presence of fast, non-rigid motion. Instead, we compute spacetime images for each frame -images whose three channels are grayscale versions of the previous, current, and next frames. This image representation</p><formula xml:id="formula_1">… … Video CNN LSTM ( Cochleagram Time Waveform</formula><p>Example-based synthesis <ref type="figure">Figure 4</ref>: We train a neural network to map video sequences to sound features. These sound features are subsequently converted into a waveform using either parametric or example-based synthesis. We represent the images using a convolutional network, and the time series using a recurrent neural network. We show a subsequence of images corresponding to one impact.</p><p>is closely related to 3D video CNNs <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b25">26]</ref>, as derivatives across channels correspond to temporal derivatives.</p><p>For each frame t, we construct an input feature vector x t by concatenating CNN features for the spacetime image at frame t and the color image from the first frame 3 :</p><formula xml:id="formula_2">x t = [φ(F t ), φ(I 1 )],<label>(2)</label></formula><p>where φ are CNN features obtained from layer fc 7 of the AlexNet architecture <ref type="bibr" target="#b26">[27]</ref> (its penultimate layer), and F t is the spacetime image at time t. In our experiments (Section 6), we either initialized the CNN from scratch and trained it jointly with the RNN, or we initialized the CNN with weights from a network trained for ImageNet classification. When we used pretraining, we precomputed the features from the convolutional layers and fine-tuned only the fully connected layers. Sound prediction model We use a recurrent neural network (RNN) with long short-term memory units (LSTM) <ref type="bibr" target="#b17">[18]</ref> that takes CNN features as input. To compensate for the difference between the video and audio sampling rates, we replicate each CNN feature vector k times, where k = ⌊T /N ⌋ (we use k = 3). This results in a sequence of CNN features x 1 , x 2 , ..., x T that is the same length as the sequence of audio features. At each timestep of the RNN, we use the current image feature vector x t to update the vector of hidden variables h t 4 . We then compute sound features by an affine transformation of the hidden variables:</p><formula xml:id="formula_3">s t = W h t + b h t = L(x t , h t−1 ),<label>(3)</label></formula><p>where L is a function that updates the hidden state <ref type="bibr" target="#b17">[18]</ref>. During training, we minimize the difference between the predicted and ground-truth predictions at each timestep:</p><formula xml:id="formula_4">E({ s t }) = T t=1 ρ( s t −˜ s t 2 ),<label>(4)</label></formula><p>where˜ s t and s t are the true and predicted sound features at time t, and ρ(r) = log(ǫ + r 2 ) is a robust loss that bounds the error at each timestep (we use ǫ = 1/25 2 ). We also increase robustness of the loss by predicting the square root of the subband envelopes, rather than the envelope values themselves. To make the learning problem easier, we use PCA to project the 42-dimensional feature vector at each timestep down to a 10-dimensional space, and we predict this lower-dimensional vector. When we evaluate the network, we invert the PCA transformation to obtain sound features. We train the RNN and CNN jointly using stochastic gradient descent with Caffe <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b11">12]</ref>. We found it helpful for convergence to remove dropout <ref type="bibr" target="#b41">[42]</ref> and to clip large gradients. When training from scratch, we augmented the data by applying cropping and mirroring transformations to the videos. We also use multiple LSTM layers (the number depends on the task; please see the supplementary material).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Generating a waveform</head><p>We consider two methods for generating a waveform from the predicted sound features. The first is the simple parametric synthesis approach of <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b29">30]</ref>, which iteratively imposes the subband envelopes on a sample of white noise (we used just one iteration). This method is useful for examining what information is captured by the audio features, since it represents a fairly direct conversion from features to sound. However, for the task of generating plausible sounds to a human ear, we find it more effective to impose a strong natural sound prior during conversion from features to waveform. Therefore, we also consider an examplebased synthesis method that snaps a window of sound features to the closest exemplar in the training set. We form a query vector by concatenating the predicted sound features s 1 , ..., s T (or a subsequence of them), searching for its nearest neighbor in the training set as measured by L 1 distance, and transferring the corresponding waveform.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experiments</head><p>We applied our sound-prediction model to several tasks, and evaluated it with a combination of human studies and <ref type="bibr" target="#b3">4</ref> To simplify the presentation, we have omitted the LSTM's hidden cell state, which is also updated at each timestep. automated metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Sound prediction tasks</head><p>In order to study the problem of detection -that is, the task of determining when and whether an action that produces a sound has occurred -separately from the task of sound prediction, we consider two kinds of videos. First, we focus on the prediction problem and consider only videos centered on audio amplitude peaks, which often correspond to impact onsets (Section 3). We train our model to predict sound for 15-frame sequences (0.5 sec.) around each peak.</p><p>For the second task, which we call the detection problem, we train our model on longer sequences (approximately 2 sec. long) sampled from the training videos with a 0.5second stride, and we subsequently evaluate this model on full-length videos. Since it can be difficult to discern the precise timing of an impact, we allow the predicted features to undergo small shifts before they are compared to the ground truth. We also introduce a two-frame lag in the RNN output, which allows the model to observe future frames before outputting sound features. Finally, before querying sound features, we apply a coloring procedure to account for statistical differences between the predicted and real sound features (e.g., under-prediction of amplitude), using the silent videos in the test set to estimate the empirical mean and covariance of the network's predictions. For these implementation details, please see the supplementary material. For both tasks, we split the full-length videos into training and test sets (75% training and 25% testing).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models</head><p>For the prediction task, we compared our model to image-based nearest neighbor search. We computed fc 7 features from a CNN pretrained on ImageNet <ref type="bibr" target="#b26">[27]</ref> for the center frame of each sequence, which by construction is the frame where the impact sound occurs. We then searched the training set for the best match and transferred its corresponding sound. We considered variations where the CNN features were computed on an RGB image, on (three-frame) spacetime images, and on the concatenation of both features. To understand the influence of different design decisions, we also considered several variations of our model. We included models with and without ImageNet pretraining; with and without spacetime images; and with examplebased versus parametric waveform generation. Finally, we included a model where the RNN connections were broken (the hidden state was set to zero between timesteps).</p><p>For the RNN models that do example-based waveform generation (Section 5.2), we used the centered impacts in the training set as the exemplar database. For the prediction task, we performed the query using the sound features for the entire 15-frame sequence. For the detection task, this is not possible, since the videos may contain multiple, overlapping impacts. Instead, we detected amplitude peaks of the parametrically inverted waveform, and matched the  <ref type="figure">Figure 5</ref>: (a) We measured the rate at which subjects chose an algorithm's synthesized sound over the actual sound. Our full system, which was pretrained from ImageNet and used example-based synthesis to generate a waveform, significantly outperformed models based on image matching. For the neural network models, we computed the auditory metrics for the sound features that were predicted by the network, rather than those of the inverted sounds or transferred exemplars. (b) What sounds like what, according to our algorithm? We applied a classifier trained on real sounds to the sounds produced by our algorithm, resulting in a confusion matrix (c.f . <ref type="figure" target="#fig_1">Figure 3</ref>(b), which shows a confusion matrix for real sounds). It obtained 22.7% class-averaged accuracy. (c) Confusions made by a classifier trained on fc7 features (30.2% class-averaged accuracy). For both confusion matrices, we used the variation of our model that was trained from scratch.</p><p>sound features in small (8-frame) windows around each peak (starting the window one frame before the peak).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Evaluating the sound predictions</head><p>We assessed the quality of the sounds using psychophysical experiments and measurements of acoustic properties.</p><p>Psychophysical study To test whether the sounds produced by our model varied appropriately with different actions and materials, we conducted a psychophysical study on Amazon Mechanical Turk. We used a two-alternative forced choice test in which participants were asked to distinguish real and fake sounds. We showed them two videos of an impact event -one playing the recorded sound, the other playing a synthesized sound. We then asked them to choose the one that played the real sound. The sound-prediction algorithm was chosen randomly on a per-video basis. We randomly sampled 15 impact-centered sequences from each full-length video, showing each participant at most one impact from each one. At the start of the experiment, we revealed the correct answer to five practice videos.</p><p>We measured the rate at which participants mistook our model's result for the ground-truth sound ( <ref type="figure">Figure 5(a)</ref>), finding that our full system -with RGB and spacetime input, RNN connections, ImageNet pretraining, and examplebased waveform generation -significantly outperformed the image-matching methods. It also outperformed a baseline that sampled a random (centered) sound from the training set (p &lt; 0.001 with a two-sided t-test). We found that the version of our model that was trained from scratch outperformed the best image-matching method (p = 0.02). Finally, for this task, we did not find the difference between our full and RGB-only models to be significant (p = 0.08).</p><p>We show results broken down by semantic category in   For some categories, such as grass and leaf, participants were frequently fooled by our results. Often when a participant was fooled, it was because the sound prediction was simple and prototypical (e.g., a simple thud noise), while the actual sound was complex and atypical. True leaf sounds, for example, are highly varied and may not be fully predictable from a silent video. When they are struck, we hear a combination of the leaves themselves, along with rocks, dirt, and whatever else is underneath them. In contrast, the sounds predicted by our model tend to be closer to prototypical grass/dirt/leaf noises. Participants also sometimes made mistakes when the onset detection failed, or when multiple impacts overlapped, since this may have defied their expectation of hearing a single impact.</p><p>We found that the model in which the RNN connections were broken was often unable to detect the timing of the hit, and that it under-predicted the amplitude of the sounds. As a result, it performed poorly on automated metrics and failed  <ref type="figure" target="#fig_3">Figure 7</ref>: Semantic analysis of psychophysical study. We show the rate that our algorithm fooled human participants for each material, action, and reaction class. The error bars show 95% confidence intervals. Our approach significantly outperforms the highest-performing image-matching method (RGB + spacetime).</p><p>to find good matches. The performance of our model with parametric waveform generation varied widely between categories. It did well on materials such as leaf and dirt that are suited to the relatively noisy sounds that the method produces but poorly on hard materials such as wood and metal (e.g., a confusion rate of 62% ± 6% for dirt and 18% ± 5% for metal). On the other hand, the example-based approach was not effective at matching textural sounds, such as those produced by splashing water <ref type="figure" target="#fig_3">(Fig. 7)</ref>.</p><p>Auditory metrics We measured quantitative properties of sounds for the prediction task. We chose metrics that were not sensitive to precise timing. First, we measured the loudness of the sound, which we took to be the maximum energy (L 2 norm) of the compressed subband envelopes over all timesteps. Second, we compared the sounds' spectral centroids, which we measured by taking the center of mass of the frequency channels for a one-frame (approx. 0.03 sec.) window around the center of the impact. We found that on both metrics, the network was more accurate than the image-matching methods, both in terms of mean squared error and correlation coefficients ( <ref type="figure">Figure 5(a)</ref>).</p><p>Oracle results How helpful is material category information? We conducted a second study that controlled for material-recognition accuracy. Using the subset of the data with material annotations, we created a model that chose a random sound from the same class as the input video. We also created a number of oracle models that used these material labels (Table 6(a)). For the best-performing imagematching model (RGB + spacetime), we restricted the pool of matches to be those with the same label as the input (and similarly for the example-based synthesis method).</p><p>We also considered a model that matched the ground-truth sound to the training set and transferred the best match. We found that, while knowing the material was helpful for each method, it was not sufficient, as the oracle models did not outperform our model. In particular, our model significantly outperformed the random-sampling oracle (p &lt; 10 −4 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Impact detection</head><p>We also used our methods to produce sounds for long, uncentered videos, a problem setting that allows us to evaluate their ability to detect impact events. We provide qualitative examples in <ref type="figure">Figure 8</ref> and videos in the supplementary material. To quantitatively evaluate its detection accuracy, we used the parametric synthesis method to produce a waveform, applied a large gain to that waveform, and then detected amplitude peaks (Section 3).</p><p>We then compared the timing of these peaks to those of the ground truth, considering an impact to be detected if a predicted spike occurred within 0.1 seconds of it. Using the predicted amplitude as a measure of confidence, we computed average precision. We compared our model to an RGB-only model, finding that the spacetime images significantly improve the result, with APs of 43.6% and 21.6% respectively. Both models were pretrained with ImageNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Learning about material and action by predicting sounds</head><p>By learning to predict sounds, did the network also learn something about material and physical interactions? To assess this, we tested whether the network's output sounds were informative about material and action class. We applied the same SVM that was trained to predict material/action class on real sound features (Sec. 4) to the sounds predicted by the model. Under this evaluation regime, it is not enough for the network's sounds to merely be distinguishable by class: they must be close enough to real sounds so as to be classified correctly by an SVM that has never seen a predicted sound. To avoid the influence of pretraining, we used a network that was trained from scratch. We note that this evaluation method is different from that of recent unsupervised learning models <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b44">45]</ref> that train a classifier on the network's feature activations, rather than on a ground-truth version of the output.</p><p>Using this idea, we classified the material category from predicted sound features. The classifier had class-averaged accuracy of 22.7%, and its confusion matrix is shown in <ref type="figure">Fig.  5(b)</ref>. This accuracy indicates that our model learned an output representation that was informative about material, even though it was only trained to predict sound. We applied a similar methodology to classify action categories from predicted sounds, obtaining 68.6% class-averaged accuracy (chance = 50%), and 53.5% for classifying reaction categories (chance = 20%). We found that material and reaction recognition accuracy improved with ImageNet pretraining (to 28.8% and to 55.2%, respectively), but that there was a slight decrease for action classification (to 66.5%).</p><p>We also tested whether the predicted sound features convey information about the hardness of a surface. We grouped the material classes into superordinate hard and soft classes, and trained a classifier on real sound features (sampling 1300 examples per class), finding that it obtained  <ref type="figure">Figure 8</ref>: Automatic sound prediction results. We show cochleagrams for a representative selection of video sequences, with a sample frame from each sequence on the left. The frame is sampled from the location indicated by the black triangle on the x-axis of each cochleagram. Notice that the algorithm's synthesized cochleagrams match the general structure of the ground truth cochleagrams. Dark lines in the cochleagrams indicate hits, which the algorithm often detects. The algorithm captures aspects of both the temporal and spectral structure of sounds. It correctly predicts staccato taps in rock example and longer waveforms for rustling ivy. Furthermore, it tends to predict lower pitched thuds for a soft couch and higher pitched clicks when the drumstick hits a hard wooden railing (although the spectral differences may appear small in these visualizations, we evaluate this with objective metrics in Section 6). A common failure mode is that the algorithm misses a hit (railing example) or hallucinates false hits (cushion example). This frequently happens when the drumstick moves erratically. Please see our video for qualitative results.</p><p>66.8% class-averaged accuracy (chance = 50%). Here we have defined soft materials to be {leaf, grass, cloth, plastic bag, carpet} and hard materials to be {gravel, rock, tile, wood, ceramic, plastic, drywall, glass, metal}.</p><p>We also considered the problem of directly predicting material class from visual features. In <ref type="table">Table 6</ref>(b), we trained a classifier using fc 7 features -both those of the model trained from scratch, and of a model trained on ImageNet <ref type="bibr" target="#b26">[27]</ref>. We concatenated color and spacetime image features, since we found that this improved performance. We also considered an oracle model that cropped a high-resolution (256 × 256) patch from the impact location using human annotations, and concatenated its features with those of the full image (we used color images). To avoid occlusions from the arm or drumstick, we cropped the patch from the final frame of the video. We found that performing these crops significantly increased the accuracy, suggesting that localizing the impact is important for classification. We also tried concatenating vision and sound features (similar to <ref type="bibr" target="#b1">[2]</ref>), finding that this significantly improved the accuracy.</p><p>The kinds of mistakes that the visual classifier (video → material) made were often different from those of the sound classifier (sound → material). For instance, the visual classifier was able to distinguish classes that have a very different appearance, such as paper and cloth. These classes both make low-pitched sounds (e.g., cardboard and cushions), and were sometimes are confused by the sound classifier. On the other hand, the visual classifier was more likely to confuse materials from outdoor scenes, such as rocks and leaves -materials that sound very different but which fre-quently co-occur in a scene. When we analyzed our model by classifying its sound predictions (video → sound → material), the resulting confusion matrix <ref type="figure">(Fig. 5(b)</ref>) contains both kinds of error: there are visual analysis errors when it misidentifies the material that was struck, and sound synthesis errors when it produces a sound that was not a convincing replica of the real sound.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Discussion</head><p>In this work, we proposed the problem of synthesizing visually indicated sounds -a problem that requires an algorithm to learn about material properties and physical interactions. We introduced a dataset for studying this task, which contains videos of a person probing materials in the world with a drumstick, and an algorithm based on recurrent neural networks. We evaluated the quality of our approach with psychophysical experiments and automated metrics, showing that the performance of our algorithm was significantly better than baselines.</p><p>We see our work as opening two possible directions for future research. The first is producing realistic sounds from videos, treating sound production as an end in itself. The second direction is to use sound and material interactions as steps toward physical scene understanding.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Greatest Hits: Volume 1 dataset. What do these materials sound like when they are struck? We collected 977 videos in which people explore a scene by hitting and scratching materials with a drumstick, comprising 46,577 total actions. Human annotators labeled the actions with material category labels, the location of impact, an action type label (hit vs. scratch), and a reaction label (shown on right). These labels were used only for analyzing what our sound prediction model learned, not for training it. We show images from a selection of videos from our dataset for a subset of the material categories (here we show examples where it is easy to see the material in question).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>(a) Cochleagrams for selected classes. We extracted audio centered on each impact sound in the dataset, computed our subband-envelope representation, and then estimated the mean for each class. (b) Confusion matrix derived by classifying sound features. Rows correspond to confusions made for a single category. The row ordering was determined automatically, by similarity in material confusions (see the supplementary material).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 6 :</head><label>6</label><figDesc>(a) We ran variations of the full system and an imagematching method (RGB + spacetime). For each model, we include an oracle model (labeled with "+ mat") that draws its sound examples from videos with the same material label. (b) Class-averaged material recognition accuracy obtained by training an SVM with different image and sound features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. For some categories, such as grass and leaf, participants were frequently fooled by our results. Often when a participant was fooled, it was because the sound prediction was simple and prototypical (e.g., a simple thud noise), while the actual sound was complex and atypical. True leaf sounds, for example, are highly varied and may not be fully predictable from a silent video. When they are struck, we hear a combination of the leaves themselves, along with rocks, dirt, and whatever else is underneath them. In contrast, the sounds predicted by our model tend to be closer to prototypical grass/dirt/leaf noises. Participants also sometimes made mistakes when the onset detection failed, or when multiple impacts overlapped, since this may have defied their expectation of hearing a single impact. We found that the model in which the RNN connections were broken was often unable to detect the timing of the hit, and that it under-predicted the amplitude of the sounds. As a result, it performed poorly on automated metrics and failed</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Scratches and hits usually satisfy this assumption, but splash sounds often do not -a problem that could be addressed with more sophisticated onset-detection methods<ref type="bibr" target="#b4">[5]</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">We use only the first color image to reduce the computational cost.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments. This work was supported by NSF grants 6924450 and 6926677, by Shell, and by a Microsoft Ph.D. Fellowship to A.O. We thank Carl Vondrick and Rui Li for the helpful discussions, and the workers at Middlesex Fells, Arnold Arboretum, and Mt. Auburn Cemetery for not asking too many questions while we were collecting the Greatest Hits dataset.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning to see by moving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Joint object-material category segmentation from audio-visual cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sapienza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Golodetz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Valentin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Miksik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Izadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The acquisition of physical knowledge in infancy: A summary in eight lessons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Baillargeon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Blackwell handbook of childhood cognitive development</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="46" to="83" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Material recognition in the wild with the materials in context database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Upchurch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bala</surname></persName>
		</author>
		<idno>abs/1412.0623</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A tutorial on onset detection in music signals. Speech and Audio Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Daudet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Abdallah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Duxbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Davies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">B</forename><surname>Sandler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1035" to="1047" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Were those coconuts or horse hoofs? visual context effects on identification and perceived veracity of everyday sounds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bonebright</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Auditory Display</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Statistical modeling of intrinsic structures in impacts sounds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cavaco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Lewicki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">121</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="3558" to="3568" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Freeman. Visual vibrometry: Estimating material properties from small motion in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">L</forename><surname>Bouman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rubinstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The visual microphone: passive recovery of sound from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rubinstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wadhwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Mysore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Unsupervised visual representation learning by context prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPR</publisher>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The estimation of the gradient of a density function, with applications in pattern recognition. Information Theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fukunaga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Hostetler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="32" to="40" />
			<date type="published" when="1975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">What in the world do we hear?: An ecological approach to auditory event perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">W</forename><surname>Gaver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ecological psychology</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning haptic representation for manipulating deformable food objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gemici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IROS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Derivation of auditory filter shapes from notched-noise data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">R</forename><surname>Glasberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Moore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Hearing research</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="103" to="138" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Unsupervised feature learning from temporal data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goroshin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.02518</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Spatial pattern of bold fmri activation reveals cross-modal information in auditory cortex</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-J</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Colas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kanwisher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of neurophysiology</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Speech enhancement based on wavelet thresholding the multitaper spectrum. Speech and Audio Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Loizou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="59" to="67" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Learning visual groups from co-occurrences in space and time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zoran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Adelson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06811</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning image representations tied to ego-motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jayaraman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">3d convolutional neural networks for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Conference on Multimedia</title>
		<meeting>the ACM International Conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="675" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Can one hear the shape of a drum? The american mathematical monthly</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kac</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep learning for acoustic modeling in parametric speech generation: A systematic review of existing techniques and future trends</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-Y</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-J</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">M</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Human sound source identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Lutfi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Auditory perception of sound sources</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="13" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Sound texture perception via statistics of the auditory periphery: evidence from sound synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Mcdermott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep learning from temporal coherence in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mobahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Multimodal deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Supersizing self-supervision: Learning to grasp from 50k tries and 700 robot hours</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.06825</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">The origins of inquiry: Inductive inference and exploration in early childhood</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Schulz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in cognitive sciences</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="382" to="389" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Theory of vibration: an introduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Shabana</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Recognizing materials using perceptually inspired features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rosenholtz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Adelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="348" to="371" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Black boxes: Hypothesis testing via indirect perceptual evidence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Siegel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Magid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">E</forename><surname>Schulz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th Annual Conference of the Cognitive Science Society</title>
		<meeting>the 36th Annual Conference of the Cognitive Science Society</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Interactive learning of the acoustic properties of household objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sinapov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wiemer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Stoytchev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Pattern playback in the 90s</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Slaney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">The development of embodied cognition: Six lessons from babies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gasser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial life</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="13" to="29" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Foleyautomatic: physically-based sound effects for interactive simulation and animation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">G</forename><surname>Doel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Kry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th annual conference on Computer graphics and interactive techniques</title>
		<meeting>the 28th annual conference on Computer graphics and interactive techniques</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="537" to="544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Anticipating the future by watching unlabeled video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.08023</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations using videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learning deep features for scene recognition using places database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
