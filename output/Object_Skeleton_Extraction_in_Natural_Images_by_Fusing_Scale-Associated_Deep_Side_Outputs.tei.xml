<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Object Skeleton Extraction in Natural Images by Fusing Scale-associated Deep Side Outputs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Shen</surname></persName>
							<email>wei.shen@shu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Key Laboratory of Specialty Fiber Optics and Optical Access Networks</orgName>
								<orgName type="institution">Shanghai University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Key Laboratory of Specialty Fiber Optics and Optical Access Networks</orgName>
								<orgName type="institution">Shanghai University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Jiang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Key Laboratory of Specialty Fiber Optics and Optical Access Networks</orgName>
								<orgName type="institution">Shanghai University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijiang</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Key Laboratory of Specialty Fiber Optics and Optical Access Networks</orgName>
								<orgName type="institution">Shanghai University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
							<email>xbai@hust.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="department">School of Electronic Information and Communications</orgName>
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Object Skeleton Extraction in Natural Images by Fusing Scale-associated Deep Side Outputs</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>2 Rapid-Rich Object Search Lab, Nanyang Technological University</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:44+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Object skeleton is a useful cue for object detection, complementary to the object contour, as it provides a structural representation to describe the relationship among object parts. While object skeleton extraction in natural images is a very challenging problem, as it requires the extractor to be able to capture both local and global image context to determine the intrinsic scale of each skeleton pixel. Existing methods rely on per-pixel based multi-scale feature computation, which results in difficult modeling and high time consumption. In this paper, we present a fully convolutional network with multiple scale-associated side outputs to address this problem. By observing the relationship between the receptive field sizes of the sequential stages in the network and the skeleton scales they can capture, we introduce a scale-associated side output to each stage. We impose supervision to different stages by guiding the scale-associated side outputs toward groundtruth skeletons of different scales. The responses of the multiple scaleassociated side outputs are then fused in a scale-specific way to localize skeleton pixels with multiple scales effectively. Our method achieves promising results on two skeleton extraction datasets, and significantly outperforms other competitors.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In this paper, we investigate an interesting and nontrivial problem in computer vision, object skeleton extraction from natural images <ref type="figure" target="#fig_0">(Fig. 1)</ref>. Here, the concept of "object" means a standalone thing with a well-defined boundary and center <ref type="bibr" target="#b1">[2]</ref>, such as an animal, a human, and a plane, as opposed to amorphous background stuff, such as sky, grass, and mountain. Skeleton, also called symmetry axis, is a useful structure-based object descriptor. Extracting * Corresponding author object skeletons directly from natural images is of broad interests to many real applications including object recognition/detection <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b27">28]</ref>, text recognition <ref type="bibr" target="#b33">[34]</ref>, road detection and blood vessel detection <ref type="bibr" target="#b26">[27]</ref>. Skeleton extraction from pre-segmented images <ref type="bibr" target="#b20">[21]</ref> used to be a hot topic, which has been well studied and successfully applied to shape-based object matching and recognition <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b8">9]</ref>. However, such methods have severe limitations when being applied to natural images, as segmentation from natural images is still an unsolved problem.</p><p>Skeleton extraction from natural images is a much more challenging problem. The main difficulties stem from three aspects: (1) Complexity of natural scenes: Natural scenes can be very cluttered. Amorphous background elements, such as fences, bricks and even the shadows of objects, exhibit somewhat self-symmetry, and thus are prone to cause distractions. (2) Diversity of objects: Objects in natural images may exhibit entirely different colors, textures, shapes and sizes. (3) Specificity of skeletons: local skeleton segments have a variety of patterns, such as straight lines, Tjunctions and Y-junctions. In addition, a local skeleton segment naturally associates with a certain scale, determined by the thickness of its corresponding object part. However, it is unknown in natural images. We term this problem as unknown-scale problem in skeleton extraction.</p><p>A number of works have been proposed to study this problem in the past decade. Broadly speaking, they can be categorized into two groups: (1) Traditional image processing methods <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b32">33]</ref>, which compute skeletons from a gradient intensity map according to some geometric constraints between edges and skeletons. Due to the lack of object prior, these methods can not handle the images with complex scenes; <ref type="bibr" target="#b1">(2)</ref> Recent learning based methods <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b29">30]</ref>, which learn a per-pixel classification or segment-linking model based on elaborately handdesigned features computed at multi-scales for skeleton extraction. Limited by the ability of traditional learning models and hand-designed features, these methods fail to extract the skeletons of objects with complex structures and cluttered interior textures. In addition, such per-pixel/segment models are usually quite time consuming for prediction. Consequently, there still remains obvious gap between these skeleton extraction methods and human perception, in both performance and speed. Skeleton extraction has its unique aspect by looking into both local and global image context, which requires much more powerful models in both multiscale feature learning and classifier learning, since the visual complexity increases exponentially with the size of the context field.</p><p>To tackle the obstacles mentioned above, we develop a holistically-nested network with multiple scale-associated side outputs for skeleton extraction. The holistically-nested network <ref type="bibr" target="#b30">[31]</ref> is a deep fully convolutional network (FC-N) <ref type="bibr" target="#b17">[18]</ref>, which enables holistic image training and prediction for per-pixel tasks. Here, we connect a scale-associated side output to each convolutional layer in the holisticallynested network to address the unknown-scale problem in skeleton extraction.</p><p>Referring to <ref type="figure">Fig. 2</ref>, imagine that we are using multiple filters with different sizes (such as the convolutional kernels in convolutional networks) to detect a skeleton pixel with a certain scale; then only the filters with the sizes larger than the scale will have responses on it, and others will not. Note that the sequential convolutional layers in a holistically-nested network can be treated as the filters with increasing sizes (the receptive field sizes on the original image of each convolutional layer are increasing from shallow to deep). So each convolutional layer is only able to capture the features of the skeleton pixels with scales less than its receptive field size. The sequential increasing receptive field sizes provide a principle to quantize the skeleton scale space. With these observations, we propose to impose supervision to each side output, optimizing it towards a scale-associated groundtruth skeleton map. More specifically, each skeleton pixel in it is labeled by a quantized scale value and only the skeleton pixels whose scales are smaller than the receptive filed size of the side output are reserved. Thus, each side output is associated with some certain scales and able to give a certain number of scale-specific skeleton score maps (the score map for one specified quantized scale value) when predicting.</p><p>The final predicted skeleton map can be obtained by fusing these scale-associated side outputs. A straightforward <ref type="bibr">Figure 2</ref>. Using filters (the green squares on images) of multiple sizes for skeleton extraction. Only when the size of the filter is larger than the scale of current skeleton part can the filter capture enough context feature to detect it. fusion method is to average them. However, a skeleton pixel with larger scale probably has a stronger response on a deeper side output, and a weaker response on a shallower side output; a skeleton pixel with smaller scale may have strong responses on both of the two side outputs. By considering this phenomenon, for each quantized scale value, we propose to use a scale-specific weight layer to fuse the corresponding scale-specific skeleton score map provided by each side output.</p><p>In summary, the core contribution of this paper is the proposal of the scale-associated side output layer, which enables both target learning and fusion in a scale-associated way. Therefore, our holistically-nested network is able to localize skeleton pixels with multiple scales.</p><p>To the best of our knowledge, there are only two datasets related to our task. One is the SYMMAX300 dataset <ref type="bibr" target="#b28">[29]</ref>, which is converted from the well-known Berkeley Segmentation Benchmark (BSDS300) <ref type="bibr" target="#b18">[19]</ref>. However, this dataset is used for local reflection symmetry detection. Local reflection symmetry <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b12">13]</ref> is a kind of low-level feature of image, regardless of the concept of "object". Some samples in this dataset are shown in <ref type="figure" target="#fig_1">Fig. 3(a)</ref>. Note that, a large number of symmetries occur in non-object parts. Generally, object skeleton is a subset of local reflection symmetry. The other one is the WH-SYMMAX dataset <ref type="bibr" target="#b22">[23]</ref>, which is converted from the Weizmann Horse dataset <ref type="bibr" target="#b6">[7]</ref>. This dataset is suitable to verify object skeleton extraction methods; however, as shown in <ref type="figure" target="#fig_1">Fig. 3</ref>(b) the limitation is that only one object category, the horse, is contained in it. To evaluate skeleton extraction methods, we construct a new dataset, named SK506 1 . There are 506 natural images in this dataset, which are selected from the recent published MS COCO dataset <ref type="bibr" target="#b7">[8]</ref>. The objects in these 506 images belong to a variety of categories, including humans, animals, such as birds, dogs and giraffes, and artificialities, such as planes and hydrants. We apply a skeletonization method <ref type="bibr" target="#b2">[3]</ref> to the provided human-annotated foreground segmentation maps of the selected images to generate the groundtruth skeleton maps. Some samples of the SK506 dataset are shown in <ref type="figure" target="#fig_1">Fig. 3</ref>(c). We evaluate several skeleton extraction methods as well as symmetry detection methods on both SK506 and WH-SYMMAX. The experimental results demonstrate that the proposed method significantly outperforms others.  <ref type="bibr" target="#b28">[29]</ref>. (b) The WH-SYMMAX dataset <ref type="bibr" target="#b22">[23]</ref>. (c) Our new dataset, the SK506 dataset. The groundtruths for skeleton or local reflection symmetry are in yellow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>Object skeleton extraction has been paid much attention in previous decades. However, most works in the early stage <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b2">3]</ref> only focus on skeleton extraction from presegmented images. As these works have a strict assumption that object silhouettes are required to be available, they cannot be applied in our task.</p><p>Some pioneers try to extract skeletons from the gradient intensity maps computed on natural images. The gradient intensity map is generally obtained by applying directional derivative operators to a gray-scale image smoothed by a Gaussian kernel. For instance, in <ref type="bibr" target="#b15">[16]</ref>, the author provides an automatic mechanism to determine the best size of the Gaussian kernel used for gradient computation, and he propose to detect skeletons as the pixels for which the gradient intensity assumes a local maximum (minimum) in the direction of the main principal curvature. Jang and Hong <ref type="bibr" target="#b11">[12]</ref> extract the skeleton from the pseudo-distance map which is obtained by iteratively minimizing an object function defined on the gradient intensity map. Yu and Bajaj <ref type="bibr" target="#b31">[32]</ref> propose to trace the ridges of the skeleton intensity map calculated from the diffused vector field of the gradient intensity map, which can remove the undesirable biased skeletons. Due to the lack of object prior, these methods are only able to handle the images with simple scenes.</p><p>Recent learning based skeleton extraction methods are more suitable to deal with the scene complexity problem in natural images. One type of them formulates skeleton ex-traction to be a per-pixel classification problem. Tsogkas and Kokkinos <ref type="bibr" target="#b28">[29]</ref> compute the hand-designed features of multi-scale and multi-orientation at each pixel, and employ the multiple instance learning framework to determine whether it is symmetry 2 or not. Shen et al. <ref type="bibr" target="#b22">[23]</ref> then improve their method by training MIL models on automatically learned scale-and orientation-related subspaces. Sironi et al. <ref type="bibr" target="#b26">[27]</ref> transform the per-pixel classification problem to a regression one to achieve accurate skeleton localization, which learns the distance to the closest skeleton segment in scale-space. Alternatively, another type of learning based methods aim to learn the similarity between local skeleton segments (represented by superpixel <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b13">14]</ref> or spine model <ref type="bibr" target="#b29">[30]</ref>), and link them by hierarchical clustering <ref type="bibr" target="#b14">[15]</ref>, dynamic programming <ref type="bibr" target="#b13">[14]</ref> or particle filter <ref type="bibr" target="#b29">[30]</ref>. Due to the limited power of the hand-designed features and traditional learning models, these methods are intractable to detect the skeleton pixels with large scales, as much more context information is needed to be handled.</p><p>Our method is inspired by <ref type="bibr" target="#b30">[31]</ref>, which develops a holistically-nested network for edge detection (HED). Edge detection does not face the unknown-scale problem. Using a local filter to detect an edge pixel, no matter what the size of the filter is, will have responses, either stronger or weaker. So summing up the multi-scale detection responses, which is adopted in the fusion layer in HED, is able to improve the performance of edge detection <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b23">24]</ref>, while bringing noises across the scales for skeleton extraction. There are two main differences between HED and our method. 1. We supervise the side outputs of the network with different scale-associated groundtruths, while the groundtruths in HED are the same. 2. We use different scale-specific weight layers to fuse the corresponding scalespecific skeleton score maps provided by the side outputs, while the side outputs are fused by a single weight layer in HED. Such two changes utilize multi stages in a network to explicitly detect the unknown scale, which HED is unable to handle with. With the extra supervision added to each layer, our method ais able to provide a more informative result, i.e., the predicted scale for each skeleton pixel, which is useful for other potential applications, such as object proposal detection (we will show this in Sec. 4.2.5). While the result of HED cannot be applied to such applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>In this section, we describe our methods for object skeleton extraction. First, we introduce the architecture of our holistically-nested network. Then, we discuss how to optimize and fuse the multiple scale-associated side outputs in the network for skeleton extraction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Network Architecture</head><p>The recent work <ref type="bibr" target="#b0">[1]</ref> has demonstrated that fine-tuning well pre-trained deep neural networks is an efficient way to obtain a good performance on a new task. Therefore, we basically adopt the network architecture used in <ref type="bibr" target="#b30">[31]</ref>, which is converted from VGG 16-layer net <ref type="bibr" target="#b25">[26]</ref> by adding additional side output layers and replacing fully-connected layers by fully-convolutional layers with 1 × 1 kernel size. Each fully-convolutional layer is then connected to an upsampling layer to ensure that the outputs of all the stages are with the same size. Here, we make several modifications for our task skeleton extraction: (a) we connect the proposed scale-associated side output layer to the last convolutional layer in each stage except for the first one, respectively conv2 2, conv3 3, conv4 3, conv5 3. The receptive field sizes of the scale-associated side output layers are 14, 40, 92, 196, respectively. The reason why we omit the first stage is that the receptive field size of the last convolutional layer in it is too small (only 5) to capture any skeleton features. There are few skeleton pixels with scales less than such a small receptive field size. (b) Each scale-associated side output layer provides a certain number of scale-specific skeleton score maps. Each scale-associated side output layer is connected to a slice layer to obtain the skeleton score map for each scale. Then from all the scale-associated side output layers, we use a scale-specific weight layer to fuse the skeleton score maps for this scale. Such a scale-specific weight layer can be achieved by a fully-convolutional layer with 1 × 1 kernel size. In this way, the skeleton score maps for different scales are fused by different weight layers. The fused skeleton score maps for each scale are concatenated together to form the final predicted skeleton map. To sum up, our holistically-nested network architecture has 4 stages with additional scale-associated side output layers, with strides 2, 4, 8 and 16, respectively, and with different receptive field sizes; it also has 5 additional weight layers to fuse the side outputs. An illustration for the network architecture is shown in <ref type="figure" target="#fig_2">Fig. 4.</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Skeleton Extraction by Fusing Scale-associated Side Outputs</head><p>Skeleton extraction can be formulated as a per-pixel classification problem. Given a raw input image X = {x j , j = 1, . . . , |X|}, our goal is to predict its skeleton mapŶ = {ŷ j , j = 1, . . . , |X|}, whereŷ j ∈ {0, 1} denotes the predicted label for each pixel x j , i.e., if x j is predicted as a skeleton pixel,ŷ j = 1; otherwise,ŷ j = 0. Next, we describe how to learn and fuse the scale-associated side outputs in the training phase as well as how to utilize the learned network in the testing phase, respectively. . The proposed network architecture for skeleton extraction, which is converted from VGG 16-layer net <ref type="bibr" target="#b25">[26]</ref>. It has 4 stages with additional scale-associated side output layers connected to the convolutional layers. Each scale-associated side output is guided by a scale-associated groundtruth skeleton map (The skeleton pixels with different quantized scales are in different colors.). Each scale-associated side output layer provides a certain number of scale-specific skeleton score maps (identified by stage numberquantized scale value pairs). The score maps of the same scales from different stages will be sliced and concatenated. Five scalespecific weighted-fusion layers are added to automatically fuse outputs from multiple stages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Training Phase</head><p>We are given a training dataset denoted by S = {(X (n) , Y (n) ), n = 1, . . . , N }, where X (n) = {x Skeleton scale quantization. According to the definition of skeleton <ref type="bibr" target="#b5">[6]</ref>, we define the scale of each skeleton pixel as the diameter of the maximal disk centered at it, which can be obtained when computing the groundtruth skeleton map from the pre-segmented image. By defining the scale of each non-skeleton pixel to be zero, we build a scale map S (n) = {s (n) j , j = 1, . . . , |X (n) |} for each Y (n) and we have y</p><formula xml:id="formula_0">(n) j = 1(s (n) j &gt; 0), where 1(·)</formula><p>is an indicator function. As we consider each image holistically, we drop the superscript n in our notation. We aim to learn a holisticallynested network with multiple stages of a convolutional layer linked with a scale-associated side output layer. Assume that there are M such stages in our network, in which the receptive field sizes of the convolutional layers increase sequentially. Let (r i ; i = 1, . . . , M ) be the sequence of the receptive field sizes. Recall that only when the receptive field size is larger than the scale of a skeleton pixel can the convolutional layer capture the features of it. Thus, the s-cale of a skeleton pixel can be quantized into a discrete value, to indicate which stages in the network are able to detect this skeleton pixel. (Here, we assume that r M is sufficiently large for capturing the features of the skeleton pixels with the maximum scale). The quantized value z of a scale s is computed by</p><formula xml:id="formula_1">z = arg min i=1,...,M i, s.t. r i &gt; λs if s &gt; 0 0 if s = 0 ,<label>(1)</label></formula><p>where λ &gt; 1 is a factor to ensure that the receptive field sizes are sufficiently large for feature computation. Scale-associated side output learning. The groundtruth skeleton map Y can be trivially converted from Z: Y = 1(Z &gt; 0), but not vice versa. So we would like to guild the network training by Z instead of Y , as more supervision can be included. This actually convert a binary classification problem to a multi-class classification, where each class corresponds a quantized scale. Towards this end, each side output layer in our network is associated with a softmax regression classifier. While according to the above discussions, one stage in our network is only able to detect the skeleton pixels with scales less than its corresponding receptive field size. Therefore, the side output is scale-associated. For the i-th side output, we guild it to a scale-associated groundtruth skeleton map:</p><formula xml:id="formula_2">Z (i) = Z • 1(Z ≤ i),</formula><p>where • is an element-wise product operator. Let K (i) denote the maximum value in Z (i) , i.e., K (i) = i, then we have</p><formula xml:id="formula_3">Z (i) = {z (i) j , j = 1, . . . , |X|}, z (i) j ∈ {0, 1, . . . , K (i) }. Let ℓ (i)</formula><p>s (W, Φ (i) ) denote the loss function for this scale-associated side output, where W and Φ (i) are the layer parameters of the network and the parameters of the classifier of this stage. As our network enables holistic image training, the loss function is computed over all pixels in the training image X and the scale-associated groundtruth skeleton map Z (i) . Generally, the distribution of skeleton pixels with different scales and non-skeleton pixels is biased in an image. Therefore, we define a weighted softmax loss function to balance the loss between these multiple classes:</p><formula xml:id="formula_4">ℓ (i) s (W, Φ (i) ) = − 1 |X| |X| j=1 K (i) k=0 β (i) k 1(z (i) j = k) log Pr(z (i) j = k|X; W, Φ (i) ),<label>(2)</label></formula><p>where β (i) k is the loss weight for the k-th class and Pr(z (i) j = k|X; W, Φ (i) ) ∈ [0, 1] is the predicted score given by the classifier for how likely the quantized scale of x j is k. N (·) denotes the number of non-zero elements in a set, then β k can be computed by</p><formula xml:id="formula_5">β (i) k = 1 N (1(Zi==k)) K (i) k=0 1 N (1(Zi==k))</formula><p>.</p><p>(</p><p>Let a (i) jk be the activation of the i-th side output associated with the quantized scale k for the input x j , then we use the softmax function <ref type="bibr" target="#b4">[5]</ref> σ(·) to compute Pr(z</p><formula xml:id="formula_7">(i) j = k|X; W, Φ (i) ) = σ(a (i) jk ) = exp(a (i) jk ) K (i) k=0 exp(a (i) jk )</formula><p>.</p><p>(4) One can show that the partial derivation of ℓ </p><formula xml:id="formula_8">(i) s (W, Φ (i) ) w.r.t. a (i) jl (l ∈ {0, 1, . . . , K (i) }) can be obtained by ∂ℓ (i) s (W, Φ (i) ) ∂a (i) jl = − 1 m β (i) l 1(z (i) j = l)− K (i) k=0 β (i) k 1(z (i) j = k)Pr(z (i) j = l|X; W, Φ (i) ) .</formula><formula xml:id="formula_9">L s (W, Φ) = M i=1 ℓ (i) s (W, Φ (i) ).<label>(6)</label></formula><p>Multiple scale-associated side outputs fusion. For an input pixel x j , each scale-associated side output provides a predicted score Pr(z (i) j = k|X; W, Φ (i) ) (if k≤K (i) ) for representing how likely its quantized scale is k. We can obtain a fused score f jk by simply summing them with weights a k = (a </p><formula xml:id="formula_10">f jk = M i=max(k,1) a (i) k Pr(z (i) j = k|X; W, Φ (i) ), s.t. M i=max(k,1) a (i) k = 1.<label>(7)</label></formula><p>We can understand the above fusion by this way: each scaleassociated side output provides a certain number of scalespecific predicted skeleton score maps, and we utilize M +1 scale-specific weight layers: A = (a k ; k = 0, . . . , M ) to fuse them. Similarly, we can define a fusion loss function by</p><formula xml:id="formula_11">L f (W, Φ, A) = − 1 |X| |X| j=1 M k=0 β k 1(z j = k) log Pr(z j = k|X; W, Φ, a k ),<label>(8)</label></formula><p>where β k is defined by the same way in Eqn. 3 and Pr(z j = k|X; W, Φ, w k ) = σ(f jk ).</p><p>Finally, we can obtain the optimal parameters by</p><formula xml:id="formula_12">(W, Φ, A) * = arg min(L s (W, Φ) + L f (W, Φ, A)).<label>(9)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Testing Phase</head><p>Given a testing image X = {x j , j = 1, . . . , |X|}, with the learned network (W, Φ, A) * , its predicted skeleton map Y = {ŷ j , j = 1, . . . , |X|} is obtained bŷ</p><formula xml:id="formula_13">y j = 1 − Pr(z j = 0|X; W * , Φ * , a 0 * ).<label>(10)</label></formula><p>Recall that z j = 0 and z j &gt; 0 mean that x j is a non-skeleton/skeleton pixel, respectively. We refer to our method as FSDS, for fusing scale-associated deep side outputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Understanding of the Proposed Method</head><p>To understand our method more deeply, we illustrate the intermediate results of our method and compare with those of HED in <ref type="figure" target="#fig_7">Fig. 5</ref>. The response of each scale-associated side output can be obtained by the similar way of Eqn. 10. We compare the response of each scale-associated side output to the corresponding one in HED (The side output 1 in HED is connected to conv1 2, while ours start from con-v2 2.). With the extra scale-associated supervision, the responses of our side outputs are indeed related to scale. For example, the first one fires on the structure with small scales, such as the legs, the interior textures and the object boundaries; while in the second one, the skeleton parts of the head and neck are clear and meanwhile the noises on small scale structure are suppressed. In addition, we perform scale-specific fusion, by which each fused scale-specific skeleton score map indeed corresponds to one scale (See the first three response maps corresponding to legs, neck and torso respectively). The side outputs in HED are not able to differentiate skeleton pixels with different scales. Consequently, the first two respond on the whole body, which bring noises to the final fusion one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results</head><p>In this section we discuss the implementation details and compare the performance of our skeleton extraction methods with competitors. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation Details</head><p>Our architecture is built on the public available implementation of FCN <ref type="bibr" target="#b17">[18]</ref> and HED <ref type="bibr" target="#b30">[31]</ref>. The whole network is fine-tuned from an initialization with the pre-trained VG-G 16-layer net <ref type="bibr" target="#b25">[26]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model parameters</head><p>The hyper parameters of our network include: mini-batch size(10), base learning rate (1 × 10 −6 ), loss weight for each side-output (1), momentum (0.9), initialization of the nested filters(0), initialization of of the scale-specific weighted fusion layer (1/n, where n is the number of sliced scale-specific map), the learning rate of the scale-specific weighted fusion layer (5 × 10 −6 ), weight decay (2 × 10 −4 ), maximum number of training iterations (20, 000).</p><p>Data augmentation Data augmentation is a principal way to generate sufficient training data for learning a "good" deep network. We rotate the images to 4 different angles (0 • , 90 • , 180 • , 270 • ) and flip with different axis(updown,left-right,no flip), then resize images to 3 different scales (0.8, 1.0, 1.2), totally leading to an augmentation factor of 36. Note that when resizing a groundtruth skeleton map, the scales of the skeleton pixels in it should be multiplied by a resize factor accordingly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Performance Comparison</head><p>We conduct our experiments by comparing our method FSDS with many others, including a traditional image processing method (Lindeberg's method <ref type="bibr" target="#b15">[16]</ref>), three learning based segment linking methods ( Levinshtein's method <ref type="bibr" target="#b14">[15]</ref>, Lee's method <ref type="bibr" target="#b13">[14]</ref> and Particle Filter <ref type="bibr" target="#b29">[30]</ref>), three per-pixel classification/regression methods (Distance Regression <ref type="bibr" target="#b26">[27]</ref>, MIL <ref type="bibr" target="#b28">[29]</ref> and MISL <ref type="bibr" target="#b22">[23]</ref>) and a deep learning based method (HED <ref type="bibr" target="#b30">[31]</ref>). For all theses meth-ods, we use the source code provided by authors under default setting. For HED and FSDS, we perform iterations of sufficient numbers to obtain optimal models, 15, 000 and 18, 000 iterations for FSDS and HED, respectively. We apply a standard non-maximal suppression algorithm <ref type="bibr" target="#b9">[10]</ref> to the response maps of HED and ours to obtain the thinned skeletons for performance evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Evaluation Protocol</head><p>We follow the evaluation protocol used in <ref type="bibr" target="#b28">[29]</ref>, under which the performances of skeleton extraction methods are measured by their maximum F-meansure ( 2·Precision·Recall Precision+Recall ) as well as precision-recall curves with respect to the groundtruth skeleton map. To obtain the precision-recall curves, the detected symmetry response is first thresholded into a binary map, which is then matched with the groundtruth skeleton map.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">SK506</head><p>We first conduct our experiments on our newly built SK506 Dataset. Object skeletons in this dataset have large variances in both structures and scales. We split this dataset into 300 training and 206 testing images. We report the Fmeasure as well as the average runtime per image of each method on this dataset in <ref type="table">Table.</ref> 1. Observed that, both traditional image processing and per-pixel/segment learning methods perform not well, indicating the difficulty of this task. In addition, the segment linking methods are extremely time consuming. Our method FSDS outperforms others significantly, even compared with the deep learning based method HED. Besides, thanks to the powerful convolution computation ability of GPU, our method can process images in real time, about 20 images per second. The precision/recall curves shown in <ref type="figure">Fig. 6</ref> evidence again that FSDS is better than the competitors, as ours shows both improved recall and precision at most of the precision-recall regimes. We illustrate the skeleton extraction results obtained by several methods in <ref type="figure" target="#fig_8">Fig. 7</ref> for qualitative comparison. These qualitative examples show that our method hits on more groundtruth skeleton points and meanwhile suppresses the false positives. The false positives in the results of HED are probably introduced across response of different scales. Benefited from scale-associated learning and scale-specific fusion, our method is able to suppress such false positives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">WH-SYMMAX</head><p>The WH-SYMMAX dataset <ref type="bibr" target="#b22">[23]</ref> totally contains 328 images, among which the first 228 are used for training and the rest are used for testing. The precision/recall curves of skeleton extraction methods are shown in <ref type="figure">Fig. 8</ref> and summary statistics are in <ref type="table" target="#tab_1">Table 2</ref>. Qualitative comparisons are </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>F-measure Avg Runtime (sec)</p><p>Lindeberg <ref type="bibr" target="#b15">[16]</ref> 0.227 4.03 Levinshtein <ref type="bibr" target="#b14">[15]</ref> 0.218 144.77 Lee <ref type="bibr" target="#b13">[14]</ref> 0.252 606.30 Particle Filter <ref type="bibr" target="#b29">[30]</ref> 0.226 322.25 † MIL <ref type="bibr" target="#b28">[29]</ref> 0.392 42.38 HED <ref type="bibr" target="#b30">[31]</ref> 0.542 0.05 † FSDS (ours) 0.623 0.05 † <ref type="figure">Figure 6</ref>. Evaluation of skeleton extractors on the SK506 dataset.</p><p>Leading skeleton extraction methods are ranked according to their best F-measure with respect to groundtruth skeletons. Our method, FSDS achieves the top result and shows both improved recall and precision at most of the precision-recall regime. See <ref type="table" target="#tab_0">Table 1</ref> for more details about the other two quantities and method citations. given in the Supp.. Both quantitative and qualitative results demonstrate that our method is clearly better than others.  <ref type="bibr" target="#b14">[15]</ref> 0.174 105.51 Lee <ref type="bibr" target="#b13">[14]</ref> 0.223 716.18 Particle Filter <ref type="bibr" target="#b29">[30]</ref> 0.334 13.9 † Distance Regression <ref type="bibr" target="#b26">[27]</ref> 0.103 5.78 MIL <ref type="bibr" target="#b28">[29]</ref> 0.365 51.19 MISL <ref type="bibr" target="#b22">[23]</ref> 0.402 78.41 HED <ref type="bibr" target="#b30">[31]</ref> 0.732 0.06 † FSDS (ours) 0.769 0.07 †</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.4">Cross Dataset Generalization</head><p>One may concern the scale-associated side outputs learned from one dataset might lead to higher generalization error when applying them to another dataset. To explore whether this is the case, we test the model learned from one dataset on another one. For comparison, we list the cross dataset generalization results of MIL <ref type="bibr" target="#b28">[29]</ref>, HED <ref type="bibr" target="#b30">[31]</ref> and our method in <ref type="table" target="#tab_2">Table 3</ref>. Our method achieves better cross dataset generalization results than both the "nondeep" method (MIL) and the "deep" method (HED). <ref type="figure">Figure 8</ref>. Evaluation of skeleton extractors on the WH-SYMMAX Dataset <ref type="bibr" target="#b22">[23]</ref>. Leading skeleton extraction methods are ranked according to their best F-measure with respect to groundtruth skeletons. Our method, FSDS achieves the top result and shows both improved recall and precision at most of the precision-recall regime. See <ref type="table" target="#tab_1">Table 2</ref> for more details about the other two quantities and method citations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.5">Usefulness Verification</head><p>To verify the usefulness of the extracted skeletons, we do an experiment on object proposal detection. Let h E B be , where ǫ is a very small number to ensure the denominator to be non-zero, and M is a part mask reconstructed by a detected skeleton segment. As shown in <ref type="figure" target="#fig_9">Fig. 9</ref>, the new scoring method achieves a better object proposal detection result than Edge Boxes. We also show that the extracted skeleton can be used for symmetric part segmentation in the Supp..</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We have presented a fully convolutional network with multiple scale-associated side outputs to extract skeletons from natural images. By pointing out the relationship between the receptive field sizes of the sequential stages in the network and the skeleton scales they can capture, we emphasized the important roles of the proposed scaleassociated side outputs in (1) guiding multi-scale feature learning and (2) fusing scale-specific responses from different stages. The encouraging experimental results demonstrate the effectiveness of the proposed method for skeleton extraction from natural images. It achieves significant improvements to all other competitors.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Object skeleton extraction in natural images. The skeletons are in yellow.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Some samples from three datasets. (a) The SYM-MAX300 dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4</head><label>4</label><figDesc>Figure 4. The proposed network architecture for skeleton extraction, which is converted from VGG 16-layer net [26]. It has 4 stages with additional scale-associated side output layers connected to the convolutional layers. Each scale-associated side output is guided by a scale-associated groundtruth skeleton map (The skeleton pixels with different quantized scales are in different colors.). Each scale-associated side output layer provides a certain number of scale-specific skeleton score maps (identified by stage numberquantized scale value pairs). The score maps of the same scales from different stages will be sliced and concatenated. Five scalespecific weighted-fusion layers are added to automatically fuse outputs from multiple stages.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>. . . , |X (n) |} is a raw input image and Y (n) = {y (n)j , j = 1, . . . , |X (n) |} (y (n) j ∈ {0, 1})is its corresponding groundtruth skeleton map. First, we describe how to compute a quantized skeleton scale map for each training image, which will be used for guiding the network training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>(We set λ = 1.2 in our experiments.) Now, for an image X, we can build a quantized scale value map Z = {z j , j = 1, . . . , |X|}}(z j ∈ {0, 1, . . . , M }).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Φ</head><label></label><figDesc>= (Φ (i) ; i = 1, . . . , M ) denotes the parameters of the classifiers in all the stages, then the loss function for all the side outputs is simply obtained by</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>k</head><label></label><figDesc>; i = max(k, 1), . . . , M ):</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 .</head><label>5</label><figDesc>The comparison between the intermediate results of FS-DS and HED. We can observe that the former are able to differentiate skeleton pixels with different scales, while the latter cannot.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 .</head><label>7</label><figDesc>Illustration of skeleton extraction results on the SK506 dataset for several selected images. The groundtruth skeletons are in yellow and the thresholded extraction results are in red. Thresholds were optimized over the whole dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 .</head><label>9</label><figDesc>Object proposal results on ETHZ Shape Classes<ref type="bibr" target="#b10">[11]</ref>. (a) The curve (IoU = 0.7). (b) Examples. Groundtruth (green), the closest proposal to groundtruth of Edgebox (red) and ours (blue).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 .</head><label>1</label><figDesc>Performance comparison between different methods on the SK506 dataset. †GPU time.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 .</head><label>2</label><figDesc>Performance comparison between different methods on the WH-SYMMAX Dataset<ref type="bibr" target="#b22">[23]</ref>. †GPU time.</figDesc><table>Method 
F-measure Avg Runtime (sec) 

Lindeberg [16] 
0.277 
5.75 
Levinshtein </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 .</head><label>3</label><figDesc>Cross-dataset generalization results. TRAIN/TEST indicates the training/testing dataset used. the objectness score of a bounding box B obtained by Edge Boxes [35], we define our objectness score by h B = ∀M∩B =∅ M∩B ∀M∩B =∅ M+ǫ ·h E B</figDesc><table>Method 
Train/Test 
F-measure 

MIL [29] 
SK506/WH-SYMMAX 
0.363 
HED [31] 
SK506/WH-SYMMAX 
0.637 
FSDS (ours) SK506/WH-SYMMAX 
0.692 
MIL [29] 
WH-SYMMAX/SK506 
0.387 
HED [31] 
WH-SYMMAX/SK506 
0.492 
FSDS (ours) WH-SYMMAX/SK506 
0.529 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">http://wei-shen.weebly.com/uploads/2/3/8/2/23825939/sk506.zip</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Although symmetry detection is not the same problem as skeleton extraction, we also compare the methods for it with ours, as skeleton can be considered a subset of symmetry.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Analyzing the performance of multilayer neural networks for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="329" to="344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">What is an object?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Alexe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="73" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Skeleton pruning by contour partitioning with discrete curve evolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Latecki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="449" to="462" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Active skeleton for non-rigid object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Latecki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="575" to="582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bishop</surname></persName>
		</author>
		<title level="m">Pattern Recognition and Machine Learning</title>
		<meeting><address><addrLine>Newyork, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Models for the perception of speech and visual form, chapter A Transformation for extracting new descriptors of shape</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Blum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1967" />
			<publisher>MIT Press</publisher>
			<biblScope unit="page" from="363" to="380" />
			<pubPlace>Boston, MA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Class-specific, top-down segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Borenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ullman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="109" to="124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Microsoft COCO captions: Data collection and evaluation server</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1405.0312</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Object recognition as many-to-many feature matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Demirci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shokoufandeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Keselman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bretzner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Dickinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="203" to="222" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Fast edge detection using structured forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1558" to="1570" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Object detection by contour segment networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="14" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A pseudo-distance map for the segmentation-free skeletonization of gray-scale images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-S</forename><surname>Hong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="18" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Curved glide-reflection symmetry detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="266" to="278" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Detecting curved symmetric parts using a deformable disc model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Dickinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1753" to="1760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multiscale symmetric part detection and grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Levinshtein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Dickinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="2162" to="2169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Edge detection and ridge detection with automatic scale selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lindeberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="117" to="156" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Computational symmetry in computer vision and computer graphics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Now publishers Inc</publisher>
			<pubPlace>Hanover, MA, US-A</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="416" to="425" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multi-scale improves boundary detection in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="533" to="545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A survey on skeletonization algorithms and their applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">K</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Borgefors</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Di Baja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Recognition of shapes by editing their shock graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">B</forename><surname>Sebastian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">N</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">B</forename><surname>Kimia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="550" to="571" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multiple instance subspace learning via partial random projection tree for local reflection symmetry in nature images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="266" to="278" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deepcontour: A deep convolutional feature learned by positivesharing loss for contour detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3982" to="3991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Shock graphs and shape matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Siddiqi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shokoufandeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Dickinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Zucker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="13" to="32" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition. CoRR, abs/1409.1556</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Multiscale centerline detection by learning a scale-space distance transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sironi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2697" to="2704" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Skeleton Search: Categoryspecific object recognition and segmentation using a skeletal shape model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">H</forename><surname>Trinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">B</forename><surname>Kimia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="215" to="240" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning-based symmetry detection in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tsogkas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="41" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Local symmetry detection in natural images using a particle filtering approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Widynski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Moevus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mignotte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5309" to="5322" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Holistically-nested edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1395" to="1403" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A segmentation-free approach for skeletonization of gray-scale images via anisotropic vector diffusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Bajaj</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="415" to="420" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Accurate centerline detection and line width estimation of thick lines using the radon transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Couloigner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="310" to="316" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Symmetry-based text line detection in natural scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2558" to="2567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Edge boxes: Locating object proposals from edges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="391" to="405" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
