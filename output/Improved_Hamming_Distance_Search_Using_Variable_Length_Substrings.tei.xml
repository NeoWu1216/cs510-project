<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improved Hamming Distance Search using Variable Length Hashing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eng-Jon</forename><surname>Ong</surname></persName>
							<email>e.ong@surrey.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Centre for Vision, Speech and Signal Processing</orgName>
								<orgName type="institution">University of Surrey</orgName>
								<address>
									<settlement>Guildford</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miroslaw</forename><surname>Bober</surname></persName>
							<email>m.bober@surrey.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Centre for Vision, Speech and Signal Processing</orgName>
								<orgName type="institution">University of Surrey</orgName>
								<address>
									<settlement>Guildford</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Improved Hamming Distance Search using Variable Length Hashing</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:41+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper addresses the problem of ultra-large-scale search in Hamming spaces. There has been considerable research on generating compact binary codes in vision, for example for visual search tasks. However the issue of efficient searching through huge sets of binary codes remains largely unsolved. To this end, we propose a novel, unsupervised approach to thresholded search in Hamming space, supporting long codes (e.g. 512-bits) with a wide-range of Hamming distance radii. Our method is capable of working efficiently with billions of codes delivering between one to three orders of magnitude acceleration, as compared to prior art. This is achieved by relaxing the equal-size constraint in the Multi-Index Hashing approach, leading to multiple hash-tables with variable length hash-keys. Based on the theoretical analysis of the retrieval probabilities of multiple hash-tables we propose a novel search algorithm for obtaining a suitable set of hash-key lengths. The resulting retrieval mechanism is shown empirically to improve the efficiency over the state-of-the-art, across a range of datasets, bit-depths and retrieval thresholds.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>At present, there is a need for efficient searching in image datasets that are increasingly larger in size, ranging from millions (e.g.Flickr 1M <ref type="bibr" target="#b4">[5]</ref>, ImageNet <ref type="bibr" target="#b0">[1]</ref>) to a billion images (e.g. ANN1B <ref type="bibr" target="#b6">[7]</ref>). Tackling this problem requires two important mechanisms: 1) efficient image representation and 2) the ability to quickly search inside the representation space.</p><p>This paper addresses the second part of efficient searching. For our purposes, we tackle the task of fast large-scale retrieval of compact binary vectors, posed as a thresholded Hamming distance search. To achieve this, we propose a novel unsupervised approach for performing thresholded search in Hamming space. Our approach is able to cope efficiently with binary code dimensionalities that are large (e.g. 512-bits) and a wide range of Hamming distance radii.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Background Review</head><p>There exists a large body of work on generating binary codes for the purpose of large scale image retrieval, in particular the method of hashing <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b13">14]</ref>. They aim to extract hashing functions that binarise high dimensional feature vectors into compact binary codes.</p><p>Nonetheless, the problem of efficiently searching through a large dataset of binary vectors remains. A linear scan is usually used at this stage, which can be accelerated by the build-in CPU hardware instructions. However, for large datasets (hundreds of millions, billions), the linear search time for a single query is still in the order of minutes. One solution is the hierarchical decomposition of the search space using multiple trees <ref type="bibr" target="#b7">[8]</ref>. Hashing using binary codes has also been used for fast approximate nearest neighbour search in image retrieval <ref type="bibr" target="#b12">[13]</ref>, where, the binary code is the hashtable key. Retrieval of related examples to a query example are the colliding hashtable entries. However, this approach is only applicable when the dimensionality of the binary code is small (i.e. less than 32), otherwise, the memory footprint of the hashtable itself becomes prohibitively large. Our work differs from this in that we can still cope with large dimensionalities.</p><p>Another approach is to divide the binary code into smaller segments and build multiple hashtables, leading to the Multi-Index Hashing (MIH) approach <ref type="bibr" target="#b2">[3]</ref> and its use for large scale search by Norouzi et al. <ref type="bibr" target="#b8">[9]</ref>. Here, a binary code is divided into equally sized substrings and separate hashtables are built from them. The configuration of substring lengths and their number is selected such that a superset of relevant examples (i.e. within some r-neighbourhood in Hamming space) are returned. Examples that are above distance r are then removed using linear scan. The resulting search speeds were significantly faster than linear scan. However, this speedup is possible only for small Hamming thresholds r. When r increases, the time spent on removing inaccurate retrievals increases very quickly and eventually, becomes very similar to exhaustive linear scans. This is due to the constraint of equal length strings.</p><p>Our work removes this constraint and we show how this improves in the retrieval time compared to the MIH approach, across the a large range of Hamming thresholds. Simultaneously, our approach also provides the option for a faster approximate search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.">Contributions and Overview</head><p>In this paper, we propose a novel approach which delivers significant efficiency improvements over the existing multi-index hashing methods, used for large scale retrieval of nearest neighbours binary vectors in Hamming metric spaces. This is achieved through the following contributions:</p><p>• Extension of the MIH method to support variable length hashkeys for the different hashtables.</p><p>• Theoretical analysis of using variable length hashkeys.</p><p>• Novel tree-based search method for locating nearoptimal set of substring lengths.</p><p>To our knowledge, we are not aware of any other work that uses multiple hashtables with variable length hashkeys for efficient retrieval. We will show that this gives a retrieval mechanism that is significantly more efficient than the state-of-the-art multi-index hashing method with no loss of accuracy.</p><p>In the rest of the paper, Section 2 details the problem statement that we tackle in this paper. The hashtable-based retrieval mechanism is described in Section 3, it also derives detailed retrieval probabilities from a combinatorial perspective. Section 4 derives upper and lower bounds for the retrieval probabilities. This leads to an efficient treebased method for finding suitable hashkey lengths, where the theoretical bounds provide crucial pruning criteria for efficient search. We then provide experimental results, evaluating the performance of the proposed method in comparison to state-of-the-art approaches in Section 5. Finally, we conclude in Section 6. To aid clarity, most of the proofs for lemmas and theorems are given in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Problem Statement</head><p>In this section, we provide a formal statement to the problem of thresholded Hamming distance search. Let us be given a dataset X = {x 1 , x 2 , ..., x N } of N number of D-dimensional binary vectors. The distance metric is assumed to be Hamming distance, denoted as || H . Let q be the D-dimensional query vector. We are also given two parameters known as the Hamming threshold, θ and minimum required recall rate β.</p><p>The aim of the paper is then to find a retrieval function R : {0, 1} D → 2 N that quickly returns a set (R) of database example indices given some example query q, which we denote as: R = R(q). We require that the Hamming distance between q and all the examples indexed in R be less than or equal to θ. Now, suppose that the true number of examples in X with Hamming distance less or equal to q is N + . We also allow for an approximate retrieval, where: 1 ≥ |R|/N + ≥ β.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Multiple Hashtables Retrieval</head><p>This section describes a multiple hashtable approach that allows us to efficiently retrieve examples that are within a predefined Hamming distance, θ, to a given query example. We find that the efficiency of retrieval can be further increased if we allow a small factor of false negative retrieval error to occur, denoted by the factor, ǫ ∈ [0, 1]. That is, suppose the number of examples within θ Hamming distance to the query is N θ , then we allow ǫN θ examples to be rejected. The factor ǫ is directly related to β as: ǫ = 1 − β.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">HL-Sets</head><p>In order to perform efficient retrieval from a large number of examples, multiple hashtables are used. The keys for the hashtables are obtained by dividing the D-dimensional binary feature vector into a number of M mutually exclusive substrings, each acting as a hashkey. The set of M hashkey lengths is denoted as M = (m i ) M i=1 . Note that in this work, M is a multiset, allowing us to have multiple elements with the same value. For the rest of the paper, we will denote a set of Hashkey Lengths M as a HL-set.</p><p>Previous work <ref type="bibr" target="#b2">[3]</ref> required that all the hashkey lengths be equal, with the lengths summing to the feature vector dimension, D. Here, we allow the hashkey lenghts m i to be different and do not require their sum to equal D. We show experimental evidence (Section 5) how both of the above improve the retrieval efficiency and accuracy, compared to existing work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Retrieval Mechanism using Hashtable Sets</head><p>The retrieval mechanism consists of M separate hashtables, which we denote as a hashtable set. In this work, we aim to configure the keys such that there is high probability of collisions in the hashtable for examples that have Hamming distances less than θ, whilst minimising the collisions for examples with distances greater than θ. Formally, we can think of each hashtable as a function,</p><formula xml:id="formula_0">H i : {0, 1} mi → 2 N . Associated with the i th hashtable H i , is a set of key vector dimensions D i = {d i,j } mi j=1 .</formula><p>The "dimension index set" D i can then be used to extract the hashtable key from the query binary vector.</p><p>Then, suppose we are given an input query q ∈ {0, 1} D . We first extract the substring keys (of length m i ) for each Input Query: q = (0,1,1,0,1,1,1,1,0,1,0) Here, the feature vector is split into 3 hashkeys of lengths m1, m2, m3 respectively. Each substring is a hashkey to the corresponding hashtables H1, H2, H3 respectively. Given a query vector q, its substrings are used to retrieve relevant example indices before a final union to give the final retrieved indices.</p><p>of the hashtables using their respective dimension index set D i as follows:</p><formula xml:id="formula_1">q i = (q di,j ) mi j=1 .</formula><p>The final set of retrieved examples, R, is then given as:</p><formula xml:id="formula_2">R = M i=1 H i (q i )<label>(1)</label></formula><p>An illustration of the multiple hashtable based retrieval in Eq. 1 can be seen in <ref type="figure" target="#fig_1">Fig. 1</ref>. Eq. 1 returns a superset of examples that have distances less than θ, thus, may return some with distances that are greater than θ, an equivalent of "false positives". Subsequently, the retrieved false positives are filtered by explicitly computing and thresholding based on their Hamming distances to the query example.</p><p>Thus, this paper proposes the retrieval function R described in the problem statement (Section 2) to be as follows:</p><formula xml:id="formula_3">R(q) = {i ∈ R : |q − x i | ≤ θ}</formula><p>where R is defined in Eq. 1, and x i is the i th example in the dataset described in Section 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Probability of Collisions: Combinatorial Perspective</head><p>In this section, we will consider the probability of retrieving examples within θ Hamming distance to a query using Eq. 1. More specifically, we seek to determine the probability of retrieval of an example with Hamming distance r from the query example, using M hashtables. The lengths of the hashkeys of these hashtables are given in the set M and corresponding dimension indices Q.</p><p>Considered from a combinatorial perspective, when the Hamming distance between an database example and query is r-bits, the number of valid "different bit" configurations is the binomial coefficient: D C r . Now, suppose we have a collision with the i th hashtable, this implies that at least m i bits between the query and the dataset example are the same. Thus, a collision in hashtable H i implies that there is only D−mi C r valid configurations left. Hence, the probability of a query having collisions with entries in hashtable H i is:</p><formula xml:id="formula_4">P mi (r) = D−mi C r D C r = (D − m i ) r D r<label>(2)</label></formula><p>where, for conciseness, we write the falling power as:</p><formula xml:id="formula_5">A r = A(A − 1)(A − 2)...(A − r + 1).</formula><p>Eq. 2 can be given a more convenient form by moving the variable r from the reducing power into multiplying factors (derivation details in Appendix C):</p><formula xml:id="formula_6">P mi (r) = (D − r) mi D mi<label>(3)</label></formula><p>In order to obtain the probability of a query containing a substring that collided with at least one entry in one hashtable, we use the assumption that all the substrings associated with the hashtables are independent and therefore the product rule can be used, resulting in the following probability retrieval curve P for different Hamming distances r:</p><formula xml:id="formula_7">P (r) = 1 − M i=1 (1 − P mi (r))<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Fixed Length vs Variable Length Hashkeys</head><p>Given a configuration of hashkey lengths, M, P (r) in Eq. 4 allows us to theoretically predict the probability of retrieval of an example with respect to their distances from a query example. This in turn allows one to predict the percentage of retrieved examples within distance θ from a query example. Importantly, P (r) (Eq. 4) predicts the proportion of examples that requires filtering using explicit Hamming distance computation. Consequently, we seek to minimise the probability of retrieval for Hamming distances above θ whilst maximimising the retrieval probabilities for Hamming distances below θ.</p><p>When the hashkey lengths are restricted to be equal and sum to D, the possible curves P (r) are limited. As an example, when D = 20 bits, only 5 such curves are possible, as shown in  In contrast, when variable hashkey lengths are used, the number of possible P (r) curves are increased greatly, allowing us more efficiently handle a much larger range of required threshold values. This can be seen in <ref type="figure" target="#fig_3">Fig. 2b</ref>, showing all possible retrieval probability curves generated by different HL-sets (i.e. hashkey length configurations). The increase in the number of retrieval probability curves is crucial in allowing us to effectively deal with different Hamming distance thresholds and minimum recall rates. This is illustrated in <ref type="figure" target="#fig_3">Fig. 2c</ref>, showing a retrieval probability curve that has lower probability of retrieving examples of distance greater than a threshold (5 in the figure) when variable length hashkeys are used. In particular, the retrieval probability when variable length hashkeys (red curve) is consistently lower than when fixed lenght hashkeys (blue curve) are used for Hamming distances over threshold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Approximation of Retrieval Probability</head><p>In the previous section, we have shown how the probability of a query feature vector colliding with an entry in at least one hashtable can be computed using Eq. 4. In practice, this equation has an inconvinient form involving falling factorials. Instead, we use an approximation to Eq. 4:</p><formula xml:id="formula_8">P M (r) = 1 − M i=1 1 − 1 − r D mi<label>(5)</label></formula><p>Empirical analysis on the probabilities calculated from Eq.5 show minimal deviation from Eq. 4. An example of this (D = 128) can be seen in <ref type="figure" target="#fig_4">Fig. 3a</ref>, with the difference between the 2 curves across all valid Hamming distances is shown in <ref type="figure" target="#fig_4">Fig.3b</ref>). The histogram of maximum differences between the approximate and exact curves is shown in <ref type="figure" target="#fig_4">Fig.3c)</ref>. We can see there that the majority of differences is less than 0.05. When all the possible retrieval curves given D = 128 are considered, the mean maximum difference is 0.01. In fact, it is possible to show that both curves con-verge as D increases. For the remainder of the paper, we shall denote P M (r) as the retrieval probability curve.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">HL-Set Tree-based Searching</head><p>The use of variable length hashkeys allow us to generate a large number of possible probability retrieval curves. However, the total number of possible HL-sets for dimension D is the partition number of the integer D. We find that the partition number (p(n)) for an integer n increases exponentially and can be approximated as:</p><formula xml:id="formula_9">p(n) ≈ 1 4n √ 3 exp π 2n 3</formula><p>As an example, when n = 1024, the partition number is on the order of 10 31 possible HL-sets. Thus, exhaustively searching for the optimial hashkey length set would be impossible, except for very small bit-depths. Unfortunately, many configurations do not lead to efficient retrieval. To address this issue, in this section, we propose a novel efficient tree-based search algorithm for finding efficient HL-sets given a Hamming distance threshold θ ∈ {1, ..., D} whilst resulting in retrievals that meet the minimum recall rate β ∈ [0, 1]. In order to make the tree-search efficient, pruning based on the lower bounds of retrieval probabilities will be employed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Retrieval Probability Lower Bounds</head><p>One important property is that by adding a new hashkey to an existing hashkey set, the original probability retrieval curve will be "lifted", in that the new retrieval probability values will be raised for all Hamming distances. This property establishes a lower-bound of the retrieval performance of a hashkey set and all its supersets. This in turn is crucial for the pruning criteria HL-set search algorithm proposed in the next section.  We find that adding a shorter hashkey to an existing hashkey set will raise the retrieval probability more than adding a longer hashkey over all Hamming distances: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Search Tree Pruning Criteria</head><p>In order to increase the efficiency of the search process, a pruning criteria based on the lower bounds of extending HL-sets is proposed. Firstly, we define the cost of the HLset, M = (m 1 , m 2 , ..., m n ) using the following function S:</p><formula xml:id="formula_10">S(M, θ) = D r=θ+1 P M (r)<label>(6)</label></formula><p>The value of the function S represents the sum of example proportions after the threshold distance θ. A larger value of S would require us to filter more examples by means of explicit computation of the Hamming distance. Consequently, the aim is to minimise S whilst ensuring that the minimum recall requirement β is still met.</p><p>With respect to an existing "minimal" cost S min , we find that if adding a hashkey of length m ′ to an existing set M results in a higher cost than S min , then adding any shorter hashkey will also result in a higher cost than S min . Additionally, any superset of M ∪ {m ′ } will have a higher cost than S min . More formally: <ref type="figure" target="#fig_1">(m 1 , m 2 , .</ref>.., m n ) be a set of hashkey lengths, with P M(r) its retrieval probability curve. Let θ be a given Hamming distance threshold and S min be a minimum hashkey set cost, and m ′ &lt; D be some integer. Then, if S(M ∪ {m ′ }, θ) ≥ S min , then for all This means, we can stop considering any branches, breadth or depth-wise when adding a new hashkey results in a cost greater than the current optimal cost S min .</p><formula xml:id="formula_11">Theorem 1. Let M =</formula><formula xml:id="formula_12">1 ≤ l ≤ m ′ , S(M ∪ {l}, θ) ≥ S min . Also, for any M ′ ⊃ M ∪ {l}, we have S(M ∪ {l}, θ) ≥ S min . Proof. Since S(M ∪ {m ′ }, θ) ≥ S min , we have D r=θ P M∪{m ′ } (r) ≥ S min (Eq. 6). From Lemma 2, P M∪{l} (r) ≥ P M∪{m ′ } (r) for all 0 ≤ l ≤ m ′ . Thus, it follows that S(M ∪ {l}, θ) = D r=θ P M∪{l} (r) ≥ D r=θ P M∪{m ′ } (r) ≥ S</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Search Algorithm</head><p>The proposed algorithm for obtaining a HL-set is given in Algorithm 1 (procedure SUBLENSEARCH). This algorithm aims to efficiently find a HL-set, M = (m 1 , m 2 , ..., m n ) with a minimal cost, S(M, θ) and have P M (θ) ≥ β. The search problem is tackled as an integer partitioning task, where the valid range of integers are considered in a tree-manner, implemented recursively. To this end, it considers every multiset of integers that will sum up to D. The requirement that the integers must sum to at most D is checked in line 12. The breadth search is done as a for loop in line 10, whilst depth searching performed recursively in line 24. To allow the algorithm to consider all HL-sets in a reasonable time, Theorem 1 provides a simultaneous breadth and depth pruning of irrelevant branches in Line 17 by breaking out of the for loop (line 10) and stopping the recursive call (line 24) from being called.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>This section describes the experiments conducted to evaluate the performance of the proposed method against return P M (r), S r . 32: end function related state-of-the-art methods. Specifically, we have performed experiments on thresholded Hamming distance retrieval using a number of large scale datasets. In particular, we tackle the problem of a retrieval of a percentage (β) of examples below a given Hamming distance (θ) to an example query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Datasets and experimental setup</head><p>The datasets considered in this section are as follows: 1 Billion SIFT 128D dataset <ref type="bibr" target="#b6">[7]</ref>, 1 Million SIFT dataset using 128-bits <ref type="bibr" target="#b5">[6]</ref> and Flickr 1 Million [4] dataset using 512 bits. For the ANN datasets, the provided 128D SIFT features were binarised using Gaussian random projection followed by binarisation as detailed in <ref type="bibr" target="#b1">[2]</ref> and as is consistent with the method used in <ref type="bibr" target="#b8">[9]</ref>. The Flickr 1 Million dataset used a 512D binary feature vector extracted using the Robust Visual Descriptor method <ref type="bibr" target="#b11">[12]</ref>.</p><p>For all the experiments, we will compare the perfor-mance of the proposed variable length hashkey method against the multi-index hashing (MIH) method using fixed length hashkeys and the linear scan method proposed in <ref type="bibr" target="#b2">[3]</ref>. For each dataset, 10000 examples were reserved for the test set. The remaining were used to build the search database.</p><p>In all experiments, the Hamming distance thresholds ( Θ ) considered lie in the range of [1, D/3] at increments of 16. We consider a set of required minimum recall rates: B = {0.999, 0.9, 0.8, 0.7}. Then, for each given threshold θ ∈ Θ, and required minimum recall rate β ∈ B, the appropriate HL-set is obtained using the proposed search method (Section 4.3).</p><p>Next, the required hashtables, as dictated by the HL-set are built. The feature dimensions for the hashkeys in each hashtable were randomly chosen, but ensured to only be used by a single hashtable. To compare against the MIH method, fixed hashkey lengths based on the length log 2 (N ), where N is the dataset size was used, as described in <ref type="bibr" target="#b2">[3]</ref>.</p><p>Following this, each example in the test set is used as a query q ∈ {0, 1} D . Using the retrieval mechanism from Section 3.2, with q as input, a set R, of |R| number of retrieved examples is obtained. The Hamming distance of members in R to q is then computed, allowing the extraction of the set of retrieved examples with Hamming distance equal or less than θ, R + :</p><formula xml:id="formula_13">R + = {r ∈ R : |r − q| H ≤ θ}</formula><p>To compute the recall rate to query q, the subset of examples N + with distance less or equal to θ from q in dataset D is obtained using linear scan. The recall rate is then: M RR = |R + |/|N + |, and is used to verify whether the minimum recall rate β is met. The retrieval size |R| is used to evaluate the improvement of the proposed method over linear scan and MIH.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Results and Analysis</head><p>For all the experiments, in terms of search time for the hashkey lengths, the proposed method took less than 10 seconds on a single threaded Intel Processor (2.2GHz) for most cases. However, for a small minority of cases, the search took up to 6 minutes for 512D vectors. However, the current implementation is in Python, and this hashkey search time can be significantly improved by implementation in C++. The largest memory footprint was 200GB for the 128D 1Billion ANN dataset.</p><p>The results of the experiments can be seen in <ref type="figure" target="#fig_9">Fig. 4</ref>. Firstly, as can be seen in <ref type="figure" target="#fig_9">Fig. 4a)</ref>,b),c), MIH will only satisfy a limited range of minimum recal rates. For example, for the 1Billion ANN dataset, with β = 0.999, the MIH method recovers at the required minimum recall rate uptill θ = 12. From then onwards, the required dataset is too small and rejects too many examples below the required threshold. In contrast, the proposed method always finds a suitable set of hashkey lengths that result in the β satisfied for any Hamming distance threshold θ. <ref type="table">Table 1</ref> shows the other values for other minimum recal rates and datasets. Next, we find that the hashtable retrieval is consistently more efficient than linear scan over all values of θ and β. This can be seen in <ref type="figure" target="#fig_9">Fig. 4d</ref>,e,f) showing the speedup factors of both the proposed method and MIH over linear scan. Since the MIH method uses only a constant hashkey length (log 2 (N )) for building the hashtables, it has a constant retrieval size, regardless of the required threshold θ or minimum recall rate, β. It is therefore shown as a constant horizontal line in <ref type="figure" target="#fig_9">Fig. 4a</ref>,b,c. Here, it can be seen that the proposed method retrieves sets that can be significantly smaller, ranging from 10 −7 the size of the dataset for 1 Billion examples to 10 −6 for 1 million examples. When the minimum recall rate is met, as detailed in <ref type="table">Table 2</ref>, we find that the proposed method on average returns retrieval sets that are smaller than those of MIH. However, we have found that this is not always the case. For the Flickr 1 Million dataset, we find that the MIH method returns retrieval sets that are smaller for Hamming thresholds between 100 and 150. This limitation is due to the assumption in the search method that examples are equally distributed across all Hamming distances. In the future we will aim to incorporate information on the Hamming distance distribution into the search method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions and Future Work</head><p>In this paper, we have proposed a novel, unsupervised approach to thresholded search in Hamming space, supporting long codes (e.g. 512-bits) with a wide-range of Hamming distance radii. Based on the theoretical analysis of the retrieval probabilities of multiple hash-tables we have proposed a novel tree-based search algorithm for obtaining a suitable set of hash-key lengths that guarantees a minimum required recall rate for retreival of examples below a given Hamming distance threshold. We have shown empirically that our method is capable of handling bit depths up to 512 bits and working efficiently up to a billion codes delivering resulting one to three orders of magnitude acceleration, as compared to the MIH method.</p><p>For future work, we aim to extend the variable length hashkey method for weighted Hamming distances. Additionally, more theoretical analysis is required for the accuracy bounds of the approximation to the retrieval probability used here. Finally, we have also experimentally observed that during the search process, hashkeys that are long in length or too short are always pruned in the search tree. It would be beneficial to obtain a better bounds for the range of hashkey lengths that will eventually be useful. Since P M1 (r) − P M2 (r) &gt; 0, we have shown that P M1 (r) &gt; P M2 (r) &gt; 0 when m ′ 1 &lt; m ′ 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Proof of Eq. 3</head><p>To get Eq. 3 from Eq. 2, we first expand the RHS of Eq. 2 into its factorial factors (index i dropped for convinience):</p><formula xml:id="formula_14">(D − m) r D r = (D − m)! (D − m − r)! × (D − r)! D!<label>(7)</label></formula><p>Next, we note that the first factor on the right hand side of Eq. 7 can be simplified as follows: </p><formula xml:id="formula_15">(D − m i ) r D r = (D − r) m D m<label>(8)</label></formula><p>Hence, Eq. 2 can be rewritten as:</p><formula xml:id="formula_16">P mi (r) = (D − r) mi D mi</formula></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Each hashtable takes a substring as the hashkey and returns a set of example indices with a similar key (i.e. colliding examples). The full set of retrieved examples is the union of the retrieved examples across all the M hashtables.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>An illustration of the hashtable based retrieval mechanism given in Eq. 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2a .</head><label>2a</label><figDesc>As a result, we find that there are only a small set of threshold values that will not result in the retrieval of a large number of irrelevant examples (i.e. distance greather than θ) or the rejection of a large number of relevant examples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 .</head><label>2</label><figDesc>Retrieval probability curves with fixed length hashkeys (a) and with variable hashkey lengths (b). (c) shows the advantage of fixed vs variable length hashkeys, where we require retrieval of examples less than distance 3. It can be seen that the variable length hashkeys (red curve) has lower probability of retrieving examples greater than distance 3 compared to fixed length hashkeys (blue curve).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 .</head><label>3</label><figDesc>An example of the retrieval probability curves for a hashkey length set (D = 128) using the approximation and exact calculation (a). The difference between the computed exact and approximation retrieval probability curves (b). The histogram of maximum difference between exact and approximate values over all valid retrieval curves when D = 128-bits.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Lemma 1 .</head><label>1</label><figDesc>Let (m 1 , m 2 , ..., m i , m i+1 ) be a HL-set, A j = 1 − (1 − r/D) mj and P i (r) = 1 − i j=1 A j . Then, P i (r) ≤ P i+1 (r) for all 0 ≤ r ≤ D. (Proof in Appendix A)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Lemma 2 .</head><label>2</label><figDesc>Let M = (m 1 , m 2 , ..., m n ) be a HL-set. Let m ′ 1 &lt; m ′ 2 and M 1 = M∪{m ′ 1 } and M 2 = M∪{m ′ 2 }. Let P M (r) = 1 − m∈M (1 − (1 − r/D) m ). Then, P M2 (r) &lt; P M1 (r) for all 0 ≤ r ≤ D. (Proof in Appendix B.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>min . Hence, we have shown that for all 1 ≤ l ≤ m ′ , S(M ∪ {l}, θ) ≥ S min . To show that for any M ′ ⊃ M ∪ {l}, then S(M ∪ {l}, θ) ≥ S min , we follow similar lines. Since M ′ ⊃ M ∪ {l}, then from Lemma 1, P M ′ (r) ≥ P M∪{l} (r), and so, S(M ′ , θ) = D r=θ P M ′ (r) ≥ D r=θ P M∪{l} (r) ≥ S min . Hence, we have shown that for any M ′ ⊃ M∪{l}, we have S(M ∪ {l}, θ) ≥ S min .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>β</head><label></label><figDesc>i ∈ m max , m max − 1, ..., cur , A cur = F indT P andArea(D, M ′</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 4 .</head><label>4</label><figDesc>Results for the ANN 1B SIFT dataset with 128-bit vectors, ANN 1M SIFT dataset and Flickr 1M Datasets. (a,b,c) show the achieved minimum recall rates (denoted as "True Positive Rate") for the variable length method and for the fixed length hashkey method. (d,e,f) show the speedup over linear scan for different minimum recall values across different Hamming thresholds.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>(</head><label></label><figDesc>D − r)! (D − m − r)! = (D − r)...(D − r − m + 1)(D − m − r)! (D − m − r)! = (D − r)(D − r − 1)...(D − r − m + 1) = (D − r) mThe RHS second factor of Eq. 7 is similarly simplified:(D − m)! D! = (D − m)! D(D − 1)...(D − m + 1)(D − m)! = 1 D mSubstituting both the above formulas into Eq. 7 gives:</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was supported by the European Commission under Grant 610691 (project BRIDGET) and TSB-funded CODAM project.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Hence, since P i (r) − P i+1 (r) ≤ 0, we have shown that P i (r) ≤ P i+1 (r).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Proof of Lemma 2</head><p>Proof. To prove that P M2 (r) &lt; P M1 (r), we consider the difference between P M1 (r) and P M2 (r):</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CVPR</title>
		<meeting>of CVPR</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Statistical analysis of binarized sift descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Diephuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Voloshynovskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Koval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Beekhof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ISPA</title>
		<meeting>ISPA</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="460" to="465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multi-index hsahing for information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Greene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Parnas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Procs of IEEE FOCS</title>
		<meeting>s of IEEE FOCS</meeting>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">New trends and ideas in visual concept detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Huiskes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Thomee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACM MIR</title>
		<meeting>of ACM MIR</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Hamming embedding and weak geometric consistency for large-scale image search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jegou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ECCV</title>
		<meeting>of ECCV</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Product quantization for nearest neighbor search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jegou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. PAMI</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Searching in one billion vectors: re-rank with source coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jegou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tavenard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Amsaleg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICASSP</title>
		<meeting>of ICASSP</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Fast matching of binary features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Muja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CRV</title>
		<meeting>of CRV</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="404" to="410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Fast exact serach in hamming space with multi-index hashing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Punjanio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fleet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1107" to="1119" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Locality-sensitive binary codes from shift-invariant kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Raginsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NIPS</title>
		<meeting>of NIPS</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Semantic hashing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Journal of Approximate Reasoning</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Robust and scalable aggregation of local features for ultra large scale retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Syed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bober</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICIP</title>
		<meeting>of ICIP</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Small codes and large image databases for recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE CVPR</title>
		<meeting>of IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Semi-supervised hashing for large-scale search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE PAMI</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">12</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
