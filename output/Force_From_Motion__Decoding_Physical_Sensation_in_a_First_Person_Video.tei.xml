<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Force from Motion: Decoding Physical Sensation in a First Person Video</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyun</forename><forename type="middle">Soo</forename><surname>Park</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Pennsylvania</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jyh-Jing</forename><surname>Hwang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Pennsylvania</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Shi</surname></persName>
							<email>jshi@seas.upenn.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Pennsylvania</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Force from Motion: Decoding Physical Sensation in a First Person Video</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:40+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A first-person video can generate powerful physical sensations of action in an observer. In this paper, we focus on a problem of Force from Motion-decoding the sensation of 1) passive forces such as the gravity, 2) the physical scale of the motion (speed) and space, and 3) active forces exerted by the observer such as pedaling a bike or banking on a ski turn.</p><p>The sensation of gravity can be observed in a natural image. We learn this image cue for predicting a gravity direction in a 2D image and integrate the prediction across images to estimate the 3D gravity direction using structure from motion. The sense of physical scale is revealed to us when the body is in a dynamically balanced state. We compute the unknown physical scale of 3D reconstructed camera motion by leveraging the torque equilibrium at a banked turn that relates the centripetal force, gravity, and the body leaning angle. The active force and torque governs 3D egomotion through the physics of rigid body dynamics. Using an inverse dynamics optimization, we directly minimize 2D reprojection error (in video) with respect to 3D world structure, active forces, and additional passive forces such as air drag and friction force. We use structure from motion with the physical scale and gravity direction as an initialization of our bundle adjustment for force estimation. Our method shows quantitatively equivalent reconstruction comparing to IMU measurements in terms of gravity and scale recovery and outperforms method based on 2D optical flow for an active action recognition task. We apply our method to first person videos of mountain biking, urban bike racing, skiing, speedflying with parachute, and wingsuit flying where inertial measurements are not accessible.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>A wingsuit BASE jumper, Jeb Corliss, dives from a cliff in Alps with his body-mounted GoPro camera 1 <ref type="figure">(Figure 1</ref>). This camera records a beautiful scenery that he had seen but also captures what he experienced and controlled via the camera egomotion. This egomotion is a resultant of <ref type="figure">Figure 1</ref>. This paper presents Force from Motion-decoding the sensation of 1) passive forces such as the gravity, 2) the physical scale of the motion (speed) and space, and 3) active forces exerted by the observer. We model egomotion with rigid body dynamics integrated in a bundle adjustment that allows us to recover the three sensations (left) via the physical scale and gravity aware reconstruction of the egomotion (right). physical interactions between passive forces from environments (e.g., gravity and air pressure) and active forces exerted by him to control his egomotion, e.g., angular momentum change along the roll axis to shift the heading direction. In this paper, we study a problem of Force from Motionreconstructing force and torque from an egocentric video to revive the physical sensation.</p><p>Extracting such forces requires to explicitly measure his muscle tension-the acceleration computed by a camera or inertial measurement unit (IMU) is not directly applicable to find active forces exerted by him because only net acceleration can be measured. Our key question is "can we extract his input in a form of active force and torque without measuring muscle tension from an egocentric video?" We show that it is possible to estimate an active force and torque profile that generates the egomotion. This requires to overcome three fundamental challenges: a) limited observations of body parts (body pose is often not visible from an egocentric video); b) scale and orientation ambiguity inherent in structure from motion; c) scene and activity variability (different appearance, camera placement, and motion).</p><p>We address these challenges by modeling the observed camera egomotion with rigid body dynamics that integrates three key sensations: 1) gravity force; 2) physical scale of the world; and 3) input force and torque.</p><p>The gravity force sensation is captured in the visual image itself. The gravity affects how physical environment is formed, i.e. trees and buildings are usually vertical and horizon perpendicular to gravity direction. We learn such image cues to predict a 2D gravity direction in a 2D image using a convolutional neural network designed to recognize the orientation of the image. The prediction of multiple frames is consolidated using 3D reconstructed camera orientation to estimate the 3D gravity direction. Note only the camera orientation information is needed in this step, and we are still affected by the unknown scale factor.</p><p>The physical scale of the space is important sensation since it tells us how fast we are going exactly. The absolute scale of our motion is revealed to us when the body is in a dynamically balanced state. During a banked turn, the torques produced by centripetal force and gravity force are balanced with the body leaning angle. This physical constraint together with the known gravity constant, i.e., <ref type="bibr" target="#b8">9</ref>.81 m/s 2 , allows us to compute the physical scale exactly.</p><p>The input force sensation includes 3D active force (thrust) and torque (roll and yaw). For each type of first person sport video, we construct a rigid body dynamics and model egomotion as a function of the input forces and gravity. Given the physical scale and gravity direction, we minimize the 2D geometrical reprojection error (in video) with respect to the unknown 3D world and egomotion governed by rigid body dynamics. The reconstructed camera egomotion that is corrected by physical scale and gravity direction is used for an initialization of the bundle adjustment for active force and torque estimation.</p><p>In total, our system takes an input, a first person sport video, and outputs active force and torque profile in metric scale as shown in <ref type="figure">Figure 1</ref>. We predict the 3D gravity direction by integrating 2D prediction by a convolutional neural network and recover physical scale using the roll torque equilibrium. These factors are embedded in the bundle adjustment that finds a plausible active force and torque profile that can simulate the camera egomotion via inverse dynamics while simultaneously minimizing reprojection error. Why Egocentric Video? As a form factor of a video camera facilitates seamless integration into body, hundreds of thousands of egocentric videos are captured and shared via online video repositories such as YouTube, Vimeo, and Facebook. For instance, currently more than 6,000 GoPro videos are posted in YouTube in a day. Many of these videos capture speed sport activities such as downhill mountain biking (1-10 m/s), glade skiing (5-12 m/s), skydiving (60-80 m/s) from first person view. These videos excite visual motion stimuli that are strongly dominated by physical sensation. Decoding such physical sensation provides a new computational representation of such videos that can be not only applied to vision tasks such as activity recognition, video indexing, content generation for virtual reality <ref type="bibr" target="#b31">[32]</ref> but also computational sport analytics <ref type="bibr" target="#b27">[28]</ref>, sensorimotor learning <ref type="bibr" target="#b38">[39]</ref>, and sport product design <ref type="bibr" target="#b6">[7]</ref>. Contributions This paper includes three core technical contributions. (1) Force from motion: we integrate rigid body dynamics into a bundle adjustment to estimate active force and torque profile; (2) Gravity direction estimation: we learn image cues to predict gravity direction and up-grade to 3D by employing the reconstructed camera orientations; (3) physical scale recovery: we recover a scale factor from the roll torque equilibrium relationship. We quantitatively evaluate our method using a controlled experiment with inertial measurement units (IMU). Our method shows quantitatively equivalent reconstruction comparing to IMU measurements in terms of gravity and scale recovery and outperforms method based on 2D optical flow for an active action recognition task. We apply our method to first person videos of mountain biking, urban bike racing, skiing, speedflying with parachute, and wingsuit flying where inertial measurements are not accessible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>This paper studies physics based human behavior modeling via egocentric vision. In this section, we briefly review the most related work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Human Behavior Modeling in 3rd Person View</head><p>Johansson's experiment <ref type="bibr" target="#b11">[12]</ref> has shown that human motion can be perceived and predicted by a sparse representation with short duration of visual observation. However, enabling such perception for a machine is still challenging without prior knowledge due to a large degree of freedom of an articulated body structure. This requires a compact representation to describe human body motion. While a large body of literature have studied this problem based on geometry <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b41">42]</ref> and statistical model <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b34">35]</ref>, we focus on physics based representation.</p><p>Markerless motion capture often benefits from physics based approaches 2 . Brubaker et al <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b2">3]</ref> explicitly modeled the ground reaction force as an impulse function during bipedal walking. Wei and Chai <ref type="bibr" target="#b37">[38]</ref> have shown a keyframe based human motion reconstruction where physics based simulation interpolates between keyframes. Vondrak et al. <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b35">36]</ref> introduced a feedback control system based on multibody dynamics that provides a Bayesian prior to track human body motion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Egocentric Perception</head><p>An egocentric camera is a powerful tool to understand human behaviors as it records what the camera wearer has experienced. Therefore, it is a viable solution for behavior science and quality of life technology <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b25">26]</ref>, and this motivates many vision tasks such as understanding fixation point <ref type="bibr" target="#b17">[18]</ref>, identifying eye contact <ref type="bibr" target="#b42">[43]</ref>, and localizing joint attention <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b23">24]</ref>.</p><p>An egocentric video is biased by camera egomotion which is highly discriminative for activity recognition. Fathi et al. <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref> used gaze and object segmentation cues to classify activities. 2D motion features were exploited by Kitani et al. <ref type="bibr" target="#b13">[14]</ref> to categorize and segment a first person sport video in a unsupervised manner. Coarse-to-fine  <ref type="figure">Figure 2</ref>. (a) We compute a maximum a posteriori estimate of the 3D gravity direction,ĝ ∈ S 2 . We model the prior using a mixture of von Mises-Fisher distributions and learn a likelihood function using a convolutional neural network (CNN). (b) We show the likelihood given an image with the red heatmap. The dotted lines are the ground truth gravity direction. The per pixel evidence <ref type="bibr" target="#b18">[19]</ref> is encoded as transparency, i.e., the stronger evidence, the more transparent. The CNN correctly predicts gravity direction while the last image produces 15 degree error due to the tilted bicycler. motion models <ref type="bibr" target="#b29">[30]</ref> and a pretrained convolutional neural network <ref type="bibr" target="#b30">[31]</ref> provided a strong cue to recognize activities. Yonetani et al. <ref type="bibr" target="#b43">[44]</ref> utilized a motion correlation between first and third person videos to recognize people's identity. Kopf et al. <ref type="bibr" target="#b14">[15]</ref> stabilized first person footage via 3D reconstruction of camera egomotion. In a social setting, joint attention was estimated via triangulation of multiple camera optical rays <ref type="bibr" target="#b23">[24]</ref> and the estimated joint attention was used to edit social video footage <ref type="bibr" target="#b0">[1]</ref>.</p><formula xml:id="formula_0">Prior, p(g) Likelihood, p({I } |g) i=1 F i Posterior, p(g|{I } ) i=1 F i 2  2  2  (a)</formula><p>Another information that the egocentric camera captures is exomotion or scene motion. Pirsiavash and Ramanan <ref type="bibr" target="#b24">[25]</ref> used an object centric representation and temporal correlation to recognize active/passive objects from a egocentric video, and Rogez et al. <ref type="bibr" target="#b28">[29]</ref> leveraged a prior distribution of body and hand coordination to estimate poses from a chest mounted RGBD camera. Lee et al. <ref type="bibr" target="#b16">[17]</ref> summarized a life-logging video by discovering important people and objects based on temporal correlation, and Xiong and Grauman <ref type="bibr" target="#b40">[41]</ref> utilized a web image prior to select a set of good images from egocentric videos. Fathi et al. <ref type="bibr" target="#b8">[9]</ref> used observed faces to identify social interactions and Pusiol et al. <ref type="bibr" target="#b25">[26]</ref> learned a feature that indicates joint attention in child-caregiver interactions. Our approach: To our best knowledge, this is the first paper that provides a computational framework to understand an egocentric video based on physical body dynamics. We leverage two motion cues: 1) 3D reconstruction from egomotion, and 2) gravity and scale recovery from exomotion. As an egocentric video has limited observation of body parts, estimating force and its control significantly differs from previous problems of physics based tracking and reconstruction. We introduce a novel Force from Motion method that computes the control input applied by the camera wearer. It also produces a scaled and oriented 3D reconstruction via dynamics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Force from Motion</head><p>Gravity, scale, and active force are three key ingredients that generate physical sensation in movement. In this section, we estimate these physical quantities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Gravity Direction</head><p>A natural image encodes gravity direction because it affects how physical environment is formed, i.e. trees and buildings are usually vertical and horizon perpendicular to gravity direction <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b9">10]</ref>. We exploit such image cues learned by a convolutional neural network <ref type="bibr" target="#b15">[16]</ref> to predict a gravity direction in a 2D image. This per image prediction is integrated over multiple frames by leveraging structure from motion.</p><p>We define a 3D unit gravity direction,ĝ(θ, φ) = sin θ cos φ sin θ sin φ cos θ T ∈ S 2 . We normalize the representation with respect to the instantaneous velocity direction such thatĝ(0, 0) = v/ v where v is the instantaneous velocity. This allows us to register different camera orientations in an unified coordinate system (with respect to the gravity). We compute the maximum a posteriori (MAP) estimate of the gravity direction given a set of images,</p><formula xml:id="formula_1">{I i } F i=1 : g * = argmax g∈S 2 p(ĝ|I 1 , · · · , I F ) = argmax g∈S 2 p(ĝ) F i=1 p(I i |ĝ),<label>(1)</label></formula><p>where p(ĝ) is a prior distribution of the gravity direction and a likelihood p(I i |ĝ) measures how well the 3D gravity direction is aligned with image, I i . The prior distribution encodes how the gravity is oriented with respect to the heading direction. Given a gravity direction in a training dataset 3 , we model this prior distribution using a mixture of von Mises-Fisher distributions:  where {m k , κ k } is a set of modes and concentration parameters that can be learned by an Expectation-Maximization algorithm as shown in Prior of <ref type="figure">Figure 2</ref>(a).</p><formula xml:id="formula_2">p(ĝ) = K k=1 κ k 4π sinh κ k exp κ kĝ</formula><formula xml:id="formula_3">F L F N F L mg θ b θ h mg {W} {W} {B} {B} F N T R T P F D F T T Y l v [ ] 3 P = KR I -C F F (c) Geometry</formula><p>The image likelihood, p(I i |ĝ) measures how well the projected 3D gravity direction onto the i th image agrees with the image cues learned from the training data. By the projection, we measure the orientation of the image, <ref type="formula" target="#formula_6">(3)</ref> is the camera orientation at the t th time instant. We learn this likelihood function using the convolutional neural network (CNN) proposed by Krizhevsky et al. <ref type="bibr" target="#b15">[16]</ref> with a few minor modifications. We correct the fisheye lens distortion and warp the image with a homography,</p><formula xml:id="formula_4">ξ = atan2(r T 2ĝ , r T 1ĝ ) ∈ S where R(t i ) = r T 1 r T 2 r T 3 T and R(t) ∈ SO</formula><formula xml:id="formula_5">H = KR v R(ϕ p )R(ϕ r )R T K −1</formula><p>where K and R are the camera intrinsic parameter and orientation matrices, respectively. R v is the rotation matrix whose Z axis aligns with the instantaneous velocity, v. The body coordinate system, {B} is defined in <ref type="figure" target="#fig_1">Figure 3</ref>(c), and R(ϕ p ) is the constant rotation about the pitch axis to minimize the area outside of the image. R(ϕ r ) is a rotation about the roll axis used for data augmentation. The warped image (1280 × 720) is resized to 320 × 180 as an input for the CNN. We train the network to predict a probability of the projected angle ξ discretized by 1 degree between −30 • and 30 • , i.e., ξ = 0 means the gravity direction is aligned with y axis of the image. We augment the data by rotating the image with R(ϕ r ) and its horizontal flip. <ref type="figure">Figure 2</ref>(b) illustrates the likelihood of the gravity directions learned by CNN as shown in the red heatmap and the ground truth gravity direction with dotted line.</p><p>Predictions on multiple images are consolidated by the 3D reconstructed camera orientations. Note that a single image cannot predict the 3D gravity direction due to 2D projection. Each image produces a streak in a likelihood distribution as shown in Likelihood of <ref type="figure">Figure 2</ref>(a)-any gravity direction along the streak is projected onto the same direction in 2D. The product of multiple image predictions in Equation (1) by leveraging the 3D reconstructed camera orientations can collapse the streak into a unimodal distribution 4 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Physical Scale</head><p>The leaning angle, θ b , at a banked turn is formed to balance the roll torque at the center of mass. The normal force, F N , produces a torque, T N = lF N cos θ b and the friction force, or centripetal force (no slip condition), F L produces an opposite directional torque T L = lF L sin θ b with respect to the center of mass where l is the distance between the center of mass to the ground contact point as shown in <ref type="figure" target="#fig_1">Figure 3(a) and 3(c)</ref>. These two torques must be balanced to maintain the leaning angle, i.e., the tangential velocity, v, is defined by the leaning angle and the curvature of the turn.</p><p>By equating these two torques, i.e., T L + T N = 0, we obtain the following relationship with gravity constant:</p><formula xml:id="formula_6">g = 9.81 m/s 2 = c |â x | tan θ b ,<label>(3)</label></formula><p>whereâ x is the linear acceleration in the lateral direction, which is measured from the reconstructed 3D camera trajectory in {W} <ref type="figure" target="#fig_1">(Figure 3(c)</ref>) and c is a scale factor that maps from the 3D reconstruction to the physical world. In <ref type="figure" target="#fig_1">Figure 3</ref>(b), we plot the scale factors measured from different time instances with their median and variance. The slope of the data points represents the scale factor of the reconstruction. We compute these data points along the video sequences that include a number of banked turns. <ref type="figure" target="#fig_1">Figure 3</ref>(a) shows the torques produced by the scale factor and two torques are roughly canceled out. Note that −T N is plotted for a direct comparison. This allows us to reconstruct physical dimension of the terrain and speed as shown in <ref type="figure" target="#fig_3">Figure 4(a)</ref>. Note that the speed profile is physically meaningful, i.e., average speed of the mountain biking ranges between 1-6 m/s 2 . Elevation (m) <ref type="bibr" target="#b4">5</ref> 10 15   <ref type="formula" target="#formula_12">(7)</ref>. (b) The bundle adjustment in Equation <ref type="formula" target="#formula_12">(7)</ref> produces plausible active force and torque profile that produces a camera trajectory concerting with the video ((c) and (d)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Physics of Rigid Body Dynamics</head><p>A single rigid body that undergoes motion as a resultant of forces and torque can written as:</p><formula xml:id="formula_7">ma = F in + F ex (4) J α + ω × J ω = T in + T ex ,<label>(5)</label></formula><p>where m is mass, a ∈ R 3 is linear acceleration, α ∈ R 3 is angular acceleration, J ∈ R 3×3 is moment of inertia, and ω ∈ R 3 is angular velocity. We denote F in and T in as active force and torque that are applied by the camera wearer (input signal). F ex and T ex are passive force and torque that are applied by external sources such as gravitation force, centripetal force, and pitch moment created by an unbalance impact between two wheels in a bicycle as shown in <ref type="figure" target="#fig_1">Figure 3</ref>(c). Note that a biking is used for an illustrative purpose while this dynamics can be applied general activities such as skiing, jetskiing, speedflying, and wingsuit flying with a few minor modifications such as body mass, moment of inertia, and air lift instead of normal force for a flying activities 5 . We represent Equation (4) in the world coordinate system, {W}, and Equation <ref type="formula" target="#formula_7">(5)</ref> in the body coordinate system, {B} 6 as shown in <ref type="figure" target="#fig_1">Figure 3</ref>(c). The active force and torque are composed of thrust force F T , roll torque T R , and yaw (steering) torque T Y :</p><formula xml:id="formula_8">F in = F T v v , T in = 0 T Y T R T ,</formula><p>where the thrust force is applied along the velocity direction, v 7 . The passive force and torque are composed of the following components: <ref type="bibr" target="#b4">5</ref> See the supplementary material for activity dependent coefficients. <ref type="bibr" target="#b5">6</ref> Forces in world coordinate system are semantically meaningful as the Y axis aligns with the gravity direction while torques in the body coordinate system are more interpretable (roll, pitch, and yaw) <ref type="bibr" target="#b20">[21]</ref>. <ref type="bibr" target="#b6">7</ref> The choice of the input force and torque components depends on the constraints of motion while it has to satisfy the controllability criterion.</p><formula xml:id="formula_9">F ex = mg + (F D + F F ) v v + F L F N 0 T T ex = 0 0 lF N sin θ b − lF L cos θ b T ,</formula><p>where g = 0 9.81 0 T m/s 2 is the gravitational acceleration, f D = −0.5C D ρA v 2 is the air drag force where C D ≈ 1.0, ρ = 1.23 kg/m 3 , and A are air drag coefficient, air density, and cross sectional area perpendicular to the velocity, respectively. F F ≤ 0 and F L are frictions along velocity and lateral directions, respectively. l is the distance between the center of mass and the ground contact point, and θ b is the body leaning angle. Equation <ref type="formula">(4)</ref> and <ref type="formula" target="#formula_7">(5)</ref> can be together written as a compact form:</p><formula xml:id="formula_10">Mq + C(q) = Ju + E,<label>(6)</label></formula><p>where M is the inertial matrix, C is the Coriolis matrix, E is the passive force and torque, and u = F T T R T Y is the active component. The state q = C T Ω T T describes the camera egomotion where C ∈ R 3 is the camera center and Ω ∈ R 3 is the axis-angle representation of camera rotation, i.e., exp [Ω] × = R ∈ SO(3) where [·] × is the skew symmetric representation of the cross product <ref type="bibr" target="#b20">[21]</ref>. J is a workspace mapping matrix written as:</p><formula xml:id="formula_11">J =   v T / v 0 0 0 0 0 1 0 0 0 0 1   T .</formula><p>Equation <ref type="formula" target="#formula_10">(6)</ref> describes motion in terms of active force and torque component, u, which allows us to directly map between input and the resulting motion. Solving for u is inverse dynamics that is integrated in our bundle adjustment in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Inverse Dynamics for Optimal Control</head><p>We integrate three ingredients for physical sensation, gravity direction, physical scale, and active force and torque into the following cost:  where D measures reprojection error, i.e., D(x 1 ,</p><formula xml:id="formula_12">minimize u(t),{Xj } i,j D (P(t i )X j , x ij ) + λ R T 0u (t) Tu (t)dt subject to P(t i ) = KR(t i ) I 3 −C(t i ) Mq + C(q) = Ju + E,<label>(7)</label></formula><formula xml:id="formula_13">x 2 ) = (x 1 /z 1 − x 2 /z 2 ) 2 + (y 1 /z 1 − y 2 /z 2 ) 2 where x = x y z T . P(t i ) ∈ R 3×4</formula><p>is the camera projection matrix at time t i instant, X ∈ P 3 is a 3D point, and x ij ∈ P 2 is the j th 2D point measurement at t i time instant. The goal is to infer both the unknown 3D world structure X, as well as control forces for the rigid body dynamics, u(t) , assuming the gravity force and the scale of the space is given. The last term in the cost function regularizes active forces such that the resulting input profile over time is continuous. λ R is a control weight for input regularization. Equation <ref type="formula" target="#formula_12">(7)</ref> consolidates a bundle adjustment cost from structure from motion with the optimal control theory that finds the optimal control profile to generate the desired output trajectory. Equation <ref type="formula" target="#formula_12">(7)</ref> is highly nonlinear due to reprojection error and rigid body dynamics, which requires a good initialization. We reconstruct 3D discrete camera pose trajectory, {P(t i )}, and a set of 3D points, {X j }, using structure from motion. The acceleration and velocity of the camera pose are approximated by differentiating the discrete camera pose. This allows us to approximate active and passive components, u and E, by solving statics, i.e., each time instant independently. Given this discrete input profile, we build a continuous piecewise linear function as an initialization of u(t).</p><p>We minimize the objective function using Levenberg-Marquardt algorithm <ref type="bibr" target="#b21">[22]</ref> where the ordinary differential equations for rigid body dynamics are solved via the Runge-Kutta 4th-order method on SE(3) <ref type="bibr" target="#b4">[5]</ref> whenever evaluating the objective function. <ref type="figure" target="#fig_3">Figure 4(b)</ref> shows comparison between initialization and the refined active force and torque. The initialization contains implausible input profile due to noisy acceleration computation. The optimization allows us to find the smooth input profile that simultaneously minimizes reprojection error, which agrees with structure from motion result as shown in <ref type="figure" target="#fig_3">Figure 4</ref>(c) and 4(d).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Result</head><p>We evaluate our algorithm on real world data. For all sequences, a camera trajectory is reconstructed by struc-ture from motion at 30 Hz. We assume all videos have the fixed resolution (1280 × 720) and intrinsic parameters (focal length, principle coordinates, and fisheye lens distortion) because 97% of first person sport videos are taken by the same mode of GoPro 2 Hero or GoPro 3 Hero. We use 29 Bike sequences ranging from 5 mins to 20 mins (about 1 million images 8 ) to fine-tune the CNN pre-trained by <ref type="bibr" target="#b15">[16]</ref> using Caffe <ref type="bibr" target="#b10">[11]</ref>. For computational efficiency, we divide a video into a set of 10 second videos (300 frames) to optimize Equation (7).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Quantitative Evaluation</head><p>We quantitatively evaluate our algorithm with a controlled experiment conducted by an experienced mountain biker with head-mounted inertial measurement unit (IMU) as shown in <ref type="figure" target="#fig_4">Figure 5</ref>(a). Additional IMU was attached on his body to measure disparity between head and body motion <ref type="bibr" target="#b8">9</ref> . Two cameras are also attached on the bike to monitor his behaviors such as pedaling and braking. Our evaluations are performed to verify our method in three criteria: gravity prediction, scale recovery, and active force and torque estimation. Gravity prediction We compare our prediction using CNN and reconstructed camera orientation with three baseline methods: a) Y axis: prediction by the image Y axis as a camera is often oriented upright; b) Y axis MLE: prediction by a) consolidated by the reconstructed camera orientation; c) ground plane normal. The ground plane is estimated by fitting a plane with RANSAC on the sparse point cloud. <ref type="figure" target="#fig_4">Figure 5</ref>(b) shows a comparison with baseline algorithms where our method produces median error 2.7 degree with 3.64 standard deviation (mean: 4.40 degree). Note that we do not compare our final MAP estimate for fair comparison. We also test our method on manually annotated data in <ref type="figure" target="#fig_4">Figure 5</ref> are used for the training data while Bike 1, 2, and 3 were not included in the training dataset. <ref type="table">Table 1</ref> summarizes the gravity prediction comparison. Scale recovery We recover the scale factor and compare the magnitude of linear acceleration with IMU, i.e., a / a m where a and a imu are acceleration of ours and IMU, respectively. Note that IMU data is noisier than our estimation but the ratio remains approximately 1 (head: 1.0278 median, 1.1626 mean, 0.6186 std.; body: 0.9999 median, 1.1600 mean, 0.7739 std.). We recover scale factors for 11 different sequences each ranges between 1 mins to 15 mins as shown in <ref type="figure" target="#fig_4">Figure 5</ref>(d). This results in overall 1.0188 median, 1.1613 mean, and 0.7003 std. Active force estimation We identify the moment that thrust force (pedaling and braking) is applied <ref type="bibr" target="#b9">10</ref> . We use a thresholding binary classifier, ξ + (t) and ξ − (t) to detect pedaling and braking, respectively: ξ</p><formula xml:id="formula_14">+ (t) = 1 if t t−1 F T (t)dt &gt; ǫ T , and 0 otherwise; ξ − (t) = 1 if t t−1 F T (t)dt &lt; −ǫ T ,</formula><p>and 0 otherwise 11 . <ref type="figure" target="#fig_6">Figure 6</ref>(a) shows active force profile and ground truth manually annotated from the videos of behavior monitoring cameras as shown in <ref type="figure" target="#fig_4">Figure 5</ref>(a). Our active force profile accords with the ground truth, i.e., pedaling when F T &gt; 0 and braking when F T &lt; 0. In <ref type="figure" target="#fig_6">Figure 6</ref>(b) and 6(c), we compare our method with net acceleration measured by IMU and structure from motion. We also compare against optical flow to measure acceleration that is often use for egocentric activity recognition tasks <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b29">30]</ref>. Also we compare with Pooled Motion Feature representation <ref type="bibr" target="#b30">[31]</ref>, which requires a pre-trained model. Our active force identification outperforms other baseline methods that do not take into account active force decomposition. This verifies that a trivial extension by attaching IMU on camera is not sufficient enough to estimate the active force applied by the camera wearer-the measured acceleration needs to be decomposed. Active torque estimation We compare the estimated angular velocity with measurements from gyroscope in <ref type="figure" target="#fig_6">Figure 6(d)</ref>. Note that the velocity computation by differentiating the reconstructed camera trajectory does not directly apply as different framerate between IMU and camera and noisy reconstruction. The optimally estimated active force and torque generate plausible angular velocity profile. Table 2 summarizes error of angular velocity measured by 11 <ref type="bibr" target="#b9">10</ref> Active force and torque are difficult to directly measure using IMU because the measured acceleration is due to net force and torque not input. This requires special force/torque sensors attached human bodies that measures muscle tension. <ref type="bibr" target="#b10">11</ref> A sophisticated classifier such as recurrent neural networks can be a complementary approach when supervision is available. different scenes. The correlation is also measured, which produces 0.87 mean correlation.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Qualitative Evaluation</head><p>We apply our method on real world data downloaded from YouTube. 5 different types of scenes are processed: 1) mountain biking (1-10 m/s); 2) Flying: wingsuit jump (25-50 m/s) and speedflying with parachute (9-40 m/s) (); 3) jetskiing at Canyon (4-20 m/s); 4) glade skiing (5-12 m/s); 5) Taxco urban downhill biking (5-15 m/s). <ref type="figure">Figure 1</ref> and 7, estimated gravity direction, physical scale of force and velocity, and active force and torque. Also passive components such as air drag, pitch torque, and normal force are shown. Thrust force is applied when climbing up the hill in Biking or when accelerating in Jetskiing. For Skiing, periodic lateral forces and roll moments are observed as the camera wearer was banking frequently. For flying case 12 , strong air drag force and lifting forces are observed. Also unstable angular momentum along the roll axis comparing to other axes is observed, which requires skillful body control to balance left and right wings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Discussion</head><p>In this paper, we present a method to reconstruct physical sensation of a first person video. We recover three ingredients for the physical sensations: gravity direction, physical scale, and active force and torque. The gravity direction is computed by leveraging a convolutional neural network integrated with the reconstructed 3D camera orientations. We recover the physical scale by using a torque equilibrium relationship along the roll axis at a bank turn. Active and passive components are modeled using rigid body dynamics which is integrated into a bundle adjustment that finds active force and torque profile concerting with the video. We quantitatively evaluate our method with controlled experiments where our method outperforms other baseline algorithms with a large margin (×2 ∼ ×10) and apply our method on real world data of various activities such as biking, skiing, flying, jetskiing, and urban bike racing.  <ref type="bibr" target="#b30">[31]</ref> (c) Deceleration  <ref type="figure">Figure 7</ref>. We compute gravity direction, physical scale factor, and active force and torque from a first person video. For each sequence, the top row shows image superimposed with speed, gravity, forces, and torque. Full trajectories of such physcial quantities are illustrated in the next row.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>(a) We recover the physical scale of a 3D reconstruction by exploiting the torque equilibrium at a banked turn where the torques generated by normal force and centripetal force TN + TL = 0 must be canceled to maintain the leaning angle, θ b . (b) The scale factor can be estimated by the slope, |ax|/ tan θ b . (c) We model the egomotion of a camera wearer using single rigid body dynamics (6 degree of freedom). Force and torque are decomposed into passive components (gravity, mg; centripetal force, FL; normal force, FN; friction force, FF; air drag, FD; pitch torque, TP) and active components (thrust, FT; roll torque, TR; yaw torque, TY).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>(a) The recovered gravity direction and scale allow us to identify physical dimension of elevation and speed. (b) We compute active force and torque by parametrizing them into a bundle adjustment in Equation</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>(a) For quantitative evaluation, we design a control experiment with an experienced mountain biker. (b) We compare our prediction with three baseline algorithms (see the description of the baseline algorithm in Section 5.1. The red heatmap indicates the likelihood at each time instant p(I|ĝ). Our predictor uses the image likelihood in conjunction with the reconstructed camera orientation. (c) We measure error across different scenes. (d) We recover physical scale and compare with IMU in terms of linear acceleration. Our method correctly estimate the scale (perfect recovery if 1; median 1.0287 with 0.6186 standard deviation).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 .</head><label>6</label><figDesc>(a) We identify active forces by manually annotating frames when pedaling or braking. (b) and (c) Our method outperforms optical flow based representation including<ref type="bibr" target="#b30">[31]</ref> with a large margin. (d) We compare our estimation with a gyroscope attached to the camera. Our estimation via active force and torque produces plausible angular velocity profile that accords with the gyroscope measurements.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>Gravity distribution</figDesc><table>Error: 0.5 degree 
Error: 15.8 degree 
Error: 1.1 degree 

Prediction 
Ground truth 

(b) CNN Visualization 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head></head><label></label><figDesc>Mean Med. Std. Mean Med. Std. Mean Med. Std. Mean Med. Std. Mean Med. Std. Mean Med. Std. Mean Med. Std. Mean Med. Std. Mean Med. Std. Mean Med. Std.</figDesc><table>(c) where our method consistently outperforms others 
significantly (×2 ∼ ×10). Note that only biking sequences 
Bike 1 

Bike 2 
Bike 3 
Bike IMU 
Ski 1 
Ski 2 
Ski 3 
Taxco 1 
Taxco 2 
Taxco 3 
Y axis 
5.62 4.44 4.72 8.10 
6.18 9.06 10.15 9.29 6.34 16.02 13.11 10.88 8.31 7.24 5.80 8.11 7.37 6.94 
6.86 5.93 4.79 8.00 4.62 13.10 5.77 4.66 4.92 9.66 7.00 8.84 
Y axis MLE 
5.92 4.57 4.66 6.08 
5.31 5.91 10.68 8.97 9.11 15.83 12.28 11.21 10.09 6.72 8.72 7.80 6.54 6.28 
7.00 6.37 4.75 6.90 4.06 12.73 5.94 4.01 5.97 10.41 6.83 10.85 
Ground plane 
7.45 6.28 5.14 12.69 10.20 8.99 11.31 8.16 11.01 11.98 10.24 9.03 
8.27 5.50 8.36 7.36 6.90 5.17 
7.87 6.86 5.84 10.44 8.13 13.04 8.07 6.79 7.44 7.09 5.67 5.44 
CNN MLE (ours) 0.76 0.61 0.60 2.53 
1.00 4.38 4.40 2.70 3.64 11.21 9.11 
8.18 
5.17 4.37 4.08 4.97 2.59 11.17 4.53 3.05 4.88 3.37 2.68 3.02 
4.60 2.89 5.06 5.86 4.26 6.80 

Table 1. Gravity prediction error (degree). Med.: median, Std.: standard deviation 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 .</head><label>2</label><figDesc></figDesc><table>Angular velocity comparison with gyroscope. Med.: me-
dian, Std.: standard deviation, Corr: correlation (perfect if 1) 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Other applications of physics based approaches have been used to infer motion<ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b39">40]</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Our training data consists of 32 Bike, 19 Ski, 23 Urban bike, 23 Jetski, 29 Wingsuit fly, and 30 Speed fly sequences and each sequence ranges between 1 mins to 38 mins. We annotate the 2D gravity direction of images in the training set and reconstruct it in 3D. This 3D reconstructed gravity allows us to propagate over 100 frames. Optionally, we also use IMU attached camera to automatically annotate the gravity. See the supplementary material for the detailed description of the training data.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">If ones goes straight without changing camera orientation, the streak remains constant as shown in Likelihood ofFigure 2(a).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">Note that the scenes change rapidly due to fast egomotion and thus, the data capture variety of scene cues.<ref type="bibr" target="#b8">9</ref> A quantitative analysis on the relationship between body and gaze orientation is included in the supplementary material.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12">Unfortunately, the gravity direction cannot properly estimated as it was even challenging to a human annotator. Instead, we manually find frames that contain the horizon to estimate the gravity direction.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Automatic editing of footage from multiple social cameras. SIGGRAPH</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Arev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Hodgins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shamir</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Recovering non-rigid 3d shape from image streams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hertzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Biermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The kneed walker for human pose tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Brubaker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Physicsbased person tracking using simplified lower-body dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Brubaker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hertzmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Numerical Methods for Ordinary Differential Equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Butcher</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>John Wiley and Sons</publisher>
		</imprint>
	</monogr>
	<note>second edition</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">People tracking using hybrid monte carlo filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Choo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Fleety</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A new bicycle design based on biomechanics and advanced technology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Monte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Leonardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Menchinelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Marini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Sport Biomechanics</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Understanding egocentric activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning to recognize daily actions using gaze</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Recognition of natural scenes from global properties: Seeing the forest without representing the trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Greene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Psychology</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<title level="m">Caffe: Convolutional architecture for fast feature embedding</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Visual perception of biological motion and a model for its analysis. Perception and Psychophyics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Johansson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1973" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">First person vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Fast unsupervised ego-action learning for first-person sports videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Kitani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Okabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sugimoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">First person hyperlapse videos. SIGGRAPH</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Discovering important people and objects for egocentric video summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning to predict gaze in egocentric video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Shape and nonrigid motion estimation through physics-based synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Terzopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">A Mathematical Introduction to Robotic Manipulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Sastry</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
			<publisher>CRC Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Numerical Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nocedal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Wright</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Modeling the shape of the scene: a holistic representation of the spatial envelope</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">3D social saliency from head-mounted cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shiekh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Recognizing activities of daily living in first-person camera views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Discovering the signatures of joint attention in child-caregiver interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pusiol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Soriano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>Frank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CogSci</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Decoding childrens social behavior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D</forename><surname>Abowd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rozga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Clements</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Essa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">Y</forename><surname>Ousley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">L</forename><surname>Presti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lantsman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bidwell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Longitudinal stability analysis of a jet-powered wingsuit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Robson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AIAA Atmospheric Flight Mechanics Conference</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">First-person pose recognition using egocentric workspaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rogez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Supancic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">First-person activity recognition: What are they doing to me</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Ryoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Matthies</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Pooled motion features for first-person videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Ryoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rothrock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Matthies</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Design of a framework for interoperable motion effects for 4d theaters using human-centered motion data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Advances in Computer Entertainment Technology</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Stochastic tracking of 3d human figures using 2d image motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sidenbladh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Nonrigid structure-from-motion: Estimating shape and motion with hierarchical priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hertzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">3d people tracking with gaussian process dynamical models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Videobased 3d motion capture through biped control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vondrak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Hodgins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Jenkins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGGRAPH</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Physical simulation for probabilistic motion tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vondrak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Jenkins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Videomocap: Modeling physically realistic human motion from monocular video sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>SIG-GRAPH</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Principles of sensorimotor learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Wolpert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Diedrichsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Flanagan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Reviews Neuroscience</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Dynamic models of human motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Wren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pentland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Face and Gesture</title>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Detecting snap points in egocentric video with a web photo prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A factorization-based approach for articulated nonrigid shape, motion and kinematic chain recovery from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Detecting bids for eye contact using a wearable camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bridges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rozga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FG</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Ego-surfing first person videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yonetani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Kitani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
