<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Canonical Time Warping</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Trigeorgis</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihalis</forename><forename type="middle">A</forename><surname>Nicolaou</surname></persName>
							<email>2m.nicolaou@gold.ac.uk</email>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Goldsmiths</orgName>
								<orgName type="institution" key="instit2">University of London</orgName>
								<address>
									<country>U K</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Center for Machine Vision and Signal Analysis</orgName>
								<orgName type="institution">University of Oulu</orgName>
								<address>
									<country key="FI">Finland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Björn</forename><forename type="middle">W</forename><surname>Schuller</surname></persName>
							<email>bjoern.schuller@imperial.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Canonical Time Warping</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:39+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Machine learning algorithms for the analysis of timeseries often depend on the assumption that the utilised data are temporally aligned. Any temporal discrepancies arising in the data is certain to lead to ill-generalisable models, which in turn fail to correctly capture the properties of the task at hand. The temporal alignment of time-series is thus a crucial challenge manifesting in a multitude of applications. Nevertheless, the vast majority of algorithms oriented towards the temporal alignment of time-series are applied directly on the observation space, or utilise simple linear projections. Thus, they fail to capture complex, hierarchical non-linear representations which may prove to be beneficial towards the task of temporal alignment, particularly when dealing with multi-modal data (e.g., aligning visual and acoustic information). To this end, we present the Deep Canonical Time Warping (DCTW), a method which automatically learns complex non-linear representations of multiple time-series, generated such that (i) they are highly correlated, and (ii) temporally in alignment. By means of experiments on four real datasets, we show that the representations learnt via the proposed DCTW significantly outperform state-of-the-art methods in temporal alignment, elegantly handling scenarios with highly heterogeneous features, such as the temporal alignment of acoustic and visual features.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The alignment of multiple data sequences is a commonly arising problem, raised in multiple fields related to machine learning, such as signal, speech and audio analysis <ref type="bibr" target="#b28">[29]</ref>, computer vision <ref type="bibr" target="#b5">[6]</ref>, graphics <ref type="bibr" target="#b4">[5]</ref> and bio-informatics <ref type="bibr" target="#b0">[1]</ref>. Example applications range from the temporal alignment of facial expressions and motion capture data <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b37">38]</ref>, to the alignment for human action recognition <ref type="bibr" target="#b33">[34]</ref>, and speech <ref type="bibr" target="#b17">[18]</ref>.</p><p>The most prominent temporal alignment method is Dynamic Time Warping (DTW) <ref type="bibr" target="#b28">[29]</ref>, which identifies the optimal warping path that minimises the Euclidean distance between two time-series. While DTW has found wide application over the past decades, the application is limited mainly due to the inherent inability of DTW to handle observations of different or high dimensionality since it directly operates on the observation space. Motivated by this limitation while recognising that this scenario is commonly encountered in real-world applications (e.g., capturing data from multiple sensors), in <ref type="bibr" target="#b36">[37]</ref> an extension to DTW is proposed. Coined Canonical Time Warping (CTW), the method combines Canonical Correlation Analysis (CCA) and DTW by aligning the two sequences in a common, latent subspace of reduced dimensionality whereon the two sequences are maximally correlated. Other extensions of DTW include the integration of manifold learning, thus facilitating the alignment of sequences lying on different manifolds <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b10">11]</ref> while in <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b37">38]</ref> constraints are introduced in order to guarantee monotonicity and adaptively constrain the temporal warping. It should be noted that in <ref type="bibr" target="#b37">[38]</ref>, a multi-set variant of CCA is utilised <ref type="bibr" target="#b13">[14]</ref> thus enabling the temporal alignment of multiple sequences, while a Gauss-Newton temporal warping method is proposed.</p><p>While methods aimed at solving the problem of temporal alignment have been successful in a wide spectrum of applications, most of the aforementioned techniques find a single linear projection for each sequence. While this may suffice for certain problem classes, in many real world applications the data are likely to be embedded with more complex, possibly hierarchical and non-linear structures. A prominent example lies in the alignment of non-linear acoustic features with raw pixels extracted from a video stream (for instance, in the audiovisual analysis of speech, where the temporal misalignment is a common problem). The mapping between these modalities is deemed highly nonlinear, and in order to appropriately align them in time this needs to be taken into account. An approach towards extracting such complex non-linear transformations is via adopting the principles associated with the recent revival of deep neural network architectural models. Such architectures have been successfully applied in a multitude of problems, including feature extraction and dimensionality reduction <ref type="bibr" target="#b15">[16]</ref>, feature extraction for object recognition and detection <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b9">10]</ref>, feature extraction for face recognition <ref type="bibr" target="#b31">[32]</ref>, acoustic modelling in speech recognition <ref type="bibr" target="#b14">[15]</ref>, as well as for extracting non-linear correlated features <ref type="bibr" target="#b1">[2]</ref>.</p><p>Interest to us is also work that has evolved around multimodal learning. Specifically, deep architectures deemed very promising in several areas, often overcoming by a large margin traditionally used methods in various emotion and speech recognition tasks <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b24">25]</ref>, and on robotics applications with visual and depth data <ref type="bibr" target="#b34">[35]</ref>.</p><p>In this light, we propose Deep Canonical Time Warping (DCTW), a novel method aimed towards the alignment of multiple sequences that discovers complex, hierarchical representations which are both maximally correlated and temporally aligned. To the best of our knowledge, this work presents the first deep approach towards solving the problem of temporal alignment, which in addition offers very good scaling when dealing with large amounts of data. In more detail, our work carries the following contributions: (i) we extend DTW-based temporal alignment methods to handle heterogeneous collections of features which may be connected via non-linear hierarchical mappings, (ii) in the process, we extend DCCA to (a) handle arbitrary temporal discrepancies in the observations and (b) cope with multiple (more than two) sequences, while finally (iii) we evaluate the proposed DCTW on a multitude of real data sets, where the performance gain in contrast to other state-of-theart methods becomes clear.</p><p>The remainder of this paper is organised as follows. In Sec. 3 we refer to related work on temporal alignment and canonical correlation analysis. In Sec. 4, we describe the proposed DCTW. We provide experiments on four real datasets in Sec.5, while we conclude the paper in Sec.7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Notation</head><p>Throughout the paper, matrices are denoted by uppercase boldface letters (e.g., X, Y), vectors are denoted by lowercase boldface letters (e.g., x, y), and scalars appear as either uppercase or lowercase letters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Canonical Correlation Analysis</head><p>Canonical Correlation Analysis (CCA) is a shared-space component analysis method, that given two data matrices X 1 , X 2 where X i ∈ R di×T recovers the loadings W 1 ∈ R d1×d , W 2 ∈ R d2×d that linearly project the data on a subspace where the linear correlation is maximised. This can be interpreted as discovering the shared information conveyed by all the datasets (or views). The correlation</p><formula xml:id="formula_0">ρ = corr(Y 1 , Y 2 ) in the projected space Y i = W ⊤ i X i can be written as ρ = E[Y 1 Y ⊤ 2 ] E[Y 1 Y ⊤ 1 Y 2 Y ⊤ 2 ]</formula><p>(1)</p><formula xml:id="formula_1">= W ⊤ 1 E[X 1 X ⊤ 2 ]W 2 W ⊤ 1 E[X 1 X ⊤ 1 ]W 1 W ⊤ 2 E[X 2 X ⊤ 2 ]W 2 (2) = W ⊤ 1 Σ 12 W 2 W ⊤ 1 Σ 11 W 1 W ⊤ 2 Σ 22 W 2 ,<label>(3)</label></formula><p>where Σ ij denotes the empirical covariance between data matrices X i and X j 1 . There are multiple equivalent optimisation problems for discovering the optimal loadings W i which maximise Eq. 3 <ref type="bibr" target="#b7">[8]</ref>. For instance, CCA can be formulated as a least-squares problem, arg min</p><formula xml:id="formula_2">W 1 ,W2 W ⊤ 1 X 1 − W ⊤ 2 X 2 2 F subject to: W ⊤ 1 X 1 X ⊤ 1 W 1 = I, W ⊤ 2 X 2 X ⊤ 2 W 2 = I,<label>(4)</label></formula><p>and equivalently as a trace optimisation problem arg max</p><formula xml:id="formula_3">W 1 ,W2 tr W ⊤ 1 X 1 X ⊤ 2 W 2 subject to W ⊤ 1 X 1 X ⊤ 1 W 1 = I, W ⊤ 2 X 2 X ⊤ 2 W 2 = I,<label>(5)</label></formula><p>where in both cases we exploit the scale invariance of the correlation coefficient with respect to the loadings in the constraints. The solution in both cases is given by the eigenvectors corresponding to the d largest eigenvalues of the generalised eigenvalue problem</p><formula xml:id="formula_4">Σ 12 Σ −1 22 Σ 21 W 1 = Σ 11 W 1 Λ.<label>(6)</label></formula><p>Note that, an equivalent solution is obtained by resorting to Singular Value Decomposition (SVD) on the matrix K = Σ −1/2 11 <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b3">4]</ref>. The optimal objective value of Eq. 5 is then the sum of the largest d singular values of K, while the optimal loadings are found by setting</p><formula xml:id="formula_5">Σ 12 Σ −1/2 22</formula><formula xml:id="formula_6">W 1 = Σ −1/2 11 U d and W 2 = Σ −1/2 22 V d , with U d and V d</formula><p>being the left and right singular vectors of K. Note that this interpretation is completely analogous to solving the corresponding generalised eigenvalue problem arising in Eq. 6 and keeping the top d eigenvectors corresponding to the largest eigenvalues.</p><p>Recently, in order to facilitate the extraction of nonlinear correlated transformations, a methodology inspired by CCA called Deep CCA (DCCA) <ref type="bibr" target="#b1">[2]</ref> was proposed. In more detail, motivated by the recent success of deep architectures, DCCA assumes a network of multiple stacked layers consisting of nonlinear transformations for each data set i, with parameters θ i = {θ 1 i , ..., θ l i }, where l is the number of layers. Assuming the transformation applied by the network corresponding to data set i is represented as f i (X i ; θ i ), the optimal parameters are found by solving arg max θ1,θ2 corr(f 1 (X 1 ; θ 1 ), f 2 (X 2 ; θ 2 )).</p><p>Let us assume that in each of the networks, the final layer has d maximally correlated units in an analogous fashion to the classical CCA 3. In particular, we consider thatX i denotes the transformed input data sets,</p><formula xml:id="formula_8">X i = f i (X i ; θ i ) and that the covariancesΣ ij are now estimated onX, i.e.,Σ ij = 1 T −1X i (I − 1 T 11 ⊤ )X ⊤ i .</formula><p>As described above for classical CCA (Eq. 5), the optimal objective value is the sum of the k largest singular values of</p><formula xml:id="formula_9">K = Σ −1/2 11Σ 12Σ −1/2 22</formula><p>, which is exactly the nuclear norm of K,</p><formula xml:id="formula_10">K * = trace( √ KK ⊤ ). Problem 7 now becomes arg max θ1,θ2 K * ,<label>(8)</label></formula><p>which is precisely the loss function that is backpropagated through the network 2 <ref type="bibr" target="#b1">[2]</ref>. Put simply, the networks are optimised towards producing features which exhibit high canonical correlation coefficients.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Time Warping</head><p>Given two data matrices X 1 ∈ R d×T1 , X 2 ∈ R d×T2 Dynamic Time Warping (DTW) aims to eliminate temporal discrepancies arising in the data by optimising Eq.9, arg min</p><formula xml:id="formula_11">∆ 1 ,∆2 X 1 ∆ 1 − X 2 ∆ 2 2 F subject to: ∆ 1 ∈ {0, 1} T1×T , ∆ 2 ∈ {0, 1} T2×T ,<label>(9)</label></formula><p>where ∆ 1 and ∆ 2 are binary selection matrices <ref type="bibr" target="#b36">[37]</ref> that encode the alignment path, effectively remapping the the samples of each sequence to a common temporal scale. Although the number of plausible alignment paths is exponential with respect to T 1 T 2 , by employing dynamic programming, DTW infers the optimal alignment path (in terms of Eq. 9) in O(T 1 T 2 ). Finally, the DTW solution satisfies the boundary, continuity, and monotonicity constraints <ref type="bibr" target="#b28">[29]</ref>.</p><p>The main limitation of DTW lies in the inherent inability to handle sequences of varying feature dimensionality, which is commonly the case when examining data acquired from multiple sensors. Furthermore, DTW is prone to failure when one or more sequences are perturbed by arbitrary affine transformations. To this end, the Canonical Time Warping (CTW) <ref type="bibr" target="#b36">[37]</ref> elegantly combines the least-squares formulations of DTW (Eq.9) and CCA (Eq.4), thus facilitating the utilisation of sequences with varying dimensionalities, while simultaneously performing feature selection and temporal alignment. In more detail, given X 1 ∈ R d1×T1 , X 2 ∈ R d2×T2 , the CTW problem is posed as arg min</p><formula xml:id="formula_12">W 1 ,W2,∆1,∆2 W ⊤ 1 X 1 ∆ 1 − W ⊤ 2 X 2 ∆ 2 2 F subject to: W ⊤ 1 X 1 ∆ 1 ∆ ⊤ 1 X ⊤ 1 W 1 = I, W ⊤ 2 X 2 ∆ 2 ∆ ⊤ 2 X ⊤ 2 W 2 = I, W ⊤ 1 X 1 ∆ 1 ∆ ⊤ 2 X ⊤ 2 W 2 = D, X 1 ∆ 1 1 = X 2 ∆ 2 1 = 0, ∆ 1 ∈ {0, 1} T1×T , ∆ 2 ∈ {0, 1} T2×T<label>(10)</label></formula><p>where the loadings W 1 ∈ R d×T1 and W 2 ∈ R d×T2 project the observations onto a reduced dimensionality subspace where they are maximally linearly correlated, D is a diagonal matrix and 1 is a vector of all 1's of appropriate dimensions. The constraints in Eq. 10, mostly inherited by CCA, deem the CTW solution translation, rotation, and scaling invariant. A solution is then subsequently obtained by alternating between solving CCA (by fixing X i ∆ i ) and DTW (by fixing W ⊤ i X i ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Deep Canonical Time Warping</head><p>The goal of Deep Canonical Time Warping (DCTW) is to discover a hierarchical non-linear representation of the data sets X i , i = {1, 2} where the transformed features are (i) temporally aligned with each other, and (ii) maximally correlated. To this end, let us consider that f i (X i ; θ i ) represents the final layer activations of the corresponding network for dataset X i . We propose to optimise the following objective,</p><formula xml:id="formula_13">arg min θ1,θ2,∆ 1 ,∆2 f 1 (X 1 ; θ 1 )∆ 1 − f 2 (X 2 ; θ 2 )∆ 2 2 F subject to: f 1 (X 1 ; θ 1 )∆ 1 ∆ ⊤ 1 f 1 (X 1 ; θ 1 ) ⊤ = I, f 2 (X 2 ; θ 2 )∆ 2 ∆ ⊤ 2 f 2 (X 2 ; θ 2 ) ⊤ = I, f 1 (X 1 ; θ 1 )∆ 1 ∆ ⊤ 2 f 2 (X 2 ; θ 2 ) = D, f 1 (X 1 ; θ 1 )∆ 1 1 = f 2 (X 2 ; θ 2 )∆ 2 1 = 0, ∆ 1 ∈ {0, 1} T1×T , ∆ 2 ∈ {0, 1} T2×T</formula><p>(11) where as defined for Eq. 10, D is a diagonal matrix and 1 is an appropriate dimensionality vector of all 1's. Clearly, the objective can be solved via alternating optimisation. Given the activation of the output nodes of each network i, DTW recovers the optimal warping matrices ∆ i which temporally align them. Nevertheless, the inverse is not so straight-forward, since we have no closed form solution for finding the optimal non-linear stacked transforma-tion applied by the network. We therefore resort to finding the optimal parameters of each network by utilising backpropagation. Having discovered the warping matrices ∆ i , the problem becomes equivalent to applying a variant of DCCA in order to infer the maximally correlated nonlinear transformation on the temporally aligned input features. This requires that the covariances are reformulated aŝ</p><formula xml:id="formula_14">Σ ij = 1 T −1 f i (X i ; θ i )∆ i C T ∆ ⊤ j f j (X j ; θ j ) ⊤ , where C T is the centring matrix, C T = I − 1 T 11 ⊤ . By defining K DCT W =Σ −1/2 11Σ 12Σ −1/2 22 , we now have that corr(f 1 (X 1 ; θ 1 )∆ 1 , f 2 (X 2 ; θ 2 )∆ 2 ) = K DCT W * .</formula><p>(12) We optimise this quantity in a gradient-ascent fashion by utilising the subgradient of Eq. 12 <ref type="bibr" target="#b2">[3]</ref>, since the gradient can not be computed analytically. By assuming that Y i = f i (X i ; θ i ) for each of network i and USV ⊤ = K DCT W is the singular value decomposition of K DCT W , then the subgradient for the last layer is defined as</p><formula xml:id="formula_15">F (pos) =Σ −1/2 11 UV ⊤Σ−1/2 22 Y 2 ∆ 2 C T F (neg) =Σ −1/2 11 USU ⊤Σ−1/2 11 Y 1 ∆ 1 C T ∂ K DCT W * ∂Y 1 = 1 T − 1 F (pos) − F (neg) .<label>(13)</label></formula><p>At this point, it is clear that CTW is a special case of DCTW. In fact, we arrive at CTW (Sec.3.2) by simply considering a network with one layer. In this case, by setting f i (X i ; θ i ) = W ⊤ i X i , Eq. 11 becomes equivalent to Eq. 10, while solving Eq. 12 by means of Singular Value Decomposition (SVD) on K DCT W provides equivalent loadings to the ones obtained by CTW via eigenanalysis.</p><p>Finally, we note that we can easily extend DCTW to handle multiple (more than 2) data sets, by incorporating a similar objective to the Multi-set Canonical Correlation Analysis (MCCA) <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b25">26]</ref>. In more detail, instead of Eq. 12 we now optimise m i,j=1</p><formula xml:id="formula_16">corr(f i (X i ; θ i )∆ i , f j (X j ; θ j )∆ j ) = m i,j K ij DCT W * .<label>(14)</label></formula><p>where m is the number of sequences and K ij DCT W = Σ −1/2 iiΣ ijΣ −1/2 jj . The subgradient of Eq. 14 can be computed in a straightforward manner by utilising Eq.13. Note that by setting ∆ i = I, Eq. 14 becomes an objective for learning transformations for multiple sequences via DCCA <ref type="bibr" target="#b1">[2]</ref>. Finally, we note that any warping method can be used in place of DTW for inferring the warping matrices ∆ i (e.g., <ref type="bibr" target="#b37">[38]</ref>). DCTW is illustrated in <ref type="figure">Fig.1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>In order to assess the performance of DCTW, we perform detailed experiments against both linear and nonlinear state-of-the-art temporal alignment algorithms. In more detail we compare against:</p><p>State of the art methods for time warping without a feature extraction step:</p><p>• Dynamic Time Warping (DTW) <ref type="bibr" target="#b28">[29]</ref> which finds the optimal alignment path given that the sequences reside in the same manifold (as explained in Sec.3.2).</p><p>• Iterative Motion Warping (IMW) <ref type="bibr" target="#b16">[17]</ref> alternates between time warping and spatial transformation to align two sequences.</p><p>State-of-the art methods with a linear feature extractor:</p><p>• Canonical Time Warping (CTW) <ref type="bibr" target="#b36">[37]</ref> as posed in section Sec. 3.2, CTW finds the optimal reduced dimensionality subspace such that the sequences are maximally linearly correlated.</p><p>• Generalized Time Warping (GTW) <ref type="bibr" target="#b37">[38]</ref> which uses a combination of CTW and a Gauss-Newton temporal warping method that parametrises the warping path as a combination of monotonic functions.</p><p>State-of-the-art methods with non-linear feature extraction process.</p><p>• Manifold Time Warping <ref type="bibr" target="#b33">[34]</ref> that employs a variation of Laplacian Eigenmaps to non-linearly transform the original sequences.</p><p>We evaluate the aforementioned techniques on four different real-world datasets, namely (i) the Weizmann database Sec. 5.2, where multiple feature sets are aligned , (ii) the MMI Facial Expression database Sec.5.3, where we apply DCTW on the alignment of facial Action Units, (iii) the XRMB database Sec.5.4 where we align acoustic and articulatory recordings, and finally (iv) the CUAVE database Sec.5.5, where we align visual and auditory utterances.</p><p>Evaluation For all experiments, unless stated otherwise, we assess the performance of DCTW utilising the the alignment error introduced in <ref type="bibr" target="#b37">[38]</ref>. Assuming we have m sequences, each algorithm infers a set of warping paths P alg = p alg 1 , p alg 2 , . . . , p alg m , where p i ∈ x ∈ N l alg |1 ≤ x ≤ n m is the alignment path for the ith sequence with a length l alg . The error is then defined as Err = dist(P alg , P ground ) + dist(P ground , P alg ) l alg + l ground ,</p><formula xml:id="formula_17">dist P 1 , P 2 = l1 i=1 min l2 j=1 p 1 (i) − p 2 (j) 2 . X 1 X 2 f 1 (·) f 2 (·) f1(x 1 1 ) f1(x 2 1 ) f1(x 3 1 ) f1(x 4 1 ) f1(x 5 1 ) f2(x 1 2 ) f2(x 2 2 ) f2(x 3 2 ) f2(x 4 2 ) · · · · · · f1(x T1 1 ) f2(x T2 2 )</formula><p>CCA loss Spatial Transformation Temporal alignment <ref type="figure">Figure 1</ref>: Illustration of the DCTW architecture with two networks, one for each temporal sequence. The model is trained end-to-end, first performing a spatial transformation of the data samples and then a temporal transformation such as the temporal sequences are maximally correlated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Experimental Setup</head><p>In each experiment, we perform unsupervised pretraining of the deep architecture for each of the available sequences in order to speed up the convergence of the optimisation procedure. In particular, we initialise the parameters of each of the layers using a denoising autoencoder <ref type="bibr" target="#b32">[33]</ref>. We utilise full-batch optimisation with AdaGrad <ref type="bibr" target="#b8">[9]</ref> for training, although similar results are obtained by utilising mini-batch stochastic gradient descent optimisation with a large mini-batch size. In contrast to <ref type="bibr" target="#b1">[2]</ref>, we utilise a leaky rectified linear unit with a = 0.03 (LReLU) <ref type="bibr" target="#b21">[22]</ref>, where f (x) = max(ax, x) and a is a small positive value. In our experiments, this function converged faster and produced better results than the suggested modified cube-root sigmoid activation function. For all the experiments (excluding Sec. 5.2 where a smaller network was sufficient) we utilised a fixed three layer 200-100-100 fully connected topology, thus reducing the number the number of free hyperparameters of the architecture. : This both facilitates the straight-forward reproducibility of experimental results, as well as helps towards avoiding overfitting (particularly since training is unsupervised).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Real Data I: Alignment of Human Actions under Multiple Feature Sets</head><p>In this experiment, we utilise the Weizmann database <ref type="bibr" target="#b12">[13]</ref>, containing videos of nine subjects performing one of ten actions (e.g., walking). We adopt the experimental protocol described in <ref type="bibr" target="#b37">[38]</ref>, where 3 different shape features are computed for each sequence, namely (1) a binary mask, (2) Euclidean distance transform <ref type="bibr" target="#b23">[24]</ref>, and (3) the solution of the Poisson equation <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b37">38]</ref>. Subsequently, we reduce the dimensionality of the frames to 70-by-35 pixels, while we keep the top 123 principle components. For all algorithms, the same hyperparameters as <ref type="bibr" target="#b37">[38]</ref> are used. Following <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b37">[38]</ref>, 90% of the total correlation is kept, while we used a topology of two layers carrying 50 neurons each. Triplets of videos where subjects are performing the same action where selected, and each alignment algorithm was evaluated on aligning the three videos based on the features described above. The ground truth of the data was approximated by running DTW on the binary mask images. Thus, the reasoning behind this experiment is to evaluate whether the methods manage to find a correlation between the three computed features, in which case they would find the alignment path produced by DTW.</p><p>In <ref type="figure" target="#fig_0">Fig. 2</ref> we show the alignment error for ten randomly generated sets of videos. As DTW, DDTW, IMW, and CTW are only formulated for performing alignment between two sequences we use their multi-sequence extension as formulated in <ref type="bibr" target="#b38">[39]</ref>, and we use the prefix p to denote the multisequence variant.</p><p>We observe that DTW and DDTW fail to align the videos correctly, while CTW, GTW, and DCTW perform quite better. This can be justified by considering that DTW and DDTW are applied directly on the observation space, while CTW, GTW and DCTW infer a common subspace of the three input sequences. The best performing methods are clearly GTW and DCTW.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Real Data II: Alignment of Facial Action Units</head><p>Next, we evaluate the performance of DCTW on the task of temporal alignment of facial expressions. We utilise the MMI Facial Expression Dataset <ref type="bibr" target="#b26">[27]</ref> which contains more than 2900 videos of 75 different subjects, each performing a particular combination of Action Units (i.e., facial muscle activations). We have selected a subset of the original dataset which contains videos of subjects which manifest the same action unit (namely, AU12 which corresponds to a smile), and for which we have ground truth annotations. We preprocessed all the images by converting to greyscale and utilised an off-the-shelf face detector along with a face alignment procedure <ref type="bibr" target="#b18">[19]</ref> in order to crop a bounding box around the face of each subject. Subsequently, we reduce the dimensionality of the feature space to 400 components using whitening PCA, preserving 99% of the energy. We clarify that the annotations are given for each frame, and describe the temporal phase of the particular AU at that frame. Four possible temporal phases of facial action units are defined: neutral when the corresponding facial muscles are inactive, onset where the muscle is activated, apex when facial muscle intensity reaches its peak, and offset when the facial muscle begins to relax, moving towards the neutral state.</p><p>Utilising raw pixels, the goal of this experiment lies in temporally aligning each pair of videos. In the context of this experiment, this means that the subjects in both videos exhibit the same temporal phase at the same time. E.g., for smiles, when subject 1 in video 1 reaches the apex of the smile, the subject in video 2 does so as well. In order to quantitatively evaluate the results, we utilise the ratio of correctly aligned frames within each temporal phase to the total duration of the temporal phase across the aligned videos. This can be formulated as |Φ1∩Φ2| |Φ1∪Φ2| , where Φ 1,2 is the set of aligned frame indices after warping the initial vector of annotations using the alignment matrices ∆ i found via a temporal warping technique.</p><p>Results are presented in <ref type="figure">Fig. 4</ref>, where we illustrate the alignment error on 45 pairs of videos across all methods and action unit temporal phases. Clearly, DTW overper-forms MW, while CCA based methods such as CTW and GTW perform better than DTW. It can be seen that the best performance in all cases is obtained by DCTW, and using a t-test with the next best method we find that the result is statistically significant (p &lt; 0.05). This can be justified by the fact that the non-linear hierarchical structure of DCTW facilitates the modelling of the complex dynamics straight from the low-level pixel intensities.</p><p>Furthermore, in <ref type="figure">Fig.3</ref> we illustrate the alignment results from a pair of videos of the dataset. The first row depicts the first sequence in the experiment, where for each temporal phase with duration [t s , t e ] we plot the frame t c = ⌈ ts+te 2 ⌉. The second row illustrates the ground truth of the second video, while the following rows compare the alignment paths obtained by DCTW, CTW and GTW respectively. By observing the corresponding images as well as the temporal phase overlap, it is clear that DCTW achieves the best alignment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Real Data III: Alignment of Acoustic and Articulatory Recordings</head><p>The third set of experiments involves aligning simultaneous acoustic and articulatory recordings from the Wisconsin X-ray Microbeam Database (XRMB) <ref type="bibr" target="#b35">[36]</ref>. The articulatory data consist of horizontal and vertical displacements of eight pellets on the speaker's lips, tongue, and jaws, yielding a 16-dimensional vector at each time point. We utilise the features provided by <ref type="bibr" target="#b1">[2]</ref>. The baseline acoustic features consist of standard 13-dimensional mel-frequency cepstral coefficients (MFCCs) <ref type="bibr" target="#b6">[7]</ref> and their first and second derivatives computed every 10ms over a 25ms window. For the articulatory measurements to match the MFCC rate, we concatenate them over a 7-frame window, thus obtaining X art ∈ R 273 and X MFCC ∈ R 112 . As the two views were recorded simultaneously and then manually synchronised <ref type="bibr" target="#b35">[36]</ref>, we use this correspondence as the ground truth and then we produce a synthetic misalignment to the sequences, producing 10 sequences of 5000 samples. We warp the auditory features using the alignment path produced by P mis (i) = i 1.1 l 0.1 MFCC for 1 ≤ i ≤ l MFCC where l MFCC is the number of MFCC samples.</p><p>Results are presented in Tab. 1. Note that DCTW outperforms compared methods by a much larger margin than other experiments here. Nevertheless, this is quite expected: the features for this experiment are highly heterogeneous and e.g., in case of MFCCs, non-linear. The multi-layered non-linear transformations applied by DCTW are indeed much more suitable for modelling the mapping between such varying feature sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Real Data IV: Alignment of Audio and Visual Streams</head><p>In our last (and arguably, most challenging) experiment, we aim to align the subject's visual and auditory utterances. To this end, we use the CUAVE <ref type="bibr" target="#b27">[28]</ref> database which contains 36 videos of individuals pronouncing the digits 0 to 9.</p><p>In particular, we use the portion of videos containing only frontal facing speakers pronouncing each digit five times, and use the same approach as in Sec. 5.4 in order to introduce misalignments between the audio and video streams. In order to learn the hyperparameters of all employed alignment techniques, we leave out 6 videos.</p><p>Regarding pre-processing, from each video frame we extract the region-of-interest (ROI) containing the mouth of the subject using the landmarks produced via <ref type="bibr" target="#b18">[19]</ref>. Each ROI was then resized to 60 x 80 pixels, while we keep the top 100 principal components of the original signal. Subsequently, we utilise temporal derivatives over the reduced vector space. Regarding the audio signal, we compute the "Zero" "One" "Two" "Three" "Four" "Five" "Six" "Seven" "Eight" "Nine" <ref type="figure">Figure 5</ref>: The alignment results of DCTW for subject #1 of the CUAVE database.</p><p>Mel-frequency cepstral coefficients (MFCC) features using a 25ms window adopting a step size of 10ms between successive windows. Finally, we compute the temporal derivatives over the acoustic features (and video frames). To match the video frame rate, 3 continuous audio frames are concatenated in a vector. The results show that DCTW out-  performs the rest of the temporal alignment methods by a large margin. Again, the justification is similar to Sec. 5.4: the highly heterogeneous nature of the acoustic and video features highlights the significance of deep non-linear architectures for the task-at-hand. It should be noted that the best results obtained for GTW utilise a combination of hyperbolic and polynomial basis, which biases the results in favour of GTW due to the misalignment we introduce. Still, it is clear that DCTW obtains much better results in terms of alignment error.</p><formula xml:id="formula_18">DTW</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Computational details and discussion</head><p>Currently the computational cost of aligning a set of m sequences each of length of T i samples each is O( m i,j T i T j + eig( i d i )), which is bounded by the cost of performing DTW and secondly the cost of the singular value decomposition to calculate the derivatives in Eq. 13. As the decomposition is performed on the last layer of the network, which is of reduced dimensionality (100 units in our case) it is very cheap to compute in practise. In contrast other non-linear warping algorithms <ref type="bibr" target="#b33">[34]</ref> require an expensive k-nearest neighbour and an extra eigendecomposition step or in the case of CTW <ref type="bibr" target="#b36">[37]</ref> an eigendecomposition on the original correlation matrix which becomes much more expensive when dealing with data of high dimensionality.</p><p>It is worthwhile to mention that although in this work we explored simple network topologies, our cost function can be optimised regardless of the number of layers or neuron type (e.g., convolutional). Finally we also note that DCTW is agnostic to the use of the method for temporally warping the sequences and other relaxed variants of DTW might be employed in practise when there is a large number of observations in each sequence as for example Fast DTW <ref type="bibr" target="#b29">[30]</ref> or GTW <ref type="bibr" target="#b37">[38]</ref> as long as it conforms to the alignment constrains, i.e., it always minimises the objective function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusions</head><p>In this paper, we study the problem of temporal alignment of multiple sequences. To the best of our knowledge, we propose the first temporal alignment method based on deep architectures, which we dub Deep Canonical Time Warping (DCTW). DCTW discovers a hierarchical nonlinear feature transformation for multiple sequences, where (i) all transformed features are temporally aligned, and (ii) are maximally correlated. By means of various experiments on four real datasets, the significance of DCTW on multiple applications is highlighted, as the proposed method outperforms, in many cases by a very large margin, compared state-of-the-art methods for temporal alignment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Acknowledgements</head><p>George Trigeorgis is a recipient of the fellowship of the Department of Computing, Imperial College London, and this work was partially funded by it. The work of Stefanos Zafeiriou was partially funded by the EPSRC project EP/J017787/1 (4D-FAB) and by the the FiDiPro program of Tekes (project number: 1849/31/2015). The work of Björn W. Schuller was partially funded by the European Community's Horizon 2020 Framework Programme under grant agreement No. 645378 (ARIA-VALUSPA). We would like to thank the NVIDIA Corporation for donating a Tesla K40 GPU used in this work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Aligning sequences of subjects performing similar actions from the Weizmann database. (left) the three computed features for each of the sequences (1) binary, (2) euclidean, (3) poisson solution. (middle) The aligned sequences using DCTW. (right) Alignment errors for each of the six techniques.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>Facial expression alignment of videos S002-005 and S014-009 from the MMI dataset (Sec. 5.3). Depicted frames for each temporal phase with duration [t s , t e ] correspond to the middle of each of the temporal phase, t c = ⌈ ts+te 2 ⌉. We also plot the temporal phases ( neutral, onset, apex, and offset) corresponding to (i) the ground truth alignment and (ii) compared methods (DCTW, CTW, and GTW). Note that the entire video is included in our supplementary material. Temporal phase detection accuracy as defined by the ratio of correctly aligned frames with respect to the total duration for each temporal phase -the higher the better.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>Alignment errors on the task of audio-visual temporal alignment. Note that videos better illustrating the results are contained in our supplementary material.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Note that we assume zero-mean data to avoid cluttering the notation.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Since the nuclear norm is non-differentiable and motivated by<ref type="bibr" target="#b2">[3]</ref>, in<ref type="bibr" target="#b1">[2]</ref> the subgradient of the nuclear norm is utilised in gradient descent.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Aligning gene expression time series with time warping algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Aach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Church</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="495" to="508" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep Canonical Correlation Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Andrew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Consistency of trace norm minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">A probabilistic interpretation of canonical correlation analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Motion signal processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bruderlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH</title>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="page" from="97" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Aligning non-overlapping sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Caspi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="39" to="51" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Comparison of parametric representations for monosyllabic word recognition in continuously spoken sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mermelstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TASSP</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A least-squares framework for component analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>De La Torre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1041" to="1055" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Adaptive Subgradient Methods for Online Learning and Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Duchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Dynamic Manifold Warping for view invariant action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Medioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="571" to="578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Shape representation and classification using the poisson equation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gorelick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Actions as space-time shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gorelick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2247" to="2253" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">On multi-set canonical correlation analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hasan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCNN</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Sig. Prog. Mag</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="82" to="97" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Reducing the dimensionality of data with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">313</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="504" to="507" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Style translation for human motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Pulli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Popović</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page">1082</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">On the hidden markov model and dynamic time warping for speech recognitiona unified view</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-H</forename><forename type="middle">F</forename><surname>Juang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AT&amp;T Bell Laboratories Technical Journal</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1213" to="1243" />
			<date type="published" when="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">One Millisecond Face Alignment with an Ensemble of Regression Trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kazemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Josephine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep learning for robust feature generation in audiovisual emotion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">M</forename><surname>Provost</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICASSP</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3687" to="3691" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Rectifier nonlinearities improve neural network acoustic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Maas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Multivariate analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mardia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1979" />
			<publisher>Academic press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A linear time algorithm for computing exact Euclidean distance transforms of binary images in arbitrary dimensions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Maurer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Raghavan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="265" to="270" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Multimodal deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Multiset canonical correlations analysis and multispectral, truly multitemporal remote sensing data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Nielsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="293" to="305" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Web-based database for facial expression analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICME</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="317" to="321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">CUAVE: A new audio-visual database for multimodal human-computer interface research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Patterson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Fundamentals of Speech Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Rabiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Juang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993" />
			<biblScope unit="volume">103</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Fastdtw: Toward accurate dynamic time warping in linear time and space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Salvador</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD-Workshop</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Isotonic CCA for sequence alignment and activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shariat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pavlovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2572" to="2578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Closing the gap to humanlevel performance in face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1701" to="1708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="3371" to="3408" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Manifold Warping: Manifold Alignment over Time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Vu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Multimodal unsupervised feature learning for RGB-D scene labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-J</forename><surname>Cham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="453" to="467" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">X-ray microbeam speech production database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Westbury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JASA</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">S1</biblScope>
			<biblScope unit="page" from="56" to="56" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Canonical time warping for alignment of human behavior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>De La Torre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Generalized time warping for multi-modal alignment of human motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>De La Torre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1282" to="1289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Generalized Canonical Time Warping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>De La Torre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
