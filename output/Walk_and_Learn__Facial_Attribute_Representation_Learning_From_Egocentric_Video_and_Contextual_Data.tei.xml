<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Walk and Learn: Facial Attribute Representation Learning from Egocentric Video and Contextual Data</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Wang</surname></persName>
							<email>jing.wang@u.northwestern.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Northwestern University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
							<email>chengyu@us.ibm.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Northwestern University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ibm</forename><forename type="middle">T J</forename><surname>Watson</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Northwestern University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rogerio</forename><forename type="middle">Schmidt</forename><surname>Feris</surname></persName>
							<email>rsferis@us.ibm.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Northwestern University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ibm</forename><forename type="middle">T J</forename><surname>Watson</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Northwestern University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Walk and Learn: Facial Attribute Representation Learning from Egocentric Video and Contextual Data</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:48+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The way people look in terms of facial attributes (ethnicity, hair color, facial hair, etc.) and the clothes or accessories they wear (sunglasses, hat, hoodies, etc.) is highly dependent on geo-location and weather condition, respectively. This work explores, for the first time, the use of this contextual information, as people with wearable cameras walk across different neighborhoods of a city, in order to learn a rich feature representation for facial attribute classification, without the costly manual annotation required by previous methods. By tracking the faces of casual walkers on more than 40 hours of egocentric video, we are able to cover tens of thousands of different identities and automatically extract nearly 5 million pairs of images connected by or from different face tracks, along with their weather and location context, under pose and lighting variations. These image pairs are then fed into a deep network that preserves similarity of images connected by the same track, in order to capture identity-related attribute features, and optimizes for location and weather prediction to capture additional facial attribute features. Finally, the network is fine-tuned with manually annotated samples. We perform an extensive experimental analysis on wearable data and two standard benchmark datasets based on web images (LFWA and CelebA). Our method outperforms by a large margin a network trained from scratch. Moreover, even without using manually annotated identity labels for pre-training as in previous methods, our approach achieves results that are better than the state of the art.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Describing people based on attributes, such as gender, age, hair style and clothing style, is an important problem for many applications, including suspect search based on eyewitness descriptions <ref type="bibr" target="#b10">[11]</ref>, fashion analytics <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b4">5]</ref>, face retrieval and verification <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b30">31]</ref>, and person reidentification <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b37">38]</ref>. In this work, we address the problem of learning rich visual representations (i.e., "good features") for modeling person attributes without manual labels, with a focus on facial attribute prediction.</p><p>The state of the art in facial attribute classification, as demonstrated by standard evaluation benchmarks <ref type="bibr" target="#b30">[31]</ref>, has been advanced by methods that use deep convolutional neural networks (CNNs) pre-trained on massive amounts of images that have been manually annotated with identity labels. In fact, it has been shown that identity-related attributes such as gender, hair color, and age are implicitly encoded in nodes of CNNs that are trained for identity discrimination <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b41">42]</ref>. Despite the excellent performance, the feature representation learned by these methods requires costly manual annotation of hundreds of thousands or even millions of images in the pre-training stage. Moreover, the pretrained network fails to encode attributes that are not related to identity, such as eyewear and different types of hats.</p><p>In this paper, we address these issues by taking a different approach. Instead of relying on manually annotated images from the web, we learn a discriminative facial attribute representation from egocentric videos captured by a person walking across different neighborhoods of a city, while leveraging discretized geo-location and weather information readily available in wearable devices as a free source of supervision. The motivation for using location and weather data to construct facial attribute representations is illustrated in <ref type="figure">Figure 1</ref>. In New York City, for example, the likelihood of meeting an Afro-American casual walker in certain regions of Harlem is more than 90%. The same is true for Hispanics in Washington Heights, East Asians in Flushing, South Asians in India Square, East Europeans in Brighton Beach, and so on. These groups are characterized by their unique facial attributes (hair color, hair length, facial and eyes shape, etc.). Moreover, the weather conditions influence the facial appearance changes due to lighting variations and also dictate the clothing and accessories people wear. As an example, on sunny and warm days, the likelihood that a person will wear sunglasses, baseball hats, tshirts, and shorts increases, whereas the presence of scarfs, beanies, and jackets is much more frequent in cold days. <ref type="figure">Figure 1</ref>: Top: Casual walkers as imaged by people with wearable cameras walking across different neighborhoods of New York City. Due to changes in demographics, the expected appearance of facial attributes is highly dependent on location. Moreover, the weather conditions change facial appearance due to different lighting, and influence the choice of outfit and the use of accessories such as sunglasses, hats, and scarfs. Bottom: Face images obtained via face detection and landmark tracking. Note the large variations in lighting, expression, face pose, ethnicities, and accessories. We exploit this information to build rich visual feature representations for facial attribute classification.</p><p>Our goal is to leverage data about location and weather as weak labels to construct rich facial attribute representations.</p><p>Overview of our Approach. Our proposed feature learning method relies on processing identity-unlabeled data and learning feature embeddings from a few supervised tasks. We first track the faces of casual walkers using facial landmark tracking in more than 40 hours of egocentric video, obtaining face images under a variety of conditions, as shown in <ref type="figure">Figure 1</ref>. These face images are then arranged into pairs, where information from tracking is used to label the pairs as belonging to the same individual or not. Nearly 5 million pairs are generated and fed into a network that encodes identity-related features through a Siamese structure with contrastive loss, while further embedding contextual features based on location and weather prediction. Finally, the obtained feature representation is fine-tuned with manual labels for the task of facial attribute classification.</p><p>Generally, our proposed feature representation learning for person attribute modeling has the following advantages over previous methods: First, it does not require costly manual annotation in the pre-training stage. Second, by leveraging location and weather information, it encodes facial features beyond identity, in contrast to methods pre-trained on large image repositories with identity labels <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b30">31]</ref>. Third, it leverages the rich appearance of faces from a large number of casual walkers at different locations and lighting conditions, which may not be captured by images available on the web.</p><p>Our main contributions can be summarized as follows: 1. We introduce a new Ego-Humans dataset containing more than 40 hours of egocentric videos captured by people with wearable cameras walking across different regions of New York City. The data covers tens of thousands of casual walkers and includes both the weather and location context associated with the videos.</p><p>2. To the best of our knowledge, this is the first time a "walk and learn" approach that leverages discretized geolocation and weather information has been proposed for constructing deep visual representations for person attribute modeling. Our method seamlessly embeds this contextual information in a Siamese network that measures similarity of face pairs automatically extracted from tracks.</p><p>3. We show that our self-supervised approach can match or exceed the performance of state-of-the-art methods that rely on supervised pre-training based on hundreds of thousands or millions of annotated images with identity labels. In addition, we show that facial attributes are implicitly encoded in our network nodes as we optimize for location, weather, and face similarity prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Egocentric Vision. First-person vision methods have received renewed attention by the computer vision community <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b34">35]</ref>. Current methods and datasets have focused on problems such as video summarization <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b44">45]</ref>, activity recognition <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b51">52]</ref>, and social interaction <ref type="bibr" target="#b8">[9]</ref>. In contrast, our work is focused on the problem of looking at people and modeling facial attributes from a first-person vision perspective. Compared to existing egocentric datasets <ref type="bibr" target="#b2">[3]</ref>, our Ego-Humans dataset is the first of its kind; it deals with a different task, it is larger in scale, and it also has associated geo-location and weather information, which could be relevant for many other tasks.</p><p>Facial Attribute Modeling. Kumar et al. <ref type="bibr" target="#b20">[21]</ref> proposed a method based on describable facial attributes to assist in face verification and attribute-based face search. Siddiquie et al. <ref type="bibr" target="#b38">[39]</ref> and Luo et al. <ref type="bibr" target="#b31">[32]</ref> exploited the inter-dependencies of facial attributes to improve classification accuracy. Chen et al. <ref type="bibr" target="#b3">[4]</ref> built a feature representation that relies on discrimination of images based on first names, and showed improved results in age and gender classification. Berg and Belhumeur <ref type="bibr" target="#b1">[2]</ref> introduced part-based onevs.-one features (POOFs) and showed that features constructed based on identity discrimination are helpful for facial feature classification. Li et al. <ref type="bibr" target="#b26">[27]</ref> proposed a method that jointly learns discriminative binary codes and attribute prediction for face retrieval.</p><p>More recently, deep convolutional neural networks have advanced the state of the art in facial attribute classification. N. Zhang et al. <ref type="bibr" target="#b54">[55]</ref> proposed pose-aligned networks (PANDA) for deep attribute modeling. Z. Zhang et al. <ref type="bibr" target="#b55">[56]</ref> proposed a deep model based on facial attributes to perform pairwise face reasoning for social relation prediction. Luo et al. <ref type="bibr" target="#b30">[31]</ref> achieved state-of-the-art performance on the LFWA and CelebA datasets using a network pre-trained on massive identity labels. Our work, instead, achieves the same or superior performance without requiring manually annotated identity labels for the pre-training step.</p><p>Geo-Tagged Image Analysis. Many methods have been proposed for geo-tagged image analysis. In particular, image geo-localization, i.e., the problem of predicting the location of a query image, has received increased attention in the past few years <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b24">25]</ref>. Other related research includes discovering architectural elements and recognizing city attributes from large geo-tagged image repositories <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b57">58]</ref> and using location context to improve image classification <ref type="bibr" target="#b43">[44]</ref>. More closely related to our work, Islam et al. <ref type="bibr" target="#b49">[50]</ref> investigated the geo-dependence of facial features and attributes; however they used off-the-shelf facial attribute classifiers for this analysis, whereas the goal of our work is to build feature representations so as to improve the accuracy of facial attribute classifiers.</p><p>Representation Learning. Most high-performance computer vision methods based on deep learning rely on visual representations that are learned based on supervised pre-training, for example, using networks trained on millions of annotated examples such as the ImageNet dataset for general object classification <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b18">19]</ref>, or relying on mas-sive amounts of identity labels for facial analysis tasks <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b41">42]</ref>. Our work, instead, is focused on building rich visual representations for person attribute classification without using manual annotations in the pre-training step.</p><p>There is a long history of methods for unsupervised learning of visual representations based on deep learning <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b56">57]</ref>. When large collections of unlabeled still images are available, auto-encoders or methods that optimize for reconstruction of the data are popular solutions to learn features without manual labeling <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b45">46]</ref>. Doersch et al. <ref type="bibr" target="#b6">[7]</ref> proposed learning supervised "pretext" tasks between patches within an image as an embedding for unsupervised object discovery. These approaches, however, have not yet proven effective in matching the performance of supervised pre-training methods.</p><p>When video data is available, additional regularization can be imposed by enforcing temporal coherence <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b46">47]</ref> or through the so called slow feature analysis <ref type="bibr" target="#b48">[49]</ref>. More recently, Srivastava et al. <ref type="bibr" target="#b39">[40]</ref> used multilayer Long Short Term Memory (LSTM) networks to learn representations of video sequences, combining auto-encoders and prediction of future video frames.</p><p>Our work is related to other methods that learn visual representations from videos captured by wearable cameras or vehicle-mounted cameras <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b0">1]</ref>, where awareness of egomotion can be used as a supervisory signal for feature learning. In contrast to those methods, however, we leverage the geo-location and weather data that are readily available in wearable sensors as a source of free supervisory signal to learn rich visual representations which are suitable to facial attribute classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Ego-Humans Dataset</head><p>Data Collection. The Ego-Humans dataset was collected in New York City over a period of two months, from August 28 to October 26 (during the summer and fall seasons). The data consists of videos captured by three people with chest-mounted cameras, walking across different neighborhoods of the city. Two camera models were used: a GoPro camera (higher-quality) and a Vievu camera (lowerquality), both with 1080p resolution, capturing data at 30 frames per second. Within the two-month period, 25 days were selected for data collection, covering different regions of Manhattan and nearby areas, including the Financial District, Times Square, Central Park, Harlem, Little Italy, Brooklyn Bridge, Chinatown, Flushing, and others. In each day, one or more hours of video were recorded, at different times of the day and in different weather conditions. In total, we recorded more than 40 hours of egocentric videos, split into chunks of 15 minutes. In association with these videos, we recorded location using a GPS sensor and detailed weather information, such as temperature, precipitation, and weather condition, using an open weather API that retrieves this information based on geographic coordinates.</p><p>Discretization of Contextual Data. Rather than relying on fine-grained GPS coordinates, our learning algorithm considers a coarse set of locations as class labels. More specifically, we cluster GPS coordinates according to published census/ethnicity data 1 . In particular, we consider four ethnical groups: White, Black, Asian, and Indian. <ref type="figure" target="#fig_0">Figure 2</ref> shows an ethnicity map segmented based on census data, where each cluster has its own peculiar predominance of facial attributes. We are currently expanding this set (including Hispanics, for example) as we capture more data in other locations. Regarding weather, our data includes a variety of temperatures and conditions, but for training we have used two classes: sunny/hot and cloudy/cold. We note that other partitions of our data could be used for other tasks. As an example, for clothing attributes, GPS clustering based on socio-economic factors could be relevant, as well as finergrained weather conditions and temperatures.</p><p>In addition to extracting the weather and location labels, it is also important to generate face pairs (similar and dissimilar) for encoding identity features, which are helpful for discriminating several facial attributes. This procedure consists of two steps: 1) tracking casual walkers via face detection and landmark tracking and 2) image pair selection.</p><p>Tracking Casual Walkers. We used the OpenCV frontal face detector and facial landmark tracking based on the supervised descent method (SDM) <ref type="bibr" target="#b50">[51]</ref> to track casual walkers in the videos. The detector was tuned to output only high-confidence detections, with virtually no false alarms, at the expense of more false negatives. We used the intraface implementation of the SDM landmark tracking 2 , which works remarkably well, greatly expanding the set of captured face poses, lighting, and expressions as illustrated in <ref type="figure">Figure 1</ref>, without drifting. In total, we collected 15,000 face tracks, for a total of 160,000 face images.</p><p>Selecting Informative Pairwise Constrains: Given the face images extracted by face detection and tracking, we 1 http://projects.nytimes.com/census/2010/explorer 2 http://www.humansensing.cs.cmu.edu/intraface/ consider the following pairwise constraints:</p><p>• Temporal information: two faces connected by the same track can be assumed to belong to the same person. Conversely, two faces detected at the same video frame at different locations do not belong to the same person. • Geo-location: two faces captured from totally different geographic areas are assumed to be from different people. Based on these constraints, we generate nearly 5 million face pairs, along with their same/not same labels. As detailed in the next section, preserving similarity of face pairs connected by the same track improves robustness to lighting and pose variation, and learning features to discriminate different individuals is important for the final facial attribute classification task. <ref type="table" target="#tab_0">Table 1</ref> summarizes the information about our data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Facial Attribute Representation Learning</head><p>In the previous section, we introduced our unique Ego-Humans dataset. Next, we describe how we use this data to build a rich visual representation for facial attribute classification, based on a deep network that encodes features related to facial similarity, as well as weather and location information, without requiring manual annotations in the pre-training stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Learning Objective</head><p>Our learning framework builds upon the millions of face pairs automatically generated based on face detection and landmark tracking as described in the previous section, along with weather and location information. Our goal is to learn good features for facial attribute classification by leveraging this data. Specifically, given a face image x i ∈ X in the original pixel space, our goal is to obtain its associated facial representation r i ∈ R N , so that a facial attribute classifier can be constructed on top of r i (e.g., via network fine-tuning with a small set of manual labels).</p><p>In our learning algorithm, we have a training set U of N u face pairs, U = {(x i , x j ); y i,j }, where y i,j ∈ {1, −1} indicates whether (x i , x j ) are images of the same person or not. In addition, we also have another two training sets L w The learned feature r i should capture identity-related attributes (embedding in U ) and also preserve the high-level factors in L w and L g . Towards this goal, the deep network is initially trained over U by minimizing the verification loss d e (·) (to be described next) for face verification using a Siamese network structure. To learn high-level features from L w and L g , we train weather and location networks independently by minimizing their own softmax loss functions. The two contextual networks are initialized by the weights from the verification-trained model on the bottom layers and fine-tuned with individual contextual labels. The feature r i is the concatenation of the learned feature vectors of the top layer from each network and is further applied to train the facial attribute model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Deep Network Structure</head><p>To learn the embedding in U we design a Siamese network. A Siamese network consists of two base networks which share the same parameters. The Siamese structure is depicted in <ref type="figure" target="#fig_1">Figure 3(a)</ref>. For our experiments, we take images with a size of 90 × 90 × 3 as input. The size of the face is constrained by the image quality and the resolution from the videos. The base network uses the GoogLeNet style architecture in <ref type="bibr" target="#b36">[37]</ref>. This deep architecture contains two con-volutional layers and six layers of inception modules <ref type="bibr" target="#b42">[43]</ref> as shown in <ref type="figure" target="#fig_1">Figure 3</ref>(b). Due to the small input size, our architecture removes 5 × 5 filters from the inception models from layer inception 4a to inception 4d. The network contains around 4 million parameters. In the Siamese network, we connect the deep architecture with three inception modules and one fully connected layer.</p><p>The weather and location models share the same base architecture as the Siamese network, but do not share parameters at the top layers. In particular, as illustrated in <ref type="figure" target="#fig_1">Figure  3</ref>(c), we feed the fully-connected layer with inception modules 4c and 4d. This allows us to capture more localized features in the weather and location models, while encoding more global similarity in the identity verification model. In the three models (identity verification, weather, and location), the output feature vectors of the top fully connected layer are all 1024-dimensional vectors and are further concatenated to form the final facial attribute feature representation. We implemented the network using the Caffe deep learning toolbox <ref type="bibr" target="#b16">[17]</ref>. The complete network structure is shown in <ref type="figure" target="#fig_1">Figure 3(a)</ref>.</p><p>Loss Function: The Siamese network used to generate identity-related attribute features uses contrastive loss to preserve visual similarity of faces connected by the same track and dissimilarity to other tracks. The contrastive loss d e (·) is defined as:   where ✶(·) is the indicator function. This contrastive loss penalizes the distance between x i and x j in positive mode, and pushes apart pairs in negative mode up to a minimum margin distance specified by the constant δ. We use the l 2 norm for the distance measure. The parameters of the network are updated using stochastic gradient descent (SGD) <ref type="bibr" target="#b47">[48]</ref> by standard error back-propagation <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b35">36]</ref>. The weather and location prediction models use the softmax loss as mentioned earlier.</p><formula xml:id="formula_0">d e (x i , x j , y i,j ) = ✶(y i,j = 1)d(x i , x j ) + (1) ✶(y i,j = −1)max(δ − d(x i , x j ), 0)</formula><p>Fine-Tuning for Attribute Learning. After we obtain our pre-trained model based on the optimization described previously, the next step is to use standard fine-tuning with images manually labeled with facial attribute labels. Additional output layers are added for fine-tuning and the crossentropy loss is used for attribute classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Ablation Studies on Wearable Data</head><p>In this section, we first analyze the effectiveness of each component of our network on our wearable dataset. We have manually annotated 2714 images from 25 egocentric videos randomly selected from the data described in Section 3. The faces in this dataset have large variations in pose and resolution. Each annotated image contains seven- teen facial attributes covering global attributes (e.g., gender, ethnicity, age) and local features (e.g., eyewear, hair color, hat). All attributes are further categorized into binary class tasks. For this dataset, we randomly select 80% of the data as the training set and keep the rest for testing.</p><p>Analysis of the Verification Model. We first consider our base network (without the location and weather models). We evaluate the performance of training (fine-tuning) this network with the few available manual labeled examples, considering the following cases: 1) training from scratch: The network is initialized with random weights and the global learning rate is set as 0.001.</p><p>2) id(Ego-Humans): Training with our pre-trained model based on identity verification with 5M image pairs automatically extracted from our Ego-Humans dataset. After preinitializing the network with the weights learned from the verification models, we set the fine-tuning global learning rate with 0.0001, but with a learning rate in the top two layers of 10 times the global learning rate.</p><p>Both cases run through the whole wearable training data with 100 epochs in attribute learning. The results in <ref type="table" target="#tab_2">Table 2</ref> demonstrate that the pre-trained model outperforms training from scratch by a large margin. This is not surprising, given the relatively small training size of the dataset and the large variations in pose and lighting. This demonstrates the richness of our verification model obtained from unlabeled egocentric videos.</p><p>Analysis of the Geo-Location and Weather Models. Now we evaluate the benefit of features learned from geolocation and weather models. We perform experiments on the annotated wearable dataset by fine-tuning the network, considering the identity verification model only (id-verify), the inclusion of geo-location (id-verify + geo-loc), weather (id-verify + weather) and geo-location and weather models concatenated (id-verify + weather + geo-loc). The results are shown in <ref type="figure" target="#fig_3">Figure 5</ref>. The performance indicates average improvements of 2%, 2% and 4% when concatenating the base-net with the features fine-tuned from the geo-location,  weather, and geo-location + weather, respectively. The geolocation model provides more complementary information to the verification network on ethnicities like East Asian and South Asian. And the weather model adds in weights for non-identity-related but weather-related attributes like sunglasses and hat. <ref type="figure" target="#fig_2">Figure 4</ref> illustrates some examples of attribute prediction in our data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Comparison with the State of the Art</head><p>In this section we evaluate the effectiveness of our network with quantitative results on two standard facial attribute datasets, CelebA and LFWA, constructed based on face datasets CelebFaces <ref type="bibr" target="#b40">[41]</ref> and LFW <ref type="bibr" target="#b14">[15]</ref>, respectively. Both datasets have forty binary facial attributes, as listed in <ref type="table" target="#tab_4">Table 3</ref>. We use the exact same partition of data as in <ref type="bibr" target="#b30">[31]</ref>: 160k images of CelebA are used to fine-tune the network. In the remaining 40k CelebA images and the LFWA dataset, 50% of the images are used to extract the learned top-layer fc features from the network and to train a linear SVM classifier for attribute classification, and the other 50% are used for testing.</p><p>We evaluate the performance of our network on the two datasets with four state-of-the-art methods: Face-Tracer <ref type="bibr" target="#b19">[20]</ref>, two versions of PANDA <ref type="bibr" target="#b54">[55]</ref> network, PANDA-w and PANDA-l, based on the setting described in <ref type="bibr" target="#b30">[31]</ref>; and LNet+ANet <ref type="bibr" target="#b30">[31]</ref>. The same data was used for all approaches. FaceTracer utilizes hand-crafted features (HOG + color histogram) on face functional regions to train an SVM classifier. LNet+ANet uses a massive set of images with manually labeled identities for pre-training and cascades two networks to automatically detect the face region and consequently learn the facial attributes from the detected part.Apart from LNet+ANet, all the methods obtain cropped faces externally either from given landmark points (FaceTracer and PANDA-l) or based on off-the-shelf detection (PANDA-w and ours).</p><p>As shown in <ref type="table" target="#tab_4">Table 3</ref>, our approach significantly outperforms the four other methods on LFWA and reaches comparable performance with LNet+ANet on CelebA on average score, without using manual labeling in the pre-training stage. Our approach achieves better results than the prior methods on most of the forty attributes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Visual Attribute Discovery</head><p>The quantitative results for the above three datasets show that the pre-trained models on identity verification, geolocation, and weather classification boost the prediction of facial attributes despite the fact they are not explicitly  Given a layer in the pre-trained net, for example inception 4d in loc-net, we get top class-related neurons with high activations across all images within a class. For each selected neuron, we further select the most related image with highest value across the whole dataset. <ref type="figure" target="#fig_4">Figure 6</ref> shows the corresponding selected images of the top nine neurons in each class. We can see that the selected images in different geo-locations are from different races, which means the neurons in loc-net are learning strong priors about the concept of ethnicity in the location classification.</p><p>To better visualize attributes discovered by neurons, we construct the deconvnet framework following the ideas in <ref type="bibr" target="#b52">[53]</ref> and project selected top neurons back to the input pixel space. <ref type="figure" target="#fig_5">Figure 7</ref> presents the visualized features of the top nine neurons of specific layers after pre-training identity verification and weather classification separately. Recovering the whole face contour with clear discriminative parts such as the eyes and mouth, the visualized results of the selected neurons in the verification model from layer inception 4d in <ref type="figure" target="#fig_5">Fig 7(a)</ref> reveal that the neurons capture global identity-related face features. Therefore, facial attributes that are intrinsic to the identity, such as "gender", can be discovered by the network. The illustrated neurons in the weather model are from layer inception 4c and 4d. The visualizations of the selected neurons partially recover the upper face and focus on similar local components. By capturing local attributes such as "sunglasses" or "hat", the visualization explicitly demonstrates that the pre-trained weather model provides complementary features on identity-nonrelated attributes to the model from identity verification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Discussion</head><p>In the next few years, the amount of egocentric video associated with contextual information will grow significantly. As an example, many police officers around the world are already using body-worn cameras in patrol operations. This growth may be even greater as wearable devices become mainstream among ordinary people. We believe our work offers novel ways to learn rich facial representations from the ever-growing pool of unlabeled egocentric videos with contextual data. Although we have considered only the case of people walking across different neighborhoods of a city, our method could be applied at different geo-scales (e.g. worldwide) to capture larger variations.</p><p>One could argue that the face pairs generated by our approach inherit some bias. To the contrary, we have shown that in practice this is not an issue. In fact, we observed that faces across the same track exhibit large variations in pose and lighting, helping our approach to be more robust against these factors.</p><p>We would like to point out that our approach only requires location and weather data at the training stage. Although this contextual information could be useful at test time to improve accuracy, it may not always be available.</p><p>Finally, we are currently applying our approach to learn representations for fine-grained clothing attribute classification <ref type="bibr" target="#b4">[5]</ref>, as weather and location clearly influence clothing choices. By learning with diverse contextual information, the framework could be also applied to other high-level analysis tasks such as urban perception <ref type="bibr" target="#b33">[34]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusions</head><p>In this paper we have proposed a novel deep learning framework for learning facial attributes. Different from previous approaches, our method can capture good representations/features for facial attributes by exploiting videos and contextual data (geo-location and weather) captured by a wearable sensor as the person walks. The proposed framework can leverage the rich appearance of faces from tens of thousands of casual walkers at different locations and lighting conditions without requiring the cost of manual labels. We demonstrate our approach in several real-world datasets, showing substantial improvement over other baselines.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Grouping of GPS coordinates based on an ethnicity map defined by census data (best viewed in color).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>(a) Overview of the proposed network for facial attribute learning. (b) The base deep architecture model. (c) The network for location/weather predictionof N w images, L w = {x k ; c k } Nw k=1 , where c k ∈ {1, ..., C w } indicates the label of weather; and L g = {x l ; c l } Ng g=1 , where c l ∈ {1, ..., C g } indicates the label of geo-location. We use discretized values for weather and location labels as described in the previous section.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Example of annotated wearable data with predicted attributes</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Results of various baseline methods on the annotated wearable dataset. The embedding of location and weather net features help boost the performance, especially on ethnicity and non-identity related attributes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Top selected images in each class with max activations on layer inception 4e after pre-training loc-net. The discretized geo-location classes according to census from left to right are: White, Black, Asian, and Indian.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Visualized feature of top ranked neurons in models after pre-training identity verification (a) and pretraining weather classification (b). Best viewed in electronic form. trained for attribute classification. To better understand the attribute-related contextual information the pre-trained nets have learned, we show some qualitative examples of the top activated neurons in the pre-training phase.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><head>Table 1 :</head><label>1</label><figDesc>Statistics of the Ego-Humans Dataset.</figDesc><table>Collection period 
08/28 -10/26 
No. of days 
25 
Video footage40 hours 
Contextual info 
GPS and Weather Data 
No. of face tracks 
15,000 
No. of face images 
160,000 
No. of generated face pairs 
4.9 million 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc>Attribute prediction results of training the base net from scratch and with models after pre-training based on identity verification using the Ego-Humans dataset and the CASIA dataset.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc>Performance comparison with state of the art methods on 40 binary facial attributes</figDesc><table></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning to see by moving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">POOF: Part-Based One-vs-One Features for fine-grained categorization, face verification, and attribute estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">N</forename><surname>Belhumeur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The evolution of first person vision methods: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Betancourt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Morerio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Regazzoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rauterberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Techn</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="744" to="760" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">What&apos;s in a name: First names as facial attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Girod</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep domain adaptation for describing people based on fine-grained clothing attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Unsupervised visual representation learning by context prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">What makes paris look like paris?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="101" />
			<date type="published" when="2012-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Social interactions: A first-person perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Hodgins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning to recognize daily actions using gaze</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Attributebased people search: Lessons learnt from a practical surveillance system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bobbitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pankanti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICMR</title>
		<meeting>ICMR</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning and calibrating per-location classifiers for visual place recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gronat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Obozinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Im2gps: estimating geographic information from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A fast learning algorithm for deep belief nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1527" to="1554" />
			<date type="published" when="2006-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Labeled faces in the wild: A database for studying face recognition in unconstrained environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
		<idno>07-49</idno>
		<imprint>
			<date type="published" when="2007" />
			<pubPlace>Amherst</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Massachusetts</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning image representations equivariant to ego-motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jayaraman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Multimedia</title>
		<meeting>ACM Multimedia</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Where to buy it: Matching street clothing photos in online shops</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Kiapour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Facetracer: A search engine for large collections of images with faces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Belhumeur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Describable visual attributes for face verification and image search. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">N</forename><surname>Belhumeur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1962" to="1977" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Re-id: Hunting attributes in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Layne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Building high-level features using large scale unsupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="541" to="551" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Predicting geoinformative attributes in large-scale image collections using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Crandall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Predicting important objects for egocentric video summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="38" to="55" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Two birds, one stone: Jointly learning binary code for large-scale face image retrieval and attributes prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Delving into egocentric actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Cross-view image geolocalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Street-toshop: Cross-scenario clothing retrieval via parts alignment and auxiliary set</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep learning face attributes in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A deep sum-product architecture for robust facial attributes analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep learning from temporal coherence in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mobahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning high-level judgments of urban perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">First-person pose recognition using egocentric workspaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rogez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Supancic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Iii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning representations by back-propagating errors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Modeling</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Transferring a semantic representation for person re-identification and search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Image ranking and retrieval based on multi-attribute queries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Siddiquie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Unsupervised learning of video representations using lstms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mansimov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deep learning face representation by joint identification-verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deeply learned face representations are sparse, selective, and robust</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Improving image classification with location context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Manohar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Rob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Lubomir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Egocentric video summarization of cultural tour based on user preferences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Varini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Serra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cucchiara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Multimedia</title>
		<meeting>ACM Multimedia</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Extracting and composing robust features with denoising autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations using videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">The general inefficiency of batch training for gradient descent learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">R</forename><surname>Martinez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1429" to="1451" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Slow feature analysis: Unsupervised learning of invariances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wiskott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">J</forename><surname>Sejnowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="715" to="770" />
			<date type="published" when="2002-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">DeepFocal: A Method for Direct Focal Length Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Workman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Greenwell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Baltenberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jacobs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Supervised descent method and its applications to face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>De La</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torre</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Temporal sequence modeling for video event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Yu Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanfu</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Choudhary</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Deconvolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Panda: Pose aligned networks for deep attribute modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Learning social relation traits from face images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Stacked what-where auto-encoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goroshin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno>abs/1506.02351</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Recognizing city identity via attribute analysis of geo-tagged images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
