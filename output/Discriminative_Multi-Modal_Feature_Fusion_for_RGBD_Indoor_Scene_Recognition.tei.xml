<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Discriminative Multi-modal Feature Fusion for RGBD Indoor Scene Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyuan</forename><surname>Zhu</surname></persName>
							<email>zhuh@i2r.a-star.edu.sg.weibel@gatech.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">*</forename><surname>Star</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Singapore</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Baptiste</forename><surname>Weibel</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Tech</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Usa</forename><surname>Jean-Baptiste</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijian</forename><surname>Lu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">*</forename><surname>Star</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Singapore</surname></persName>
						</author>
						<title level="a" type="main">Discriminative Multi-modal Feature Fusion for RGBD Indoor Scene Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:39+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>RGBD scene recognition has attracted increasingly attention due to the rapid development of depth sensors and their wide application scenarios. While many research has been conducted, most work used hand-crafted features which are difficult to capture high-level semantic structures. Recently, the feature extracted from deep convolutional neural network has produced state-of-the-art results for various computer vision tasks, which inspire researchers to explore incorporating CNN learned features for RGBD scene understanding. On the other hand, most existing work combines rgb and depth features without adequately exploiting the consistency and complementary information between them. Inspired by some recent work on RGBD object recognition using multi-modal feature fusion, we introduce a novel discriminative multi-modal fusion framework for rgbd scene recognition for the first time which simultaneously considers the inter-and intra-modality correlation for all samples and meanwhile regularizing the learned features to be discriminative and compact. The results from the multimodal layer can be back-propagated to the lower CNN layers, hence the parameters of the CNN layers and multimodal layers are updated iteratively until convergence. Experiments on the recently proposed large scale SUN RGB-D datasets show that our method achieved the state-of-the-art without any image segmentation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Indoor scene recognition based on RGBD camera has attracted increasingly attention due to its wide applications in computer vision and robotics and the popularity of depth sensors. Although much progress has been achieved in the past few years, indoor scene recognition is still challenging due to the large intra-object variation and spatial layout changes, not to mention the challenges caused by the occlusion and low-light condition.</p><p>Given an input indoor image, we human can quickly recognize the scene category and generalize the brained learned recognizer to the new place which is not seen be-fore. The key to success of our brain lies in three aspects:</p><p>(1) its exposure to the dense and diversity sampling of our visual world; (2) its versatile capacity to abstract compact and discriminative representations of different complexities; (3) its high efficiency to fuse information from multimodalities to perform high-level reasoning.</p><p>To improve current scene recognition systems, a large dataset is very helpful to learn meaningful representations and prevent overfitting. Early scene-centric datasets <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b12">13]</ref> for outdoor and indoor scene recognition either has a small number of scene categories or dataset size compared with the counterparts in object centric datasets, such as Im-ageNet <ref type="bibr" target="#b13">[14]</ref>. Recently, Zhou et al <ref type="bibr" target="#b21">[22]</ref> proposed the Places dataset which contains 7 million images from 476 places categories, making it the largest scene and place database, which allows us to train data hungry models. Although depth sensors has become cheaper, the number of influential RGBD dataset is still relative smaller than RGB counterparts. The first generation benchmarks, such as NYU D2 <ref type="bibr" target="#b15">[16]</ref> and Berkeley B3DO dataset <ref type="bibr" target="#b5">[6]</ref>, have bootstrapped the initial progress of RGBD scene understanding. Recently, Song et al <ref type="bibr" target="#b16">[17]</ref> proposed the large scale SUN-RGBD dataset, which is the first RGBD dataset that has a similar scale as the PASCAL VOC dataset <ref type="bibr" target="#b2">[3]</ref>, hence make it possible to borrow the success technique in RGB scene understanding to RGBD research.</p><p>Besides a large dataset, features are also vital for scene recognition. Many features have been proposed for scene recognition, here we just briefly mention some recent influential and related works due to the space limits. Early scene recognition uses hand-crafted features such as SIFT <ref type="bibr" target="#b10">[11]</ref>, GIST <ref type="bibr" target="#b11">[12]</ref> and CENTRIST <ref type="bibr" target="#b19">[20]</ref>, which achieved reasonable performance for certain tasks. On the other hand, as the hand-crafted features models low-level activations from V1 cells, they can ignore the more discriminative information in higher hierarchies which are vital for scene understanding. Moreover, the hand-craft features also have a low generalization capability when migrating to new tasks. To avoid the limitations of hand-craft features, the unsupervised feature learning has been proposed, such as deep belief nets <ref type="bibr" target="#b4">[5]</ref>, deep Boltzmann machines <ref type="bibr" target="#b14">[15]</ref>, deep autoen-  <ref type="figure">Figure 1</ref>. Pipeline of our multi-modal feature fusion framework for RGBD indoor scene recognition. The input to our system are small batches of RGB images and corresponding HHA[4] encoded depth images. The CNNs for RGB and depth modalities are then separately trained and the features from the second fully connected layers are fed into the multi-modal learning layer F for learning two projection matrix W1 and W2. F formulates an objective function which maximizes the inter-and intra-modality associations for samples from the same class and vice versa for samples from different classes, and meanwhile regularize the learned features to be discriminative and compact. The blue solid lines denote the forward training of multi-modal layer F . The blue dashed lines denote the parameter backpropagation process. The golden lines denote the final feature fusion between two modalities for final rgbd scene recognition.</p><p>coders <ref type="bibr" target="#b7">[8]</ref>, convolutional deep belief networks <ref type="bibr" target="#b8">[9]</ref>, hierarchical sparse coding. The recent work of Bo et al <ref type="bibr" target="#b0">[1]</ref> on Hierarchical Matching Pursuit and its Multipath variant <ref type="bibr" target="#b1">[2]</ref> has achieved good performance for various tasks, including RGBD object recognition. While, the feature hierarchies learned by unsupervised feature learning is still comparatively shallower than the recent popular deep convolutional neural network <ref type="bibr" target="#b6">[7]</ref>, which first appears in ImageNet classification challenges ILSVRC-2012. Recently, <ref type="bibr" target="#b9">[10]</ref> explored to incorporate segmentation in CNN learning framework and achieved the state-of-the-art for rgbd scene recognition.</p><p>To learn features for RGBD scene recognition, one can apply existing methods to color and depth modalities separately, or simply treat RGBD as un-differentiated fourchannel data. Such separate learning and un-differential handling can ignore the consistency and complementary information between the two modalities and their relative importance for various tasks. Hence, the relationship between different modalities have not been thoroughly investigated for rgbd scene recognition. To resolve the above issue, we proposed a discriminative multi-modal feature fusion framework for RGBD scene recognition. The proposed framework is illustrated in <ref type="figure">Fig.1</ref>. The basic idea is that we seek to transform the activations from the trained rgb and depth CNNs to a common subspace, such that we can discover the discriminative features for both modalities and simultaneously increase the association between same class' samples and decrease the association between different classes' samples for both intra-and inter-modalities. Our experiments on SUN-RGBD dataset shows that our method out-performed <ref type="bibr" target="#b9">[10]</ref> without any image segmentation. More importantly, our research highlights the potential of appropriate feature fusion for RGBD scene recognition, which is worthwhile for further research.</p><p>Our work is inspired by two recent works <ref type="bibr" target="#b18">[19]</ref> and <ref type="bibr" target="#b17">[18]</ref> for RGBD object recognition, which applied multi-modal feature learning to fuse the response from the CNNs trained with RGB image and surface normal images. There are several differences between our work and these two works. Firstly, the task domain is quite different. <ref type="bibr" target="#b18">[19]</ref> and <ref type="bibr" target="#b17">[18]</ref> targets the RGBD object recognition for the prop-like images in controlled environment, while for scene recognition, the image is much more cluttered. In terms of theoretical side, <ref type="bibr" target="#b18">[19]</ref> enforced the intra-and inter-modalities correlations between pairwise samples and <ref type="bibr" target="#b17">[18]</ref>, enforce the correlations between the features of each sample individually. While, we  consider both information in our formulation. In addition, our formulation includes the regularization for the withinclass and between-class inter-view associations, which is ignored in <ref type="bibr" target="#b18">[19]</ref>. We prove the necessity of this regularization by testing our implementation of <ref type="bibr" target="#b18">[19]</ref> on the SUN-RGBD dataset. The results demonstrate our methods' theoretical and performance advantages. Moreover, in terms of initial feature formulation, we include CNN features fine tuned on the HHA encoded depth layer <ref type="bibr" target="#b3">[4]</ref>, which has proved to be more discriminative than raw depth image and surface normal image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Proposed method</head><formula xml:id="formula_0">Let X = [x 1 , x 2 , ..., x N ] ∈ R d1×N</formula><p>and Y = [y 1 , y 2 , ..., y N ] ∈ R d2×N denote the d 1 -and d 2 -dimensions of the activations from the second fully connected (fc2) layer of color and depth CNN in one data batch of N images. Let W i ∈ R Mi×di be the transformation matrix for the modality i, (i = 1, 2), and F 1 ∈ R M1×N and F 2 ∈ R M2×N be the learned features for the rgb and depth modalities, respectively. M i is the projected feature dimension for color (i = 1) and depth features (i = 2).</p><p>Our task is to learn a new representation F 1 = W 1 X and F 2 = W 2 Y such that the correlation for same class' samples are maximized for both inter-and intra-modalities, and vice versa for different classes' samples, and require the learned features to be compact and discriminative according to the input samples. Finally, the learned features F s = [F 1 F 2 ] are fed to the SVM to train the final scene classifier. Noted, our framework can incorporate any stateof-the-art features as input and any state-of-the-art classifiers for final prediction, e.g. we can incorporate the sparse logistic regression to from an end-to-end learning system and back-propagate the parameters to the lower layers as in <ref type="bibr" target="#b17">[18]</ref>. We choose SVM in our work due to its robustness to outliers. An illustration of our method is shown in <ref type="figure" target="#fig_1">Fig. 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Formulation</head><p>To learn features for both modalities, our objective function is formulated as</p><formula xml:id="formula_1">min {W1,W2,F1,F2,α1,α2} F = 2 i=1 α p i [t 1 D i + t 2 R i ] + β(C 1 − λ 2 C 2 ) subject to α 1 + α 2 = 1, α 1 ≥ 0, α 2 ≥ 0 (1) where D i := D i (W i ) is the intra-modality discriminative term and R i := R i (W i , F i )</formula><p>is the intra-modality reconstruction term for modality i, i = {1, 2}, which are to be elaborated in following parts. These two terms are balanced by the trade-off factor t 1 and t 2 respectively. α 1 and α 2 are self-adjusted weights for each modality. The hyperparameter p is introduced to avoid trivial sub-optimal solution when only one modality is selected before such nonlinear handling <ref type="bibr" target="#b18">[19]</ref>. C 1 and C 2 are the inter-modality correlation terms for within-class and between-class samples. Intra-modality discriminative term: The discriminative term D 1 (W 1 ) for RGB modality is intended to find a W 1 to project the RGB CNN activations X to a space in which the distance between x i and x j is small if they are of the same class, otherwise large if they are from different classes. D 2 (W 2 ) is similarly defined for the depth CNN activations Y . The constraint can be defined as: if two objects are from the same class (y ij = 1), their relative feature distance should be smaller than a given threshold µ 1 − τ 1 , otherwise (y ij = −1) the distance should be larger than µ 1 + τ 1 , which is similarly defined as in <ref type="bibr" target="#b18">[19]</ref>. Mathemati-cally, this can be expressed as</p><formula xml:id="formula_2">y ij (µ 1 − d W1 (x i , x j )) &gt; τ 1<label>(2)</label></formula><p>where the distance between a pair of the CNN activations x i and x j is computed as</p><formula xml:id="formula_3">d W1 (x i , x j ) = (W 1 x i − W 1 x j ) T · (W 1 x i − W 1 x j ) (3)</formula><p>and the discriminative term is defined as follows</p><formula xml:id="formula_4">D 1 (W 1 ) = ∀i,j h(τ 1 − y ij (µ 1 − d W1 (x i , x j ))) (4)</formula><p>where h is a smoothed hinge loss function h(x) = max(0, x), where D 2 (W 2 ) is similarly defined for the depth modality. Intra-modality reconstruction term: The reconstruction term R 1 (W 1 , F 1 ) for color modality is defined as</p><formula xml:id="formula_5">R 1 (W 1 , F 1 ) = ||W 1 X − F 1 || 2 F + ||W T 1 F 1 − X|| 2 F + λ 1 g(F 1 )<label>(5)</label></formula><p>this term enforces the learned feature F 1 to be similar to the W 1 transformed X, while the second term encourages F 1 to reconstruct X when back-transformed via W T 1 , and the third term g is the smooth L 1 penalty function <ref type="bibr" target="#b17">[18]</ref>. With the reconstruction term, the supervised information is introduced to allow W 1 better fit the observed training data individually. Likewise, R 2 (W 2 , F 2 ) corresponds to the reconstruction error for the depth modality Y .</p><p>Inter-modality correlation term: With the discriminative term and reconstruction term, the distances between same class samples are decreased and the distances between different classes samples are increased for each modality. However, the data captured from different modalities may suffer from missing information or noise pollution, hence we seek to exploit the correlation between different modalities to reduce misclassification, such that the association of within-class samples are maximized, while the association between different classes are minimized for inter-views. Such regularization is introduced by adding two inter-view correlation terms C 1 and C 2 which minimize the pairwise distances between the color and depth modalities of the same class and vice versa for the samples from different classes.</p><formula xml:id="formula_6">C 1 (W 1 , W 2 ) = c ∀i,j∈c [ (d W1 (x i , x j ))− (d W2 (y i , y j ))] 2<label>(6)</label></formula><p>and</p><formula xml:id="formula_7">C 2 (W 1 , W 2 ) = c =d c,d ∀i∈c,j∈d [ (d W1 (x i , x j ))− (d W2 (y i , y j ))] 2<label>(7)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Alternating optimization</head><p>To our knowledge, there is no closed-form solution to Eq.1 because we need to solve W k , F k and a k jointly. To address this, we adopt the alternating approach to optimize W k , F k , a k . The pseudocode of our algorithm is illustrated as in Algorithm 1:</p><p>Algorithm 1 Optimizing our proposed feature fusion framework Input: Training set with two modalities: X, Y , the corresponding label Output: Feature projection matrix: W 1 , W 2</p><p>Step 1 (Initialization):  <ref type="formula" target="#formula_2">( 12)</ref> end for</p><formula xml:id="formula_8">Initializae W 1 , W 2 , F 1 , F 2 , a 1 ,</formula><p>In Step 2.1 and 2.2, we update the other variables using the gradient descent algorithm, where the same learning rate γ is used. In Step 2.1, we update F 1 , F 2 , the derivate of F respect to F 1 are shown below:</p><formula xml:id="formula_9">∂F ∂F 1 = 2a p 1 t 2 [(F 1 − W 1 X) + W 1 (W T 1 F 1 − X) + λ 1 g ′ (F 1)]<label>(8)</label></formula><p>then F 1 is updated as</p><formula xml:id="formula_10">F 1 ← F 1 − γ ∂F ∂F 1<label>(9)</label></formula><p>In step 2.2, when F 1 , F 2 , a 1 , a 2 are fixed, W i is updated, e.g.</p><p>∂F</p><formula xml:id="formula_11">∂W 1 = 2α p 1 t 1 [(W 1 X − F 1 )X T + W 1 (W T 1 F 1 − X)] + 2t 2 W 1 [α p 1 ∀i,j y ij h ′ (τ 1 − y ij (µ 1 − dW 1 (x i , x j ))A i,j ) + β( c ∀i,j∈c (1 − dW 2 (y i , y j ) dW 1 (x i , x j ) )A i,j − λ 2 c =d c,d ∀i∈c,j∈d (1 − dW 2 (y i , y j ) dW 1 (x i , x j ) )A i,j ))]<label>(10)</label></formula><p>where</p><formula xml:id="formula_12">A i,j = (x i − x j ) T (x i − x j ) W 1 ← W 1 − γ ∂F ∂W 1<label>(11)</label></formula><p>Then by fixing W 1 , W 2 , F 1 , F 2 , we can update α 1 , α 2 accordingly by attaching Lagrange multiplier based on :</p><formula xml:id="formula_13">L(α, η) =α p 1 T 1 + α p 2 T 2 − η(α 1 + α 2 − 1) =α p 1 (t 1 D 1 (W 1 ) + t 2 R 1 (W 1 , F 1 )) +α p 2 (t 1 D 2 (W 2 ) + t 2 R 2 (W 2 , F 2 )) + − η(α 1 + α 2 − 1)<label>(12)</label></formula><p>By setting ∂L(α,η) α and ∂L(α,η) η to 0, α i can be updated as:</p><formula xml:id="formula_14">α i = (1/T i ) 1/(p−1) 2 i=1 (1/T i ) 1/(p−1)<label>(13)</label></formula><p>Finally, the learned weight can be back-propagated to the lower layer of CNN by</p><formula xml:id="formula_15">∂F ∂x i = α p 1 t 1 ∂D 1 (W 1 ) ∂x i +β ∂C ∂x i +α p 1 t 2 ∂R 1 (W 1 , F 1 ) ∂x i<label>(14)</label></formula><p>where</p><formula xml:id="formula_16">∂D 1 (W 1 ) ∂x i = ∀j 2y ij W T 1 W 1 (x i − x j ) h ′ (τ 1 − y ij (µ 1 − d W1 (x i , x j ))) (15) ∂C(W 1 , W 2 ) ∂x i = 2W T 1 W 1 c ∀i,j∈c (x i − x j ) d W1 (x i , x j ) − d W2 (y i , y j ) d W1 (x i , x j ) − λ 2 c =d c,d ∀i∈c,j∈d (x i − x j ) d W1 (x i , x j ) − d W2 (y i , y j ) d W1 (x i , x j )<label>(16)</label></formula><formula xml:id="formula_17">∂R 1 (W 1 , F 1 ) ∂x i = W T 1 (W 1 x i − f i ) − (W T 1 f i − x i ) (17)</formula><p>for color modality X and is similarly defined for depth modality Y .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experiments</head><p>To evaluate the effectiveness of our proposed method, we perform experiments on recently proposed SUN-RGBD dataset <ref type="bibr" target="#b16">[17]</ref>. The details of the experiments and the results are discussed in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Datasets and experiment setup</head><p>SUN-RGBD Dataset: The SUN-RGBD dataset is the first large scale dataset which has a similar scale as PAS-CAL VOC <ref type="bibr" target="#b2">[3]</ref>. The dataset was captured by four different sensors, and contains 10,335 RGB-D images in 47 scene categories. For scene categorization, the benchmark of scene classification is conducted on 19 subsets of the dataset with more than 80 samples.</p><p>Architecture of CNNs The architecture of the CNN for scene classification are exactly the same as the AlexNet <ref type="bibr" target="#b6">[7]</ref>. The network contains eight layers with weights, with five convolutional layers and the three fully-connected layers. The network has about 60 million parameters. For scene classification, the network was initialized by using the network of PlacesCNN <ref type="bibr" target="#b21">[22]</ref>, which was trained from 205 categories of places with minimum 5,000 and maximum 15,000 images per-category. The last fully-connected layer is removed and the second fully-connected layer (fc2) is used for feature extraction. Then we fine-tune the network for the RGB images in SUNRGBD dataset, which form the indoor scene centric CNN. To train the depth CNN, we first encode the depth images with the HHA encoding method proposed by Gupta et al. <ref type="bibr" target="#b3">[4]</ref>, which generates three channels at each pixel with the information of horizontal disparity, height above ground and the angle the pixel's local surface normal makes with the inferred gravity direction. HHA encodes the properties of geometric pose that emphasize complementary discontinuities in the image. After the HHA encoding, we fine-tune the RGB CNN on the encoded depth dataset. We use the recent popular deep learning platform Caffe, the network was trained on a Titan X.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Parameter setting and analysis</head><p>For our multi-modal feature fusion framework, the dimension M i of the projected features are set to the same M i = 2048 for both modality, the effects of different number of M i can be observed in <ref type="figure" target="#fig_2">Fig.3</ref>, one can see that both excessive small or large M i lead to performance drop. The parameters β = 1e −10 , λ 1 = 1, λ 2 = 1e −11 , µ 1 = 100, µ 2 = 1000, τ 1 = 1, τ 2 = 1, K = 300 are tuned empirically on the validation set and then fixed during testing.</p><p>We also evaluate the contribution of each term by incrementally including them in our objective function, which is achieved by turning the related trade-off factors λ 2 and t 2 from 0 to our validated parameters. We also selfimplemented Wang et al.'s method <ref type="bibr" target="#b18">[19]</ref> and tested its performance on the SUN RGB-D dataset with the parameters tuned on the validation set. From <ref type="table">Table.</ref>1, one can observe that by considering within-class and between-class correlation for inter-modality and intra-modality explicitly, we achieved 6.3% improvement over Wang et al. <ref type="bibr" target="#b18">[19]</ref>. Our full model achieved around 8.7% improvement over the model with just discriminative term and correlation term. The underlying reason is that the discriminative term and correlation term learns discriminative feature for pairwise samples, however final classification is evaluated on each sample individually. Hence the introduction of reconstruction term allow the supervised information to regularize on the learned transformation matrix, such that it can better fit the observed samples, therefore achieves large performance improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Accuracy(%) Wang et al <ref type="bibr" target="#b18">[19]</ref> 26.5 Discriminative term + Correlation term 32.8 Full model 41.5 <ref type="table">Table 1</ref>. The illustration of the contribution of each term. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Convergence analysis</head><p>To show the convergence of our methods, we plot the value of our objective function for the 300 iterations with all the parameters fixed. From the graph, we can see that the objective function converge quickly with around first 200 iterations and converge slowly for next 100 iterations, as shown in <ref type="figure" target="#fig_3">Fig.4</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Comparison with other methods</head><p>We compare our method with different baselines. Specifically, we compare with the hand-craft descriptor GIST <ref type="bibr" target="#b11">[12]</ref>, unsupervised feature learning descriptor HMP <ref type="bibr" target="#b0">[1]</ref> and supervised AlexNet <ref type="bibr" target="#b6">[7]</ref>, PlaceCNN <ref type="bibr" target="#b21">[22]</ref> and the state-of-the-art SSCNN <ref type="bibr" target="#b9">[10]</ref> on SUNRGBD dataset. The accuracy figures of GIST, AlexNet, PlaceNN, SSCNN on RGB or RGB+D are cited from <ref type="bibr" target="#b9">[10]</ref>. For HMP, we compute the descriptor on both RGB and HHA encoded depth, then the features are concatenated to train the classifier.</p><p>The introduction of the our comparing methods is given as follow:</p><p>• GIST <ref type="bibr" target="#b11">[12]</ref>. GIST is a famous hand-craft scene descriptor, which computes the spectral information in an image through Discrete Fourier Transform (DFT). The spectral signals are then compressed by the Karhunen-Loeve Transform (KLT).</p><p>• Hierarchical Matching Pursuit (HMP) <ref type="bibr" target="#b0">[1]</ref> is an recent proposed unsupervised feature learning method. It builds feature hierarchy layer-by-layer using matching pursuit encoder. We use the original code provided by <ref type="bibr" target="#b0">[1]</ref> for best performance.</p><p>• AlexNet <ref type="bibr" target="#b6">[7]</ref>: Since the performance of our method, PlacesCNN and SSCNN are based on AlexNet, hence the performance of original AlexNet is included. AlexNet was trained on the object centric ImageNet dataset, while our method <ref type="bibr" target="#b13">[14]</ref>, PlacesCNN and SS-CNN on the scene centric dataset, such as Places <ref type="bibr" target="#b21">[22]</ref> and SUN-RGBD <ref type="bibr" target="#b16">[17]</ref>.</p><p>• Places-CNN <ref type="bibr" target="#b21">[22]</ref>. Place-CNN is pre-trained on 2.5 million scene images using Alexnet. In <ref type="bibr" target="#b21">[22]</ref>, both Linear SVM and RBF Kernel SVM are considered to train and classify the Place-CNN extracted features on RGB and RGBD.</p><p>• SSCNN [10] makes use of a slightly modified AlexNet that trained with the SUN-RGBD dataset. The network is divided into two branches, one for semantic segmentation, the other for image classification. The image classification branch is regularized by the semantic segmentation.</p><p>• Our model. We also use our projected features for RGB and RGBD to train linear SVM.</p><p>The experiment result is shown in <ref type="table">Table.2 and Table.</ref>3, where the accuracy is the mean accuracy of 19 scene classes. The confusion matrix is also shown in <ref type="figure" target="#fig_4">Fig.5</ref>, where the diagonal represent the recognition accuracy of each scene. From the tables, it can be seen that the CNN trained with scene centric databases, such as Place-CNN, SSCNN and our model, out-perform the object-centric AlexNet and those hand-craft GIST and HMP based on unsupervised feature learning, which proves the advantage of scene specific modeling. The other notable phenomenon is that the performance of the classifiers trained by combining the rgb and depth features always out-perform the rgb counterparts, which confirms the common intuition that there are some consistency and complementary information between rgb and depth modalities.</p><p>Moreover, the SVM trained with our projected features, outperform the PlaceCNN RBF baseline by around 2.5% in terms of accuracy, which means the features learned by our method has preserved most of the discriminative feature from the original RGB and depth CNN, while eliminating a lot of noisy and redundant features, which proves the validity of feature fusion. Our method's slightly outperformed SSCNN by 0.2%. Our projected color features out-performed the SSCNN's color version by 0.9%. Actually, our method and SSCNN present different tools toward RGB-D scene recognition problem. SSCNN goes along feature engineering, while we go along feature fusion, and these two methods are both effective and they could be combined together, which is left for future study.</p><p>In terms of learned feature dimension, SSCNN is the most compact, which has only 512 dimensions. The dimen-sion of our learned feature (4096 dimensions) lies in between SSCNN and HMP (28000 dimensions). But regarding the generalization capability, our method out-perform SSCNN as our framework only needs image-wise groundtruth, while SSCNN need to prepare pixel-wise groundtruth which is restrictive when applied to a new task. Moreover, SSCNN requires image segmentation, which would introduce extra computational overhead. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Conceptual illustration of our method, ( and △) denote the samples from different classes, and the blue and orange colors denote the samples from rgb and depth modalities separately.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>The effect of choosing different Mi.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Convergence of the objective function.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>The confusion matrix of our method's scene recognition performance on SUN-RGBD datasset. The vertical axis shows the ground-truth classes, the horizontal axis show the predicted classes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head></head><label></label><figDesc>Table 3. Scene recognition accuracy for different classes.Table 2. Scene recognition accuracy of different methods.</figDesc><table>Class 

bathroom 
bedroom 
classroom 
computer room 
conference room 
corridor 
Accuracy (%) 
89.1 
69.0 
54.1 
41.5 
20.2 
69.0 
Class 
dining area 
dining room 
discussion area 
furniture store 
home office 
kitchen 
Accuracy (%) 
33.7 
41.3 
6.7 
87.4 
11.5 
66.5 
Class 
lab 
lecture theatre 
library 
living room 
office 
rest space 
Accuracy (%) 
12.4 
25.6 
28.8 
22.4 
43.1 
59.3 
Class 
study space 
Average Accuracy 
Accuracy (%) 
7.5 
41.5 

Model 
Input 
Accuracy (%) 
GIST [12] + 
RGB 
19.7 
RBF Kernel SVM RGB+D 
23.0 
HMP [1] + 
RGB 
21.7 
Linear SVM 
RGB+D 
25.7 
AlexNet [7] 
RGB 
24.3 
RGB+D 
30.7 
Place-CNN [22] + 
RGB 
35.6 
Linear SVM 
RGB+D 
37.2 
Place-CNN [22] + 
RGB 
38.1 
RBF Kernel SVM RGB+D 
39.0 
SSCNN [10] 
RGB 
36.1 
RGBD 
41.3 
Our Model + 
RGB 
37.0 
Linear SVM 
RGB+D 
41.5 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusion</head><p>In this work, we propose a novel discriminative feature fusion framework for RGBD scene recognition. Our framework considers the inter-and intra-modalities correlation for all class samples and meanwhile regularize the learned feature to be discriminative and compact. Our method outperforms other state-of-the-arts over the recently proposed SUN-RGBD dataset in terms of the accuracy, feature length and learning overhead. Overall, our work shows the potential of feature fusion for RGBD scene recognition, which is worthwhile for further research.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Hierarchical matching pursuit for image classification: Architecture and fast algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multipath sparse coding using hierarchical matching pursuit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (VOC) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J V</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning rich features from RGB-D images for object detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A fast learning algorithm for deep belief nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1527" to="1554" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A category-level 3-d object dataset: Putting the kinect to work</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Janoch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshop</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Building high-level features using large scale unsupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranganath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Understand scene categories by objects: A semantic regularized scene classifier using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kodagoda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<idno>abs/1509.06470</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Modeling the shape of the scene: A holistic representation of the spatial envelope</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="145" to="175" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Recognizing indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Quattoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Imagenet large scale visual recognition challenge. CoRR, abs/1409.0575</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Indoor segmentation and support inference from RGBD images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">SUN RGB-D: A RGB-D scene understanding benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Lichtenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multi-modal sharable and specific feature learning for rgb-d object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">C</forename><surname>Mmss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Large-margin multi-modal deep learning for RGB-D object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TMM</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1887" to="1898" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">CENTRIST: A visual descriptor for scene categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1489" to="1501" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">SUN database: Large-scale scene recognition from abbey to zoo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">A</forename><surname>Ehinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning deep features for scene recognition using places database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">À</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
