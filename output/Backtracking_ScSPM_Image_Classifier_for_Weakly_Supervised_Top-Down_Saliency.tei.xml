<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Backtracking ScSPM Image Classifier for Weakly Supervised Top-down Saliency</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hisham</forename><surname>Cholakkal</surname></persName>
							<email>hisham002@ntu.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="laboratory">Multimedia Lab</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jubin</forename><surname>Johnson</surname></persName>
							<email>jubin001@ntu.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="laboratory">Multimedia Lab</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepu</forename><surname>Rajan</surname></persName>
							<email>asdrajan@ntu.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="laboratory">Multimedia Lab</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Backtracking ScSPM Image Classifier for Weakly Supervised Top-down Saliency</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:38+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Top-down saliency models produce a probability map that peaks at target locations specified by a task/goal such as object detection. They are usually trained in a supervised setting involving annotations of objects. We propose a weakly supervised top-down saliency framework using only binary labels that indicate the presence/absence of an object in an image. First, the probabilistic contribution of each image patch to the confidence of an ScSPM-based classifier produces a Reverse-ScSPM (R-ScSPM) saliency map. Neighborhood information is then incorporated through a contextual saliency map which is estimated using logistic regression learnt on patches having high R-ScSPM saliency. Both the saliency maps are combined to obtain the final saliency map. We evaluate the performance of the proposed weakly supervised top-down saliency and achieves comparable performance with fully supervised approaches. Experiments are carried out on 5 challenging datasets across 3 different applications.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>A saliency map can be thought of as a probability map in which the probabilities of pixels belonging to salient regions are mapped to intensities. It helps to reduce the search space for further processing, for example, in object segmentation. Bottom-up saliency aims to locate regions in an image that capture human fixations within first few milliseconds after the stimulus is presented <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b13">14]</ref>. Here, feature contrast at a location plays the central role, with no regard to the notion of an object, although high-level concepts like face have been used in conjunction with visual cues like color and shape <ref type="bibr" target="#b22">[23]</ref>. Lack of prior knowledge about the target in goal-oriented applications such as object detection and object class segmentation limits its utility. For example, the saliency maps produced by recent bottom-up approaches <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b38">39]</ref> cannot discriminate between bus, person and bicycle in <ref type="figure" target="#fig_0">Fig. 1(b, c)</ref>.</p><p>On the contrary, top-down approaches utilize prior knowledge about the target for better estimation of saliency. The top-down saliency models produce a probability map that peaks at target/object locations <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b35">36]</ref>. Our objective is to generate top-down saliency maps like those shown in <ref type="figure" target="#fig_0">Fig. 1</ref>(d, e, f). They were generated using the proposed method to identify probable image regions that belong to bus, bicycle and person separately. Most methods for top-down saliency detection learn object classes in a fully supervised manner, where an exact object annotation is available. The learning component enables either discrimination between object classes or generation of saliency models of objects. Weakly supervised learning (WSL) alleviates the need for such userintensive annotation by providing only class labels for an image during learning. The top-down saliency method using WSL <ref type="bibr" target="#b24">[25]</ref> employs iterative refinement of object hypothesis on a training image. The proposed weakly supervised top-down saliency approach <ref type="figure">(Fig. 2 (e)</ref>) does not require these iterative steps, but produces a saliency map that is even comparable to fully supervised approaches as shown in <ref type="figure">Fig. 2(b, c, d)</ref>.</p><p>In the proposed method, first, an ScSPM-based image classifier <ref type="bibr" target="#b36">[37]</ref> is trained for an object category. On a val- <ref type="figure">Figure 2</ref>. Our weakly supervised top-down saliency map in comparison with fully supervised methods. (a) Input images, person and bicycle saliency maps of (b) <ref type="bibr" target="#b35">[36]</ref>, (c) <ref type="bibr" target="#b19">[20]</ref>, (d) <ref type="bibr" target="#b4">[5]</ref> and (e) proposed method are shown in row 1 and row 2 respectively.</p><formula xml:id="formula_0">(a) (b) (c) (d) (e)</formula><p>idation/test image, the classifier gives a confidence score indicating the presence of the object. The probabilistic contribution of each patch in the image to this confidence score is analyzed to estimate its Reverse-ScSPM (R-ScSPM ) saliency. The patches having high R-ScSPM saliency are generally from object regions, but they lack contextual information. For high-level understanding of the surrounding spatial region, contextual information of the patch is required. Hence, we incorporate a contextual saliency module that computes the probability of object presence in a patch using logistic regression trained on contextual max-pooled vectors <ref type="bibr" target="#b4">[5]</ref>. The training of contextual saliency needs a set of positive patches from the object region and a set of random negative patches from images that do not contain the object. Since a patch-level annotation is not available, we use patches from the positive training images having high R-ScSPM saliency to train the contextual saliency. The contextual saliency inferred on a test image is combined with the R-ScSPM saliency to form the final saliency map. R-ScSPM saliency considers the spatial location of patch through backtracking max-pooled vector whereas contextual saliency considers its spatial neighborhood information, thereby complementing one another. We also propose a classifier confidence-based refinement to the saliency map. Besides illustrating the accuracy of saliency maps produced by the proposed method, we demonstrate its effectiveness in applications like weakly supervised object annotation and class segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Kanan et al. <ref type="bibr" target="#b16">[17]</ref> proposed a top-down saliency approach which uses object appearance cues along with location prior. It fails if the object appears at random locations. Closer to our framework, <ref type="bibr" target="#b35">[36]</ref> proposed a fully supervised top-down saliency model that jointly learns a conditional random field (CRF) and dictionary using sparse codes of SIFT features as latent variables. The inability to discriminate similar objects (e.g. car and bike) and lack of contextual information causes a large number of false detections. <ref type="bibr" target="#b19">[20]</ref> improves upon this by considering the first and second order statistics of color, edge orientation and pixel location within a superpixel, along with objectness <ref type="bibr" target="#b2">[3]</ref> instead of SIFT features. Khan and Tappan <ref type="bibr" target="#b17">[18]</ref> use label and location-dependent smoothness constraint in a sparse code formulation to improve the pixel-level accuracy of <ref type="bibr" target="#b35">[36]</ref> at the expense of increased computational complexity. Zhu et al. <ref type="bibr" target="#b40">[41]</ref> proposed a contextual pooling based approach where LLC <ref type="bibr" target="#b34">[35]</ref> codes of SIFT features are max-pooled in a local neighborhood followed by log-linear model learning. By replacing LLC codes with locality-constrained contextual sparse coding (LCCSC), <ref type="bibr" target="#b4">[5]</ref> improves <ref type="bibr" target="#b40">[41]</ref> with a carefully chosen category-specific dictionary learned from the annotated object area. The proposed method uses a smaller dictionary which is not category-specific. Discriminative models <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b31">32]</ref> often represent a few patches on the object as salient and not the entire object. Hence, such models end up with low recall rates as compared to generative models <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b35">36]</ref>. The proposed framework addresses this using contextual saliency. In <ref type="bibr" target="#b31">[32]</ref>, the task of image classification is improved using discriminative spatial saliency to weight visual features. A fully supervised CNN-based approach <ref type="bibr" target="#b39">[40]</ref> achieves high accuracy in category-independent saliency datasets such as PASCAL-S <ref type="bibr" target="#b21">[22]</ref> by training on large datasets such as ImageNet <ref type="bibr" target="#b29">[30]</ref>, which is known to be computationally intense.</p><p>The use of weak supervision in top-down saliency has largely been left unexamined. DSD <ref type="bibr" target="#b11">[12]</ref> uses a weakly supervised setting where bottom-up features are combined with discriminative features that maximize the mutual information to the category label. The lack of contextual feature information limits its performance in images containing background clutter. In <ref type="bibr" target="#b24">[25]</ref>, a joint framework using classifier and top-down saliency is used for object categorization by sampling representative windows containing the object. Their iterative strategy leads to inaccurate saliency estimation if the initialized windows do not contain the object.</p><p>Since we demonstrate the usefulness of our saliency maps for object annotation and class segmentation, we review closely related works in these areas. The class segmentation approach of <ref type="bibr" target="#b10">[11]</ref> uses superpixels as the basic unit to build a classifier using histogram of local features within each superpixel. Aldavert et al. <ref type="bibr" target="#b1">[2]</ref> computes multiclass object segmentation of an image through an integral linear classifier. A much larger codebook of 500,000 atoms is used compared to only 1536 atoms in our framework. Training of shape mask <ref type="bibr" target="#b23">[24]</ref> requires images to be marked as difficult or truncated in addition to the object annotation. The proposed method produces better results by just using a binary label indicating the presence or absence of object of interest. Co-segmentation approaches <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b18">19]</ref> segment out the common objects among a set of input images. Semantic object selection <ref type="bibr" target="#b0">[1]</ref> collects images with white background from the internet using tag-based image retrieval. A saliency model is proposed in <ref type="bibr" target="#b32">[33]</ref> to address the object annotation task <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b33">34]</ref>. ∈ Ω as it does not contribute positively to classifier confidence. For a patch A ∈ Ω, R-ScSPM saliency P (rA, cA, pA) is evaluated by setting the sparse codes of all patches except A to 0 forming UA followed by a scalar product (·) with the classifier weight W . Similar procedure is followed for all patches in Ω. The patch A is selected as object patch since P (rA, cA, pA) ≥ 0.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed method</head><p>In this section, we first present the weakly supervised R-ScSPM framework <ref type="figure" target="#fig_1">(Fig. 3</ref>) to obtain R-ScSPM saliency. We then introduce contextual saliency (Sec. 3.2) that estimates object presence in a patch by considering its neighborhood information. Training of contextual saliency requires object patches that are selected using the R-ScSPM saliency map. Finally, during inference (Sec. 3.3), our framework combines both the saliency maps to generate the final saliency map.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">R-ScSPM saliency</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Our ScSPM implementation and notations</head><p>In our ScSPM-based classifier, dense-SIFT features are extracted from gray-scale image patches. K-means clustering of the SIFT features from training images are used to form a dictionary D of d elements (atoms). The SIFT features</p><formula xml:id="formula_1">X = [x 1 , x 2 ...x M ] from M patches of an image are sparse coded using D to U = [u 1 , u 2 ...u m , ...u M ].</formula><p>Here, u m is a d-dimensional vector representing the sparse code of a feature x m from the m th image patch. The spatial distribution of the features in the image is encoded in the max-pooled image vector Z through a multi-scale max-pooling operation F (u 1 , u 2 , ..., u M ) of the sparse codes on a 3-level spatial pyramid <ref type="bibr" target="#b20">[21]</ref> as shown in <ref type="figure" target="#fig_1">Fig. 3</ref>. The i th element z i of Z is a max-pooled value derived using maximum operation on j th elements of all patches in a spatial region R defined by i and j = 1 + (i − 1) mod d. It is represented as</p><formula xml:id="formula_2">z i = max{|u 1j |, |u 2j |, ....|u qj |}, s.t. 1, 2...q ∈ R. (1)</formula><p>Let the label Y k ∈ {1, −1} indicate the presence or absence of an object O in the k th image. If Y k = 1 it is a positive image, else it is a negative image. Image-label pairs (Z k , Y k ) of L training images are used to train a linear binary SVM classifier by minimizing following objective function <ref type="bibr" target="#b7">[8]</ref> arg min</p><formula xml:id="formula_3">W W 2 + C L k=1 max(0, 1 − Y k (W ⊤ Z k + bias)),<label>(2)</label></formula><p>where W = [w 1 , w 2 ....w N ] ⊤ and bias are the SVM weight vector and bias respectively. W is learnt separately for each object category. N is the length of the max-pooled image vector Z k and C is a constant. Given a validation/test image with max-pooled vector Z, the classifier score W ⊤ Z +bias indicates the confidence of the presence of object O in it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">R-ScSPM saliency formulation</head><p>In an ScSPM image classifier, both the linear-SVM and multi-scale max-pooling operations can be traced back to the patch level. This enables us to analyze the contribution of each patch towards the final classifier score which is then utilized to generate the R-ScSPM saliency map for an object. Since a common dictionary D is learned for all objects by unsupervised clustering of random SIFT features from training images, the correspondence of a particular dictionary atom to an object or background is unknown. ScSPM stipulates that a patch is representative of its image if its sparse code makes the largest contribution (max-pooling) to a particular dictionary atom among other patches in the same spatial region R. The representativeness, r m , of a patch m for an image is indicated by the number of times the elements of that patch's sparse code made it to the maxpooled vector. Representative patches may either contribute positively or negatively to the classifier score with higher contribution indicating more relevance of the patch to an object O. The relevance of the patch to the object is denoted c m .</p><p>It is possible that among the elements of the sparse code of a patch that contributes positively to the classifier confidence, there are other elements that may contribute negatively. For example, let [u m1 , 0, ... u mj ...0, u md ] ⊤ be the sparse code of a patch m with its j th element u mj being a local maximum in its spatial pyramid region. Although u mj contributes positively to the classifier confidence W ⊤ Z + bias, the other non-zero elements u m1 or u md may contribute negatively, indicating absence of the object in that patch. So, the relevance of a patch to the object requires its contribution to be computed in the absence of other patches; this relevance is denoted p m . The probability of a patch m belonging to an object, which in turn indicates the saliency G of the object, depends on the three parameters-r m , c m and p m as G = P (r m , c m , p m ) = P (p m |r m , c m )P (c m |r m )P (r m ).</p><p>(3) The representative elements of the sparse code u m is identified by</p><formula xml:id="formula_4">Ψ m = {iδ(F −1 (z i ), u mj )}, ∀i ∈ {1, 2, ..N },<label>(4)</label></formula><p>where δ is the Kronecker delta function and F −1 is the inverse operation of spatial pyramid max-pooling and the location of z i in Z identifies the region R in the spatial pyramid and its position j in the sparse code u m . The probability of representativeness of the m th patch to the image is then defined as</p><formula xml:id="formula_5">P (r m ) = card(Ψ m )/N,<label>(5)</label></formula><p>where card(.) represents cardinality and N is the length of Z.</p><p>The classifier confidence is a score indicating the presence of the object in the image, which proportionally increases from a definite absence (score ≤ −1) to definite presence (score ≥ 1). Normalizing the confidence scores between 0 and 1 using parameters β = 0.5 and b = β(bias + 1) , we can represent it as the probability</p><formula xml:id="formula_6">P (Y = 1|F (u 1 , u 2 , ..., u M )) = βW ⊤ F (u 1 , u 2 , ..., u M ) + b, = βW ⊤ Z + b = β ∀i∈{1,..N } w i z i + b, = β ∀i∈Ψm w i z i + β ∀i∈{1,..N }\Ψm w i z i + b, = f (c m |r m ) + β ∀i∈{1,..N }\Ψm w i z i + b,</formula><p>where f (c m |r m ) is the contribution of the patch m to the image classifier confidence.</p><p>Given that the patch is representative of the image, the probability of it belonging to the object is</p><formula xml:id="formula_7">P (c m |r m ) = f (c m |r m ), if f (c m |r m ) ≥ 0, 0, otherwise.<label>(6)</label></formula><p>Using the above probabilities, we select a set Ω of all patches that contribute positively to the classifier confidence as</p><formula xml:id="formula_8">Ω = {P (c t |r t )P (r t ) &gt; 0}, ∀t = 1, 2, ..., M.<label>(7)</label></formula><p>The net contribution of a patch m ∈ Ω in the absence of other patches is</p><formula xml:id="formula_9">P (p m |r m , c m ) = βW ⊤ F ( 0.., u m , ..., 0) + b,<label>(8)</label></formula><p>where F ( 0.., u m , ..., 0) is the max-pooling operation performed by replacing the sparse codes of all other patches with a zero vector 0 of size d to form a max-pooled vector Z m . Implementation details. <ref type="figure" target="#fig_1">Fig. 3</ref> illustrates three patches A, B and C on an image and their corresponding sparse codes. The classifier score W ⊤ Z + bias indicates the confidence of the presence of object as mentioned earlier. Each element z i of Z has a corresponding weight w i . The elements from the Hadamard product W • Z with w i z i &gt; 0 mark the patches A and B that contribute positively to the classifier confidence through a F −1 (.) operation, i.e the set Ω. The contribution of patch A in the absence of other patches is evaluated using max-pooling operations F ( 0.., u A , .., 0) on sparse code vectors U A in which sparse codes of all other patches except u A are replaced with 0 forming max-pooled vector Z A . The R-ScSPM saliency of a patch m is given by</p><formula xml:id="formula_10">P (r A , c A , p A ) = βW ⊤ F ( 0.., u A , .., 0) + b if A ∈ Ω, 0 otherwise.<label>(9)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Contextual saliency training</head><p>The purpose of contextual saliency is to include neighborhood information of a patch. Previous top-down saliency approaches <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b40">41]</ref> use a fully supervised setting to select object patches to train their contextual saliency module. In our approach, we remove this requirement by using object patches that are extracted by R-ScSPM saliency. From positive training images, patches with R-ScSPM saliency G &gt; 0.5 are selected as positive patches with label l = +1, while random patches are selected from negative images with patch label l = −1. The selected patches are indicative of belongingness to an object category. In <ref type="figure" target="#fig_1">fig 3,</ref> patch A having P (r A , c A , p A ) ≥ 0.5 is selected for contextual model training while patch B having P (r B , c B , p B ) &lt; 0.5 is removed.</p><p>For the selected patches, a 13 × 13 neighborhood of surrounding patches are divided into a 3 × 3 spatial grid followed by max pooling of sparse codes over each grid and concatenated to form a context max pooled vector ρ. A logistic regression model with weight v and bias b v is learned using positive and negative patches from the training images to form the contextual saliency model <ref type="bibr" target="#b4">[5]</ref>. Since the sparse codes for every patch is already computed for R-ScSPM, max pooling over the context of a patch followed by logistic regression learning is the only additional computation required for this contextual saliency model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Saliency Inference</head><p>On a test image, the contextual saliency L is inferred using the logistic regression by</p><formula xml:id="formula_11">P (l = 1 | ρ, v) = 1 1 + exp(−(v T ρ + b v ) ,<label>(10)</label></formula><p>where P (l = 1 | ρ, v) indicates the probability of presence of an object in a patch and ρ is the contextual max-pooled vector for a test patch. For each patch, the contextual and R-ScSPM saliency values are combined as GL + 0.5(G + L) and normalized to values in [0, 1] to form the saliency S. We choose this combination criteria instead of a product between the two, since the R-ScPSM saliency values are non-zero only for R-ScSPM selected patches.</p><p>Classifier-based refinement. The saliency map is refined using the same image classifier used for R-ScSPM saliency having SVM parameters (W, bias). Given a test image, we compute its classifier confidence W ⊤ Z + bias. The test image could either contain a single class or multiple classes. For the former, as in the Graz-02 dataset, the classifier estimates the presence or absence of an object. However, for multiple classes, thresholding of classifier confidence determines the presence or absence of an object. During training, we compute the average classifier confidences W ⊤ Z j for all positive training images j in each run of Kfold learning (K=15). The mean of K such values is used it is less probable that the object O is present in that image, and therefore, there will be no salient object marked in the image. However, if W ⊤ Z &gt; th O , the saliency of the patch is retained as S. Pixel-level saliency maps are generated from patch-level saliency S using Gaussian-weighted interpolation as in <ref type="bibr" target="#b4">[5]</ref>.</p><formula xml:id="formula_12">(a) (b) (c) (d) (e) (f)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental evaluation</head><p>We evaluate the performance of our weakly supervised top-down saliency model on 5 challenging datasets across three applications. We compare with other topdown saliency approaches using Graz-02 <ref type="bibr" target="#b26">[27]</ref> and PASCAL VOC-07 <ref type="bibr" target="#b5">[6]</ref> segmentation datasets. The top-down saliency map is applied to the tasks of class segmentation, object annotation and action-specific patch discovery on Object Discovery dataset <ref type="bibr" target="#b28">[29]</ref>, PASCAL VOC-07 detection dataset and PASCAL VOC-2010 action dataset <ref type="bibr" target="#b6">[7]</ref> respectively. All these datasets are challenging, especially from a weakly supervised training perspective, due to heavy background clutter, occlusion and viewpoint variation.</p><p>We maintain the same parameters across the datasets. Following <ref type="bibr" target="#b35">[36]</ref>, SIFT features are extracted from 64 × 64 patches on a grayscale image with grid spacing of 16 pixels. The dictionary size for sparse coding is set to 1536 disregarding individual object categories whereas in <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b19">20]</ref> separate dictionaries of size 512, corresponding to each object category are iteratively learned. The size of the contextpooled vector is 9 × 1536 = 13824. <ref type="figure" target="#fig_2">Fig. 4</ref> shows a visual comparison of the effect of each stage in our proposed method. For a test image, the patches in Ω (refer <ref type="figure" target="#fig_1">Fig. 3</ref>) from the R-ScSPM pipeline are shown in <ref type="figure" target="#fig_2">Fig. 4(b)</ref> and the R-ScSPM saliency map is shown in <ref type="figure" target="#fig_2">Fig. 4(d)</ref>. This saliency map is thresholded at 0.5 to obtain the most relevant patches weeding out the false detection in Ω as shown in <ref type="figure" target="#fig_2">Fig. 4(c)</ref>. <ref type="figure" target="#fig_2">Fig. 4(f)</ref> shows the final saliency map formed by combining the contextual <ref type="figure" target="#fig_2">(Fig. 4(e)</ref>) and R-ScSPM saliency maps. At non-textured patches of the car (top), the lower R-ScSPM saliency is boosted by high con-  textual saliency in the final saliency map. The smearing of saliency in the contextual saliency map for small objects (bottom) is removed when combined with the R-ScSPM saliency map. <ref type="table" target="#tab_0">Table 1</ref> analyzes the contribution of each component of the proposed saliency model on Graz-02 dataset (dataset details are in Sec. 4.3). The effectiveness of the proposed method in selecting positive patches is demonstrated by comparing its performance to that using random selection of patches from positive images in training contextual saliency. The mean precision rate at EER (%) of 39% is much lower than 60.52% obtained using the complete framework. This can be attributed to poor model learning in categories like car where the object size could be much smaller relative to the image, whereby random selections are more probable to pick out patches from the background. By training the contextual model with the R-ScSPM selected patches, the result improved to 58.9%, which shows that the R-ScSPM patch selection is effective in localizing object patches in an unannotated positive image. The contribution of R-ScSPM saliency is studied by removing the contextual saliency component from the framework. The results are poorer compared to 'R-ScSPM trained contextual saliency' due to lack of contextual information. Our complete framework gives 60.52% which shows that both the contextual and R-ScSPM saliency maps complement one another. R-ScSPM utilizes the patch location and contextual saliency utilizes its neighborhood information and hence they complement each other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Analysis of individual components</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparison with various levels of supervision</head><p>Previous WSL localization and top-down saliency works <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b24">25]</ref> select initial negative patches from either the boundaries or at random locations from the postive training images. They need to iteratively refine their model in order to remove potentially erroneous negative patches. Since the  <ref type="bibr" target="#b19">[20]</ref>, LCCSC <ref type="bibr" target="#b4">[5]</ref>) and weakly supervised (DSD <ref type="bibr" target="#b11">[12]</ref>) top-down saliency approaches on car and person images.</p><p>training of the proposed method is not iterative, we need to select negative patches only from negative images. We analyze the influence of negative patches extracted from positive images on the performance of contextual saliency using different supervision settings in Graz-02. Each positive training image is divided into regular sized grids varying from 2 × 2 to 32 × 32 and each grid is manually labeled to indicate if an object is present or not. We also consider the case of a rectangular bounding box around the object. The contextual saliency model is learned using the additional label information. We maintain the same number of positive and negative patches throughout the experiment. Each category's model is evaluated on its respective test images. Pixel-level precision rates at EER (%) is averaged over all categories and shown in <ref type="figure" target="#fig_3">Fig. 5</ref>. The model trained using negative patches from both positive and negative images (green) outperforms the result when only negative images are used (blue), with the performance increasing with increasing scale of supervision. It indicates that if an iterative learning is used in our method, the results can be improved considerably. The proposed weakly supervised method (red) matches the performance of the 16 × 16 supervised setting (a label for every 40 × 40 pixels) learned using negative patches from negative images despite having the label at the image level. We outperform the results of a labeled bounding box using the same learning settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Graz-02 dataset</head><p>Graz dataset contains 3 object categories and a background category with 300 images per category. We split the images into training and testing sets following <ref type="bibr" target="#b35">[36]</ref>. We report our results on 3 test set configurations. First, pixellevel results of the proposed saliency model and recent topdown saliency models <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b4">5]</ref> are evaluated on all 600 test images of the dataset. Second, for comparison with related approaches <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b10">11]</ref>, each object category is evaluated on test images from its respective category and the pixellevel results are reported. Finally, to compare with <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b16">17]</ref>, the patch-level results on 300 test images <ref type="bibr" target="#b35">[36]</ref> is evaluated, where 150 test images are from a single category and the remaining 150 are from the background class. <ref type="table" target="#tab_1">Table 2</ref> compares our pixel-level results with recent topdown saliency approaches <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b19">20]</ref> and related object segmentation and localization approaches <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b23">24]</ref>. SV indicates supervision level with US, WS, FS referring to unsupervised, weakly supervised and fully supervised training respectively. <ref type="bibr" target="#b35">[36]</ref> and <ref type="bibr" target="#b19">[20]</ref> are fully supervised (FS), needing 20 iterations of CRF learning with sparse codes relearned at each iteration. Separate dictionaries are used for each object category. On the contrary, our weakly supervised method does not require any iterative learning and sparse codes are computed just once on a single dictionary. <ref type="bibr" target="#b4">[5]</ref> uses a larger dictionary of 2048 atoms for Graz-02 as compared to 1536 atoms in our approach. When their model is evaluated on the entire 600 test images, the mean precision at EER is 52.2% and 52.21% respectively. The discriminative capability of <ref type="bibr" target="#b19">[20]</ref> does not improve by incorporating objectness <ref type="bibr" target="#b2">[3]</ref> and superpixel features to <ref type="bibr" target="#b35">[36]</ref>. The proposed method achieves 54.76% with better discrimination against objects of other categories in a weakly supervised setting. <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b1">2]</ref> reports results in which each model is tested on images from its own category. Pixel-level results of our proposed model is evaluated using same setting. It is seen that <ref type="bibr" target="#b19">[20]</ref> is better (row 13) than proposed weakly supervised approach (row 15), however is inferior to our model (row 5) in removing false positives (row 3). <ref type="bibr" target="#b1">[2]</ref> uses 500,000 dictionary atoms in their fully supervised framework to produce 65.13% accuracy as compared to 60.52% in our weakly supervised approach that uses only 1536 atoms. Our results are far superior compared to the fully supervised shape mask <ref type="bibr" target="#b23">[24]</ref>. As expected, recent bottomup saliency model <ref type="bibr" target="#b38">[39]</ref> produces poor performance when compared to our result.</p><p>For fair comparison with fully supervised approaches, we report the results of our model in a fully supervised setting as well (FS version), i.e. the contextual saliency model is trained on object patches from training images using patch-level object annotations as in <ref type="bibr" target="#b35">[36]</ref>, instead of R-ScSPM. With this supervised setting, our model achieves state-of-the art results in top-down saliency. <ref type="table" target="#tab_2">Table 3</ref> compares the patch-level precision at EER of the proposed saliency model on 300 test images with other representative patch-level methods. As evident from <ref type="figure" target="#fig_4">Fig. 6</ref>, DSD <ref type="bibr" target="#b11">[12]</ref> has limited capability to remove background clutter, resulting in poor performance of their model. Feature learning using independent component analysis helped SUN <ref type="bibr" target="#b16">[17]</ref> to perform better than DSD, but substantially poorer than the proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">PASCAL VOC-07 segmentation dataset</head><p>Following <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b35">36]</ref>, training uses object detection images and testing is performed on 210 segmentation test images. Also, to reduce the computational complexity of sparse coding, a common dictionary of 1536 atoms is used for all object classes, which is much smaller than (20×512) atoms of <ref type="bibr" target="#b4">[5]</ref> . For each object category, separate sparse codes are computed in <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b19">20]</ref>. <ref type="table">Table. 4</ref> shows the patch-level performance comparison between the proposed WS method and FS top-down saliency approaches <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b4">5]</ref>. Category-level results and comparisons are available in the supplementary material. Knowledge about object presence inferred from the classifier refinement helped the saliency map to outperform <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b4">5]</ref> in classes like aeroplane and train. However, the use of a fixed context neighborhood size and lack of object annotation limits the performance in smaller objects like bottle and bird. Our method outperforms the fully supervised approach <ref type="bibr" target="#b35">[36]</ref> and achieves a higher mean precision rates at EER ( %) computed over all 20 classes.</p><p>Khan and Tappen <ref type="bibr" target="#b17">[18]</ref> report their pixel-level precision rates at EER only for cow category (8.5%) which is lower than the proposed weakly supervised approach (9.7%). We did not compare with <ref type="bibr" target="#b19">[20]</ref> since they manually assign an all zero map if the object of interest is absent <ref type="bibr" target="#b4">[5]</ref>. By simple thresholding at EER, the proposed saliency maps outperform segmentation approaches <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b1">2]</ref>. The results are available in the supplementary material. The presence of multiple, visually similar object classes in a single image is challenging for a weakly supervised approach, yet we achieve patch-level precision rate at EER comparable to that of state-of-the art fully supervised approach <ref type="bibr" target="#b4">[5]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Computation time</head><p>Training of the proposed framework is significantly faster compared to <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b19">20]</ref>, since we do not use iterative dictionary learning. MATLAB implementations of all approaches were evaluated on a PC running on Intel Xeon 2.4GHz processor. Our unoptimized implementation needs only 3.5 seconds for inference on a test image, which is faster when compared to 5.5 seconds for <ref type="bibr" target="#b35">[36]</ref> and 28 seconds for <ref type="bibr" target="#b19">[20]</ref>. In our framework, all the saliency models share a common sparse code and contextual max-pooled vector. Inferring another model on same image needs an additional 1 second only. However, <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b19">20]</ref> needs to calculate sparse codes for each model separately. <ref type="bibr" target="#b4">[5]</ref> uses a larger dictionary of different sizes in both datasets. It took 3.85 seconds for inference in Graz-02 dataset and 17 seconds in PASCAL VOC-07.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Applications</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.1">Object class segmentation</head><p>The saliency maps obtained for a particular class are thresholded as in <ref type="bibr" target="#b12">[13]</ref> followed by Grab-Cut <ref type="bibr" target="#b27">[28]</ref>. Cosegmentation aims to segment the common object from a given set of images, which is similar to the image-level label provided in our weakly supervised training which enables a fair comparison with our approach. We train airplane, car and horse models using 130 images per category from PAS-CAL VOC 2010 detection dataset and evaluated on object discovery dataset <ref type="bibr" target="#b28">[29]</ref>. Qualitative results 1 are shown in <ref type="figure" target="#fig_5">Fig. 7</ref> and quantitative comparisons with co-segmentation approaches are shown in table 5. The jaccard similiarity, i.e, intersection over union (IOU ) with the ground-truth is evaluated as in <ref type="bibr" target="#b28">[29]</ref>. Although <ref type="bibr" target="#b28">[29]</ref> performs better than our method on the horse class, we achieve better precision (84.09% vs 82.81%) which indicates that the proposed method can remove false detections on negative images. <ref type="bibr" target="#b0">1</ref> More results are available at https://goo.gl/8bvHpZ. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.2">Object annotation</head><p>We generated rectangular boxes from our saliency maps using coherent sampling <ref type="bibr" target="#b32">[33]</ref> to annotate PASCAL VOC-07 detection images. As in <ref type="bibr" target="#b32">[33]</ref> we select the first object location proposal in each image as the annotation of the object of interest. If IOU &gt; 0.5 it is labeled as a correct annotation. <ref type="table">Table 6</ref> shows average annotation accuracy across 20 object categories. It illustrates that proposed approach is better than a deformable part-based model <ref type="bibr" target="#b8">[9]</ref> trained on saliency maps of <ref type="bibr" target="#b32">[33]</ref>, which in turn uses several iterations of weakly supervised training to produce the reported result. It can be observed 1 from <ref type="figure" target="#fig_5">Fig. 7</ref> that inspite of low contrast with the background, the proposed method can successfully annotate (yellow boxes) bird and cat images. Green colored rectangular box indicates the ground truth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.3">Action-specific patch discovery</head><p>We aim to automatically identify patches that help to describe the action. Qualitative evaluation of our R-ScSPM patch selection strategy on PASCAL VOC-2010 action dataset indicates that it is effective in identifying the most representative patches of different action categories as shown 1 in <ref type="figure" target="#fig_5">Fig. 7</ref>. The representative patches of an action category include class-specific objects as well as the actionspecific orientation of human body parts. The patches corresponding to the body part performing the action, namely the hand, and the objects with which the hand interacts, namely the phone, camera and instrument have been extracted correctly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, a weakly supervised top-down saliency approach is presented that requires just a binary label indicating the presence/absence of the object in an image for training. A novel R-ScSPM framework produces a saliency map that enables selection of representative patches for contextual saliency which is shown to improve the final saliency map. Extensive experimental evaluations show that the proposed method performs comparably with that of fullysupervised top-down saliency approaches.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Comparison of our top-down saliency with bottom-up methods. (a) Input image, (b) saliency map of [10], (c) [39]; proposed top-down saliency maps for (d) bus (e) bicycle and (f) person categories.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Illustration of our R-ScSPM saliency estimation and patch selection for dog category. Red arrows indicate the proposed R-ScSPM framework. The elements zi of Z having (wizi &gt; 0) are traced back to the image patches A, B and are added to Ω. The patch C /</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Illustration of individual stages of the proposed model. (a) Input image, (b) patches in Ω and (c) patches selected by thresholding (d) R-ScSPM saliency map. (e) contextual saliency map and (f) final saliency map. as the final threshold th O to indicate the presence of object O on a test image. To avoid situations where false negative values drastically reduce the threshold, we maintain the lowest possible confidence value as −0.5. If W ⊤ Z &lt; th O ,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Effect of patch selection strategy for training. X-axis specifies the supervision settings, Y-axis denotes the mean of precision at EER (%) across 3 categories.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>Comparison of the proposed weakly supervised method with other fully supervised (Yang&amp;Yang [36] ,Kocak et al.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .</head><label>7</label><figDesc>Applications of the proposed saliency maps for (a) class segmentation, (b) object annotation and (c) action-specific patch selection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><head>Table 1 .</head><label>1</label><figDesc>Components analysis : Pixel-level precision rates at EER (%).</figDesc><table>Bike Car 
Person Mean 

Random trained contextual saliency 
51.2 27.3 
38.3 
39 
R-ScSPM trained contextual saliency 
66.9 55.3 
54.5 
58.9 
R-ScSPM saliency 
61.6 46.6 
54.8 
54.3 
Complete (R-ScSPM trained contextual 
saliency + R-ScSPM saliency) 
67.5 56.48 57.56 
60.52 

41.1 

47.3 

53.7 

59.4 

62.9 

54.5 
44.3 

53.13 

60.21 

66.21 

69.83 

61.9 
58.9 

35 

40 

45 

50 

55 

60 

65 

70 

75 

2x2 
4x4 
8x8 
16x16 
32x32 
Rectangular box 

Negative pathes from negative images only 

Negative patches from positive and negative images 

R-ScSPM patch selecion 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 2 .</head><label>2</label><figDesc>Pixel-level precision rates at EER (%) on Graz-02.</figDesc><table>Method 
SV Test set 
Bike 
Car 
Person Mean 
1 -Zhang et al. [39] 
US 
31.77 18.66 30.71 
27.1 
2 -Yang and Yang [36] 
FS 
59.4 
47.4 
49.8 
52.2 
3 -Kocak et al. [20] 
FS 
59.92 45.18 51.52 
52.21 
4 -LCCSC [5] 
FS 
69.07 58.39 58.22 
61.89 
5 -Proposed WS 
WS 
63.96 45.11 55.21 
54.76 
6 -FS version 
FS 

All 
test 
images 

71.5 
56.6 
62.3 
63.51 
7 -Zhang et al. [39] 
US 
54.67 39.03 52.04 
48.58 
8 -Aldavert et al. [2] 
FS 
71.9 
64.9 
58.6 
65.13 
9 -Fulkerson et al. [11] 
FS 
72.2 
72.2 
66.1 
70.16 
10 -Shape mask [24] 
FS 
61.8 
53.8 
44.1 
53.23 
11 -Yang and Yang [36] 
FS 
62.4 
60 
62 
61.33 
12 -Khan and Tappen [18] FS 
72.1 
-
-
-
13 -Kocak et al. [20] 
FS 
73.9 
68.4 
68.2 
70.16 
14 -LCCSC [5] 
FS 
76.19 71.2 
64.13 
70.49 
15 -Proposed WS 
WS 
67.5 
56.48 57.56 
60.52 
16 -FS version 
FS 

Test 
images 
from 
respective 
category 

77.61 71.91 66.95 
72.16 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 3 .</head><label>3</label><figDesc>Patch-level precision rates at EER (%) on 300 test images.</figDesc><table>Bike Car 
Person Mean 

DSD [12] 
62.5 37.6 48.2 
49.4 
SUN [17] 
61.9 45.7 52.2 
53.27 
Proposed WS 76.0 53.7 66.7 
65.43 

Table 4. Patch-level precision rates at EER(%) on PASCAL VOC-
07. 

Method 
Yang and Yang [36] 
(FS) 

LCCSC [5] 
(FS) 

Proposed 
(WS) 

Mean of 20 classes 16.7 
23.4 
18.6 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 5 .Table 6 .</head><label>56</label><figDesc>Comparison with segmentation approaches on Object Discovery dataset. Comparison with weakly supervised object annotation approaches on PASCAL -07 detection datasetMethod   Nguyen et al.<ref type="bibr" target="#b25">[26]</ref> Siva and Xing<ref type="bibr" target="#b33">[34]</ref> Siva et al.<ref type="bibr" target="#b32">[33]</ref> Proposed</figDesc><table>Method 
Airplane 
Car 
Horse 

Joulin et al. [15] 
15.36 
37.15 30.16 
Joulin et al. [16] 
11.72 
35.15 29.53 
Kim et al. [19] 
7.9 
0.04 
6.43 
Object Discovey [29] 
55.81 
64.42 51.65 
Proposed 
57.27 
67.42 50.51 

Annotation accuracy 
(Avg. of 20 Classes) 
22.4 
30.4 
32.0 
36.22 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgment: The authors thank K.R Jerripothula for sharing the code of <ref type="bibr" target="#b12">[13]</ref>.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Semantic object selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Price</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Fast and robust object segmentation with the integral linear classifier</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Aldavert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ramisa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>De Mantaras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Toledo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">What is an object</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Alexe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2010</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Analysis of scores, datasets, and models in visual saliency prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Tavakoli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Top-down saliency with locality-constrained contextual sparse coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cholakkal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">The PASCAL Visual Object Classes Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<ptr target="http://www.pascal-network.org/challenges/VOC/voc2007/workshop/index.html.5" />
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<ptr target="http://www.pascal-network.org/challenges/VOC/voc2010/workshop/index.html.5" />
		<title level="m">The PASCAL Visual Object Classes Challenge 2010 (VOC2010) Results</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Liblinear: A library for large linear classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R.-E</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1871" to="1874" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained partbased models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Traditional saliency reloaded: A good old model in new shape</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Frintrop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Werner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>García</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Class segmentation and object localization with superpixel neighborhoods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fulkerson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Discriminant saliency, the detection of suspicious coincidences, and applications to visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Automatic image co-segmentation using geometric mean saliency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">R</forename><surname>Jerripothula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Category-independent object-level saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Discriminative clustering for image co-segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multi-class cosegmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Sun: Top-down saliency using natural statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Cottrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Visual Cognition</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">6-7</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Discriminative dictionary learning with spatial priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Tappen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Distributed cosegmentation via submodular optimization on anisotropic diffusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Top down saliency estimation via superpixel-based discriminative dictionaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kocak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cizmeciler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Erdem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">E</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The secrets of salient object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning to detect a salient object</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Accurate object recognition with shape masks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marszałek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning saliency maps for object categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moosmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Larlus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jurie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV workshop</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Weakly supervised discriminative localization and classification: a joint learning process</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>De La Torre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Generic object recognition with boosting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Opelt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fussenegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Auer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>PAMI</publisher>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="416" to="431" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Grabcut: Interactive foreground extraction using iterated graph cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kolmogorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="309" to="314" />
			<date type="published" when="2004" />
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Unsupervised joint object discovery and segmentation in internet images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rubinstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
		<respStmt>
			<orgName>ImageNet Large Scale Visual Recognition Challenge. IJCV</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Objectcentric spatial pooling for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno>ECCV. 2012. 6</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Discriminative spatial saliency for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jurie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Looking beyond the image: Unsupervised learning for object saliency and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Siva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Agapito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Weakly supervised object detector learning with model drift detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Siva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Locality-constrained linear coding for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Top-down visual saliency via joint crf and dictionary learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Linear spatial pyramid matching using sparse coding for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Combining randomization and discrimination for fine-grained image categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Minimum barrier salient object detection at 80 fps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mȇch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Saliency detection by multi-context deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Top-down saliency detection via contextual pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Signal Processing Systems</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
