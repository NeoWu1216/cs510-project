<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Optimal Relative Pose with Unknown Correspondences</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Fredriksson</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Lund University</orgName>
								<address>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viktor</forename><surname>Larsson</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Lund University</orgName>
								<address>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Olsson</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Lund University</orgName>
								<address>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fredrik</forename><surname>Kahl</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Lund University</orgName>
								<address>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Chalmers University of Technology</orgName>
								<address>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Optimal Relative Pose with Unknown Correspondences</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:44+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Previous work on estimating the epipolar geometry of two views relies on being able to reliably match feature points based on appearance. In this paper, we go one step further and show that it is feasible to compute both the epipolar geometry and the correspondences at the same time based on geometry only. We do this in a globally optimal manner. Our approach is based on an efficient branch and bound technique in combination with bipartite matching to solve the correspondence problem. We rely on several recent works to obtain good bounding functions to battle the combinatorial explosion of possible matchings. It is experimentally demonstrated that more difficult cases can be handled and that more inlier correspondences can be obtained by being less restrictive in the matching phase.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The ability to compute the relative pose between two cameras is of fundamental importance in multiple view geometry. Almost all structure from motion systems rely on this subproblem for building an overall reconstruction. Efficient minimal solvers <ref type="bibr" target="#b16">[17]</ref> as well as globally optimal branch and bound methods <ref type="bibr" target="#b9">[10]</ref> exist. Like most other multiple view geometry problems, relative orientation is typically computed from a set of image point correspondences which are generated using image appearance alone. To obtain reliable correspondences the matching criterion is quite restrictive and will in practice only allow feature points with unique appearance. For example, <ref type="bibr" target="#b14">[15]</ref> requires that the best matching score is significantly better than the second best one. As a consequence the number of available point correspondences can be small in scenes with ambiguous and/or repeated texture. Despite a restrictive matching criterion one has to expect that a portion of the feature points are incorrectly matched due to noise and appearance changes. Since these mismatches do not obey the Gaussian noise assumption they can be devastating to the reconstruction quality. To detect and remove such outliers, minimal solvers <ref type="bibr" target="#b16">[17]</ref> in combination with RANSAC <ref type="bibr" target="#b5">[6]</ref> are often employed. While it is very efficient in cases with a relatively low number of outliers RANSAC does not offer any optimality guarantees. Recently it has been shown that in many applica- tions RANSAC tends to fail when the number of outliers is high <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b6">7]</ref>. To address this problem researchers have started to consider methods that are guaranteed to find the largest possible inlier set. In <ref type="bibr" target="#b6">[7]</ref> a method for determining the relative translation that maximizes the number of inliers is presented. The approach has a worst case complexity of O(n 2 log(n)) where n is the number of putative matches. In <ref type="bibr" target="#b7">[8]</ref> the same problem is addressed in a branch and bound framework. While the worst case complexity is exponential it is shown to outperform <ref type="bibr" target="#b6">[7]</ref> in practice. For the full relative pose problem, <ref type="bibr" target="#b4">[5]</ref> and <ref type="bibr" target="#b18">[19]</ref> propose methods that also maximize the number of inliers.</p><p>While the above approaches all assume unique putative matches, in this paper, we take a step further and consider problems with multiple possible matches. In the extreme case, we consider the search space of all possible matching pairs. This gives a way to aid the matching in settings with ambiguous texture by incorporating geometric knowledge into the matching process. If the camera motion is known then the search for point correspondences is reduced to a search over known epipolar lines <ref type="bibr" target="#b15">[16]</ref>. On the other hand, if the motion is unknown then we need to solve for geometry and point correspondences simultaneously, which is much harder. We propose a simple technique for solving this problem. To our knowledge it has previously not been attempted. Compared to regular inlier maximization there are two difficulties that have to be overcome. First, the space of matches to search grows quickly when we allow more than just the best match for each point. Second, with several feasible matches for each point we need to enforce one-toone constraints. Our approach can be seen as a branch and bound method in two levels. Rather than searching over all model parameters like in standard branch and bound, we first consider only relative translation estimation with unknown correspondences at the lower level. By solving a bipartite matching problem we create a bounding function that can be evaluated very rapidly. The above subproblem is then used at the higher level in order to estimate rotation parameters.</p><p>From a practical point of view the enlarged space of possible matches introduces solution ambiguities. Given an epipolar line any point lying on it is a potential match. Hence points can switch matches along the epipolar line without effecting the camera motion yielding multiple solutions.</p><p>The literature dealing with geometric vision problems with unknown matches is sparse. A method for subspace clustering is proposed in <ref type="bibr" target="#b11">[12]</ref>. In <ref type="bibr" target="#b2">[3]</ref> a branch and bound framework is applied to estimate in-plane Euclidean transformations and globally optimal matching solutions. In <ref type="bibr" target="#b13">[14]</ref> a method for estimating 3D-3D rigid transformation is presented. While the approach effectively searches a 6-dimensional space its applicability is somewhat limited since it requires that all points in the source and target pointclouds have a match. An integer programing approach for geometric problems with linear residuals is presented in <ref type="bibr" target="#b1">[2]</ref>. The approach can in principle be applied to relative pose with affine cameras but with an algebraic error function. The unknown correspondence problem with known relative motion is dealt with in <ref type="bibr" target="#b15">[16]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Introductory example</head><p>In this introductory example we study the extreme case where we use no image information and hence each point in one image can be matched to every point in the other. In <ref type="figure" target="#fig_0">Figure 1</ref> we show two images of the same scene. Applying our method to this problem, we are able to recover the relative pose in an optimal manner, see <ref type="figure" target="#fig_1">Figure 2</ref>. Although the global optimum of the estimated relative motion corresponds to the expected (true) camera motion, two of the correspondence pairs are mismatched (red lines). We will later show that this is due to an inherent ambiguity in the problem, which cannot be resolved using geometric constraints alone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.">Problem statement</head><p>In this work we use the spherical camera model where a 3D point X is projected into a camera (R, t) by simply normalizing the transformed point, i.e.</p><formula xml:id="formula_0">v = R(X − t) R(X − t) .</formula><p>For spherical cameras the most natural error metric is the angular reprojection error. Definition 1. Given an error threshold ǫ ∈ R + , relative rotation R and translation t, the corresponding image point-</p><formula xml:id="formula_1">pair (v 1 , v 2 ) is called an inlier if there exists some X ∈ R 3 such that 1 ∠(v 1 , X) ≤ ǫ and ∠(v 2 , R(X − t)) ≤ ǫ. (1)</formula><p>The classical relative pose problem can then be stated.</p><p>Problem 1. Given a threshold ǫ and a set of putative point matches between two images find the relative rotation R and translation t which maximize the number of inliers.</p><p>Let I 1 be the index set for the image points in the first image and I 2 the index set for the second image. Define E ⊆ I 1 × I 2 to be the set of possible correspondences. If we allow every point in the first image to match to every point in the other image we have E = I 1 × I 2 .</p><p>Definition 2. A set of putative matches M ⊂ E is feasible if each i ∈ I 1 appears in at most one pair (i, j) ∈ M and each j ∈ I 2 appears in at most one pair (i, j) ∈ M . Now we can state the problem we solve in this paper.</p><p>Problem 2. Given a set of possible correspondences E and ǫ &gt; 0, find the relative rotation R and translation t which have a maximal set of feasible inlier matches M ⊂ E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Known relative rotation</head><p>First we consider the case when the relative rotation is known and we only want to estimate the relative translation.</p><p>Problems with known relative rotation occur naturally in applications such as robotics where rotation is either known to be fixed or can be estimated through other sensors.</p><p>This subproblem will also be used as a building block when performing branch and bound over the rotation space in Section 3. Since there is a scale ambiguity we can without loss of generality assume that the translation is of unit length. Similarly by rotating the image points in the second image we can assume the rotation to be identity, i.e. R = I.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Epipolar geometry under translation</head><p>The epipolar geometry for known relative rotation has been thoroughly investigated in prior works such as <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8]</ref> and we only state the needed results here.</p><p>If the camera only has undergone a pure translation, then the epipolar constraint is reduced to the image points v 1 and v 2 and the translation t being coplanar with the origin. Since we assumed t to be of unit length this is equivalent to requiring that v 1 , v 2 and t lie on a great circle on the unit sphere. If we allow for a reprojection error ǫ, then the translation must lie inside a wedge determined by the ǫ-cones around v 1 and v 2 . This is illustrated in <ref type="figure">Figure 3</ref>. <ref type="figure">Figure 3</ref>: The image points v 1 and v 2 (arrows in the image) determine two wedges on the sphere. The wedges are formed by constructing great circles tangent to the ǫ-cones around the image points. If the translation t lies inside the wedges it will have the image pair is an inlier. Only one of the wedges correspond to a reconstruction with the 3D point X in front of both cameras.</p><formula xml:id="formula_2">w t v 2 v 1</formula><p>For a point pair (v 1 , v 2 ) the normals of the two great circles which define the wedge are given by</p><formula xml:id="formula_3">n ± = sin(β/2)(n × w) ± cos(β/2)n (2)</formula><p>where α is the angle between the image points and β satisfies sin(β/2) = sin(ǫ)/ sin(α/2)</p><p>and</p><formula xml:id="formula_5">w = v 1 + v 2 v 1 + v 2 , n = v 1 × v 2 v 1 × v 2 .<label>(4)</label></formula><p>Checking if a translation t has the pair (v 1 , v 2 ) as an inlier is then simply done by verifying that n + · t ≥ 0 and n − · t ≥ 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Branch and bound for translation estimation</head><p>In <ref type="bibr" target="#b7">[8]</ref> the authors perform branch and bound on the unit sphere to solve the relative translation problem. In their approach they subdivide the unit sphere into spherical triangles. To compute upper bounds for any triangle on the sphere they count the number of wedges which either intersect the triangle or completely enclose it. This will of course be an upper bound for the number of wedges any translation t inside the triangle can satisfy. Finding the intersection of the wedges and a triangle is simple due to the fact that the edges of spherical triangles lie along great circles. The intersection points are easily computed by forming the cross products of the normals. To find a lower bound they simply evaluate the center point of the triangle.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Bounding the optimal feasible inlier set</head><p>In the case of multiple potential matches the lower bound computed in <ref type="bibr" target="#b7">[8]</ref> is not valid since it does not guarantee one-to-one matches. The authors of <ref type="bibr" target="#b7">[8]</ref> also presented a heuristic improvement to address unknown correspondences. However it only guarantees that points in one of the images are uniquely assigned.</p><p>Similarly to <ref type="bibr" target="#b7">[8]</ref> we perform branch and bound on the sphere using spherical triangles representing a set of feasible translations. For each subdivision of the search space we need to compute an upper and lower bound for the size of the largest feasible inlier set any translation within the triangle can have. To compute the upper bound we find the largest feasible inlier set under the assumption that all wedges which either intersect the triangle or completely enclose it are inliers. Clearly this will be an upper bound for the largest feasible set for any translation inside the triangle. To find a lower bound, we check which wedges contain the center point of the triangle and compute the largest feasible set for these.</p><p>To ensure that we only search feasible matchings that fulfill the one-to-one constraint we construct a bipartite graph where the nodes in the graph correspond to the image points. For each possible inlier wedge we add an edge between the corresponding points. The problem is then reduced to finding a maximum-cardinality matching in this bipartite graph, see <ref type="figure" target="#fig_2">Figure 4</ref>.</p><p>Matching in bipartite graphs is a well studied problem which can be efficiently solved. In the implementation we use the Hopcroft-Karp <ref type="bibr" target="#b10">[11]</ref> algorithm but any standard solver can be applied. The Hopcroft-Karp algorithm has a worst case complexity of O(|E| |V |) but has been shown for sparse graphs to have an average complexity of O(|E| log |V |) <ref type="bibr" target="#b0">[1]</ref>. Typically for small spherical triangles the number of possible inlier wedges which are contradictory will be small and the matching problem can be solved extremely quickly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Example with multiple matches</head><p>It is common practice in structure from motion to include a point pair in the set of possible correspondences if the point from the first image has no other good candidates <ref type="bibr" target="#b14">[15]</ref>. This is often a good idea since it removes a lot of contradictory point pairs and decreases the amount of outliers. Unfortunately in scenes with repeated structures this often results in few matches between the images, which in turn could lead to low quality solutions. If we instead include all matches which are sufficiently good we do not need to discard possibly correct matches. While the problem becomes more difficult to solve, the optimal solution has the potential to contain more inliers.</p><p>In this example we compare matching with Lowe's criterion 2 with simply including the 10000 best candidate matches. Note that this means that some points might have multiple matches in the other image. <ref type="figure">Figure 5</ref> shows an example of an image pair with a lot of windows and the possible matches found using the two matching approaches. The inlier matches for the optimal solutions are shown in <ref type="figure">Figure  6</ref>. <ref type="bibr" target="#b1">2</ref> The threshold was set to 0.6. The optimal number of inliers for the normal SIFT-matching, using <ref type="bibr" target="#b7">[8]</ref>. The running time was 20 ms and Lowe's criterion 0.6. Right: The optimal number of inliers when solving the problem using the proposed method on the 10000 best matches. The running time was 7 s. The ǫ corresponds to 3 pixels. The incorrect inliers lie on the epipolar lines and in the model these are indistinguishable from the correct inliers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.">Comparison to other methods</head><p>In this section we evaluate our method for translation estimation on the dataset of Fredriksson et al. <ref type="bibr" target="#b6">[7]</ref>. The dataset contains 136 image pairs taken without any relative rotation. For each image pair we selected the best candidate matches. We ran the translation estimation multiple times with increasing amounts of matches included.</p><p>The minimal case for translation estimation requires two points,</p><formula xml:id="formula_7">t = (v 1 × v 2 ) × (v ′ 1 × v ′ 2 ).<label>(6)</label></formula><p>We compare our method against two different versions of RANSAC. In the first version we randomly select two point correspondences and evaluate the number of inlier matchings without regard for the feasibility requirement. In the other version (RANSAC-bpt) we compute the number of actual inliers by solving a bipartite matching problem in each iteration. For the two RANSAC algorithms we run 500 and 50000 iterations, which corresponds to a 99% chance of finding at least one minimal case where both point pairs are inliers in situations with 90% and 99% outliers respectively. We also compare with the method from Fredriksson et al. <ref type="bibr" target="#b7">[8]</ref>. In <ref type="bibr" target="#b7">[8]</ref> the authors also presented a heuristic improvement for dealing with unknown correspondences. We include results both with and without this heuristic. <ref type="table">Table 1</ref> shows the average inlier count for different number of matches included. <ref type="figure" target="#fig_4">Figure 7</ref> shows the running times for all methods.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Matches</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Full relative pose</head><p>Now we consider the the full relative pose problem with unknown correspondences. The method we propose is based on performing a branch and bound over rotation space similarly to <ref type="bibr" target="#b9">[10]</ref>. For each subdivision of rotation space we compute the bounds using the method presented in Section 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Branch and bound over rotation space</head><p>To handle the full relative pose problem we use the rotation bounds derived by Hartley and Kahl in <ref type="bibr" target="#b9">[10]</ref>. They optimally solve the relative pose problem without outliers by using branch and bound over the space of rotations.</p><p>Each rotation can be represented by the axis of rotation and the angle. This allows the set of rotations to be parameterized by the set {r ∈ R 3 , r ≤ π} where the rotation angle is represented by the length of the vector. For ease of implementation <ref type="bibr" target="#b9">[10]</ref> uses this parametrization but searches over the cube [−π, π] 3 . To obtain bounds the following result is used. Lemma 1. Let r 1 correspond to the rotation R 1 and r 2 to</p><formula xml:id="formula_8">R 2 . Then 3 d(R 1 , R 2 ) ≤ r 1 − r 2 .<label>(7)</label></formula><p>This means that if we have a cube inside [−π, π] 3 with side length σ centered at R c then any rotation R inside the cube satisfies</p><formula xml:id="formula_9">d(R c , R) ≤ √ 3 2 σ.<label>(8)</label></formula><p>To find bounds for any relative pose with a rotation inside this cube <ref type="bibr" target="#b9">[10]</ref> transforms the image points in the second image by R T c and search for the translation while allowing for a larger error in the second image,</p><formula xml:id="formula_10">ǫ 2 = ǫ + √ 3 2 σ.<label>(9)</label></formula><p>This additional error accounts for the uncertainty in the rotation. Note that this larger threshold is only used for the second image point. In <ref type="bibr" target="#b9">[10]</ref> the bounds are computed by solving a linear program to determine the optimal translation. This is possible since correspondences are known with no outliers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Extension to unknown correspondences</head><p>We extend the framework of <ref type="bibr" target="#b9">[10]</ref> to handle unknown correspondences. The idea is to perform branch and bound over rotation space similarly to <ref type="bibr" target="#b9">[10]</ref> but to compute the bounds using the method presented in Section 2. This is possible since the translation estimation can be solved very quickly.</p><p>During the outer branch and bound (over rotations) we need to compute the bounds for a given rotation cube in [−π, π] 3 . To find the upper bound we perform an inner branch and bound over translation space where we account for the uncertainty in rotation. <ref type="figure" target="#fig_5">Figure 8</ref> shows a wedge where we allow for a larger error in the second image point (as in <ref type="formula" target="#formula_10">(9)</ref>). In <ref type="bibr" target="#b9">[10]</ref> it is shown that the normals to the wedges where the image points have different ǫ are given by n ± = sin(β/2)(n × w) ± cos(β/2)n <ref type="bibr" target="#b9">(10)</ref> where α is the angle between the image points,</p><formula xml:id="formula_11">w = sin(ǫ 2 )v 1 + sin(ǫ 1 )v 2 sin(ǫ 2 )v 1 + sin(ǫ 1 )v 2 , n = v 1 × v 2 v 1 × v 2<label>(11)</label></formula><p>and β satisfies sin 2 (β/2) = sin 2 (ǫ 1 ) + 2 sin(ǫ 1 ) sin(ǫ 2 ) cos(α) + sin 2 (ǫ 2 ) sin 2 (α) . To find a lower bound we solve the translation problem again but with the original wedges (same ǫ for both images). This corresponds to finding the optimal translation for the rotation in the center of the cube.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Evaluation on synthetic data</head><p>In this section we evaluate the method for full relative pose on synthetic data. In these experiments we see how different error thresholds affect the quality of the solution. We also compare stopping the algorithm when some specific optimality gap (difference between upper bound and lower bound) is achieved. The optimality gap gives a bound for the maximum difference in inliers between the returned solution and the optimal solution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Narrow field of view</head><p>In this experiment we test the method on problems with narrow field of view. Since the image points are closer together, the narrow fields of view are more difficult to solve using branch and bound based algorithms. We synthesize 50 different 3D points, located uniformly in a cube of side length four around the origin. The cameras are placed randomly at distance four to the origin, resulting in about 60 • field of view. We add Gaussian noise with standard deviation 0.033 • to the image points. In <ref type="table" target="#tab_2">Table 2 and Table 3</ref> the results can be seen for ǫ = 0.5 • and ǫ = 0.25 • .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Narrow field of view: Planar scene</head><p>In this experiment we use the same experimental setup as above, but all the points are now in the same plane, defined by the square (±1, ±1, √ 6). The first camera is located in the origin and the the second at the the unit circle parallel to the plane. We use 50 synthesized 3D points, randomly <ref type="bibr" target="#b2">3</ref> Here d(·, ·) denotes the angle between the rotations.   distributed in the plane. Noise are added to the image points as in the previous case. In <ref type="table" target="#tab_5">Table 4</ref> and   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">Omni-directional</head><p>In this synthetic experiment we instead use omni-directional cameras. We again use 50 points, this time they are randomly distributed on the sphere of radius two. The camera centres are randomly distributed on the unit sphere. The added noise here is the same as in the narrow field of view. The result for ǫ = 1 • and ǫ = 0.5 • can be seen in <ref type="table" target="#tab_8">Table  6</ref> and <ref type="table" target="#tab_9">Table 7</ref>. Note that for the same error threshold and gap, the solution to the omni-directional problem has much higher quality then the narrow field of view.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Ambiguities along epipolar lines</head><p>We now again consider the image pair shown in <ref type="figure" target="#fig_0">Figure 1</ref> (Section 1.1). For this image pair there exist multiple optimal solutions and <ref type="figure">Figure 9</ref> shows another optimal solution with the same relative rotation and translation but with different point matches. This ambiguity occurs since there can be multiple possible matches along the epipolar lines for the optimal rotation and translation. See <ref type="figure" target="#fig_0">Figure 10</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Evaluation on the KITTI benchmark dataset</head><p>Next we evaluate our method on the KITTI dataset <ref type="bibr" target="#b8">[9]</ref>. The dataset consists of images taken from a camera mounted on the roof of a car. Since the car typically only moves in a plane we can restrict our search to rotations around the y-axis.</p><p>The dataset also contains ground truth poses obtained through other sensors. We consider four image sequences <ref type="figure">Figure 9</ref>: Another optimal solution. Note that the solution has the same rotation and translation as in <ref type="figure" target="#fig_1">Figure 2</ref> but different point matches. which were taken 2011-09-26. Some example images from one of the sequences can be seen in <ref type="figure" target="#fig_0">Figure 11</ref>.</p><p>For each sequence all image pairs in the sequence which differed by ten frames were used. We consider two types of matching strategies.</p><p>The first strategy is a standard symmetric matching strategy where for each point we find the best match in the other image. The match is kept if it is sufficiently good 4 , it is sufficiently better than the next best match (Lowe's criterion) and the match was symmetric. The symmetry requirement ensures that there are no ambiguous matches.</p><p>If there is repeated texture in the scene a point might have multiple good matches in the other image. Due to the ambiguous matching such points are discarded in the first <ref type="figure" target="#fig_0">Figure 11</ref>: Example images from the KITTI vision dataset. For the experiment the grayscale images were used. matching strategy. In the second strategy we instead allow points to have multiple matches in the other image. For each point we include all matches that are sufficiently good and such that the worst of the included matches satisfy Lowe's criterion to the next best match. In <ref type="table" target="#tab_11">Table 8</ref>   Using the proposed method we estimated the rotation and translation for the matches generated by both strategies. The resulting errors can be seen in <ref type="table" target="#tab_13">Table 9 and Table  10</ref>. For comparison we show the result from 100 iteration of RANSAC using the 3 point solver from <ref type="bibr" target="#b17">[18]</ref>.</p><p>Note that for the second sequence (0005) we see a significant improvement when allowing for multiple matches. For the other sequences the results are comparable. The average running time for our method for the two strategies were 1.3 seconds and 2.1 seconds.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusions</head><p>In this paper, we have demonstrated that it is possible to optimally compute the relative pose with unknown correspondences. We also show that keeping ambiguous point matches can improve the accuracy of the estimated camera motion and increase the number of inliers. We have also shown that problem instances can be very different in nature, ranging from very hard instances with ambiguous solutions and long running times, to well-posed situations where our algorithm returns a solution within a couple of seconds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Acknowledgments</head><p>This work has been funded by the Swedish Research Council (grant no. 2012-4213) and the Crafoord Foundation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Example of two images of the same scene. The goal is to recover both the relative pose and the correct matching between the points.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>The correspondences found by the proposed method. The inliers have less than 0.1 degrees angular error. The red lines correspond to incorrectly matched points (compared to a manually created ground truth matching).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Computing bounds for a triangle. The left column of nodes correspond to points in the first image and the right column to the second image. For each subdivision of the search space a subset of the wedges are possible inliers. The possible inlier correspondences are illustrated as solid lines and the correspondences which can be ruled out are illustrated as dashed lines. For the solid lines a maximal set of independent edges are selected. These are illustrated by the red lines in (b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :Figure 6 :</head><label>56</label><figDesc>Left: Normal SIFT-matching with Lowe's criterion 0.6. Right: The 10000 best candidate matches. Left:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 :</head><label>7</label><figDesc>The average running times for different number of point pairs included.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 :</head><label>8</label><figDesc>The wedges of possible translations when we allow for larger reprojection error in the second image point.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 10 :</head><label>10</label><figDesc>The epipolar lines in the second image. The ambiguous matches inFigure 9are along the epipolar lines.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>The results for the proposed method on 10 syn-
thetic narrow field of view instances. The synthetic data 
contains 50 3D points. The error threshold ǫ is 0.5 • . 

ǫ = 0.25 • 

Gap Rotation Translation Mean time Median time 

5 
3.7 • 
2.6 • 
879s 
189s 
3 
2.9 • 
1.8 • 
881s 
189s 
0 
2.2 • 
1.5 • 
1015s 
201s 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>The results for the proposed method on 10 syn-
thetic narrow field of view instances. The synthetic data 
contains 50 3D points. The error threshold ǫ is 0.25 • . 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 5</head><label>5</label><figDesc>the results can be seen for ǫ = 0.5 • and ǫ = 0.25 • .</figDesc><table>ǫ = 0.5 • 

Gap Rotation Translation Mean time Median time 

5 
31 • 
28 • 
43s 
45s 
3 
31 • 
28 • 
42 
45s 
0 
10 • 
12 • 
83s 
45s 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="true"><head>Table 4 :</head><label>4</label><figDesc>The results for the proposed method on 10 synthetic narrow field of view instances, where all points are located in the same plane. The synthetic data contains 50 3D points. The error threshold ǫ is 0.5 • .</figDesc><table>ǫ = 0.25 • 

Gap Rotation Translation Mean time Median time 

5 
12 • 
16 • 
91 
94s 
3 
8 • 
12 • 
137s 
103s 
0 
5 • 
9 • 
218s 
161s 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>The results for the proposed method on 10 syn-
thetic narrow field of view instances, where all points are 
located in the same plane. The synthetic data contains 50 
3D points. The error threshold ǫ is 0.25 • . 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 6 :</head><label>6</label><figDesc>Gap Rotation Translation Mean time Median time</figDesc><table>The results for the proposed method on 10 syn-
thetic omni-directional instances. The problem has 50 
points. 

ǫ = 0.5 • 

10 
1.6 • 
2.5 • 
266s 
241s 
5 
1.2 • 
1.4 • 
264s 
241s 
3 
1.1 • 
1.0 
264s 
241s 
0 
0.9 • 
1.0 • 
266s 
249s 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="false"><head>Table 7 :</head><label>7</label><figDesc>The results for the proposed method on 10 synthetic omni directional instances. The problem has 50 points.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" validated="false"><head></head><label></label><figDesc>some statistics for the sequences and matching strategies can be found.</figDesc><table>I 
II 

Sequence images matches matches amb. matches 

0001 
108 
65.4 
82.2 
9.1 
0005 
154 
62.4 
79.3 
9.3 
0015 
297 
61.9 
83.2 
12.9 
0048 
22 
137.8 
179.6 
33.4 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11" validated="false"><head>Table 8 :</head><label>8</label><figDesc>The number of images and the average number of matches generated by the two strategies. For the second strategy we show also the average number of ambiguously matched points.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13" validated="true"><head>Table 9 :</head><label>9</label><figDesc>The average rotation error in degrees for the two matching strategies for the different sequences.</figDesc><table>I 
II 

Sequence 
Our 
RANSAC Our RANSAC 

0001 
7.79 
2.16 
4.77 
2.10 
0005 
13.81 
21.61 
9.99 
20.21 
0015 
9.99 
4.00 
5.87 
4.23 
0048 
2.17 
4.19 
2.13 
3.75 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14" validated="false"><head>Table 10 :</head><label>10</label><figDesc>The average translation error in degrees for the two matching strategies for the different sequences.</figDesc><table></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">∠(a, b) denotes the angle between the vectors a and b.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Determined by a threshold on the angle between the SIFT descriptors.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Matching algorithms are fast in sparse random graphs. Theory of Computing Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bast</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mehlhorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Schafer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tamaki</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A branch-and-bound approach to correspondence and grouping problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bazin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Demonceaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vasseur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ikeuchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Analysis and Machine Intelligence (PAMI)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Implementation techniques for geometric branch-and-bound matching methods. Computer Vision and Image Understanding (CVIU)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Breuel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Tractable algorithms for robust model estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Enqvist</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ask</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Åström</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Two view geometry estimation with outliers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Enqvist</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Kahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Random sample consensus: a paradigm for model fitting with application to image analysis and automated cartography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Fischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Bolles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. Assoc. Comp. Mach</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Fast and reliable two-view translation estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fredriksson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Enqvist</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Kahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Practical robust two-view translation estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fredriksson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Larsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Olsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Vision meets robotics: The kitti dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Global optimization through rotation space search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">I</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Kahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">An nˆ5/2 algorithm for maximum matchings in bipartite graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Hopcroft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Karp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on computing</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="1973" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Robust motion segmentation with unknown correspondences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Consensus set maximization with guaranteed global optimality for robust geometry estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The 3d-3d registration problem revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hartley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Robust point correspondence by concave minimization. Image and Vision Computing (IVC)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Maciel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Costeira</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">An efficient solution to the five-point relative pose problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Nistér</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Analysis and Machine Intelligence (PAMI)</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Solving for relative pose with a partially known rotation is a quadratic eigenvalue problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sweeney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Turk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2nd International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="483" to="490" />
		</imprint>
	</monogr>
	<note>3D Vision (3DV)</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Optimal essential matrix estimation via inlier-set maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
