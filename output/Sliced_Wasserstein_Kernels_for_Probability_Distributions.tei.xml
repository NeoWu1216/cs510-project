<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Sliced Wasserstein Kernels for Probability Distributions</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soheil</forename><surname>Kolouri</surname></persName>
							<email>skolouri@andrew.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Carnegie Mellon University</orgName>
								<orgName type="institution" key="instit2">Carnegie Mellon University</orgName>
								<orgName type="institution" key="instit3">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zou</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Carnegie Mellon University</orgName>
								<orgName type="institution" key="instit2">Carnegie Mellon University</orgName>
								<orgName type="institution" key="instit3">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustavo</forename><forename type="middle">K</forename><surname>Rohde</surname></persName>
							<email>gustavor@cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Carnegie Mellon University</orgName>
								<orgName type="institution" key="instit2">Carnegie Mellon University</orgName>
								<orgName type="institution" key="instit3">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Sliced Wasserstein Kernels for Probability Distributions</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:46+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Optimal transport distances, otherwise known as Wasserstein distances, have recently drawn ample attention in computer vision and machine learning as powerful discrepancy measures for probability distributions. The recent developments on alternative formulations of the optimal transport have allowed for faster solutions to the problem and have revamped their practical applications in machine learning. In this paper, we exploit the widely used kernel methods and provide a family of provably positive definite kernels based on the Sliced Wasserstein distance and demonstrate the benefits of these kernels in a variety of learning tasks. Our work provides a new perspective on the application of optimal transport flavored distances through kernel methods in machine learning tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Many computer vision algorithms rely on characterizing images or image features as probability distributions which are often high-dimensional. This is for instance the case for histogram-based methods like the Bag-of-Words (BoW) <ref type="bibr" target="#b19">[21]</ref>, feature matching <ref type="bibr" target="#b15">[17]</ref>, co-occurence matrices in texture analysis <ref type="bibr" target="#b13">[15]</ref>, action recognition <ref type="bibr" target="#b41">[43]</ref>, and many more. Having an adequate measure of similarity (or equivalently discrepancy) between distributions becomes crucial in these applications. The classic distances or divergences for probability densities include Kullback-Leibler divergence, Kolmogorov distance, Bhattacharyya distance (also known as the Hellinger distance), etc. More recently, however, the optimal transportation framework and the Wasserstein distance <ref type="bibr" target="#b39">[41]</ref> also known as the Earth Mover Distance (EMD) <ref type="bibr" target="#b32">[34]</ref> have attracted ample interest in the computer vision <ref type="bibr" target="#b37">[39]</ref>, machine learning <ref type="bibr" target="#b9">[11]</ref>, and biomedical image analysis <ref type="bibr" target="#b1">[3]</ref> communities. The Wasserstein distance computes the optimal warping to map a given input probability measure µ to a second one ν. The optimality corresponds to a cost function which measures the expected value of the displacement in a warping. Informally, thinking about µ and ν as piles of dirt (or sand), the Wasserstein distance measures a notion of displacement of each sand particle in µ times its mass to warp µ into ν.</p><p>The Wasserstein distance has been shown to provide a useful tool to quantify the geometric discrepancy between two distributions. Specifically, they've been used as distances in content-based image image retrieval <ref type="bibr" target="#b32">[34]</ref>, modeling and visualization of modes of variation of image intensity values <ref type="bibr" target="#b1">[3,</ref><ref type="bibr" target="#b43">45,</ref><ref type="bibr" target="#b35">37,</ref><ref type="bibr" target="#b4">6]</ref>, estimate the mean of a family of probability measures (i.e. barycenters of distributions) <ref type="bibr">[2,</ref><ref type="bibr" target="#b30">32]</ref>, cancer detection <ref type="bibr" target="#b28">[30,</ref><ref type="bibr" target="#b38">40]</ref>, super-resolution <ref type="bibr" target="#b21">[23]</ref>, amongst other applications. Recent advances utilizing variational minimization <ref type="bibr" target="#b14">[16,</ref><ref type="bibr" target="#b7">9]</ref>, particle approximation <ref type="bibr" target="#b42">[44]</ref>, multi-scale schemes <ref type="bibr" target="#b25">[27,</ref><ref type="bibr" target="#b27">29]</ref>, and entropy regularization <ref type="bibr" target="#b9">[11,</ref><ref type="bibr" target="#b37">39,</ref><ref type="bibr" target="#b2">4]</ref>, have enabled transport metrics to be efficiently applied to machine learning and computer vision problems. In addition, Wang et al. <ref type="bibr" target="#b43">[45]</ref> described a method for computing a transport distance (denoted as linear optimal transport) between all image pairs of a dataset of N images that requires only N transport minimization problems. Rabin et al. <ref type="bibr" target="#b30">[32]</ref> and Bonneel et al. <ref type="bibr" target="#b5">[7]</ref> proposed to leverage the fact that these problems are easy to solve for one-dimensional distributions, and introduced an alternative distance called the Sliced Wasserstein distance. Finally, recent work <ref type="bibr" target="#b29">[31,</ref><ref type="bibr" target="#b20">22,</ref><ref type="bibr" target="#b22">24]</ref> has shown that the transport framework can be used as an invertible signal/image transformation framework that can render signal/image classes more linearly separable, thus facilitating a variety of pattern recognition tasks.</p><p>Due to the benefits of using the transport distances outlined above, and given the flexibility and power of kernelbased methods <ref type="bibr" target="#b16">[18]</ref>, several methods using transport-related distances in constructing kernel matrices have been described with applications in computer vision, and EEG data classification <ref type="bibr" target="#b45">[47,</ref><ref type="bibr" target="#b10">12]</ref>. Since positive definite RBF kernels require the metric space induced by the distance to be 'flat' (zero curvature) <ref type="bibr" target="#b11">[13]</ref>, and because the majority of the transport-related distances mentioned above, in particular distances for distributions of dimension higher than one utilizing the L 2 cost, do not satisfy this requirement, few options for provably positive definite transport-based kernels have emerged. Cuturi <ref type="bibr" target="#b8">[10]</ref> for example, suggested utilizing the permanent of the transport polytope, thus guaranteeing the positivity of the derived kernels. More recently, Gardner et al. <ref type="bibr" target="#b12">[14]</ref> have shown that certain types of earth mover's distances (e.g. with the 0-1 cost function) can yield kernels which are positive definite.</p><p>Here we expand upon these sets of ideas and show that the Sliced Wasserstein distance satisfies the basic requirements for being used as a positive definite kernel <ref type="bibr" target="#b16">[18]</ref> in a variety of regression-based pattern recognition tasks, and have concrete theoretical and practical advantages. Based on the recent works on kernel methods <ref type="bibr" target="#b17">[19,</ref><ref type="bibr" target="#b18">20,</ref><ref type="bibr" target="#b11">13]</ref>, we exploit the connection between the optimal transport framework and the kernel methods and introduce a family of provably positive definite kernels which we denote as Sliced Wasserstein kernels. We derive mathematical results enabling the application of the Sliced Wasserstein metric in the kernel framework and, in contrast to other work, we describe the explicit form for the embedding of the kernel, which is analytically invertible. Finally, we demonstrate experimentally advantages of the Sliced Wasserstein kernels over the commonly used kernels such as the radial basis function (RBF) and the polynomial kernels for a variety of regression.</p><p>Paper organization. We first review the preliminaries and formally present the p-Wasserstein distance, the Sliced Wasserstein distance, and review some of the theorems in the literature on positive definiteness of kernels in Section 2. The main theorems of the paper on the Sliced Wasserstein kernels are stated in Section 3. In Section 4 we review some of the kernel-based algorithms including the kernel k-means clustering, the kernel PCA, and the kernel SVM. Section 5 demonstrates the benefits of the Sliced Wasserstein kernel over the commonly used kernels in a variety of pattern recognition tasks. Finally we conclude our work in Section 6 and lay out future directions for research in the area.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">The p-Wasserstein distance</head><p>Let σ and µ be two probability measures on measurable spaces X and Y and their corresponding probability density functions I 0 and I 1 , dσ(x) = I 0 (x)dx and dµ(y) = I 1 (y)dy. In computer vision and image processing applications one often deals with compact d-dimensional Euclidean spaces, hence X = Y = [0, 1] d .</p><p>Definition 1. The p-Wasserstein distance for p ∈ [1, ∞) is defined as,</p><formula xml:id="formula_0">W p (σ, µ) := (inf π∈Π(σ,µ) X×Y (x − y) p dπ(x, y)) 1 p (1)</formula><p>where Π(σ, µ) is the set of all transportation plans, and π ∈ Π(σ, µ), that satisfies the following,</p><formula xml:id="formula_1">π(A × Y ) = σ(A) for any Borel subset A ⊆ X π(X × B) = µ(B) for any Borel subset B ⊆ Y (2)</formula><p>Due to Brenier's theorem <ref type="bibr" target="#b6">[8]</ref>, for absolutely continuous probability measures σ and µ (with respect to Lebesgue measure) the p-Wasserstein distance can be equivalently obtained from,</p><formula xml:id="formula_2">W p (σ, µ) = (inf f ∈M P (σ,µ) X (f (x) − x) p dσ(x)) 1 p<label>(3)</label></formula><p>where, M P (σ, µ) = {f : X → Y | f # σ = µ} and f # σ represents the pushforward of measure σ and is characterized as,</p><formula xml:id="formula_3">f −1 (A) dσ = A dµ for any Borel subset A ⊆ Y</formula><p>In the one-dimensional case, the 2-Wasserstein distance has a closed form solution as the mass preserving (MP) transport map, f ∈ M P , is unique. We will show this in the following theorem. </p><formula xml:id="formula_4">f (x) := min{t ∈ R : F µ (t) ≥ F σ (x)}<label>(4)</label></formula><p>or equivalently f (x) = F −1 µ (F σ (x)). The 2-Wasserstein distance is then obtained from,</p><formula xml:id="formula_5">W 2 (σ, µ) = ( X (f (x) − x) 2 I 0 (x)dx) 1 2 .<label>(5)</label></formula><p>Note that throughout the paper we use W 2 (σ, µ) and W 2 (I 0 , I 1 ) interchangeably.</p><p>Proof. Assume there exist more than one transport maps, say f and g, such that f, g ∈ M P (σ, µ), then we can write,</p><formula xml:id="formula_6">f (x) −∞ I 1 (τ )dτ = x −∞ I 0 (τ )dτ = g(x) −∞ I 1 (τ )dτ</formula><p>Above is equivalent to F µ (f (x)) = F µ (g(x)), but I 1 is positive everywhere, hence the CDF is monotonically increasing, therefore F µ (f (x)) = F µ (g(x)) implies that f (x) = g(x). Therefore the transport map in one dimension is unique.</p><p>The closed-form solution of the Wasserstein distance in one dimension is an attractive property, as it alleviates the need for often computationally intensive optimizations. Recently there has been some work on utilizing this property of the Wasserstein distance to higher dimensional problems <ref type="bibr" target="#b30">[32,</ref><ref type="bibr" target="#b5">7,</ref><ref type="bibr" target="#b20">22]</ref> (i.e. images). We review such distances in the following section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">The Sliced Wasserstein distance</head><p>The idea behind the Sliced Wasserstein distance is to first obtain a family of one-dimensional representations for a higher-dimensional probability distribution through projections/slicing, and then calculate the distance between two input distributions as a functional on the Wasserstein distance of their one-dimensional representations. In this sense, the distance is obtained by solving several one-dimensional optimal transport problems, which have closed-form solutions.</p><formula xml:id="formula_7">Definition 2. The d-dimensional Radon transform R maps a function I ∈ L 1 (R d ) where L 1 (R d ) := {I : R d → R| R d |I(x)|dx ≤ ∞} into</formula><p>the set of its integrals over the hyperplanes of R n and is defined as,</p><formula xml:id="formula_8">RI(t, θ) := R I(tθ + γθ ⊥ )dγ (6)</formula><p>here θ ⊥ is the subspace or unit vector orthogonal to θ. Note that R :</p><formula xml:id="formula_9">L 1 (R d ) → L 1 (R × S d−1 ), where S d−1 is the unit sphere in R d .</formula><p>We note that the Radon transform is an invertible, linear transform and we denote its inverse as R −1 . For brevity we do not define the inverse Radon transform here, but the details can be found in <ref type="bibr" target="#b26">[28]</ref>. Next, following <ref type="bibr" target="#b30">[32,</ref><ref type="bibr" target="#b5">7,</ref><ref type="bibr" target="#b20">22]</ref> we define the Sliced Wasserstein distance. Definition 3. Let µ and σ be two continuous probability measures on R d with corresponding positive probability density functions I 1 and I 0 . The Sliced Wasserstein distance between µ and σ is defined as,</p><formula xml:id="formula_10">SW (µ, σ) := ( S d−1 W 2 2 (RI 1 (., θ), RI 0 (., θ))dθ) 1 2 = ( S d−1 R (f θ (t) − t) 2 RI 0 (t, θ)dtdθ) 1 2 (7)</formula><p>where f θ is the MP map between RI 0 (., θ) and RI 1 (., θ) such that, <ref type="bibr" target="#b6">(8)</ref> or equivalently in the differential form,</p><formula xml:id="formula_11">f θ (t) −∞ RI 1 (τ, θ)dτ = t −∞ RI 0 (τ, θ)dτ, ∀θ ∈ S d−1</formula><formula xml:id="formula_12">∂f θ (t) ∂t RI 1 (f θ (t), θ) = RI 0 (t, θ), ∀θ ∈ S d−1 .<label>(9)</label></formula><p>The Sliced Wasserstein distance as defined above is symmetric, and it satisfies subadditivity and coincidence axioms, and hence it is a true metric (See <ref type="bibr" target="#b20">[22]</ref> for proof).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">The Gaussian RBF Kernel on Metric Spaces</head><p>The kernel methods and specifically the Gaussian RBF kernel have shown to be very powerful tools in a plethora of applications. The Gaussian RBF kernel was initially designed for Euclidean spaces, however, recently there has been several work extending the Gaussian RBF kernel to other metric spaces. Jayasumana et al. <ref type="bibr" target="#b18">[20]</ref>, for instance, developed an approach to exploit the Gaussian RBF kernel method on Riemannian manifolds. In another interesting work, Feragen et al. <ref type="bibr" target="#b11">[13]</ref> considered the Gaussian RBF kernel on general geodesic metric spaces and showed that the geodesic Gaussian kernel is only positive definite when the underlying Riemannian manifold is flat or in other words it is isometric to a Euclidean space. Here we review some definitions and theorems that will be used in the rest of the paper.</p><formula xml:id="formula_13">Definition 4. A positive definite (PD) (resp. conditional negative definite) kernel on a set M is a symmetric function k : M × M → R, k(I i , I j ) = k(I j , I i ) for all I i , I j ∈ M ,</formula><p>such that for any n ∈ N , any elements I 1 , ..., I n ∈ X, and numbers c 1 , ..., c n ∈ R, we have</p><formula xml:id="formula_14">n i=1 n j=1 c i c j k(I i , I j ) ≥ 0 (resp. ≤ 0)<label>(10)</label></formula><p>with the additional constraint of n i=1 c i = 0 for the conditionally negative definiteness.</p><p>Above definition is used in the following important theorem due to Isaac J. Schoenberg <ref type="bibr" target="#b33">[35]</ref>, The detailed proof of above theorem can be found in Chapter 3, Theorem 2.2 of <ref type="bibr" target="#b3">[5]</ref>.</p><p>Following the work by Jayasumana et al. <ref type="bibr" target="#b18">[20]</ref>, here we state the theorem (Theorem 6.1 in <ref type="bibr" target="#b18">[20]</ref>) which provides the necessary and sufficient conditions for obtaining a positive definite Gaussian kernel from a given distance function defined on a generic metric space. </p><formula xml:id="formula_15">I i , I j ) = ψ(I i ) − ψ(I j ) V .</formula><p>Proof. The detailed proof is presented in <ref type="bibr" target="#b18">[20]</ref>. The gist of the proof, however, follows from Theorem 2 which states that positive definiteness of k(., .) for all γ &gt; 0 and conditionally negative definiteness of d 2 (., .) are equivalent conditions. Hence, for d( </p><formula xml:id="formula_16">I i , I j ) = ψ(I i ) − ψ(I j ) V it is straightforward to show that d 2 (.,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Sliced Wasserstein Kernels</head><p>In this section we present our main theorems. We first demonstrate that the Sliced Wasserstein Gaussian kernel of probability measures is a positive definite kernel. We proceed our argument by showing that there is an explicit formulation for the nonlinear mapping to the kernel space (aka feature space) and define a family of kernels based on this mapping.</p><p>We start by proving that for one-dimensional probability density functions the 2-Wasserstein Gaussian kernel is a positive definite kernel. Proof. In order to be able to show this, we first show that for absolutely continuous one-dimensional positive probability density functions there exists an inner product space V and a function ψ :</p><formula xml:id="formula_17">M → V such that W 2 (I i , I j ) = ψ(I i ) − ψ(I j ) V .</formula><p>Let σ, µ, and ν be probability measures on R with corresponding absolutely continuous positive density functions I 0 , I 1 , and I 2 . Let f, g, h : R → R be transport maps such that f # σ = µ, g # σ = ν, and h # µ = ν. In the differential form this is equivalent to f ′ I 1 (f ) = g ′ I 2 (g) = I 0 and h ′ I 2 (h) = I 1 where I 1 (f ) represents I 1 • f . Then we have,</p><formula xml:id="formula_18">W 2 2 (I 1 , I 0 ) = R (f (x) − x) 2 I 0 (x)dx, W 2 2 (I 2 , I 0 ) = R (g(x) − x) 2 I 0 (x)dx, W 2 2 (I 2 , I 1 ) = R (h(x) − x) 2 I 1 (x)dx.</formula><p>We follow the work of Wang et al. <ref type="bibr" target="#b43">[45]</ref> and Park et al. <ref type="bibr" target="#b29">[31]</ref> and define a nonlinear map with respect to a fixed probability measure, σ with corresponding density I 0 , that maps an input probability density to a linear functional on the corresponding transport map. More precisely, ψ σ (I 1 (.)) := (f (.) − id(.)) I 0 (.) where id(.) is the identity map and f ′ I 1 (f ) = I 0 . Notice that such ψ σ maps the fixed probability density I 0 to zero, ψ σ (I 0 (.)) = (id(.) − id(.)) I 0 (.) = 0 and it satisfies,</p><formula xml:id="formula_19">W 2 (I 1 , I 0 ) = ψ σ (I 1 ) 2 W 2 (I 2 , I 0 ) = ψ σ (I 2 ) 2 .</formula><p>More importantly, we demonstrate that W 2 (I 2 , I 1 ) = ψ σ (I 1 ) − ψ σ (I 2 ) 2 . To show this, we can write,</p><formula xml:id="formula_20">W 2 2 (I 2 , I 1 ) = R (h(x) − x) 2 I 1 (x)dx = R (h(f (τ )) − f (τ )) 2 f ′ (τ )I 1 (f (τ ))dτ = R (g(τ ) − f (τ )) 2 I 0 (τ )dτ = R ((g(τ ) − τ ) − (f (τ ) − τ )) 2 I 0 (τ )dτ = ψ σ (I 1 ) − ψ σ (I 2 ) 2 2</formula><p>where in the second line we used the change of variable f (τ ) = x. In the third line, we used the fact that composition of transport maps is also a transport map, in other words, since</p><formula xml:id="formula_21">f # σ = µ and h # µ = ν then (h • f ) # σ = ν.</formula><p>Finally, from Theorem 1 we have that the one-dimensional transport maps are unique, therefore if (h • f ) # σ = ν and g # σ = ν then h • f = g. We showed that there exists a nonlinear map ψ σ : M → V for which W 2 (I i , I j ) = ψ σ (I i )−ψ σ (I j ) 2 and therefore according to Theorem 3, k(I i , I j ) := exp(−γW 2 2 (I i , I j )) is a positive definite kernel.</p><p>Combining the results in Theorems 4 and 2 will lead to the following corollary.</p><p>Corollary 1. The squared 2-Wasserstein distance for continuous one-dimensional positive probability density functions, W 2 2 (., .), is a conditionally negative definite function.</p><p>Moreover, following the work of Feragen et al. <ref type="bibr" target="#b11">[13]</ref>, Theorem 4 also states that the Wasserstein space in one dimension (the space of one-dimensional absolutely continuous positive probability densities endowed with the 2-Wasserstein metric) is a flat space (it has zero curvature), in the sense that it is isometric to the Euclidean space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">The Sliced Wasserstein Gaussian kernel</head><p>Now we are ready to show that the Sliced Wasserstein Gaussian kernel is a positive definite kernel. Proof. First note that for an absolutely continuous positive probability density function, I ∈ M , each hyperplane integral, RI(., θ), ∀θ ∈ S d−1 is a one dimensional absolutely continuous positive probability density function. Therefore, following Corollary 1 for I 1 , ..., I N ∈ M we have,</p><formula xml:id="formula_22">N i=1 N j=1 c i c j W 2 2 (RI i (., θ), RI j (., θ)) ≤ 0, ∀θ ∈ S d−1 (11) where N i=1 c i = 0.</formula><p>Integrating the left hand side of above inequality over θ leads to,</p><formula xml:id="formula_23">S d−1 ( N i=1 N j=1 c i c j W 2 2 (RI i (., θ), RI j (., θ))dθ) ≤ 0 ⇒ N i=1 N j=1 c i c j ( S d−1 W 2 2 (RI i (., θ), RI j (., θ))dθ) ≤ 0 ⇒ N i=1 N j=1 c i c j SW 2 (I i , I j ) ≤ 0 (12)</formula><p>Therefore SW 2 (., .) is conditionally negative definite, and hence from Theorem 2 we have that k(I i , I j ) := exp(−γSW 2 (I i , I j )) is a positive definite kernel for γ &gt; 0.</p><p>The following corollary follows from Theorems 3 and 5.</p><p>Corollary 2. Let M be the set of absolutely continuous positive probability density functions and let SW (., .) be the sliced Wasserstein distance, then there exists an inner product space V and a function φ : M → V such that</p><formula xml:id="formula_24">SW (I i , I j ) = φ(I i ) − φ(I j ) V , for ∀I i , I j ∈ M .</formula><p>In fact, using a similar argument as the one we provided in the proof of Theorem 4 it can be seen that for a fixed absolutely continuous measure, σ, with positive probability density function I 0 , we can define,</p><formula xml:id="formula_25">φ σ (I i ) := (f i (t, θ) − t) RI 0 (t, θ)<label>(13)</label></formula><p>where f i satisfies ∂fi(t,θ)</p><formula xml:id="formula_26">∂t RI i (f i (t, θ), θ) = RI 0 (t, θ)</formula><p>. It is straightforward to show that such φ σ also satisfies the following,</p><formula xml:id="formula_27">SW (I i , I 0 ) = φ σ (I i ) 2 (14) SW (I i , I j ) = φ σ (I i ) − φ σ (I j ) 2<label>(15)</label></formula><p>for a detailed derivation and proof of the equations above please refer to <ref type="bibr" target="#b20">[22]</ref>. More importantly, such nonlinear transformation φ σ : M → V is invertible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">The Sliced Wasserstein polynomial kernel</head><p>In this section, using Corollary 2 we define a polynomial Kernel based on the Sliced Wasserstein distance and show that this kernel is positive definite. Theorem 6. Let M be the set of absolutely continuous positive probability density functions and let σ be a template probability measure with corresponding probability density function I 0 ∈ M . Let φ σ : M → V be defined as in Equation <ref type="bibr" target="#b11">(13)</ref>. Define a kernel function k :</p><formula xml:id="formula_28">M × M → R to be k(I i , I j ) := ( φ σ (I i ), φ σ (I j ) ) d for d ∈ {1, 2, ...} then k(., .) is a positive definite kernel.</formula><p>Proof. Given I 1 , ..., I N ∈ M and for d = 1 we have,</p><formula xml:id="formula_29">N i=1 N j=1 c i c j φ σ (I i ), φ σ (I j ) = N i=1 c i φ σ (I i ), N j=1 c j φ σ (I j ) = N i=1 c i φ σ (I i ) 2 2 ≥ 0<label>(16)</label></formula><p>and since k(</p><formula xml:id="formula_30">I i , I j ) = φ σ (I i ), φ σ (I j )</formula><p>is a positive definite kernel and from Mercer's kernel properties it follows that k(I i , I j ) = ( φ σ (I i ), φ σ (I j ) ) d and k(I i , I j ) = ( φ σ (I i ), φ σ (I j ) + 1) d are also positive definite kernels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">The Sliced Wasserstein Kernel-based algorithms 4.1. Sliced Wasserstein Kernel k-means</head><p>Considering clustering problems for data with the form of probability distributions, we propose the Sliced Wasserstein k-means. For a set of input data I 1 , ..., I N ∈ M , the Sliced Wasserstein k-means with kernel k(I i , I j ) = φ σ (I i ), φ σ (I j ) transforms the input data to the kernel space via φ σ : M → V and perform k-means in this space. Note that since φ σ (I i ) − φ σ (I j ) 2 = SW (I i , I j ), performing k-means in V is equivalent to performing kmeans with Sliced Wasserstein distance in M . The kernel k-means with the Sliced Wasserstein distance, essentially provides k barycenters for the input distributions. In addition, the Gaussian and polynomial Sliced Wasserstein kernel k-means are obtained by performing Gaussian and polynomial kernel k-means in V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Sliced Wasserstein Kernel PCA</head><p>Now we consider the key concepts of the kernel PCA. The Kernel-PCA <ref type="bibr" target="#b34">[36]</ref> is a non-linear dimensionality reduction method that can be interpreted as applying the PCA in the kernel-space (or feature-space), V. Performing standard PCA on φ σ (I 1 ), ..., φ σ (I N ) ∈ V provides the Sliced Wasserstein kernel PCA. In addition, applying Gaussian and polynomial Sliced Wasserstein kernel PCA to I 1 , ..., I N ∈ M is also equivalent to applying Gaussian and polynomial kernel PCA on φ σ (I 1 ), ..., φ σ (I N ) ∈ V. We utilize the cumulative percent variance (CPV) as a quality measure for how well the principal components are capturing the variation of the dataset in M and similarly in V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Sliced Wasserstein Kernel SVM</head><p>For a binary classifier, given a set of training examples</p><formula xml:id="formula_31">{I i , y i } N i=1</formula><p>where I i ∈ M and y i ∈ {−1, 1}, support vector machine (SVM) searches for a hyperplane in M that separates training classes while maximizing the separation margin where separation is measured with the Euclidean distance. A kernel-SVM , on the other hand, searches for a hyperplane in V which provides maximum margin separation between φ σ (I i )s which is equivalent to finding a nonlinear classifier in M that maximizes the separation margin according to the Sliced Wasserstein distance. Note that the kernel SVM with the Sliced Wasserstein Gaussian and polynomial kernels are essentially equivalent to applying kernel SVM, with the same kernels in the transformed Sliced Wasserstein space V. It is worthwhile to mention that, since φ σ is invertible, the Sliced Wasserstein kernel SVM learned from kernel k(I i , I j ) = φ σ (I i ), φ σ (I j ) can be sampled along side the orthogonal direction to the discriminating hyperplane in V and then inverted through φ −1 σ to directly get the discriminating features in the space of the probability densities, M . Finally, for multiclass classification problems, the problem can be turned into several binary classification tasks using pairwise coupling as suggested by Wu et al. <ref type="bibr" target="#b44">[46]</ref> or a one versus all approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experimental Results</head><p>For our experiments we utilized two image datasets, namely the University of Illinois Urbana Champaign (UIUC) texture dataset <ref type="bibr" target="#b23">[25]</ref> and the LHI animal face dataset <ref type="bibr" target="#b36">[38]</ref>. The texture dataset contains 25 classes of texture images with 40 images per class, which includes a wide range of variations. The animal face dataset contains 21 classes of animal faces with the average number of images per class being 114. <ref type="figure" target="#fig_0">Figures 1 (a)</ref> and 2 (a) show sample images from image classes for the texture and the LHI dataset, respectively. For the texture dataset we extracted the gray-level co-occurence matrices for texture images and normalized them to obtain empirical joint probability density functions of co-occuring gray levels (See <ref type="figure" target="#fig_0">Figure 1 (b)</ref>). On the other hand, for the animal face dataset, we used the normalized HOGgles images <ref type="bibr" target="#b40">[42]</ref> as probability distribution representations of RGB animal face images (See <ref type="figure" target="#fig_1">Figure 2 (b)</ref>). We acknowledge that the HOGgles are not designed for feature extraction but rather for visualization of the extracted HoG features <ref type="bibr" target="#b40">[42]</ref>, but our goal here is to show that for any ex-tracted probability density features from images the Sliced Wasserstein kernels often outperform commonly used kernels.</p><p>Regarding the implementation, the fixed density I 0 for each dataset is chosen to be the average distribution over the entire dataset. The Radon transform was used to slice the probability distributions at discrete angles θ ∈ {0, 1, ..., 179}. For each slice, RI i (., θ k ), the transport map f i that satisfies Equation <ref type="formula" target="#formula_12">(9)</ref> is calculated using Equation <ref type="bibr" target="#b2">(4)</ref>. Finally, the kernel representations φ σ (I i ) for the extracted probability distributions were calculated using Equation <ref type="bibr" target="#b11">(13)</ref>. The calculated representations, φ σ (I i ), are shown in <ref type="figure" target="#fig_0">Figures 1 (c) and 2 (c)</ref>. The Matlab code <ref type="bibr" target="#b24">[26]</ref> and detailed description of our implementation can be found in <ref type="table">[</ref> It should be mentioned that, for higher dimensional PDFs, more projections (larger M) is required to capture the underlying variations of the distributions.</p><p>First, the PCA of the input data, I 1 , ..., I N ∈ M (i.e. the co-occurence matrices for the texture images and the HOGgles images for the LHI dataset) as well as the kernel-PCA of the data with the Sliced Wasserstein kernel, RBF kernel, and the Sliced Wasserstein Gaussian kernel were calculated for both datasets. <ref type="figure" target="#fig_2">Figure 3</ref> shows the cumulative percent variance (CPV) of the dataset captured by different kernels for both datasets. It can be seen that the variations in the datasets are captured more efficiently in the Sliced Wasserstein kernel space. We note that the CPV generally depends on the choice of the kernel parameters (e.g. radius of the RBF), however, for the Sliced Wasserstein kernel, k(I i , I j ) = φ σ (I i ), φ σ (I j ) , there are no parameters and the comparison between PCA and Sliced Wasserstein kernel PCA is parameter free.</p><p>Next, we performed classification tasks on the texture and animal face datasets. Linear SVM, RBF kernel SVM, Sliced Wasserstein Gaussian kernel SVM, and the Sliced Wasserstein polynomial kernel were utilized for classification accuracy comparison. A five fold cross validation scheme was used, in which 20% of each class was held out for testing and the rest was used for training and parameter estimation. The hyperparameters of the kernels are calculated through grid search. The classification experiments were repeated 100 times and the means and standard deviations of the classification accuracies for both datasets are  reported in <ref type="figure" target="#fig_4">Figure 4</ref>.</p><p>Finally, we performed clustering on the UIUC texture and the LHI animal face dataset. We utilized the k-means algorithm and the spectral clustering method on the normalized co-occurence matrices and the HOGgles images, I ∈ M , and their corresponding representations in the kernel space, φ σ (I) ∈ V (i.e. kernel k-means). We note that, since the mapping φ σ () is known, any clustering algorithm can be performed in the Sliced Wasserstein kernel space. We repeated the k-means experiment 100 times and mea-  sured the V-measure <ref type="bibr" target="#b31">[33]</ref>, which is a conditional entropybased external cluster evaluation measure, at each iteration for both datasets. In addition, we performed spectral clustering in the kernel space φ σ (I) ∈ V and measured the Vmeasure. <ref type="figure" target="#fig_5">Figure 5</ref> shows the mean and standard deviation of the V-measure for k-means, Sliced Wasserstein kernel kmeans, spectral clustering, and Sliced Wasserstein spectral clustering. It can be seen that for both methods, the Sliced Wasserstein kernel provides better clusters which match the texture and animal face classes better, as it leads to higher V-measure values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Discussion</head><p>In this paper, we have introduced a family of provably positive definite kernels for probability distributions based on the mathematics of the optimal transport and more precisely the Sliced Wasserstein distance. We denoted our proposed family of kernels as the Sliced Wasserstein kernels. Following the work of <ref type="bibr" target="#b29">[31,</ref><ref type="bibr" target="#b20">22]</ref>, we provided an explicit nonlinear and invertible formulation for mapping probability distributions to the kernel space (aka feature space). Our experiments demonstrated the benefits of the Sliced Wasserstein kernels over the commonly used RBF and Polynomial kernels in a variety of pattern recognition tasks on probability densities.</p><p>More specifically, we showed that utilizing a dimensionality reduction scheme like PCA with the Sliced Wasserstein kernel leads to capturing more of the variations of the datasets with fewer parameters. Similarly, clustering methods can benefit from the Sliced Wasserstein kernel as the clusters have higher values of V-measure and lower value of inertia. Finally, we demonstrated that the classification accuracy for a kernel classifier like the kernel SVM can also benefit from the Sliced Wasserstein kernels.</p><p>Finally, the experiments in this paper were focused on two-dimensional distributions. However, the proposed framework can be extended to higher-dimensional probability densities. We therefore intend to investigate the application of the Sliced Wasserstein kernel to higher-dimensional probability densities such as volumetric MRI/CT brain data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Acknowledgement</head><p>This work was financially supported by the National Science Foundation (NSF), grant number 1421502.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Theorem 1 .</head><label>1</label><figDesc>Let σ and µ be absolutely continuous probability measures on R with corresponding positive density functions I 0 and I 1 , and corresponding cumulative distribution functions F σ (x) := σ((−∞, x]) and F µ (x) := µ((−∞, x]). Then, there only exists one monotonically increasing transport map f : R → R such that f ∈ M P (σ, µ) and it is defined as,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Theorem 2 .</head><label>2</label><figDesc>Let M be a nonempty set and f : (M ×M ) → R be a function. Then kernel k(I i , I j ) = exp(−γf (I i , I j )) for all I i , I j ∈ M is positive definite for all γ &gt; 0 if and only if f (., .) is conditionally negative definite.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Theorem 3 .</head><label>3</label><figDesc>Let (M, d) be a metric space and define k : M × M → R by k(I i , I j ) := exp(−γd 2 (I i , I j )) for all I i , I j ∈ M . Then k(., .) is a positive definite kernel for all γ &gt; 0 if and only if there exists an inner product space V and a function ψ : M → V such that d(</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>.) is conditionally negative definite and therefore k(., .) is positive definite. On the other hand, if k(., .) is positive definite then d 2 (., .) is conditionally negative definite, and since d(I i , I i ) = 0 for all I i ∈ M a vector space V exists for which d(I i , I j ) = ψ(I i ) − ψ(I j ) V for some ψ : M → V<ref type="bibr" target="#b18">[20,</ref><ref type="bibr" target="#b3">5]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Theorem 4 .</head><label>4</label><figDesc>Let M be the set of absolutely continuous onedimensional positive probability density functions and define k : M ×M → R to be k(I i , I j ) := exp(−γW 2 2 (I i , I j )) then k(., .) is a positive definite kernel for all γ &gt; 0.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Theorem 5 .</head><label>5</label><figDesc>Let M be the set of absolutely continuous positive probability density functions and define k : M × M → R to be k(I i , I j ) := exp(−γSW 2 (I i , I j )) then k(., .) is a positive definite kernel for all γ &gt; 0.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>1]. The computational complexity of calculating the transport map for a pair of one-dimensional probability density functions (PDFs), presented as vectors of length N , is O(N logN ). On the other hand, the computational complexity of calculating M slices (projections) of a ddimensional PDF, which is presented as a N × N × ... × N tensor, is O(M N d ). Therefore, the overall computational complexity of calculating the explicit mapping φ σ (.) is dominated by computational complexity of calculating the M projections of the distribution and is equal to O(M N d ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 1 .</head><label>1</label><figDesc>The UIUC texture dataset with 25 classes (a), the corresponding calculated co-occurence matrices (b), and the kernel representation (i.e. φσ) of the co-occurence matrices (c).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 2 .</head><label>2</label><figDesc>The LHI animal face dataset with 21 classes (a), the corresponding calculated HOGgles representation (b), and the kernel representation (i.e. φσ) of the HOGgles images (c).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 3 .</head><label>3</label><figDesc>Percentage variations captured by eigenvectors calculated from PCA, RBF kernel-PCA, Sliced Wasserstein kernel PCA, and Sliced Wasserstein Gaussian kernel (SW-RBF) PCA for the texture dataset (a) and for the animal face dataset (b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 4 .</head><label>4</label><figDesc>Kernel SVM classification accuracy with linear kernel, radial basis function kernel (RBF), Sliced Wasserstein Gaussian Kernel (SW-RBF), and Sliced Wasserstein Polynomial Kernel (SW-Polynomial).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 5 .</head><label>5</label><figDesc>Cluster evaluation for k-means, Sliced Wasserstein kernel k-means, spectral clustering, and spectral clustering k-means using the V-measure.</figDesc></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Barycenters in the Wasserstein space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Agueh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Carlier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Mathematical Analysis</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="904" to="924" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Detecting and visualizing cell phenotype differences from microscopy images using transport-based morphometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kolouri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">K</forename><surname>Rohde</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="3448" to="3453" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Iterative Bregman projections for regularized transportation problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-D</forename><surname>Benamou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Carlier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cuturi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Nenna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Peyré</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Scientific Computing</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1111" to="1138" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Harmonic analysis on semigroups</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P R</forename><surname>Christensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ressel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1984" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Geodesic PCA in the Wasserstein space by convex PCA</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bigot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gouet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>López</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Sliced and Radon Wasserstein barycenters of measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bonneel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rabin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Peyré</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pfister</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note>Journal of Mathematical Imaging and Vision</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Polar factorization and monotone rearrangement of vector-valued functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Brenier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Communications on pure and applied mathematics</title>
		<imprint>
			<date type="published" when="1991" />
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="375" to="417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A gradient descent solution to the Monge-Kantorovich problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chartrand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wohlberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Vixie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Bollt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Mathematical Sciences</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">22</biblScope>
			<biblScope unit="page" from="1071" to="1080" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Permanents, transportation polytopes and positive definite kernels on histograms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cuturi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Sinkhorn distances: Lightspeed computation of optimal transport</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cuturi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2292" to="2300" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Kernel Earth Mover&apos;s distance for EEG classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Daliri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Clinical EEG and neuroscience</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="182" to="187" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Geodesic Exponential Kernels: When curvature and linearity conflict</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Feragen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lauze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hauberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Earth Mover&apos;s distance yields positive definite kernels for certain ground distances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename><surname>Duncan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kanno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Selmic</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1510.02833</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Analysis of co-occurrence texture statistics as a function of gray-level quantization for classifying breast ultrasound</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gómez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F C</forename><surname>Infantosi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1889" to="1899" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note>Medical Imaging</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Optimal mass transport for registration and warping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Haker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tannenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Angenent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="225" to="240" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Image matching using local symmetry features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Hauagge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="206" to="213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Kernel methods in machine learning. The annals of statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1171" to="1220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Kernel methods on the riemannian manifold of symmetric positive definite matrices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jayasumana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Harandi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2013 IEEE Conference on</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="73" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jayasumana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Harandi</surname></persName>
		</author>
		<title level="m">Kernel methods on Riemannian manifolds with Gaussian RBF kernels. IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Improving bagof-features for large scale image search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The Radon Cumulative Distribution Transform and its application to image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kolouri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rohde</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="920" to="934" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Transport-based single frame super resolution of very low resolution face images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kolouri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">K</forename><surname>Rohde</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4876" to="4884" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Rohde. A continuous linear optimal transport approach for pattern analysis in image datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kolouri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Tosun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Ozolek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">K</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="453" to="462" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A sparse texture representation using local affine regions. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1265" to="1278" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">The MathWorks Inc</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>MATLAB. version 8.4.0</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A multiscale approach to optimal transport</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Mérigot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
		<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="1583" to="1592" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">The mathematics of computerized tomography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Natterer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1986" />
			<biblScope unit="volume">32</biblScope>
			<pubPlace>Siam</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">An efficient linear programming method for optimal transportation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Oberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ruan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.03668</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Accurate diagnosis of thyroid follicular lesions from nuclear morphology using supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Ozolek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Tosun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kolouri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">K</forename><surname>Rohde</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical image analysis</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="772" to="780" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">The Cumulative Distribution Transform and linear pattern classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kolouri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rohde</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.05936</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Wasserstein barycenter and its application to texture mixing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rabin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Peyré</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Delon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Scale Space and Variational Methods in Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">V-Measure: A conditional entropy-based external cluster evaluation measure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hirschberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-CoNLL</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="410" to="420" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">The Earth Mover&apos;s distance as a metric for image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rubner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tomasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="99" to="121" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Metric spaces and completely monotone functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Schoenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Mathematics</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="811" to="841" />
			<date type="published" when="1938" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Kernel principal component analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-R</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Neural Networks-ICANN&apos;97</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1997" />
			<biblScope unit="page" from="583" to="588" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Principal geodesic analysis for probability measures under the optimal transport metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Seguy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cuturi</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning hybrid image templates (HIT) by information projection. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1354" to="1367" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Convolutional Wasserstein Distances: Efficient optimal transportation on geometric domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Solomon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>De Goes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Studios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Peyré</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cuturi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Butscher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SIGGRAPH 2015)</title>
		<meeting>SIGGRAPH 2015)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Detection of malignant mesothelioma using nuclear structure of mesothelial cells in effusion cytology specimens</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Tosun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Yergiyev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kolouri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Silverman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">K</forename><surname>Rohde</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cytometry Part A</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="326" to="333" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Optimal transport: old and new</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Villani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>Springer Science &amp; Business Media</publisher>
			<biblScope unit="volume">338</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Hoggles: Visualizing object detection features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Malisiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV), 2013 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Dense trajectories and motion boundary descriptors for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kläser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="60" to="79" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">An optimal transportation approach for nuclear structure-based pathology. Medical Imaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ozolek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Slepcev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">K</forename><surname>Rohde</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="621" to="631" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A linear optimal transportation framework for quantifying and visualizing variations in sets of images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Slepcev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Ozolek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">K</forename><surname>Rohde</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Probability estimates for multi-class classification by pairwise coupling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Weng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="975" to="1005" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Local features and kernels for classification of texture and object categories: A comprehensive study. International journal of computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marszałek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="page" from="213" to="238" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
