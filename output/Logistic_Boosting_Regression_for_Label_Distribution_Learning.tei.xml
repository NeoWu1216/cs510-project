<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Logistic Boosting Regression for Label Distribution Learning *</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Xing</surname></persName>
							<email>xingchao@seu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="laboratory">Key Lab of Computer Network and Information Integration (Ministry of Education</orgName>
								<orgName type="institution">Southeast University</orgName>
								<address>
									<postCode>211189</postCode>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Geng</surname></persName>
							<email>xgeng@seu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="laboratory">Key Lab of Computer Network and Information Integration (Ministry of Education</orgName>
								<orgName type="institution">Southeast University</orgName>
								<address>
									<postCode>211189</postCode>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Xue</surname></persName>
							<email>hxue@seu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="laboratory">Key Lab of Computer Network and Information Integration (Ministry of Education</orgName>
								<orgName type="institution">Southeast University</orgName>
								<address>
									<postCode>211189</postCode>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Logistic Boosting Regression for Label Distribution Learning *</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:43+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Label Distribution Learning (LDL) is a general learning framework which includes both single label and multi-label learning as its special cases. One of the main assumptions made in traditional LDL algorithms is the derivation of the parametric model as the maximum entropy model. While it is a reasonable assumption without additional information, there is no particular evidence supporting it in the problem of LDL. Alternatively, using a general LDL model family to approximate this parametric model can avoid the potential influence of the specific model. In order to learn this general model family, this paper uses a method called Logistic Boosting Regression (LogitBoost) which can be seen as an additive weighted function regression from the statistical viewpoint. For each step, we can fit individual weighted regression function (base learner) to realize the optimization gradually. The base learners are chosen as weighted regression tree and vector tree, which constitute two algorithms named LDLogitBoost and AOSO-LDLogitBoost in this paper. Experiments on facial expression recognition, crowd opinion prediction on movies and apparent age estimation show that LDLogitBoost and AOSO-LDLogitBoost can achieve better performance than traditional LDL algorithms as well as other LogitBoost algorithms.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Learning with ambiguity is a hot topic in recent machine learning and computer vision research. A learning method is essentially building a mapping from the instances to the label space. We can use a description degree d y</p><p>x as a numerical indicator to measure the relationship of the label y to the instance x and indicate the relative label intensity. As can be seen from <ref type="figure" target="#fig_0">Fig.1</ref>, there are mainly three ways to label an instance in existing learning paradigms: For the single-label case (a), the label y 2 fully describes the instance, so d y2 x = 1. For the multi-label case (b), each of the two positive labels y 2 and y 4 by default describes 50% of the instance, so d y2 x = d y4 x = 0.5. Finally, (c) represents a general case of label distribution, which satisfies the constraints d y</p><p>x ∈ [0, 1] and y d y x = 1. d y x of the label distribution is neither the label confidence nor label population. It actually represents the description degree that the corresponding label y describes the instance x. In other words, label distribution can not only model the ambiguity of "what describes the instance", but also deal with the more general ambiguity of "how to describe the instance". This example illustrates that label distribution is more general than both the single-label and multi-label cases, and thus can provide more flexibility in the learning process. Inspired by this observation, Geng and Ji <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b6">7]</ref> proposed a general method named Label Distribution Learning (LDL) which learning a mapping from the instance to its label distribution. There are a lot of LDL algorithms which can fit some real applications well. For example, Geng et al. <ref type="bibr" target="#b11">[12]</ref> proposed two algorithms named IIS-LDL and CPNN to deal with facial age estimation problems by constructing the age distribution, because the faces at close ages look quite similar. Geng and Ji <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b6">7]</ref> proposed BFGS-LDL algorithm by using the effective quasi-Newton optimization to further improve IIS-LDL. Geng et al. <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b12">13]</ref> proposed the AGES algorithm based on the subspace trained on a data structure called aging pattern vector. This algorithm was further extended to Adaptive Label Distribution Learning (ALDL) by Geng et al. <ref type="bibr" target="#b9">[10]</ref>. Yang et al. <ref type="bibr" target="#b21">[22]</ref> tried to combine the LDL with deep learning methods called Deeply Label Distribu-tion Learning (DLDL) to deal with the apparent age estimation problem. Zhang et al. <ref type="bibr" target="#b22">[23]</ref> proposed a LDL method for crowd counting in public video surveillance. As for expression recognition, LDL can also be adapted to the emotion distribution learning <ref type="bibr" target="#b23">[24]</ref>, because an expression rarely expresses pure emotion, but often a mixture of different emotions, and each emotion has its own intensity. Geng and Xia <ref type="bibr" target="#b10">[11]</ref> extended label distribution to multivariate label distributions and proposed two algorithms to learn from the multivariate label distributions based on the Jeffery divergence. Futher more, LDL can also be combined with SVR named LDSVR <ref type="bibr" target="#b7">[8]</ref>. This algorithm was used for pre-release prediction of crowd opinion on movies. Note that the usage of LDL algorithms is not limited to the prediction of label distribution, but includes marketing strategy, the design of computer vision system, interests recommendation, etc.</p><p>One of the main assumptions made in traditional LDL algorithms is that the description degree d y x can be represented by the form of maximum entropy model <ref type="bibr" target="#b1">[2]</ref>. In the exponential part of this model, it uses a linear combination of the input space assigned to the instance. Such an approach is limited for the following reasons: firstly, the linear combination of the exponential part has limited description which is not applicable to all LDL problems. Secondly, for particular application, in order to improve the performance with this specific model, we need to improve the loss function. This will make the loss function more complex with the complicate optimization strategies and may lead to over-fitting. Alternatively, replacing this exponential part with a general function to approximate this parametric model can avoid the potential influence of the specific model. This general function is variable such as decision tree or the linear combination. So we can use this general function to represent any specific function which constitutes a LDL model family. Thus in order to find a unified learning framework for these various models of LDL model family with the same optimization strategy, we can assume that the model is the additive. Therefore a unique method called Logistic Boosting Regression (LogitBoost) <ref type="bibr" target="#b5">[6]</ref> can be used to learn this general model family all together. On the one hand, LogitBoost is a combination of the boosting method and the logistic regression <ref type="bibr" target="#b3">[4]</ref>. It uses the coordinate descent optimization method over the function space based on the second derivative Hessian matrix. On the other hand, LogitBoost can be seen as an additive weighted function regression from a statistical viewpoint. We can fit individual weighted regression function (base learner) to realize the optimization step by step. Although any regression function of the model family can be used for LDL, it has been reported in the literature <ref type="bibr" target="#b19">[20]</ref> that traditional LogitBoost turns out to have some drawbacks because of the over approximation such as using a second-order (diagonal) approximation to fit individual regression function. Therefore, Li <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b15">16]</ref> proposed robust LogitBoost and ABC-LogitBoost which choose the 'base class' for improving single-label classification accuracy adaptively. Sun et al. <ref type="bibr" target="#b19">[20]</ref> proposed a base learner named vector tree which can fit an united regression function of each class at the same time. This model used by LogitBoost is called AOSO-LogitBoost. To some extent, ABC-LogitBoost is a special case of AOSO-LogitBoost with a less flexible tree model.</p><p>In this paper, we extend the maximum entropy model of traditional LDL to a more general LDL model family. Then we use LogitBoost method to learn this model. In this way, the boosting method can also be applied to LDL which can "boost" the accuracy of any given learning algorithm especially simple learning algorithms and avoid over fitting <ref type="bibr" target="#b18">[19]</ref>. Next, two base learners are chosen for LDL model family, namely weighted regression tree <ref type="bibr" target="#b2">[3]</ref> and vector tree <ref type="bibr" target="#b19">[20]</ref>, to constitute two algorithms called LDLogitBoost as well as AOSO-LDLogitBoost. The rest of the paper is organized as follows. Section 2 introduces the LogitBoost for the LDL and proposes two LDL algorithms named LDLog-itBoost and AOSO-LDLogitBoost. In Section 3, the experimental results are reported. Finally, several conclusions are drawn in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Proposed Algorithms</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">LDL model family</head><p>We begin with the basic settings for the LDL Problems. First of all, all the labels Y = {y 1 , . . . , y j , . . . , y C } with a non-zero description degree are actually the "correct" labels to describe the instance x ∈ X but just with the different importance measured by d y x , where X represents the input space X = R q . C is the number of the labels. Secondly, we can normalize the description degree to make d y x ∈ [0, 1] and y d y x = 1 to constitute the label distribution. Measuring the description degree is very important, as one can know how much each label description degree is and how many labels are related to the particular instance. Consequently, the LDL can be formulated that given a training</p><formula xml:id="formula_0">set S = {(x 1 , D 1 ), . . . , (x i , D i ), . . . , (x N , D N )}, where x i ∈ X and D i = {d y1 xi , . . . , d yj xi , . . . , d y C xi } is the label dis- tribution associated with the instance x i .</formula><p>N is the number of the instances. The goal of LDL is to learn a conditional probability mass function from S:</p><formula xml:id="formula_1">p(y j |x i ) = 1 Z exp(F j (x i )) (1) where Z = C s=1 exp(F s (x i )) is the normalization factor. Here F j (x) is an element of C-dimensional function vector F(x) = {F 1 (x), . . . , F j (x), . . . , F C (x)}.</formula><p>It is any function which constitutes a LDL model family. From a viewpoint of LogitBoost, F(x) of this LDL model family can also be called base (weak) learner. For example, we can assume</p><formula xml:id="formula_2">that F j (x i ) = β T j x i (2)</formula><p>Substituting Eq. (2) to Eq. (1), we can get traditional LDL model:</p><formula xml:id="formula_3">p(y j |x i ) = 1 Z exp(β T j x i ) (3) where Z = C s=1 exp(β T s x i ).</formula><p>This model is maximum entropy model which can be learned by traditional LDL algorithms such as IIS-LDL and BFGS-LDL <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b6">7]</ref>. On the one hand, traditional LDL model may have a potential limitation on the performance of the algorithm. On the other hand, traditional LDL model could not be adapted to various applications. Therefore, this LDL model family can avoid the potential influence of the specific model and different function model can be selected for different applications.</p><p>There are different criteria that can be used to measure the distance or similarity between two distributions. For example, if the Kullback-Leibler divergence (K-L) is used as the distance measure between the true label distributions and the predicted label distributions, then the loss of this model is determined by:</p><formula xml:id="formula_4">L = N i=1 loss(x i ) (4) loss(x) = C j=1 d yj x log d yj x p(y j |x)<label>(5)</label></formula><p>Minimizing Eq. (5) is equivalent to minimizing the negative log-likelihood loss:</p><formula xml:id="formula_5">loss(x) = − C j=1 d yj x log p(y j |x)<label>(6)</label></formula><p>Substituting Eq. (1) to Eq. (6), we can get:</p><formula xml:id="formula_6">loss(x; F) = − C j=1 d yj x log 1 Z exp(F j (x i ))<label>(7)</label></formula><p>Eq. <ref type="formula" target="#formula_6">(7)</ref> is the target function for the optimization. However, different models or loss functions may lead to different optimization strategies. Thus in order to find a unified learning framework for these various models or loss functions with the same optimization strategy, we can use Logistic Boosting Regression (LogitBoost) <ref type="bibr" target="#b5">[6]</ref> to avoid this problem, which assume F is a flexible additive function:</p><formula xml:id="formula_7">F (T ) (x) = T t=1 v t f (t) (x, a t )<label>(8)</label></formula><p>where v t is the shrinkage and T is the total steps of the additive model. f (t) (x, a t ) are the t-th C-dimensional base(weak) function learners which correspond with the function F(x). Owing to its iterative nature of Eq. <ref type="formula" target="#formula_7">(8)</ref>, in each step of the optimization, the best</p><formula xml:id="formula_8">f (t) (x) is added only based on F (t−1) (x) = (t−1) s=1 v s f (s) (x)</formula><p>. Formally:</p><formula xml:id="formula_9">f (t) (x) = arg min f N i=1 loss(x i ; F (t−1) + f (t) ) (9)</formula><p>The function of f (t) (x) can be obtained by using a strategy similar to the greedy stage wise algorithm <ref type="bibr" target="#b5">[6]</ref>, which is a well-know algorithm for LogitBoost. This algorithm suggests to a second-order (diagonal) approximation, which needs to calculate the first two derivatives of the log-likelihood loss function Eq. (6) with respective to the function F j (x). We can derive:</p><formula xml:id="formula_10">g j (x) = ∂loss(x) ∂F j = −(d yj x − p(y j |x)) (10) h jj (x) = ∂ 2 loss(x) ∂F 2 j = p(y j |x)(1 − p(y j |x)) (11) h js (x) = ∂ 2 loss(x) ∂F j ∂F s = −p(y j |x)p(y s |x)<label>(12)</label></formula><p>In this way, we can fit an individual regression function:</p><formula xml:id="formula_11">N i=1 f j (x i ) ← − N i=1 g j (x i ) N i=1 h jj (x i ) ← − N i=1 gj (xi) hjj (xi) h jj (x i ) N i=1 h jj (x i ) ← −E w ( g j (x) h jj (x) )<label>(13)</label></formula><p>where E w (·) indicates a weighted expectation of x with the weight w. Equivalently, the Newton update f (x) solves the weighted least-squares approximation about F(x) to the log-likelihood:</p><formula xml:id="formula_12">min fj (x) E wj (x) f j (x) − (− g j (x) h jj (x) ) 2<label>(14)</label></formula><p>Thus it can be summarized as: At the each step, we can fit a weighted regression function on the training instances x i for the j-th label class, with the target:</p><formula xml:id="formula_13">z i,j = − g j (x i ) h jj (x i )<label>(15)</label></formula><p>and the weight:</p><formula xml:id="formula_14">w i,j = h jj (x i )<label>(16)</label></formula><p>For identifiability, C j=1 F j (x) = 0, i.e., the sum-tozero constraint, is usually adopted in <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17]</ref>. Therefore, in order to satisfy the sum-to-zero constraint, we need to change the updated form:</p><formula xml:id="formula_15">F (t) j (x) ← F (t−1) j (x) + v t C − 1 C f (t) j (x) − 1 C C s=1 f (t) s (x)<label>(17)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">LDLogitBoost</head><p>We can choose any function of LDL model family for this individual regression function. The weighted regression tree is very common in LogitBoost <ref type="bibr" target="#b2">[3]</ref>. It can be concluded: At the each step, we can fit individual weighted regression function on the training instances x i for the j-th label class, with the target z i,j and the weight w i,j . This method can be called LDLogitBoost and its pseudo code is given in Algorithm 1.   Calculate p(y j |x i ) by Eq. (1) 12: end for</p><formula xml:id="formula_16">Algorithm 1 LDLogitBoost 1: F j (x) = 0, p i,j = 1 C , j = 0 to C and i = 1 to N 2: for t = 1 to T do 3: for j = 1 to C do</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">AOSO-LDLogitBoost</head><p>Although any regression function of the model family can be used for LDL, it has been reported in the literature <ref type="bibr" target="#b19">[20]</ref> that vector tree model may lead a better performance than other regression functions. Each node in the vector tree model is the vector where the unique classifier output is guaranteed by adding a sum-to-zero constraint. <ref type="figure" target="#fig_4">Fig. 2</ref> gives us a vector tree model for a 3-class problem. A class pair is selected for each tree node adaptively.</p><p>For each internal node (filled), the pair is for computing split gain; For terminal nodes (unfilled), it is for node vector update. That is, at each step t, we can construct a vector tree model such that x i ∈ R t m :</p><formula xml:id="formula_17">F (t) j (x) ← F (t−1) j (x) + v t M m=1 a t m,j I(x i ∈ R t m ) (18)</formula><p>where I(·) is the indication function and v t is the shrinkage factor. So the loss function of the node becomes:</p><formula xml:id="formula_18">N odeLoss(a; R m ) = xi∈Rm loss(x i ; F)<label>(19)</label></formula><p>As we can see, Eq. <ref type="formula" target="#formula_18">(19)</ref> is equivalent to Eq. (4) if the vector tree only has the root node. Minimizing the Eq. <ref type="bibr" target="#b18">(19)</ref> allows us to obtain a set of nodes {a m , R m } M m=1 using the following procedures:</p><p>In the first step, to obtain the values a m from a given R m , we can simply take the minimization of Eq. <ref type="formula" target="#formula_18">(19)</ref>:</p><formula xml:id="formula_19">a m = arg min a N odeLoss(a; R m )<label>(20)</label></formula><p>where a m is the vector value of the related terminal node as</p><formula xml:id="formula_20">a m,k =      +(−ḡ/h) if k=r −(−ḡ/h) if k=s 0 otherwise (21) withḡ = − xi∈Rm (g r (x i ) − g s (x i ));<label>(22)</label></formula><formula xml:id="formula_21">h = xi∈Rm (h rr (x i ) + h ss (x i ) + 2h rs (x i ));<label>(23)</label></formula><p>as well as the class pair r and s can be selected by:</p><formula xml:id="formula_22">r = arg max j {−g j }<label>(24)</label></formula><formula xml:id="formula_23">s = arg max j { g r − g j h rr + h jj − 2h rj }<label>(25)</label></formula><p>In the second step, to obtain the partition {R m } M m=1 , we recursively perform binary splitting until there are Mterminal nodes. Suppose an internal node with n training instances, we fix on some feature and re-index all the n instances to their sorted feature values. Now we need to find the index n ′ with 1 &lt; n ′ &lt; n that maximizes the node gain defined as loss reduction after a division between the n ′ -th and (n ′ + 1)-th instances. At the same time, we can calculate the NodeLoss as:</p><formula xml:id="formula_24">N odeLoss(a * ; R m ) = −ḡ 2 2h<label>(26)</label></formula><p>which contributes to the biggest node gain approximately as:</p><formula xml:id="formula_25">N odeGain(n ′ ) = −ḡ 2 L 2h L + −ḡ 2 R 2h R − −ḡ 2 2h<label>(27)</label></formula><p>The selection of a class pair (r, s) for a vector tree is adaptive. And the vector tree is a binary tree with the vector value. By using this model as the base (weak) learner, we can generate a new algorithm called AOSO-LDLogitBoost and its pseudo code is given in Algorithm 2, where "AOSO" means "Adaptive one vs one".  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experiment</head><p>To demonstrate the effectiveness of the proposed LD-LogitBoost and AOSO-LDLogitBoost algorithms, we perform experiments on three different databases: s-BU 3DFE (scores-Binghamton University 3D Facial Expression) <ref type="bibr" target="#b23">[24]</ref>, COPM (Crowd Opinion Prediction on Movies) <ref type="bibr" target="#b7">[8]</ref> and ChaLearn Age Estimation Competition Data Set (CAECD) <ref type="bibr" target="#b4">[5]</ref>. The first two data sets are popular label distribution data sets, and the last one is a single label data set. The additional prior knowledge of mean value and the standard deviation of the label distribution is given in the last data set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Facial Expression Recognition</head><p>Most existing facial expression recognition methods assume the availability of a single emotion for each expres-sion in training set. However, in practical applications, an expression rarely expresses pure emotion, but often a mixture of different emotions, and each emotion has its own intensity. Facial expression recognition is a LDL problem which can be seen from <ref type="figure" target="#fig_7">Fig. 3</ref>. The emotion distribution is represented by a curve. There are six values at the horizontal axis labeled by the six basic emotions: happiness (HAP), sadness (SAD), surprise (SUR), anger (ANG), disgust (DIS) and fear <ref type="bibr">(FER)</ref>. The values at the vertical axis represent the description degrees of each emotion. We need to learn a mapping from the facial expression image to its emotion distribution.  Here we use s-BU 3DFE data set which has 2500 instances. 23 students are asked to score the s-BU 3DFE database on 6 basic emotions (i.e., happiness, sadness, surprise, fear, anger and disgust) with a 5 level scale (5 represents the highest emotion intensity, while 1 represents the lowest emotion intensity). The average score of each emotion is used to represent the specific emotion intensity. The images are cropped manually so that the eyes are at the same positions, and then the cropped images are resized to 110*140 pixels. Features are extracted by the method of Local Binary Patterns (LBP) <ref type="bibr" target="#b0">[1]</ref>.</p><p>A nature choice of evaluation measures is the average similarity or difference between the real label distributions P j and the predicted distributions Q j . There are many measures for similarity/distance between two distributions, Name Formula Kullback-Leibler(K-L)  <ref type="table" target="#tab_1">Table 1</ref>. For the four distance measures, "↓" indicates "the smaller is the better" and for the two similarity measures, "↑" indicates "the larger the better". LDLogitBoost and AOSO-LDLogitBoost are compared with seven existing LDL methods. For each compared method, several parameter configurations are tested and the best performance is reported by 10-fold cross validation. AOSO-LDLogitBoost uses maximum 5000 steps with v = 0.05, while LDLogitBoost uses maximum 2000 steps with v t = 0.05(when t &gt; 500, we changed v t = 0.01). For the traditional LDL methods, K in AA-KNN is set to 6. Linear kernel is used in PT-SVM. The number of hidden-layer neurons for AA-BP is set to 60. The insensitivity parameter ε is set to 0.1 of LDSVR. At the same time, the maximum steps in BFGS-LDL is 300 and IIS-LDL is 5000. <ref type="table">Table 2</ref> reports the experimental results of LDLogit-Boost, AOSO-LDLogitBoost and other LDL methods. The best performance on each measure is highlighted by boldface. The two-tailed T-tests with 0.05 significance level are performed to see whether the differences are statistically significant. As can be seen, AOSO-LDLogitBoost performs best on all criteria followed by LDLogitBoost. This implies that vector tree model is more competitive than weighted regression tree as well as maximum entropy model on this data set.</p><formula xml:id="formula_26">Dis 1 = C j=1 P j ln Pj Qj Euclidean Dis 2 = C j=1 (P j − Q j ) 2 Sørensen Dis 3 = C j=1 |Pj −Qj | C j=1 (Pj +Qj ) SquaredX 2 Dis 4 = C j=1 (Pj −Qj ) 2 Pj +Qj Fidelity Sim 1 = C j=1 min(P j , Q j ) Intersection Sim 2 = C j=1 P j Q j</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Crowd Opinion Prediction on Movies</head><p>The prediction of crowd opinion on movies is an interesting problem. Thousands of new movies are produced and shown in movie theaters each year, among which some are successful, many are not. On the one hand, the increasing cost and competition boosts the investment risk of the movie producers. On the other hand, the prevalent immodest advertisement and promotion makes it hard for movie audiences to choose a movie worth watching. Therefore, both sides demand a reliable prediction of what people will think about a particular movie before it is actually released or even during its planning phase. <ref type="figure" target="#fig_8">Fig. 4</ref> gives a typical example of such case. The movies T wilight (a) and I, F rankenstein (b) both have the same average rating 5.2/10. But the top two popular ratings in the rating distribution of T wilight are the lowest rating 1 (15.7%) and the highest rating 10 (15.3%), respectively, while those of I, F rankenstein concentrate at the medium ratings 6 (21.4%) and 5 (20.1%). As a result, the budget/gross ratio of T wilight is $37M/$191M and that of I, F rankenstein is $65M/$19M. Obviously, the former movie is more worthy to invest and watch. This example illustrates that the average rating is not a good indicator of the crowd opinion. Therefore, the prediction of the rating distribution is more useful which is not limited to a gross prediction, but includes marketing strategy, advertising design, movie recommendation, etc. Here we use the database named COPM (Crowd Opinion Prediction on Movies) which has 7755 movies and 54,242,292 ratings from 478,656 different users. The ratings come from Netflix, which are on a scale from 1 to 5 integral stars. Each movie has, on average, 6,994 ratings. The rating distribution is calculated for each movie as an indicator for the crowd opinion on that movie. The prerelease metadata is crawled from IMDB according to the unique movie IDs. There are both numeric and categorical attributes in this data set. Finally, all the attributes are normalized to the same scale through the min-max normalization.</p><p>To solve this problem, LDLogitBoost and AOSO- LDLogitBoost are compared with six existing LDL methods. For each compared method, several parameter configurations are tested and the best performance is reported by 10-fold cross validation. AOSO-LDLogitBoost uses maximum 500 steps with v = 0.05, while LDLogitBoost uses maximum 200 steps with v t = 0.3. For the LDL methods, K in AA-KNN is set to 10 and the number of hidden-layer neurons for CPNN is set to 80. The insensitivity parameter ε is set to 1 of LDSVR. The number of hidden-layer neurons for AA-BP is set to 60. At the same time, the maximum steps in BFGS-LLD and IIS-LLD is 50. <ref type="table">Table 3</ref> reports the experimental results of LDLogit-Boost, AOSO-LDLogitBoost and other LDL methods. As can be seen, AOSO-LDLogitBoost performs best on all of other algorithms. But LDLogitBoost has the same performs with LDSVR. The reason might be two-fold. Firstly, LDSVR takes advantage of the large margin regression by a support vector machine. Secondly, the application of the kernel trick makes it possible for LDSVR to solve this problem in a higher-dimensional.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">ChaLearn Age Estimation</head><p>In the age estimation competition organized by ChaLearn <ref type="bibr" target="#b4">[5]</ref>, apparent ages of images are provided. Uncertainty of each apparent age is induced because each image is labeled by multiple individuals. Such uncertainty makes this age estimation task be different from common chronological age estimation tasks. The age of each image in the data set is labeled by multiple individuals rather than its chronological age. For each image, its mean age µ and the standard deviation σ are given. As can be seen from <ref type="figure">Fig.5</ref>, the horizontal axis shows the apparent ages and the vertical axis represents the description degree d y x of each age y assigned with the facial image x. In this data set, totally 3,615 facial images (2,479 in the training data set and 1,136 in the validation data set), with their apparent ages and standard deviations, are provided. There are three steps of preprocessing of images. The facial region of each image is detected by the DPM model described in <ref type="bibr" target="#b17">[18]</ref>. Then the detected face is fed to a public available facial point detector software <ref type="bibr" target="#b20">[21]</ref> to detect five facial key points including the left/right eye centers, nose tip and left/right mouth corners. Finally, based on these facial points, we employed facial alignment for these facial images and resized to 256*256 pixels. The features are extracted by the method of Biologically Inspired Features (BIF) <ref type="bibr" target="#b14">[15]</ref>.  <ref type="figure">Figure 5</ref>. LDL of Apparent Age Estimation</p><p>In this competition, on the one hand, traditional singlelabel method can be used to learn a mapping from the apparent image to the given mean age µ. Thus, we can use traditional LogitBoost <ref type="bibr" target="#b5">[6]</ref> and AOSO-LogitBoost <ref type="bibr" target="#b19">[20]</ref> algorithms. On the other hand, we can use the mean age µ and the standard deviation σ to generate an age distribution assigned with each image. AOSO-LDLogitBoost, LDLogit-Boost and BFGS-LDL can be used to learn a mapping from the apparent age image to its age distribution.</p><p>The performance of the age estimation is evaluated by Mean Absolute Error (MAE) and the formula provided by competition organization (named ǫ-Error), which is:</p><formula xml:id="formula_27">ǫ = 1 − exp(− (t − µ) 2 2σ 2 )<label>(28)</label></formula><p>where t is the predicted age, µ is the mean apparent age and σ is the standard deviation. Each prediction is evaluated using the above formula, getting an error value between 0  <ref type="table">Table 3</ref>. Results on COPM. AOSO-LDLogitBoost is better.</p><p>(correct) and 1 (far from age). Not predicted images are evaluated with 1. <ref type="table">Table 4</ref> reports the experimental results of LDLogit-Boost, AOSO-LDLogitBoost, BFGS-LDL and LogitBoost as well as AOSO-LogitBoost. AOSO-LDLogitBoost and AOSO-LogitBoost use maximum 5000 steps with v = 0.05, while LDLogitBoost uses maximum 800 steps with v t = 0.1. For the LDL method, BFGS uses maximum 200 steps. As can be seen, AOSO-LDLogitBoost and LDLogitBoost perform better than all of other algorithms. It implies LD-LogitBoost and AOSO-LDLogitBoost can make full use of the standard deviation among the labels which can lead to a better performances than traditional LogitBoost and AOSO-LogitBoost algorithms. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusion</head><p>In this paper, we propose a LDL model family which extends the traditional maximum entropy model of LDL as this special case. Logistic Boosting Regression can be used to learn this general model which uses the coordinate descent optimization method over the function space based on the second derivative Hessian matrix. The base (weak) learners can be chosen as weighted regression tree and vector tree, which constitute two algorithms named LD-LogitBoost and AOSO-LDLogitBoost, respectively. On the one hand, experiments on expression recognition and crowd opinion prediction on movies show that LDLogitBoost and AOSO-LDLogitBoost are more effective than traditional LDL algorithms. On the other hand, experiment on apparent age estimation shows that LDLogitBoost and AOSO-LDLogitBoost can lead to a better performances than traditional LogitBoost and AOSO-LogitBoost algorithms by making full use of the prior knowledge such as the standard deviation among the labels.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Three ways to label an instance</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>j</head><label></label><figDesc>(x) on the training instances x i with targets z i,j and weights w i,j .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 .</head><label>2</label><figDesc>{a m , R m } M m=1 , where a m = {a m1 , . . . , a mj , . . . , a mC } is a vector value with the sum of 0 and M is the number of terminal nodes. It updates the values of F(x) all together by first computing a rectangular t1 = (0, t1 ,-t1) vector tree model partition of the feature space {R t m } M m=1 and corresponding node vector value {a t m } M m=1 at the same time, then incrementing F i by a t m where m is the index of the region R t m</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>F j = 0, p i,j = 1 C , j = 0 to C and i = 1 to N 2: for t = 1 to T do3: Obtain {R t,m } M m=1 by recursive region partition. Node split gain is computed as Eq. (27), where the class pair (r,s) is selected using Eq. (24) and Eq. (25). 4: Compute {a t,m } M m=1 by Eq. (21), where the class pair (r,s) is selected using Eq. (24) and Eq. (25).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>(y j |x i ) by Eq. (1) 7: end for</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 3 .</head><label>3</label><figDesc>LDL of Expression Recognition</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 4 .</head><label>4</label><figDesc>LDL of Crowd Opinions on movies T wilight (a) and I, F rankenstein (b)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 .</head><label>1</label><figDesc>Evaluation Measure for LDL Algorithms which are summarized in</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head></head><label></label><figDesc>AOSO-LDLogitBoost .0855±.0037 .1547±.0030 .1521±.0032 .0837±.0034 .9778±.0009 .8478±.0031 LDLogitBoost .0900±.0038 .1585±.0032 .1552±.0031 .0875±.0034 .9767±.0009 .8448±.0031 LDSVR .0918±.0047 .1583±.0035 .1559±.0035 .0884±.0042 .9765±.0011 .8440±.0035 BFGS-LDL .0989±.0040 .1666±.0036 .1639±.0034 .0960±.0037 .9744±.0011 .8360±.0034 AA-KNN .1274±.0069 .1917±.0045 .1899±.0047 .1246±.0062 .9664±.0018 .8101±.0047 AA-BP .1276±.0038 .2026±.0038 .1990±.0039 .1285±.0040 .9653±.0011 .8010±.0039 IIS-LDL .1288±.0070 .1866±.0041 .1828±.0044 .1195±.0054 .9676±.0014 .8172±.0044 CPNN .1826±.0274 .2209±.0148 .2153±.0150 .1625±.0206 .9551±.0061 .7847±.0150</figDesc><table>Method 

K-L ↓ 
Euclidean ↓ 
Sørensen ↓ 
SquaredX 2 ↓ 
Fidelity ↑ 
Intersectioin ↑ 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head></head><label></label><figDesc>Table 4. Results on Charlearn Age Estimation.AOSO-LDLogitBoost is better.</figDesc><table>Method 
MAE ↓ ǫ-Error ↓ 
AOSO-LDLogitBoost 7.2949 
0.5483 
LDLogitBoost 
7.3449 
0.5507 
BFGS-LDL 
7.4243 
0.5518 
AOSO-LogitBoost 
8.0361 
0.5758 
LogitBoost 
9.0458 
0.6044 

</table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Face description with local binary patterns: Application to face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ahonen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hadid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pietikainen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2037" to="2041" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A maximum entropy approach to natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">J D</forename><surname>Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A D</forename><surname>Pietra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="39" to="71" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Classification and regression trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Olshen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1984" />
			<publisher>CRC press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Logistic regression, adaboost and bregman distances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="253" to="285" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">ChaLearn 2015 apparent age and cultural event recognition: Datasets and results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Escalera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fabian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pardo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Baró</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gonzàlez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">J</forename><surname>Escalante</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision, ChaLearn Looking at People workshop</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Additive logistic regression: a statistical view of boosting (with discussion and a rejoinder by the authors). The annals of statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="337" to="407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Label distribution learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Geng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>in press</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Pre-release prediction of crowd opinion on movies by label distribution learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Joint Conference on Artificial Intelligence</title>
		<meeting>the International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3511" to="3517" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Label distribution learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th IEEE International Conference on Data Mining Workshops</title>
		<meeting>the 13th IEEE International Conference on Data Mining Workshops</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="377" to="383" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Facial age estimation by adaptive label distribution learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2234" to="2240" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Head pose estimation based on multivariate label distribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1837" to="1842" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Facial age estimation by learning from label distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2401" to="2412" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Automatic age estimation based on facial aging patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Smith-Miles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2234" to="2240" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning from facial aging patterns for automatic age estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th annual ACM international conference on Multimedia</title>
		<meeting>the 14th annual ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="307" to="316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Human age estimation using bio-inspired features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="112" to="119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Abc-boost: Adaptive base class boost for multi-class classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Annual International Conference on Machine Learning</title>
		<meeting>the 26th Annual International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="625" to="632" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Robust logitboost and adaptive base class (abc) logitboost</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Uncertainty in Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Face detection without bells and whistles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pedersoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="720" to="735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A brief introduction to boosting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Joint Conference on Artificial Intelligence</title>
		<meeting>the International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page" from="1401" to="1406" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">An improved multiclass logitboost using adaptive-one-vs-one</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="295" to="326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep convolutional network cascade for facial point detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3476" to="3483" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep label distribution learning for apparent age estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-B</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-W</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Geng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ChaLearn LAP 2015 workshop at ICCV</title>
		<meeting>ChaLearn LAP 2015 workshop at ICCV</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Crowd counting in public video surveillance by label distribution learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Geng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">166</biblScope>
			<biblScope unit="page">151163</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Emotion distribution recognition from facial expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Geng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM International Conference on Multimedia</title>
		<meeting>the 23rd ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
