<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Trust No One: Low Rank Matrix Factorization Using Hierarchical RANSAC</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Magnus</forename><surname>Oskarsson</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Centre for Mathematical Sciences</orgName>
								<orgName type="institution">Lund University</orgName>
								<address>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><forename type="middle">Batstone</forename><surname>Kalleåström</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Centre for Mathematical Sciences</orgName>
								<orgName type="institution">Lund University</orgName>
								<address>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Trust No One: Low Rank Matrix Factorization Using Hierarchical RANSAC</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:47+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we present a system for performing low rank matrix factorization. Low-rank matrix factorization is an essential problem in many areas, including computer vision with applications in affine structure-from-motion, photometric stereo, and non-rigid structure from motion. We specifically target structured data patterns, with outliers and large amounts of missing data. Using recently developed characterizations of minimal solutions to matrix factorization problems with missing data, we show how these can be used as building blocks in a hierarchical system that performs bootstrapping on all levels. This gives a robust and fast system, with state-of-the-art performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>We will in this paper address the problem of robust estimation of low rank factorizations of matrices with missing data and outliers. Many problems in geometric computer vision can be formulated as such. Two examples are</p><p>• Affine structure-from-motion (SfM), where the observation matrix containing feature tracks can be factorized into the camera motions and the 3D structure.</p><p>• Photometric stereo, where the directions of light sources and the surface normals are separated by factorizing the measurement matrix composed of pixel intensities under a Lambertian model.</p><p>Other applications can be found in <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b36">36]</ref>. These problems often lead to measurement matrices with highly structured data, in terms of which measurements that are available. In this paper we specifically target problems that exhibit such structured data patterns. Without missing data, the optimal solution to low rank matrix factorization, under the L 2 -norm, is given by truncating the singular value decomposition of the measurement matrix, see <ref type="bibr" target="#b9">[10]</ref>. When * This work has been financed by ELLIIT, MAPCI and eSSENCE.</p><p>there are measurements missing in the data, there is no closed form solution to the L 2 -norm minimization problem. The Wiberg algorithm <ref type="bibr" target="#b35">[35]</ref> was the first method to handle missing data. A modified version of the Wiberg algorithm was presented in <ref type="bibr" target="#b28">[29]</ref>. In <ref type="bibr" target="#b4">[5]</ref>, a damped Newton method is proposed to handle the missing data. If there are gross outliers in the data, optimizing the L 2 -norm can give poor results. In <ref type="bibr" target="#b0">[1]</ref>, Aanaes et al. proposed an iteratively re-weighted least squares approach to optimize the objective function for robustness to outliers. Using more robust norms to outliers was considered in <ref type="bibr" target="#b19">[20]</ref>, where algorithms based on alternating optimization under the Hubernorm and the L 1 -norm were introduced. Eriksson and Hengel generalized the Wiberg algorithm in <ref type="bibr" target="#b10">[11]</ref>, to using the L 1 -norm. Sometimes, extra constraints can be posed on the factorization matrices. In <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b37">37]</ref> constraints that the solution should lie on a certain manifold are considered and incorporated in the formulation. Due to the non-convexity of the matrix factorization, most methods mentioned above are based on alternating optimization, and are prone to get trapped in local minima. To address this issue, several works, such as <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b29">30]</ref> re-formulate the problem to minimize the convex surrogate of the rank function -the nuclear norm. This makes it possible to use convex optimization to find the global optimum of the approximated objective function. These approaches can handle the problems when the rank is not known a priori. However, for applications with a given rank, the nuclear norm based methods usually perform inferior to the bilinear formulation-based methods <ref type="bibr" target="#b6">[7]</ref>. The convex formulations often have problems with very high amounts of missing data and outliers. A way of handling highly structured data matrices is to divide the whole matrix into overlapping sub-blocks and combine the sub-block solutions, see <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b15">16]</ref>. Most of these methods do not consider both outliers and missing data at the same time. There are a number of works that target specific computer vision applications for incomplete data. Examples are relative orientation problems, <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b30">31]</ref>, batch structure from motion estimation, <ref type="bibr" target="#b12">[13]</ref>, multi-dimensional scaling, <ref type="bibr" target="#b31">[32]</ref>, and shape estimation, <ref type="bibr" target="#b18">[19]</ref>. It has also been shown that the specific problem of structure from motion with missing data is NP-hard, <ref type="bibr" target="#b27">[28]</ref>. In this paper we largely build upon the work in <ref type="bibr" target="#b15">[16]</ref> where minimal solvers for low rank factorizations of matrices with missing data were introduced. The emphasis was on how to analyze, describe and solve minimal problems. In this paper we address a number of algorithmic challenges (speed, accuracy, avoidance of local minima, robustness to outliers). Our contribution is a system that estimates a low rank factorization of a measurement matrix with, large amounts of missing data, in highly structured data patterns. It is based on bootstrapping minimal solvers, which gives speed and robustness to outliers. Running the solvers in a hierarchical manner gives tractable behaviour for larger input matrices, and easy parallelization. The system makes it possible to add additional constraints on the solution, throughout the pipeline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Problem formulation</head><p>We will look at the problem of finding a low rank matrix approximation to a given matrix X. Any rank K matrix X of size M × N can be written as U V = X, where U is a rank K matrix of size M × K and V is a rank K matrix of size K × N . If X represents measurements of something, that in theory should have rank K, we can not expect this equation to hold exactly due to noise. We could then instead look at the norm of the residual between the measurement matrix X and the model U V , i.e.</p><formula xml:id="formula_0">e = X − U V F .<label>(1)</label></formula><p>In many cases one does not have access to all measurements, i.e. not all entries of X are known. We can then represent which measurements are known by the index matrix W of size M × N , where the entries are either zero, if the corresponding measurement is unknown, or one if the corresponding measurement is known. We can then write the corresponding residual norm as,</p><formula xml:id="formula_1">e = (X − U V ) ⊙ W F ,<label>(2)</label></formula><p>where ⊙ represents element-wise multiplication. In addition to measurement noise, one can also have gross outliers in the data, in some applications. In this case minimizing an error norm based on an L 2 -distance often gives bad results. In order to decrease the influence of the outliers, robust norms are used, such as the L 1 -norm or the Huber norm. In <ref type="bibr" target="#b2">[3]</ref> a more refined loss function is proposed. If we assume that the inlier residuals approximately follow a Gaussian distribution, whereas outlier residuals have approximately uniformly distributed errors, then this leads to the loss function</p><formula xml:id="formula_2">l(r) = − log(c + exp(−r 2 )),<label>(3)</label></formula><p>where r is the residual error. Truncating the squared error</p><formula xml:id="formula_3">l(r) = r 2 if |r| ≤ ǫ ǫ 2 otherwise (4)</formula><p>gives a good approximation. If we denote the residual matrix R = (X −U V )⊙W , with entries r ij , we can formulate our problem as</p><formula xml:id="formula_4">minimize U,V i,j l(r ij ).<label>(5)</label></formula><p>The final error depends on ǫ which we set as a parameter in our algorithm. This is the bound that differentiates an inlier from an outlier measurement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Matrix factorization with missing data</head><p>We will use the characterization of low rank matrix factorization problems that was described in <ref type="bibr" target="#b15">[16]</ref>. For completeness and readability, we will in short describe some of the results from that paper.</p><p>A key question is to study for which index matrices W of type M × N , the problem of calculating the rank K matrix X = U V is minimal and well-defined. For this we introduce the manifold Ω = R M ×K × R K×N of possible solutions z = (U, V ). The solution set Ω X,W ⊂ Ω for a given data matrix X and a index matrix W is defined as</p><formula xml:id="formula_5">Ω X,W = {z = (U, V ) ∈ Ω|W ⊙ (X − U V ) = 0}. (6)</formula><p>Typically if the index matrix W has to many non-zero elements, then there are too many constraints in W ⊙ (X − U V ) = 0 and the solution set Ω X,W is empty for general X. If there is a solution X = U V , then we are interested to known if the solution is unique up to the so called Gauge</p><formula xml:id="formula_6">freedom z = (U, V ) vs z = (U H, H −1 V ), where H is a general invertible K × K matrix.</formula><p>Assume that input data matrix X = U 0 V 0 of size M × N has been generated by multiplying matrices U 0 of size K × M and V 0 of size K × N . Assume also that both of these matrices are general in the sense that all K × K submatrices of both U 0 and V 0 have rank K. Furthermore assume that M ≥ K and N ≥ K. An index matrix W is said to be rigid, if the solution set Ω X,W locally around the point z 0 = (U 0 , V 0 ) only consists of the set z(H) =</p><formula xml:id="formula_7">{(U 0 H, H −1 V 0 )|H invertible K × K matrix}.</formula><p>Since we are assuming that every sub-minor of U 0 and V 0 has full rank, one may actually fix the Gauge freedom by keeping one such sub-minor fixed. For example we could study the solutions for the points z = (U, V ) such that the first K rows of U are equal to those of U 0 .</p><p>For two index matrices W 1 and W 2 of the same size we say that W 1 ≤ W 2 if the inequality holds for every element. We say that W 1 &lt; W 2 if W 1 ≤ W 2 and W 1 = W 2 . It is trivial to see that if W is rigid and if W ≤ W ′ then W ′ is also rigid. It also can be shown that if W ′ is rigid and overdetermined, then there is at least one W &lt; W ′ that is rigid and minimal. We say that an index matrix W is minimal if it is rigid and satisfies ij W (i, j) = M K + N K − K 2 . For a minimal index matrix W and for general data X the solution set Ω X,W consists of a finite number of points n W up to the gauge freedom.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Henneberg extensions</head><p>We will now describe how to generate the minimal problems. The inspiration comes from rigid graph theory, where the Henneberg construction is used to generate the Laman graph, see <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b13">14]</ref>. The idea is that one starts with the smallest minimal index matrix, and by a series of extensions generate every minimal index matrix. For the rank K problem the smallest index matrix is a matrix of size K ×K consisting of ones only.</p><p>There are both constructive extensions and nonconstructive extensions. For a constructive extension from W to W ′ , one can infer the number of solutions n W ′ from n W and construct the solver, denoted by f W ′ from f W . For non-constructive extensions, it can be shown that W is minimal if and only if W ′ is minimal. However, we can in general neither infer the number of solutions n W ′ from n W nor derive a solver f W ′ from f W . Certain of these constructive extensions are particularly fast and efficient. The simplest one is as follows.</p><p>Given a minimal index matrix W for a rank-K problem of size M × N , an extended minimal index matrix W ′ of size M × (N + 1) is formed by adding a column with exactly K elements set to one. The number of solutions are identical, i.e. n W = n W ′ . Extending an algorithm from f W to f W ′ is straightforward. A similar extension can be done by adding a row with K indices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Henneberg reductions</head><p>There is also a simple recursive method to check if an index matrix W can be generated using only <ref type="bibr">Henneberg 1</ref> extensions. The procedure is as follows. Start with an index matrix of size M × N . If M = N = K then the index matrix is minimal if and only if the matrix consists only of ones. If M or N is larger than K, we calculate the minimal number of ones for a row or column. If this number is less than K, then it can be shown that the index set in question is non-rigid. If this number is larger than K it can be shown that the index set (if minimal) cannot be generated by Henneberg 1 extensions only. Finally if the number is K, then we can remove the row (or column) with exactly K ones and study this index matrix, which now is of smaller size.</p><p>The algorithm terminates after at most M + N − 2K steps. After running the algorithm we determine if the index set is minimal and can be constructed by a series of Henneberg 1 extensions. But we also obtain the pattern of extensions. Thus we obtain an efficient method of calculating the unique solution (U, V ) from a data matrix X so that W ⊙ (X − U V ) = 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Glues</head><p>Assume that the solutions to two sub-problem {W 1 , X 1 } and {W 2 , X 2 } are given by {U 1 , V 1 } and {U 2 , V 2 } respectively. To construct the solution to {W, X}, the idea is to find a transformation matrix H ∈ R K×K to transform the subspace U 2 to the same coordinate framework as the subspace U 1 . Using this transformation we have</p><formula xml:id="formula_8">U 2 V 2 = (U 2 H)(H −1 V 2 ).<label>(7)</label></formula><p>Now U 2 H and H −1 V 2 are in the same coordinate framework as U 1 and V 1 respectively. The remaining problem is to solve for H. We have the following constraint, that states that U 1 and U 2 H should coincide for the overlapping rows as</p><formula xml:id="formula_9">U 1 (I 12 , :) = U 2 (I 12 , :)H,<label>(8)</label></formula><p>where I 1 and I 2 denotes the indices of overlapping rows in U 1 and U 2 respectively and U (I, :) denotes the sub-matrix of U by taking the rows given by I. Similarly we have the overlapping constraints for V 1 and H −1 V 2 as</p><formula xml:id="formula_10">HV 1 (:, J 12 ) = V 2 (:, J 12 ),<label>(9)</label></formula><p>where J 1 and J 2 denotes the indices of overlapping columns in V 1 and V 2 respectively. If we have enough constraints from <ref type="formula" target="#formula_9">(8)</ref> and <ref type="formula" target="#formula_10">(9)</ref>, H can be solved linearly. Two examples are if there are at least K overlapping rows or K overlapping columns. For the cases where the overlap doesn't give sufficiently many constraints, we need some extra constraint outside W 1 and W 2 to solve for the transformation matrix H.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Building blocks</head><p>In this section we will describe the basic components that are used in our matrix factorization method. Our full system will be described Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Initializing solutions</head><p>We use RANSAC to find small initial solutions. We choose a sub-problem to problem 5, by choosing a submatrix W i of W . We further randomly select a minimal number of measurements, represented by W m &lt; W i . These index-matrices have corresponding measurement matrices X i and X m . Even though we have chosen a minimal subset of measurements, this need not represent a well posed minimal problem. In order to check this, we perform Henneberg reductions as described in Section 3.2. If W m indeed represents a minimal problem, we can, if we have a solver for this case, solve the corresponding matrix factorization of X m . This gives a minimal solution (U m , V m ). We can now look at how well this solution matches the other measurements in W i by looking at the residuals (U m V m −X i )⊙W i . Repeating this process gives a set of initial good solutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Extending solutions</head><p>If we have a solution, represented by (U i , V i ) we can minimally extend this solution row-or columnwise using Henneberg-1 extensions, for every column (or row) that has at least K measurements. For every such column a, we randomly select K rows that are represented in the corresponding index sub-matrix W i , and use the Henneberg-1 extension to find the new column v a so thatV</p><formula xml:id="formula_11">i = V i v a .</formula><p>To handle outliers we check how many of the measurements that fit this newŪ i . If we have a substantial enough number of inliers we keep this solution, otherwise we repeat the process a number of times.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Glueing solutions</head><p>If we have two solutions, represented by (U i , V i ) and (U j , V j ) we can, depending on the overlap of the two solutions, glue these solutions into one using the methods described in Section 3. Basically this can be done if there is enough information to estimate the K × K transformation matrix H so that U i H and H −1 V i are given in the same coordinate frame as U j and V j . Using a randomly selected minimal set of measurements to estimate H gives a new solution (U k , V k ) where U k is the union of U i H and U j , and V k is the union of H −1 V i and V j . To handle outliers we check how many inliers we get for this new solution. Again, if we have a substantial enough number of inliers we keep this solution, otherwise we repeat the process a number of times.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Refining solutions</head><p>Since we use minimal solvers, and extend these solutions iteratively, we need to refine our solutions in order to avoid error propagation. We non-linearly refine our solutions, by minimizing (5) iteratively using Gauss Newton descent. We handle the truncation, at each step, by only optimizing over the inlier set, and then updating the inlier set using the new estimate of U and V . Since the error on the inlier set is quadratic in U and V the derivatives with respect to U and V are easily obtained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Sampling scheme</head><p>Using the building blocks from the previous section, we can now describe our full sampling scheme. The basic idea behind our method is that we will have several solutions competing against each other. These solutions will expand and merge, but at all steps we will try to be robust against outliers and inlier errors, so that we do not propagate errors. We do this by random sampling at all instances.</p><p>We assume that we have four functions available: INIT, EXTEND, GLUE and REFINE, that do initialization, extensions, glues and non-linear refinement respectively as described in Sections 4.1-4.4. We start by initializing a number of seed solutions. For each solution i, we have U i and V i . We then, for each of these seed solutions, attempt to extend it and refine it. If two solutions overlap, we try to glue them together. We repeat this procedure until we have at least one solution that covers the whole data matrix X. Sometimes the errors of a solution will grow during this process, and we remove solutions that have a residual norm larger than some fixed threshold. This means that we could end up with an empty solution set. In this case we re-initialize a number of seed solutions. The steps of our procedure are summarized in Algorithm 1. There S i = EXTEND S i (row-wise) <ref type="bibr" target="#b7">8</ref>: end if 20: end while is of course no guarantee that Algorithm 1 will converge, but given a well posed problem and relevant parameter settings, it is our experience that it will. We terminate after a fixed number of iterations or when we have at least one solution that covers the whole measurement matrix. We will in the experimental section validate this, on both synthetic and real data. There are a number of parameters that need to be set in order for the algorithm to work properly. The most important one is the error bound, i.e. the reprojection error that differentiates between an inlier and an outlier. In order to increase the robustness, we have introduced an absolute threshold on the number of inliers for the EXTEND, and GLUE functions. For each row and column the num- ber of inliers should exceed this threshold. We have used the rank plus a small integer as threshold. We also need to set the number of RANSAC iterations for each of these functions, but this will mainly affect the total running time. If we assume structured data, it makes sense to smooth the index matrix W with a two-dimensional Gaussian, to obtain W sm . We then sample initialization matrices guided by W sm as a probability measure. Sampled positions are removed from W sm to avoid multiple initial points. In <ref type="bibr" target="#b15">[16]</ref> they do a manual block subdivision (with relatively large blocks e.g. 50 × 50). This means that the model that is fitted is very large (e.g. 50K + 50K − K 2 parameters for a rank K problem) algorithm. In our approach there is no need for any explicit block subdivision, and the initial models that are fitted are much smaller. This gives a much more tractable and robust algorithm.</p><formula xml:id="formula_12">S i = REFINE S i 9: if (U i V i − X i ) ⊙ W i F &gt; C<label>then</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experiments</head><p>We have applied our method to a number of different matrix factorization problems, in order to show the usefulness of it in terms of robustness, accuracy and speed.</p><p>We have compared our results to a number of state-ofthe-art methods for low rank matrix factorization, namely the method of Larsson et al., <ref type="bibr" target="#b23">[24]</ref>, the method of Jiang et al., <ref type="bibr" target="#b15">[16]</ref>, the Truncated Nuclear Norm Regularization (TNNR-ADMM) <ref type="bibr" target="#b14">[15]</ref>, OptSpace <ref type="bibr" target="#b20">[21]</ref>, the damped Wiberg algorithm using the implementation of Okatani et al. <ref type="bibr" target="#b28">[29]</ref>, and the L 1 Wiberg algorithm, <ref type="bibr" target="#b10">[11]</ref>, using the C++ implementation from <ref type="bibr" target="#b32">[33]</ref>. In the results we have included the relevant comparisons, in order to make the tables and graphs more readable. It has been previously reported (in <ref type="bibr" target="#b15">[16]</ref>) that the methods of <ref type="bibr" target="#b14">[15]</ref> and <ref type="bibr" target="#b20">[21]</ref> perform much worse for structured data patterns with high amounts of missing data. This is also our experience end hence we have omitted these results. Most of the methods do not handle outliers, and in the absence of outliers the Wiberg algorithm gives best accuracy. For this reason we mainly focus on comparison with the L 1 -and L 2 -versions of the Wiberg algorithm in the synthetic experiments in Section 6.3. All tests were conducted on a desktop computer running Ubuntu, with an Intel Core i7 3.6 GHz processor. The Matlab implementation of our method is available at https://github.com/ hamburgerlady/miss-ranko.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Affine structure from motion</head><p>Given a full data matrix and an affine camera model it is possible to solve the structure from motion problem using factorization. The standard way of doing this is by first removing the centroid of each point in the images. This leads to a rank 3 factorization problem. When there is missing data, we can neither use SVD for factorization nor remove the centroid since this is not known. We can still write the problem as a rank 4 matrix factorization problem with missing data, see e.g. <ref type="bibr" target="#b34">[34]</ref>. An affine camera is of the form</p><formula xml:id="formula_13">P i =   A i B i 0 0 0 1   .<label>(10)</label></formula><p>If we collect the N (homogeneous) 3D points in the 4 × N matrix V, one usually writes the rank 4 problem as U V = X, where U is the 2M × 4 matrix containing the stacked camera rows A i and B i , and the 2M ×N matrix X contains the x and y coordinates of image points. However, solving this rank 4 matrix factorization problem ignores the fact that the last row of the camera matrices should be equal to [0 0 0 1], and that the last coordinate of each homogeneous point should be equal to one. In our model we include this constraint by simply adding a row in the measurement matrix with just ones. We have found this to be a very powerful constraint, since we know it should hold even for missing data, and we use it throughout our pipeline. If the constraint is fulfilled, we can simply upgrade to the affine model by U a = U H so that the last row of U a is equal to [0 0 0 1].</p><p>We have run Algorithm 1 on the well known Dinosaur sequence. This is a sequence that contains very little outliers, but a large amount of missing data (88%). Even though the underlying structure from motion can be (and has been) solved using a multitude of methods, the Dinosaur sequence works well as a benchmark problem for matrix factorization. We have compared our results with to those of Larsson et al., <ref type="bibr" target="#b23">[24]</ref>. In their work they also did experiments on using the nuclear norm and we have included these results here. In Jiang et al., <ref type="bibr" target="#b15">[16]</ref> it was reported that the Truncated Nuclear Norm Regularization (TNNR-ADMM) <ref type="bibr">[</ref>  and OptSpace <ref type="bibr" target="#b20">[21]</ref> failed to recover the 2D tracks from the Dinosaur sequence. This is also our experience, as we consistently failed to recover a reasonable solution using these methods. In addition we have also run the damped Wiberg algorithm using the implementation of Okatani et al. <ref type="bibr" target="#b28">[29]</ref>. In <ref type="table" target="#tab_1">Table 1</ref> the Frobenius norm of the final factorizations are shown. The average running time for the Wiberg algorithm was 144 seconds, compared to around 3 seconds for our method. We have also run our method on a larger point set. For this set we didn't have access to results from <ref type="bibr" target="#b23">[24]</ref>. We have run our method multiple times, and we always end up in the same optimum. Here the running time for the Wiberg algorithm was 4087 seconds, compared to 5.4 seconds for our method. A histogram of the residuals (in pixels) from our reconstruction is shown in <ref type="figure" target="#fig_3">Figure 2</ref>. Using the calibration of the projective camera model, we can upgrade our affine reconstruction to an orthographic. The resulting calibrated affine reconstruction is shown to the right in <ref type="figure" target="#fig_4">Figure 3</ref>. In a second experiment, we recorded a sequence of a statue of Carl Linnaeus. We extracted Harris corner points, and tracked these using the Kanade-Lucas-Tomasi (KLT) tracker <ref type="bibr" target="#b25">[26]</ref>. This resulted in a sequence with 86 images   <ref type="table" target="#tab_1">Table 1</ref>. The running time for the full dataset was 39.8 s. The extracted points (orange circles) and the reprojected points (white dots) can be seen in <ref type="figure" target="#fig_5">Figure 4</ref>. The sequence constains approximately 1% outliers. As for the Dinosaur sequence, the Truncated Nuclear Norm Regularization (TNNR-ADMM) <ref type="bibr" target="#b14">[15]</ref> and OptSpace <ref type="bibr" target="#b20">[21]</ref> failed to recover reasonable 2D tracks from the Linnaeus sequence. We were not able to run the damped Wiberg algorithm on the full dataset, but the results for a subset of around half the points can be seen in </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Linear shape basis estimation</head><p>If we have non-rigid structures in a scene, a linear shape basis can be used to model the deformations. The underlying assumption is that the non-rigid deformation of the object can be represented as a linear combination of a set of shapes. Typically the size of the shape basis is much smaller than either the number of frames, or the tracked points, so the measurement matrix containing the point tracks can be factorized into a coefficient matrix and a shape basis matrix.</p><p>For our experiments we used the datasets from <ref type="bibr" target="#b23">[24]</ref>, Book and Hand. In these experiments the image points are tracked using a standard Kanade-Lucas-Tomasi (KLT) tracker <ref type="bibr" target="#b25">[26]</ref>. Due to occlusions, the tracker fails after a number of frames for a subset of points, which leads to missing data. To compare with the results in <ref type="bibr" target="#b15">[16]</ref> we use the same setup as they do, using a subset of 42 frames with 60 tracked points from the Book and 38 frames with 203 points from the Hand dataset. We then find rank-3 and rank-5 factorizations of the two datasets respectively. We ran our algorithm, and also the Wiberg algorithm using the implementation in <ref type="bibr" target="#b28">[29]</ref>. The results can be seen in Table 2. The Wiberg algorithm was initialized randomly. Our method and the Wiberg minimization achieve the same optima, which are slightly better than the other methods. The reason is probably that the Wiberg and our method finds the same optimum-that we believe is the global one -since the set is practically outlier free. The other methods do not in this case find an equally good optimum. For these smaller problems the Wiberg algorithm works well, but for larger problems it becomes intractable in terms of running time, as described in Section 6.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Performance tests</head><p>We have conducted a number of synthetic tests to test the performance of our method. The basic setup was done by randomly drawing two matrices U M ×K and V K×N . The product X 0 = U V was then perturbed with Gaussian noise, i.e. X = X 0 + ǫ ij with ǫ ij ∈ N (0, σ). A band diagonal matrix W M ×N was used to prescribe which measurements were available. The bandwidth b for the matrix W (with entries w ij ) is defined using w ij = 0 for j &lt; i−b or j &gt; i+b. Finally, a certain percentage of the seen measurements were replaced with entries drawn randomly from a uniform distribution, to simulate gross outliers. In a first experiment we tested the sensitivity to the proportion of outliers in the measurement matrix. We used a 300 × 300 measurement matrix X, with bandwidth 20. The entries were approximately between minus one and plus one, with Gaussian noise with standard deviation 1e − 3. A certain percentage of the measurements were then replaced with gross outliers. We ran Algorithm 1 and compared the results with the L 1 -Wiberg algorithm, <ref type="bibr" target="#b10">[11]</ref>, using the C++ implementation from <ref type="bibr" target="#b32">[33]</ref>. The results can be seen to the left, in <ref type="figure">Figure 5</ref>. We have also constructed an oracle ground truth solution to compare the results. This solution was attained by running the non-linear refinement, using the ground truth inlier set, and the ground truth U and V as starting solution. This will in general give a better optimum than U and V . One can see that both the tested algorithms perform on par. The average running times are depicted in <ref type="figure">Figure 5</ref>. The middle graph shows both algorithms in the same plot, and the right hand plot shows a magnification of the running times for the proposed algorithm. The L 1 -Wiberg algorithm has very unattractive time complexity in terms of the size of the input measurement matrix, and we failed to run it for larger sizes than 200 × 200. Our method works well for moderate amounts of outliers, but as the outlier percentage increases, the RANSAC initialization will take longer and longer time, and for this test the break-down point for our method was around 20%. In a second experiment we used a similar setup, but instead of varying the outlier rate, we varied the size of the input measurement matrix X. Here we used a fixed outlier ratio of 5%. In <ref type="figure">Figure 6</ref> the results can be seen. We show two versions of our algorithm, with and without using the GLUE step. The left of the figure shows the truncated L 2 -error for the two versions, compared with the oracle ground truth solution, obtained in the same way as in the previous experiment. The right shows the average running times, as a function of the number of rows (equal to the number of columns) in the measurement matrix. One can see that for smaller problems, there is no need for the GLUE. For larger problems using the GLUE leads to (in this case) near linear time complexity. As can be seen from the error plot, using the GLUE method doesn't lead to any error accumulation. We have also investigated our algorithm's dependence on the bandwidth of the measurement matrix. Using a similar setup as in the previous experiments, we constructed measurement matrices with varying bandwidth. We did not include any outliers in the data, and compared our results with the damped Wiberg algorithm. The results can be seen in <ref type="figure">Figure 7</ref>. Both algorithms give final norms very close to the oracle solution, but for smaller bandwidths the Wiberg algorithm has worse convergence. For very small bandwidths our algorithm becomes unstable, and the RANSAC loops will take excessive amounts of time. In this case, the breakdown point for our algorithm was for a bandwidth around 5. This will depend on the rank Outlier percentage s Proposed <ref type="figure">Figure 5</ref>. Results from the outlier test, as described in Section 6.3. Left: The graph shows the mean L 1 error for the proposed method compared to the L 1 Wiberg algorithm, <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b32">33]</ref> and the oracle ground truth solution, as functions of the outlier rate. Middle: The average running times, as functions of the outlier rate, for the proposed method compared to the L 1 Wiberg algorithm, <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b32">33]</ref>. Right: A magnification of the timing results for the proposed method. of the solution, which in this case was 4. Algorithm 1 is highly parallelizable. We have conducted a simple timing experiments using our Matlab implementation. We simply change the for-loop to Matlabs parforloop. This runs the extensions and refinement of the initial solutions in parallel, but not the initialization. The initialization could of course also be run in parallel, but for  <ref type="table">Table 3</ref>. Timings for the linear shape basis estimation using the Book and Hand dataset. The results are based on running parfor in Matlab with different number of cores on an Intel Core i7 3.6 GHz processor.</p><p>most of our conducted experiments the initialization takes a smaller fraction of the total running time. The average running times, using different number of parallel cores, are shown in <ref type="table">Table 3</ref>. We get slightly less than linear speed-ups.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>We have in this paper presented a method for doing robust low rank matrix factorization for structured data patterns. It can handle very large amounts of missing data, and moderate amounts of outliers. It gives results on par, or better than, using the L 1 -Wiberg algorithm or the damped L 2 -Wiberg algorithm, with substantial speed-up. The presented method is also trivially parallelizable. Future work includes investigating two interesting properties of our method, that we have not exploited in any detail in this paper. Firstly we have the ability to solve for multiple models in the data. For instance, if we have two rigid motions in a structure from motion sequence, we could -at least in theory -run our algorithm and find the two solutions corresponding to the two motions. Another property, that easily can be incorporated in our framework, is the ability to add additional constraints on the solution space, e.g. for the affine structure from motion setting we can constrain the pair-wise rows of U to be orthogonal to model a scaled orthographic camera.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Algorithm 1</head><label>1</label><figDesc>Matrix factorization sampling scheme 1: Given an M × N data matrix X with index matrix W , 2: initialize a solution set S = INIT, 3: (S = {S 1 , S 2 , . . . , S n }, Si = (U i , V i )). 4: while no S i is of size M × N do</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 .</head><label>1</label><figDesc>Results from running Algorithm 1 on the Dinosaur experiment (See Section 6.1 for details). Measured data points in blue, and the extent of the solutions are depicted as green boxes. The figure shows from left to right how the solution set evolves.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 .</head><label>2</label><figDesc>Left: Histogram of the final (non-truncated) residuals from the Dinosaur experiment using our proposed method. The results are from the larger dataset, with approximately 2000 3D points. Right: Histogram of the final (non-truncated) residuals from the Linnaeus experiment using our proposed method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 .</head><label>3</label><figDesc>Results from the Dinosaur experiment, with approximately 2000 points. Left: Measured tracks (green) and reconstructed tracks (blue). Right: The calibrated affine reconstruction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>The figure shows three frames of the Linnaeus sequence. Also shown are the tracked Harris points (orange circles) and the reprojected points (white dots) using the proposed method. For visibility a random subset of 250 points are shown and a total of 3977 3D points. This sequence also contains closed loops, i.e. points were tracked from the last frames to the first. Three frames of the sequence can be seen inFigure 4. The results of running our algorithm on this sequence can be seen in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 .Figure 7 .</head><label>67</label><figDesc>Timing results from the size test, as described in Section 6.3. The left graph shows the truncated L 2 -error, as functions of the image size, for the proposed method (with and without using GLUE) compared to the oracle ground truth optimum. The right graph shows the average running times for the proposed method, with and without using GLUE. Timing results from the missing data test, as described in Section 6.3. The left graph shows the L 2 -error, as functions of the bandwidth of the data, for the proposed method compared to the damped Wiberg algorithm and oracle ground truth optimum. The right graph shows the average running times for the proposed method, compared to the damped Wiberg algorithm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 .</head><label>1</label><figDesc></figDesc><table>The running time for our 
</table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Robust factorization. Trans. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Aanaes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fisker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Åström</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carstensen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Photometric stereo with general, unknown lighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Basri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kemelmacher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="239" to="257" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Visual Reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987" />
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Recovering non-rigid 3d shape from image streams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hertzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Biermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conf. Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Damped newton algorithms for matrix factorization with missing data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Buchanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Fitzgibbon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR&apos;05)</title>
		<meeting>the 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR&apos;05)<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">02</biblScope>
			<biblScope unit="page" from="316" to="322" />
		</imprint>
	</monogr>
	<note>CVPR &apos;05</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Bilinear modeling via augmented lagrange multipliers (balm)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Bue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M F</forename><surname>Xavier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>De Agapito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paladini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1496" to="1508" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Unifying nuclear norm and bilinear factorization approaches for low-rank matrix decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cabral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">D</forename><surname>La Torre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Costeira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bernardino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Computer Vision</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Robust principal component analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Candès</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of ACM</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Exact matrix completion via convex optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Candès</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
		<idno>abs/0805.4471</idno>
		<imprint>
			<date type="published" when="2008" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The approximation of one matrix by another of lower rank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Eckart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="218" />
			<date type="published" when="1936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Efficient computation of robust low-rank matrix approximations in the presence of missing data using the l 1 norm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Eriksson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="771" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Dense variational reconstruction of non-rigid surfaces from monocular video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roussos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Agapito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2013 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1272" to="1279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Batch recovery of multiple views with missing data using direct sparse solvers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Guilbert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Die graphische Statik der starren Systeme</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Henneberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BG Teubner</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="1911" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Fast and accurate matrix completion via truncated nuclear norm regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">On the minimal problems of low-rank matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oskarsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Åström</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conf. Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">An iterative multiresolution scheme for sfm with missing data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Julià</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Sappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lumbreras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Serrat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>López</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematical Imaging and Vision</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="240" to="258" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Projective reconstruction from minimal missing data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Heyden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Quan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="418" to="424" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Mdl patch correspondences on unlabeled images with occlusions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Karlsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Åström</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Robust L1 norm factorization in the presence of outliers and missing data by alternative convex programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conf. Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Matrix completion from a few entries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">H</forename><surname>Keshavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Montanari</surname></persName>
		</author>
		<idno>abs/0901.3150</idno>
		<imprint>
			<date type="published" when="2009" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">On graphs and rigidity of plane skeletal structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Laman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Engineering mathematics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="331" to="340" />
			<date type="published" when="1970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Convex envelopes for low rank approximation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Larsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Olsson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Rank minimization with structured data patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Larsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Olsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Bylow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Kahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">The augmented lagrange multiplier method for exact recovery of a corrupted low-rank matrices. CoRR, abs/1009</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">5055</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">An iterative image registration technique with an application to stereo vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">D</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th International Joint Conference on Artificial Intelligence</title>
		<meeting>the 7th International Joint Conference on Artificial Intelligence<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="1981" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Divide-andconquer matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">W</forename><surname>Mackey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Talwalkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1134" to="1142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Structure from motion with missing data is N P -hard</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Nistér</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Stewénius</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Computer Vision</title>
		<meeting><address><addrLine>Rio de Janeiro, Brazil</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Efficient algorithm for low-rank matrix factorization with missing components and performance comparison of latest algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Okatani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yoshida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Deguchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Computer Vision</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A convex approach to low rank matrix approximation with missing data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Olsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oskarsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Scandinavian Conf. on Image Analysis</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Classifying and solving minimal structure and motion problems with missing data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oskarsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Åström</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">C</forename><surname>Overgaard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision, 2001. ICCV 2001. Proceedings. Eighth IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2001" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="628" to="634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Prime rigid graphs and multidimensional scaling with missing data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oskarsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Åström</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torstensson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition (ICPR), 2014 22nd International Conference on</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="750" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">General and nested wiberg minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Strelow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<biblScope unit="page">2012</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<title level="m">IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1584" to="1591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Robust rank-4 affine factorization for structure from motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Zelek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bajcsy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Applications of Computer Vision (WACV), 2013 IEEE Workshop on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="180" to="185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Computation of principal components when data are missing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wiberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Second Symp</title>
		<meeting>Second Symp</meeting>
		<imprint>
			<date type="published" when="1976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">A factorization-based approach for articulated nonrigid shape, motion and kinematic chain recovery from video. Trans. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Practical low-rank matrix approximation under robust L1-norm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sugimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Okutomi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conf. Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
