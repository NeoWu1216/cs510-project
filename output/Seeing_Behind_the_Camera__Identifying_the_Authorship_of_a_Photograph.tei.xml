<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Seeing Behind the Camera: Identifying the Authorship of a Photograph</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Thomas</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Pittsburgh</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Kovashka</surname></persName>
							<email>kovashka@cs.pitt.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Pittsburgh</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Seeing Behind the Camera: Identifying the Authorship of a Photograph</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:45+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce the novel problem of identifying the photographer behind a photograph. To explore the feasibility of current computer vision techniques to address this problem, we created a new dataset of over 180,000 images taken by 41 well-known photographers. Using this dataset, we examined the effectiveness of a variety of features (low and high-level, including CNN features) at identifying the photographer. We also trained a new deep convolutional neural network for this task. Our results show that high-level features greatly outperform low-level features. We provide qualitative results using these learned models that give insight into our method's ability to distinguish between photographers, and allow us to draw interesting conclusions about what specific photographers shoot. We also demonstrate two applications of our method.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>"Motif Number 1", a simple red fishing shack on the river, is considered the most frequently painted building in America. Despite its simplicity, artists' renderings of it vary wildly from minimalistic paintings of the building focusing on the sunset behind it to more abstract portrayals of its reflection in the water. This example demonstrates the great creative license artists have in their trade, resulting in each artist producing works of art reflective of their personal style. Though the differences may be more subtle, even artists practicing within the same movement will produce distinct works, owing to different brush strokes, choice of focus and objects portrayed, use of color, portrayal of space, and other features emblematic of the individual artist. While predicting authorship in paintings and classifying painterly style are challenging problems, there have been attempts in computer vision to automate these tasks <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b4">5]</ref>.</p><p>While researchers have made progress towards matchings the human ability to categorize paintings by style and authorship <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b1">2]</ref>, no attempts have been made to rec- <ref type="bibr">(a)</ref> (b) (c) <ref type="figure">Figure 1</ref>: Three sample photographs from our dataset taken by Hine, Lange, and Wolcott, respectively. Our topperforming feature is able to correctly determine the author of all three photographs, despite the very similar content and appearance of the photos.</p><p>ognize the authorship of photographs. This is surprising because the average person is exposed to many more photographs daily than to paintings. Consider again the situation posed in the first paragraph, in which multiple artists are about to depict the same scene. However this time instead of painters, imagine that the artists are photographers. In this case, the stylistic differences previously discussed are not immediately apparent. The stylistic cues (such as brush stroke) available for identifying a particular artist are greatly reduced in the photographic domain due to the lessened authorial control in that medium (we do not consider photomontaged or edited images in this study). This makes the problem of identifying the author of a photograph significantly more challenging than that of identifying the author of a painting. <ref type="figure">Fig. 1</ref> shows photographs taken by Lewis Hine, Dorothea Lange, and Marion Wolcott, three iconic American photographers. 1 All three images depict child poverty and there are no obvious differences in style, yet our method is able to correctly predict the author of each.</p><p>The ability to accurately extract stylistic and authorship information from artwork computationally enables a wide array of useful applications in the age of massive online image databases. For example, a user who wants to retrieve more work from a given photographer, but does not know his/her name, can speed up the process by querying with a sample photo and using "Search by artist" functionality that first recognizes the artist. Automatic photographer identification can be used to detect unlawful appropriation of others' photographic work, e.g. in online portfolios, and could be applied in resolution of intellectual property disputes. It can also be employed to analyze relations between photographers and discover "schools of thought" among them. The latter can be used in attributing historical photographs with missing author information. Finally, understanding a photographer's style might enable the creation of novel photographs in the spirit of a known author.</p><p>This paper makes several important contributions: 1) we propose the problem of photographer identification, which no existing work has explored; 2) due to the lack of a relevant dataset for this problem, we create a large and diverse dataset which tags each image with its photographer (and possibly other metadata); 3) we investigate a large number of pre-existing and novel visual features and their performance in a comparative experiment in addition to human baselines obtained from a small study; 4) we provide numerous qualitative examples and visualizations to illustrate: the features tested, successes and failures of the method, and interesting inferences that can be drawn from the learned models; 5) we apply our method to discover schools of thought between the authors in our dataset; and 6) we show preliminary results on generating novel images that look like a given photographer's work. <ref type="bibr" target="#b1">2</ref> The remainder of this paper is structured as follows. Section 2 presents other research relevant to this problem and delineates how this paper differs from existing work. Section 3 describes the dataset we have assembled for this project. Section 4 explains all of the features tested and how they were learned, if applicable. Section 5 contains our quantitative evaluation of the different features and an analysis of the results. Section 6 provides qualitative examples, as well as two applications of our method. Section 7 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>The task of automatically determining the author of a particular work of art has always been of interest to art historians whose job it is to identify and authenticate newly discovered works of art. The problem has been studied by vision researchers, who attempted to identify Vincent van Gogh forgeries, and to identify distinguishing features of painters <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b9">10]</ref>. While the early application of art analysis was for detecting forgeries, more recent research has studied how to categorize paintings by school (e.g., "Impressionism" vs "Secession") <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b6">7]</ref>. <ref type="bibr" target="#b31">[32]</ref> explored a variety of features and metric learning approaches for computing the similarity between paintings and styles. Features based on visual appearance and image transformations have found some success in distinguishing more conspicuous painter and style differences in <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b20">21]</ref>, all of which explored low level-image features on simple datasets. Recent research has suggested that when coupled with object detection features, the inclusion of low-level features can yield state-of-the-art performance <ref type="bibr" target="#b4">[5]</ref>. <ref type="bibr" target="#b1">[2]</ref> used the Classeme <ref type="bibr" target="#b33">[34]</ref> descriptor as their semantic feature representation. While it is not obvious that the object detections captured by Classemes would distinguish painting styles, Classemes outperformed all of the low-level features. This indicates that the objects appearing in a painting are also a useful predictor of style.</p><p>Our work also considers authorship identification, but the change of domain from painting to photography poses novel challenges that demand a different solution than that which was applied for painter identification. The distinguishing features of painter styles (paint type, smooth or hard brush, etc.) are inapplicable to the photography domain. Because the photographer lacks the imaginative canvas of the painter, variations in photographic style are much more subtle. Complicating matters further, many of the photographers in our dataset are from roughly the same time period, some even working for the same government agencies with the same stated job purpose. Thus, photographs taken by the subjects tend to be very similar in appearance and content, making distinguishing them particularly challenging, even for humans.</p><p>There has been work in computer vision that studies aesthetics in photography <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b10">11]</ref>. Some work also studies style in architecture <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b22">23]</ref>, vehicles <ref type="bibr" target="#b23">[24]</ref>, or yearbook phootgraphs <ref type="bibr" target="#b14">[15]</ref>. However, all of these differ from our goal of identifying authorship in photography. Most related to our work is the study of visual style in photographs, conducted by <ref type="bibr" target="#b19">[20]</ref>. Karayev et al. conducted a broad study on both paintings and photographs. The 20 style classes and 25 art genres considered in their study are coarse (HDR, Noir, Minimal, Long Exposure, etc.) and much easier to distinguish than the photographs in our dataset, many of which are of the same types of content and have very similar visual appearance. While <ref type="bibr" target="#b19">[20]</ref> studied style in the context of photographs and paintings, we explore the novel problem of photographer identification. We find it unusual that this problem has remained unexplored for so long, given that photographs are more abundant than paintings, and there has been work in computer vision to analyze paintings. Given the lower level of authorial control that the photographer possesses compared to the painter, we believe that the photographer classification task is more challenging, in that it often requires attention to subtler cues than brush stroke, Adams 245</p><p>Brumfield <ref type="table" target="#tab_1">1138  Capa  2389  Bresson  4693 Cunningham  406  Curtis  1069  Delano  14484  Duryea  152  Erwitt  5173  Fenton  262  Gall  656  Genthe  4140  Glinn  4529  Gottscho  4009  Grabill  189  Griffiths  2000 Halsman 1310 Hartmann 2784  Highsmith  28475  Hine  5116 Horydczak 14317  Hurley  126  Jackson  881 Johnston 6962  Kandell  311  Korab  764  Lange  3913  List  2278  McCurry  6705  Meiselas 3051 Mydans 2461 O'Sullivan 573  Parr  20635 Prokudin-Gorsky 2605  Rodger  1204  Rothstein 12517 Seymour 1543  Stock  3416  Sweet  909  Van Vechten  1385  Wolcott  12173   Table 1</ref>: Listing of all photographers and the number of photos by each in our dataset.</p><p>for example. Besides our experimental analysis of this new problem, we also contribute the first large dataset of wellknown photographers and their work. In Sec. 6.3, we propose a method for generating a new photograph in the style of an author. This problem is distinct from style transfer <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b2">3]</ref> which adjusts the tone or color of a photograph. Using <ref type="bibr" target="#b2">[3]</ref> on our generated photographs did not produce a visible improvement in their quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Dataset</head><p>A significant contribution of this paper is our photographer dataset. <ref type="bibr" target="#b2">3</ref> It consists of 41 well known photographers and contains 181,948 images of varying resolutions. We searched Google for "famous photographers" and used the list while also choosing authors with large, curated collections available online. <ref type="table">Table 1</ref> contains a listing of each photographer and their associated number of images in our dataset. The timescale of the photos spans from the early days of photography to the present day. As such, some photos have been developed from film and some are digital. Many of the images were harvested using a web spider with permission from the Library of Congress's photo archives and the National Library of Australia's digital collection's website. The rest were harvested from the Magnum Photography online catalog, or from independent photographers' online collections. Each photo in the dataset is annotated with the ID of the author, the URL from which it was obtained, and possibly other meta-data, including: the title of the photo, a summary of the photo, and the subject of the photo (if known). The title, summary, and subject of the photograph were provided by either the curators of the collection or by the photographer. Unlike other datasets obtained through web image search which may contain some incorrectly labeled images, our dataset has been painstakingly assembled, authenticated, and described by the works' curators. This rigorous process ensures that the dataset and its associated annotations are of the highest quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Features</head><p>Identification of the correct photographer is a complex problem and relies on multiple factors. Thus, we explore a broad space of features (both low and high-level). The term "low-level" means that each dimension of the feature vector has no inherent "meaning." High-level features have articulatable semantic meaning (i.e. the presence of an object in the image). We also train a deep convolutional neural network from scratch in order to learn custom features specific to this problem domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Low-Level Features</head><p>• L*a*b* Color Histogram: To capture color differences among the photographers, we use a 30-dimensional binning of the L*a*b* color space. Color has been shown useful for dating historical photographs <ref type="bibr" target="#b29">[30]</ref>.</p><p>• GIST: GIST <ref type="bibr" target="#b28">[29]</ref> features have been shown to perform well at scene classification and have been tested by many of the prior studies in style and artist identification <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b31">32]</ref>. All images are resized to 256 by 256 pixels prior to having their GIST features extracted.</p><p>• SURF: Speeded-up Robust Features (SURF) <ref type="bibr" target="#b5">[6]</ref> is a classic local feature used to find patterns in images and has been used as a baseline for artist and style identification <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b1">2]</ref>. We use k-means clustering to obtain a vocabulary of 500 visual words and apply a standard bag-of-words approach using normalized histograms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>High-Level Features</head><p>• Object Bank: The Object Bank <ref type="bibr" target="#b24">[25]</ref> descriptor captures the location of numerous object detector responses. We believe that the spatial relationships between objects may carry some semantic meaning useful for our task.</p><p>• Deep Convolutional Networks:</p><p>-CaffeNet: This pre-trained CNN <ref type="bibr" target="#b17">[18]</ref> is a clone of the winner of the ILSVRC2012 challenge <ref type="bibr" target="#b21">[22]</ref>. The network was trained on approximately 1.3M images to classify images into 1000 different object categories.</p><p>-Hybrid-CNN: This network has recently achieved state-of-the-art performance on scene recognition benchmarks <ref type="bibr" target="#b37">[38]</ref>. It was trained to recognize 1183 scene and object categories on roughly 3.6M images.</p><p>-PhotographerNET: We trained a CNN with the same architecture as the previous networks to identify the author of photographs from our dataset. The network was trained for 500,000 iterations on 4 Nvidia K80 GPUs on our training set and validated on a set disjoint from our training and test sets.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experimental Evaluation</head><p>To tested the effectiveness of the aforementioned features on the photographer classification task, using our new photographer dataset. We randomly divided our dataset into a training set (90%) and test set (10%). Because a validation set is useful when training a CNN to determine when learning has peaked, we created a validation set by randomly sampling 10% of the images from the training set and excluding them from the training set for our CNN only. The training of our PhotographerNET was terminated when performance started dropping on the validation set.</p><p>For every feature in <ref type="table" target="#tab_1">Table 2</ref> (except TOP which assigns the max output in FC8 as the photographer label) we train a one-vs-all multiclass SVM using the framework provided by <ref type="bibr" target="#b12">[13]</ref>. All SVMs use linear kernels. <ref type="table" target="#tab_1">Table 2</ref> presents the results of our experiments. We report the F-measure for each of the features tested. We observe that the deep features significantly outperform all low-level standard vision features, concordant with the findings of <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b31">32]</ref>. Additionally, we observe that Hybrid-CNN features outperform CaffeNet by a small margin on all features tested. This suggests that while objects are clearly useful for photographer identification given the impressive performance of CaffeNet, the added scene information of Hybrid-CNN provides useful cues beyond those available in the purely object-oriented model. We observe that Pool5 is the best feature within both CaffeNet and Hybrid-CNN. Since Pool5 roughly corresponds to parts of objects <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b16">17]</ref>, we can conclude that seeing the parts of objects, not the full objects, is most discriminative for identifying photographers. This is intuitive because an artistic photograph contains many objects, so some of them may not be fully visible.</p><p>The Object Bank feature achieves nearly the same performance as C-FC8 and H-FC8, the network layers with explicit semantic meaning. All three of these features encapsulate object information, though Object Bank detects significantly fewer classes (177) than Hybrid-CNN (978) or CaffeNet (1000). Despite detecting fewer categories, Object Bank encodes more fine-grained spatial information about where the objects detected were located in the image, compared to H-FC8 and C-FC8. This finer-grained information could be giving it a slight advantage over these CNN object detectors, despite its fewer categories.</p><p>One surprising result from our experiment is that Pho-tographerNET does not surpass either CaffeNet or Hybrid-CNN, which were trained for object and scene detection on different datasets. <ref type="bibr" target="#b3">4</ref> PhotographerNET's top-performing feature (FC7) outperforms the deepest (FC8) layers in both CaffeNet and Hybrid-CNN, which correspond to object and scene classification, respectively. However, P-FC7 performs worse than their shallower layers, especially H-Pool5. Layers of the network shallower than P-FC7, such as P-FC6 and P-Pool5, demonstrate a sharp decrease in performance (a trend opposite to what we see for CaffeNet and Hybrid-CNN), suggesting that PhotographerNET has learned different and less predictive intermediate feature extractors for these layers than CaffeNet or Hybrid-CNN. Attributing a photograph to the author with highest P-FC8 response (TOP) is even weaker because unlike the P-FC8 method, it does not make use of an SVM. It may be that the task PhotographerNET is trying to learn is too high-level and challenging. Because PhotographerNET is learning a task even more high-level than object classification and we observe that the full-object-representation is not very useful for this task, one can conclude that for photographer identification, there is a mismatch between the high-level nature of the task, and the level of representation that is useful.</p><p>In <ref type="figure" target="#fig_1">Fig. 2</ref>, we provide a visualization that might explain the relative performance of our top-performing Photogra-pherNET feature (P-FC7) and the best feature overall (H-Pool5). We compute the t-distributed stochastic neighborhood embeddings <ref type="bibr" target="#b34">[35]</ref> for P-FC7 and H-Pool5. We use the embeddings to project each feature into 2-D space. We then plot the embedded features by representing them with their corresponding photographs.</p><p>We observe that H-Pool5 divides the image space in semantically meaningful ways. For example, we see that pho-tos containing people are grouped mainly at the top right, while buildings and outdoor scenes are at the bottom. We notice H-Pool5's groupings are agnostic to color or border differences. In contrast, PhotographerNET's P-FC7 divides the image space along the diagonal into black and white vs. color regions. It is hard to identify semantic groups based on the image's content. However, we can see that images that "look alike" by having similar borders or similar colors are closer to each other in the projection. This indicates that PhotographerNET learned to use lower-level features to perform photographer classification, whereas Hybrid-CNN learned higher-level semantic features for object/scene recognition. One possible explanation for this is that because the photos within each class (photographer) of our dataset are so visually diverse, the network is unable to learn semantic features for objects which do not occur frequently enough. In contrast, networks trained explicitly for object recognition only see images of that object in each class, enabling them to more easily learn object representations. Interestingly, these semantic features learned on a different problem outperform the features learned on our photographer identification problem.</p><p>To establish a human baseline for the task of photographer identification, we performed two small pilot experiments. We created a website where participants could view 50 randomly chosen images training images for each photographer. The participants were asked to review these and were allowed to take notes. Next, they were asked to classify 30 photos chosen at random from a special balanced test set. Participants were allowed to keep open the page containing the images for each photographer during the test phase of the experiment. In our first experiment, one participant studied and classified images for all 41 photographers and obtained an F1-score of 0.47. In a second study, a different participant performed the same task but was only asked to study and classify the ten photographers with the most data, and obtained an F1-score of 0.67. Our topperforming feature's performance in <ref type="table" target="#tab_1">Table 2</ref> (on all 41 photographers) surpasses both human F1-scores even on the smaller task of ten photographers, demonstrating the difficulty of the photographer identification problem on our challenging dataset.</p><p>Finally, to demonstrate the difficulty of the photographer classification problem and to explore the types of errors different features tend to make, we present several examples of misclassifications in <ref type="figure" target="#fig_2">Fig. 3</ref>. Test images are shown on the left. Using the SVM weights to weigh image descriptors, we find the training image (1) from the incorrectly predicted class (shown in the middle) and (2) from the correct class (shown on the right), with minimum distance to the test image. The first row <ref type="figure" target="#fig_2">(Fig. 3a-3c</ref>) depicts confusion using SURF features. All three rooms have visually similar decor and furniture, offering some explanation to  3a's misclassification as a Gottscho image. The second row <ref type="figure" target="#fig_2">(Fig. 3d-3f)</ref> shows a misclassification by CaffeNet. Even though all three scenes contain people at work, CaffeNet lacks the ability to differentiate between the scene types (indoor vs. outdoor and place of business vs. house). In contrast, Hybrid-CNN was explicitly trained to differenti- ate these types of scenes. The final row shows the type of misclassification made by our top-performing feature, H-Pool5. Hybrid-CNN has confused the indoor scene in <ref type="figure" target="#fig_2">Fig.  3g</ref> as a Highsmith. However, we can see that Highsmith took a similar indoor scene containing similar home furnishings <ref type="figure" target="#fig_2">(Fig. 3h)</ref>. These examples illustrate a few of the many confounding factors which make photographer identification challenging.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Qualitative Results</head><p>The experimental results presented in the previous section indicate that classifiers can exploit semantic information in photographs to differentiate between photographers at a much higher fidelity than low-level features. At this point, the question becomes not if computer vision techniques can perform photographer classification relatively reliably but how they are doing it. What did the classifiers learn? In this section, we present qualitative results which attempt to answer this question and enable us to draw interesting insights about the photographers and their subjects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Photographers and objects</head><p>Our first set of qualitative experiments explores the relationship of each photographer to the objects which they photograph and which differentiate them. Each dimension of the 1000-dimensional C-FC8 vector produced by Caf-feNet represents a probability that its associated ImageNet synset is the class portrayed by the image. While C-FC8 does not achieve the highest F-measure, it has a clear semantic mapping to ImageNet synsets and thus can be more easily used to reason about what the classifiers have learned. Because the C-FC8 vector is high-dimensional, we "collapse" the vector for purposes of human consideration. To do this, we map each ImageNet synset to its associated WordNet synset and then move up the WordNet hierarchy until the first of a number of manually chosen synsets 5 are encountered, which becomes the dimension's new label. This reduces C-FC8 to 54 coarse categories by averaging all dimensions with the same coarse label. In <ref type="figure" target="#fig_3">Fig. 4</ref>, we show the average response values for these 54 coarse object categories for each photographer. Green indicates positive values and red indicates negative values. Darker shades of each color are more extreme.</p><p>We apply the same technique to collapse the learned SVM weights. During training, each one-vs-all linear SVM learns a weight for each of the 1000 C-FC8 feature dimensions. Large positive or negative values indicate a feature that is highly predictive. Unlike the previous technique which simply shows the average object distribution per photographer, using the learned weights allows us to see what categories specifically distinguish a photographer from others. We show the result in <ref type="figure" target="#fig_4">Fig. 5</ref>.</p><p>Finally, while information about the 54 types of objects photographed by each author is useful, finer-grained detail is also available. We list the top 10 individual categories with highest H-FC8 weights (which captures both objects and scenes). To do this, we extract and average the H-FC8 vector for all images in the dataset for each photographer. We list the top 10 most represented categories for a select group of photographers in <ref type="table" target="#tab_3">Table 3</ref>, and include example photographs by each photographer.</p><p>We make the following observations about the photographers' style from Figs. 4 and 5 and <ref type="table" target="#tab_3">Table 3</ref>. <ref type="figure" target="#fig_3">From Fig. 4</ref>, we conclude that Brumfield shoots significantly fewer people than most photographers. Instead, Brumfield shoots many "buildings" and "housing." Peering deeper, Brumfield's top ten categories in <ref type="table" target="#tab_3">Table 3</ref> reveal that he frequently shot architecture (such as mosques and stupas). In fact, Brumfield is an architectural photographer, particularly of Russian architecture. In contrast, Van Vechten has high response values for categories such as "clothing", "covering", "headdress" and "person". Van Vechten's photographs are almost exclusively portraits of people, so we observe a positive SVM weight for "person" in <ref type="figure" target="#fig_4">Fig. 5</ref>.</p><p>Comparing Figs. 4 and 5, we see that there is not a clear correlation between object frequency and the object's SVM weight. For instance, the "weapon" category is frequently  represented given <ref type="figure" target="#fig_3">Fig. 4</ref>, yet is only predictive of a few photographers <ref type="figure" target="#fig_4">(Fig. 5)</ref>. The "person" category in <ref type="figure" target="#fig_4">Fig. 5</ref> has high magnitude weights for many photographers, indicating its utility as a class predictor. Note that the set of objects distinctive for a photographer does not fully depend on the photographer's environment. For example, Lange and Wolcott both worked for the FSA, yet there are notable differences between their SVM weights in <ref type="figure" target="#fig_4">Fig. 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Schools of thought</head><p>Taking the idea of photographic style one step further, we wanted to see if meaningful genres or "schools of thought" of photographic style could be inferred from our results. We know that twelve of the photographers in our dataset were members of the Magnum Photos cooperative. We cluster the H-Pool5 features for all 41 photographers into a dendrogram, using agglomerative clustering, and discover that nine of those twelve cluster together tightly, with only one non-Magnum photographer in their cluster. We find that three of the four founders of Magnum form their own even tighter cluster. Further, five photographers in our dataset that were employed by the FSA are grouped in our dendrogram, and two portrait photographers (Van Vechten and Curtis) appear in their own cluster. See the supplementary file for the figure. These results indicate that our techniques are not only useful for describing individual photographers but can also be used to situate photographers in broader "schools of thought."</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">New photograph generation</head><p>Our experimental results demonstrated that object and scene information is useful for distinguishing between photographers. Based on these results, we wanted to see whether we could take our photographer models yet another step further by generating new photographs imitating photographers' styles. Our goal was to create "pastiches" assembled by cropping objects out of each photographer's data and pasting them in new scenes obtained from Flickr. We first learned a probability distribution over the 205-scene types detected by Hybrid-CNN for each photographer. We then learned a distribution of objects and their most likely spatial location for each photographer, conditioned on the scene type. To do this, we trained a Fast-RCNN <ref type="bibr" target="#b15">[16]</ref> object detector on 25 object categories which frequently occurred across all photographers in our dataset using data we obtained from ImageNet. We then sampled from our joint probability distributions to choose which scene to use and which objects should appear in it and where. We randomly selected a detection (in that photographer's data) for each object probabilistically selected to appear, then cropped out the detection and segmented the cropped region using <ref type="bibr" target="#b25">[26]</ref>. We inserted the segment into the pastiche according to that photographer's spatial model for that object.</p><p>We show six pastiches generated using this approach in <ref type="figure">Fig. 6</ref>. The top row shows generated images for six photographers, and the bottom shows real images from the corresponding photographer that resemble the generated ones.   <ref type="figure">Figure 6</ref>: Generated images for six photographers (top row) and real photographs by these authors (bottom row). Although results are preliminary, we observe interesting similarities between the synthetic and real work.</p><p>For example, Delano takes portraits of individuals in uniforms and of "common people," Erwitt photographs people in street scenes without their knowledge or participation, and Rothstein photographs people congregating. Highsmith captures large banner ads and Americana, Hine children working in poor conditions, and Horydczak buildings and architecture. While these are preliminary results, we see similarities between the synthetic and authentic photos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>In this paper, we have proposed the novel problem of photograph authorship attribution. To facilitate research on this problem, we created a large dataset of 181,948 images by renowned photographers. In addition to tagging each photo with the photographer, the dataset also provides rich metadata which could be useful for future research in computer vision on a variety of tasks.</p><p>Our experiments reveal that high-level features perform significantly better overall than low-level features or humans. While our trained CNN, PhotographerNET, performs reasonably well, early proto-object and scene-detection fea-tures perform significantly better. The inclusion of scene information provides moderate gains over the purely objectdriven approach explored by <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b31">32]</ref>. We also provide an approach for performing qualitative analysis on the photographers by determining which objects respond strongly to each photographer in the feature values and learned classifier weights. Using these techniques, we were able to draw interesting conclusions about the photographers we studied as well as broader "schools of thought." We also showed initial results for a method that creates new photographs in the spirit of a given author.</p><p>In the future, we will develop further applications of our approach, e.g. teaching humans to better distinguish between the photographers' styles. We will also continue our work on using our models to generate novel photographs of known photographers' styles.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>t-SNE embeddings for two deep features. We observe that PhotographerNET relies more heavily on lowerlevel cues (like color) than higher-level semantic details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Confused images. The first column shows the test image, the second shows the closest image in the predicted class, and the third shows the closest image from the correct class. Can you tell which one doesn't belong?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Average C-FC8 collapsed by WordNet. Please zoom in or view the supplementary file for a larger image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>C-FC8 SVM weights collapsed by WordNet. Please zoom in or view supplementary for a larger image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>Color GIST SURF-BOW Object Bank Pool5 FC6 FC7 FC8 Pool5 FC6 FC7 FC8 Pool5 FC6 FC7 FC8 TOP</figDesc><table>Low 

High 
CaffeNet 
Hybrid-CNN 
PhotographerNET 
0.31 
0.33 
0.37 
0.59 
0.73 
0.7 
0.69 
0.6 
0.74 
0.73 0.71 0.61 
0.25 
0.25 0.63 0.47 0.14 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 2 :</head><label>2</label><figDesc>Our experimental results. The F-measure of each feature is reported. The best feature overall is in bold, and the best one per CNN in italics. Note that high-level features greatly outperform low-level ones. Chance performance is 0.024.</figDesc><table>To disambiguate layer names, we prefix them with a C, 
H, or P depending on whether the feature came from Caf-
feNet, Hybrid-CNN, or PhotographerNET, respectively. 
For all networks, we extract features from the Pool5, 
FC6, FC7 and FC8 layers, and show the result of using 
those features during SVM training in Table 2. The score 
in the TOP column for PhotographerNET is produced by 
classifying each test image as the author who corresponds 
to the dimension with the maximum response value in 
PhotographerNET's output (FC8). 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc>Top ten objects and scenes for select photographers, and sample images.</figDesc><table>(a) Delano 
(b) Erwitt 
(c) Highsmith 
(d) Hine 
(e) Horydczak 
(f) Rothstein 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Both Lange and Wolcott worked for the Farm Security Administration (FSA) documenting the hardship of the Great Depression, while Hine worked to address a number of labor rights issues.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Automatically creating a novel Rembrandt painting<ref type="bibr" target="#b0">[1]</ref> gained media attention in April 2016, five months after we submitted our work.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">It can be downloaded at http://www.cs.pitt.edu/ chris/photographer.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">We also tried fine-tuning the last three layers of CaffeNet and Hybrid-CNN with our photographer data, but we did not obtain an increase in performance.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">These synsets were manually chosen to form a natural human-like grouping of the 1000 object categories. Because the manually chosen synsets are on multiple levels of the WordNet hierarchy, synsets are assigned to their deepest parent.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgement. This work used the Extreme Science and Engineering Discovery Environment (XSEDE) and the Data Exacell at the Pittsburgh Supercomputing Center (PSC), supported by National Science Foundation grants ACI-1053575 and ACI-1261721.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>The Next Rembrandt</surname></persName>
		</author>
		<ptr target="https://www.nextrembrandt.com/.Ac-cessed" />
		<imprint>
			<date type="published" when="2016-04-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Towards automated classification of fine-art painting style: A comparative study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Arora</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
		<respStmt>
			<orgName>Rutgers University-Graduate School-New Brunswick</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Fast local laplacian filters: Theory and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aubry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Paris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Hasinoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">167</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Two-scale tone management for photographic look</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Paris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="637" to="645" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Classification of artistic styles using binarized features derived from a deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV) Workshops</title>
		<meeting>the European Conference on Computer Vision (ECCV) Workshops</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="71" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Computer Vision and Image Understanding (CVIU)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="page" from="346" to="359" />
		</imprint>
	</monogr>
	<note>Speeded-up robust features (SURF)</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Using machine learning for identification of art paintings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blessing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
		<respStmt>
			<orgName>Stanford University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning photographic global tonal adjustment with a database of input/output image pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Bychkovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Paris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="97" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Artistic image classification: an analysis on the printart database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Carneiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">P</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Costeira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="143" to="157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Report on digital image processing for art historians</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cornelis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dooms</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Daubechies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Schelkens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SAMPTA&apos;09</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">High level describable attributes for predicting aesthetics and interestingness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1657" to="1664" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">What makes paris look like paris?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Liblinear: A library for large linear classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R.-E</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1871" to="1874" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Signal Processing Magazine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Farid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="16" to="25" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note>Image forgery detection</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A century of portraits: A visual historical record of american high school yearbooks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ginosar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rakelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sachs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV) Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV) Workshops</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Salicon: Reducing the semantic gap in saliency prediction by adapting deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Boix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="262" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Conference on Multimedia</title>
		<meeting>the ACM International Conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="675" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Image processing for artist identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Johnson</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hendriks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Berezhnoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Daubechies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Postma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="37" to="48" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note>IEEE</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Recognizing image style</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Trentacoste</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hertzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Winnemoeller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference (BMVC)</title>
		<meeting>the British Machine Vision Conference (BMVC)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Recognizing image &quot;style&quot; and activities in video using local features and naive bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Keren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">16</biblScope>
			<biblScope unit="page" from="2913" to="2922" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Linking past to present: Discovering style in two centuries of architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Maisonneuve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Crandall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computational Photography</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Style-aware mid-level representation for discovering visual connections in space and time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1857" to="1864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Object bank: A high-level image representation for scene classification &amp; semantic feature sparsification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1378" to="1386" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The secrets of salient object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rehg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="280" to="287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Assessing the aesthetic quality of photographs using generic image descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Marchesotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Larlus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Csurka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1784" to="1791" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Ava: A large-scale database for aesthetic visual analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Marchesotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2408" to="2415" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Modeling the shape of the scene: A holistic representation of the spatial envelope</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="145" to="175" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Dating historical color images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Palermo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="499" to="512" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Detection of forgery in paintings using supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Polatkan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jafarpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Brasoveanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Daubechies</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Image Processing (ICIP)</title>
		<meeting>the IEEE International Conference on Image Processing (ICIP)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="2921" to="2924" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Large-scale classification of fine-art paintings: Learning the right metric on the right feature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1505.00855</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Impressionism, expressionism, surrealism: Automated recognition of painters and schools of art</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Macura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Orlov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Eckley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">G</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Applied Perception (TAP)</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Efficient object category recognition using classemes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Szummer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fitzgibbon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="776" to="789" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Visualizing data using t-SNE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">85</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Understanding intra-class knowledge inside cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Freeman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.02379</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="818" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning deep features for scene recognition using places database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="487" to="495" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
