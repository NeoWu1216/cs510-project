<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Similarity Learning with Spatial Constraints for Person Re-identification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dapeng</forename><surname>Chen</surname></persName>
							<email>dapengchenxjtu@foxmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Xi&apos;an Jiaotong University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zejian</forename><surname>Yuan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Xi&apos;an Jiaotong University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Badong</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Xi&apos;an Jiaotong University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanning</forename><surname>Zheng</surname></persName>
							<email>nnzheng@mail.xjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Xi&apos;an Jiaotong University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Similarity Learning with Spatial Constraints for Person Re-identification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:46+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Pose variation remains one of the major factors that adversely affect the accuracy of person re-identification. Such variation is not arbitrary as body parts (e.g. head, torso, legs)  have relative stable spatial distribution. Breaking down the variability of global appearance regarding the spatial distribution potentially benefits the person matching. We therefore learn a novel similarity function, which consists of multiple sub-similarity measurements with each taking in charge of a subregion. In particular, we take advantage of the recently proposed polynomial feature map to describe the matching within each subregion, and inject all the feature maps into a unified framework. The framework not only outputs similarity measurements for different regions, but also makes a better consistency among them. Our framework can collaborate local similarities as well as global similarity to exploit their complementary strength. It is flexible to incorporate multiple visual cues to further elevate the performance. In experiments, we analyze the effectiveness of the major components. The results on four datasets show significant and consistent improvements over the state-of-the-art methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Person re-identification refers a task of associating a same person in different camera views. It plays a crucial role for applications such as long-term person tracking <ref type="bibr" target="#b8">[9]</ref>, multi-person association <ref type="bibr" target="#b27">[28]</ref>, group behavior analysis <ref type="bibr" target="#b37">[38]</ref>, etc. Similarity measuring serves as a major step for a person re-identification system. An ideal measurement is a matching rule that yields higher matching score for the image pairs belonging to the same person than the pairs belonging to different persons. The similarity measurement can be pre-defined or be learned. The former type adopts the offthe-shelf distance metric such as Euclidean distance <ref type="bibr" target="#b9">[10]</ref>, Bhattacharyya distance <ref type="bibr" target="#b5">[6]</ref>, and covariance distance <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b0">1]</ref>, while the latter type tries to exploit the inherent invariance between image pairs <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b23">24]</ref>. By making use of the training data, learning-based models generally enjoy bet-ter performance than the learning-free methods. However, most similarity learning just focus on a "holistic" measurement, which discards the geometric structure of pedestrians and can not further exploit the discriminative power.</p><p>We argue that similarity learning should obey certain spatial constraints, which indicates the matching of certain body part should follow its spatial distribution. For example, the region containing the head of a person should be compared with the region containing the head rather than the region containing the feet. With such constraints, each region has its own similarity measurement that excels at handling the special intra-person variation within it. The combination of multiple measurements is more flexible to exploit the global invariance than a holistic one. Besides, enforcing the matching within the corresponding region can effectively reduce the risk of mismatching and become more robust to partial occlusion.</p><p>We combine such constraints with recently proposed polynomial feature maps <ref type="bibr" target="#b3">[4]</ref>. As each feature map can describe the matching within each local region, we employ multiple feature maps to represent the different regions simultaneously and inject all of them into a unified learning framework. The framework not only outputs the similarity measurements for each local region, but also makes a better consistency among these measurements. Our framework is able to collaborate the local measurements with global measurement to exploit their complementary power, and it is flexible to incorporate multiple visual cues to further improve the performance.</p><p>The main contributions are threefold: (1) We present a novel similarity function to investigate how the spatial constraints can benefit the similarity learning for person reidentification. <ref type="bibr" target="#b1">(2)</ref> We propose a convex objective function as well as an efficient optimization algorithm for it. (3) We operate in-depth experiments to analyze various aspects of our approach, and the final results outperform the state-ofthe-arts over four benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>A comprehensive survey can be found in <ref type="bibr" target="#b10">[11]</ref>. Here, we briefly review the most relevant works. Spatial constraints. As pedestrians have relative stable geometric structure, spatial constraints have been widely adopted for person image representation <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b33">34]</ref>. Wang et al. <ref type="bibr" target="#b28">[29]</ref> utilized shape and appearance context to capture the spatial relations between appearance labels. Farenzena et al. <ref type="bibr" target="#b9">[10]</ref> considered the symmetric and asymmetric prior of human body part, and extracted local features from each part. Methods <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b35">36]</ref> adopting SPM-like <ref type="bibr" target="#b16">[17]</ref> representation separated the image into several horizontal stripes, and used unsupervised Bagof-Words model to represent each stripe region. Many other works extracted dense local features <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b36">37]</ref>, and concatenated those descriptors to implicitly encode the spatial layout of the person. After feature extraction, all these methods usually employ a "holistic" similarity measurement for all the extracted descriptors without further utilizing the spatial relation.</p><p>Only a few works imposed the spatial constraints for similarity measuring. Zhao et al. <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b32">33]</ref> matched each patch in one image with the neighbouring patches in the other image, where the matching rule is pre-defined and computational cost is large. Different from their work, we impose a similarity function with spatial constraints, the similarity function is much more efficient than exhaustive matching and the spatial constraints can better exploit the discriminative ability from data. Similarity learning. Similarity learning has gradually shown its effectiveness in person re-identification. Most works learn a similarity measurement based upon Mahalanobis distance. Among them, Hirzer et al. <ref type="bibr" target="#b12">[13]</ref> relaxed the PSD constraint of the metric and obtained a simplified formulation with reasonable effectiveness. Köstinger et al. <ref type="bibr" target="#b14">[15]</ref> proposed an efficient metric computation method motivated by the log likelihood ratio test of two Gaussian distributions. In <ref type="bibr" target="#b17">[18]</ref>, Li et al. proposed Locally-Adaptive Decision Function, which can be viewed as a joint model of a distance metric and a locally adapted thresholding rule. Zheng et al. <ref type="bibr" target="#b36">[37]</ref> made use of the triplet relationship between a positive pair and a negative pair, optimizing the relative distance comparison.</p><p>Recently, a similarity measurement built on polynomial feature map has been proposed <ref type="bibr" target="#b3">[4]</ref>. The feature map explicitly represents the matching of two images, and its regularized form is connected to Mahalanobis distance and crosspatch similarity. Our work takes advantage of the feature map to represent the matching within each sub-region. The linear form of the feature maps allows us to conveniently exploit the complementary strength of different local regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Our Approach</head><p>In this section, we first revisit the polynomial feature map. Based upon the map, we impose spatial constraints for similarity learning, and formulate the learning problem specifically designed for person re-identification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Polynomial Feature Map [4]</head><p>To measure the similarity between image descriptors x a , x b ∈ R d×1 , we aim to learn a basic similarity function f (x a , x b ) that can yield high score when x a and x b are similar. The similarity function is in linear form:</p><formula xml:id="formula_0">f (x a , x b ) = φ(x a , x b ), W F .<label>(1)</label></formula><p>where ·, · F is the Frobenius inner product. To take advantage of both Mahalanobis distance and a bilinear similarity metric, we decompose Eq. 1 to be</p><formula xml:id="formula_1">f (xa, x b ) = φM (xa, x b ), WM F + φB(xa, x b ), WB F , (2) where W = [W M , W B ] and φ(x a , x b ) = [φ M (x a , x b ), φ B (x a , x b )]. More specifically, φ M (x a , x b ) = (x a −x b )(x a −x b ) ⊤ , φ B (x a , x b ) = x a x ⊤ b +x b x ⊤ a . The part φ M (x a , x b ), W M F = (x a − x b ) ⊤ W M (x a − x b )</formula><p>, is connected to Mahalanobis distance. As we want to achieve high score when x a and x b are similar,</p><formula xml:id="formula_2">W M should be negative semi-definite. The part φ B (x a , x b ), W B F = x ⊤ a W B x b + x ⊤ b W B</formula><p>x a , corresponds to bilinear similarity <ref type="bibr" target="#b2">[3]</ref>. Both parts ensure the effectiveness of f (x a , x b ).</p><p>The</p><formula xml:id="formula_3">feature map φ(x a , x b ) is composed by the elements in ϕ(x a , x b ) = [x a x ⊤ a , x b x ⊤ b , x a x ⊤ b , x b x ⊤ a ]. Particular- ly, ϕ(x a , x b ) is induced by the second order polynomi- al kernel k(z, z ′ ) = (z ⊤ z ′ ) 2 = ϕ(z), ϕ(z ′ ) F , where z = [x ⊤ a , x ⊤ b ]. φ(x a , x b )</formula><p>re-organizes the elements in ϕ(x a , x b ), thus it is a regularized form of polynomial feature map.</p><p>φ(x a , x a ) conveys the matching information of x a and x b . In the case that x a and x b are patch-wise descriptors of an image (each entry or sub-vector corresponds to a block of the image), φ M (x a , x b ) focus on measuring the similarity for descriptors at the same position. φ B (x a , x b ) matches each patch in one image with all the patches in the other image, and all the cross-patch similarities are attained as</p><formula xml:id="formula_4">x a x ⊤ b and x b x ⊤ a .</formula><p>To reduce the dimensionality of the feature map, method <ref type="bibr" target="#b3">[4]</ref> performs PCA for x a and x b before forming the feature map, which keeps the effectiveness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Spatially Constrained Similarity Function</head><p>The flowchart of the overall similarity function is illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>. We explain it with more details as follows. Regional feature map. An image is partitioned into R non-overlap horizontal stripe regions. For each region, we divide it into a collection of overlapped patches, and extract color and texture histograms from each patch. The extracted histograms belonging to a same stripe region are concatenated together. After that, we apply PCA to reduce the dimensionality and obtain the region descriptor x r for the r-th stripe, where r ∈ {1, ..., R}.</p><p>A stripe region r can be described by C visual cues {x r,1 , ..., x r,c , ..., x r,C }, thus x a and x b accordingly form C polynomial feature maps for the r-th region, i.e.,</p><formula xml:id="formula_5">{φ r,1 (x a , x b ), ..., φ r,c (x a , x b ), ..., φ r,C (x a , x b )}, where φ r,c (x a , x b ) = φ(x r,c</formula><p>a , x r,c b ). As different feature maps can describe the matching in different aspects of view, multiple feature maps can encode more comprehensive information about the matching. Local similarity integration. In order to exploit the complementary strengths of multiple visual cues within a local region, we employ a linear similarity function to combine them together for the r-th region:</p><formula xml:id="formula_6">s r (xa, x b ) = C c=1 φ r,c (xa, x b ), W r,c F ,<label>(3)</label></formula><p>where</p><formula xml:id="formula_7">W r,c = [W r,c M , W r,c B ], and W r,c M , W r,c B corresponds to φ r,c M (x a , x b ) and φ r,c B (x a , x b )</formula><p>respectively. For all the R regions, the integrated local similarity score is simply represented as:</p><formula xml:id="formula_8">s local (xa, x b ) = R r=1 s r (xa, x b ).<label>(4)</label></formula><p>Global-local collaboration. The feature maps of a local region can not describe the matching of large patterns across the stripes. To compensate the insufficiency of local similarity, we also make use of the polynomial feature map for the whole image, yielding global similarity:</p><formula xml:id="formula_9">s global (xa, x b ) = C c=1 φ G,c (xa, x b ), W G,c F ,<label>(5)</label></formula><formula xml:id="formula_10">where φ G,c (x a , x b ) = φ(x G,c a , x G,c b ) and x G,c a , x G,c b</formula><p>are the c-th type global visual descriptors for image a and image b. The global similarity and local similarity are linearly combined, and the overall similarity score is given by:</p><formula xml:id="formula_11">s(xa, x b ) = s local (xa, x b ) + γs global (xa, x b ).<label>(6)</label></formula><p>Here γ is the hyper-parameter that mediates the local similarity and global similarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Learning for Person Re-identification</head><formula xml:id="formula_12">Regularization. As W M , φ M (x a , x b ) F corresponds to negative Mahalanobis distance (discussed in Sec. 3.1), it is reasonable for W M to be negative semi-definite: W r,c M , W G,c M ∈ S d − r = 1, ..., R, c = 1, ..., C,<label>(7)</label></formula><p>where S d − denote the set containing negative semi-definite matrices with the size of d × d.</p><p>Considering the construction of φ M (x a , x b ) and φ B (x a , x b ), both feature maps are generated by the out product of certain feature vector. If some dimensions of the vector are not effective for discrimination, the elements in corresponding columns or rows of the feature map tend to be less effective. The assumption indicates that the effective elements in polynomial feature map would appear in group, we therefore impose mixed norm for the corresponding coefficient matrices.</p><formula xml:id="formula_13">R(W) = W∈W W 2,1,<label>(8)</label></formula><p>where <ref type="bibr" target="#b31">[32]</ref>, and W is the coefficient set defined by</p><formula xml:id="formula_14">W 2,1 := d i=1 W i. 2</formula><formula xml:id="formula_15">W = {W 1,c M , W 1,c B , ..., W r,c M , W r,c B , ..., W R,c M , W R,c B , W G,c M , W G,c B } C c=1</formula><p>. Relaxed loss term. The training data for person reidentification can be organized as follows. Given the descriptors of probe images X = {x 1 , ..., x n , ..., x N }, x n is associated with two sets of gallery images: a positive set X + n composed of the descriptors about the same person with x n and a negative set X − n composed of the descriptors about different persons to x n . We consider to enforce the relative comparison and propose a relaxed loss term:</p><formula xml:id="formula_16">L(W) = 1 N N n=1 [1− x i ∈X + n ,x j ∈X − n s(xn, xi) − s(xn, xj) |X + n | · |X − n | ]+,<label>(9)</label></formula><p>where [.] + denotes the hinge loss. Given a probe descriptor x n , instead of forcing every positive pair to achieve a higher score than negative pairs <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b26">27]</ref>, we require the average score of positive pairs should be higher than the average score of the negative pairs at least by a margin 1. The relaxed loss term only consists of N constraints, largely accelerating the training compared with the non-relaxed one.</p><p>Objective function. According to Eqs. 7, 8 and 9, the objective function for person re-identification is given by:</p><formula xml:id="formula_17">min W L(W) + λR(W) s.t. W r,c M ,W G,c M ∈ S d − , c = 1, ..., C, r = 1, ..., R.<label>(10)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Discussion</head><p>Our method is related to Spatial Pyramid Matching (SP-M) <ref type="bibr" target="#b16">[17]</ref>. Both of them employ subregions to encode the spatial layout information for matching. SPM employs the intersection kernel to compute the similarity for each subregion, defines a pyramid match kernel to combine the similarities in different pyramid layers, and takes the direct sum of the similarities of different type features. Instead of designing the three factors, we learn them from the data. The learned similarity measurements can adapt to each regions each visual cue, and can effectively combine them together. From another aspect of view, SPM is a more general measurement that is used for image classification, while our similarity function encodes more details about a specific class thus is suitable for identification or verification.</p><p>Our method is compared with previous ensemble approaches <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b29">30]</ref>. Although employing multiple measurements for re-identification, both the similarity functions and the learning approaches are quite different. Firstly, the multiple measurements in other methods are for the whole person image, while our measurements are with certain spatial attributes. Secondly, approaches <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b29">30]</ref> build their final similarity function in two-stage. They first learn the similarity measurements of different features independently, then utilize ensemble techniques to combine the independent scores. In contrast, we learn the multiple measurements simultaneously, and can better exploit the consistency between different types of features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Optimization</head><p>To clarify the notation, we first concatenate the C feature maps in each sub-region together:</p><formula xml:id="formula_18">φ r (xa, x b ) = [φ r,1 (xa, x b ), ..., φ r,c (xa, x b ), ..., φ r,C (xa, x b )], φ G (xa, x b ) = [φ G,1 (xa, x b ), ..., φ G,c (xa, x b ), ..., φ G,C (xa, x b )]. Accordingly, W r = [W r,1 , ..., W r,c , ..., W r,C ] and W G = [W G,1 , ..., W G,c , ..., W G,C ] are the coeffi- cients for φ r (x a , x b ) and φ G (x a , x b ). Let Φ(x a , x b ) = [φ 1 (x a , x b ), ..., φ r (x a , x b ), ..., φ R (x a , x b ), γφ G (x a , x b )]</formula><p>and U = [W 1 , ..., W r , ..., W R , W G ], the similarity function of Eq. 6 can be simply computed as:</p><formula xml:id="formula_19">s(x a , x b ) = Φ(x a , x b ), U F .<label>(11)</label></formula><p>Algorithm 1 The ADMM optimization. </p><formula xml:id="formula_20">1: Input: Dateset D = {x i , X + i , X − i } n i=1</formula><formula xml:id="formula_21">U ← U L 3</formula><p>We note that the content within the hinge loss is linear w.r.t. coefficient U. By defining</p><formula xml:id="formula_22">Ψ(xn) = x i ∈X + n ,x j ∈X − n Φ(xn, xi) − Φ(xn, xj) |X + n | · |X − n | ,<label>(12)</label></formula><p>Eq. 9 is rewritten as L</p><formula xml:id="formula_23">(U) = 1 N N n=1 [1− Ψ(x n ), U F ] + .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">ADMM Optimization</head><p>Our objective function forms a convex optimization problem. For the ease of optimization, we transform Eq. 10 to an equivalent problem:</p><formula xml:id="formula_24">min U 1 ,U 2 ,U 3 g1(U1)+g2(U2)+g3(U3), s.t.U1 = U2 = U3, (13) where g 1 (U) = L(U), g 2 (U) = λR(U), and g 3 (U) = ∞δ[U / ∈ C].</formula><p>Here, C is a closed convex set defined from the constraints in Eq. 7, and δ[·] is an indicator function which takes one if the argument is true and zeros otherwise. By performing ADMM, we have following iterations:</p><formula xml:id="formula_25">U l+1 1 = arg min U 1 g1(U1)+ ρ 2 U1 −(U l 3 − Λ l 1 ) 2 F<label>(14)</label></formula><p>U l+1 2 = arg min</p><formula xml:id="formula_26">U 2 g2(U2)+ ρ 2 U2 −(U l 3 − Λ l 2 ) 2 F (15) U l+1 3 = arg min U 3 g3(U3)+ρ U3 − 1 2 (U l+1 1 +U l+1 2 +Λ l 1 +Λ l 2 ) 2 F Λ l+1 1 = Λ l 1 +U l+1 1 −U l+1 3 , Λ l+1 2 = Λ l 2 +U l+1 2 −U l+1 3 ,</formula><p>where ρ is a scalar value called the penalty parameter, and Λ 1 and Λ 2 are scaled dual variables. The whole update procedures are summarized in Algorithm. 1. The details about the updates of U 1 , U 2 and U 3 are presented below:</p><p>The update of U 1 . Eq. 14 is a convex problem. We consider to optimize U l+1 1 from its dual form.</p><formula xml:id="formula_27">max α − 1 2ρ α ⊤ Hα−b ⊤ α, s.t. 0 ≤ α n ≤ 1 N , ∀n,<label>(16)</label></formula><p>where α ∈ R N ×1 are dual variables and α n is its nth element. The element of b ∈ R N ×1 is defined as:</p><formula xml:id="formula_28">b n = U l 3 − Λ l 1 , Ψ(x n ) F − 1. H ∈ R N ×N is the kernel matrix with H ij := Ψ(x i ), Ψ(x j ) F .</formula><p>Eq. 16 is a standard quadratic programming problem. As H can be pre-computed, optimizing the dual form is quite efficient. With optimal α * , U l+1</p><formula xml:id="formula_29">1 is updated by 1 ρ N n=1 α * n Ψ(x n ) + U l 3 − Λ l 1 .</formula><p>The update of U 2 . As U 2 ∈ R d×2dC(R+1) , the problem of Eq.15 can be decomposed into 2C(R+1) subproblems with each optimizing the coefficients corresponding to a feature map with the size of d × d. Let U s 2 ∈ R d×d denote the sth sub-matrix of U 2 and A s ∈ R d×d be the corresponding sub-matrix of U l 3 − Λ l 2 , a separate problem of Eq. 15 is:</p><formula xml:id="formula_30">min U s 2 λ U s 2 2,1 + ρ 2 U s 2 − A s 2 F .</formula><p>The optimal solution is given by a prox-operator <ref type="bibr" target="#b15">[16]</ref>:</p><formula xml:id="formula_31">(U s 2 )ij = A s ij 1 − λ/ρ A s i. 2 + ,<label>(17)</label></formula><p>where A s i. is the i-th row of A s . We apply the prox-operator for all the sub-matrices, obtaining U l+1 2 . The update of U 3 . U 3 is updated through the projection:</p><formula xml:id="formula_32">U l+1 3 = ΠC 1 2 (U l+1 1 +U l+1 2 +Λ l 1 +Λ l 2 ) ,<label>(18)</label></formula><p>where Π C is the Euclidean projection onto set C. Note that sub-matrices of 1</p><formula xml:id="formula_33">2 (U l+1 1 +U l+1 2 +Λ l 1 +Λ l 2 ) that correspond- s to {W r,c B , W G,c B } R,C r=1</formula><p>,c=1 may not be symmetric, directly projecting a non-symmetric matrix onto S d − is difficult. We operate a separated ADMM, including two iterative projection steps. One is to project the sub-matrices onto the S d by f (W) := 1 2 (W + W ⊤ ), the other is to project symmetric matrices onto S d − by cropping the positive eigenvalues to be zeros. The details about the updating procedures are relegated to the supplementary files.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Experimental Setup</head><p>Visual cues. We divide each subregion into a set of local patches as shown in <ref type="figure" target="#fig_0">Fig. 1a</ref>. From each patch, we extract 6 types of basic feature HSV 1 , HSV 2 , LAB 1 , LAB 2 , HOG and SILTP. Among them, HSV 1 and LAB 1 are 8×8×8 joint histograms, and HSV 2 and LAB 2 are 48 bin concatenated histograms with each channel having 16 bins, HOG <ref type="bibr" target="#b6">[7]</ref> and SILPT <ref type="bibr" target="#b19">[20]</ref> are texture descriptors.</p><p>The four visual cues C 1 ,C 2 ,C 3 ,C 4 concatenate both color and texture features, which are organized as HSV 1 /HOG, HSV 2 /SILPT, LAB 1 /SILPT, LAB 2 /HOG. We employ PCA to reduce their dimension, and do a whitening process to limit the impact of co-occurrence <ref type="bibr" target="#b13">[14]</ref>. The resulting descriptors are normalized to have unit L 2 norms. Parameter setting. We empirically set the number of local region R = 4, the parameter for ADMM learning ρ = 0.001. The PCA reduced dimension d depends on the size of training data, we set d to be 120, 60, 60 and 500 for VIPeR <ref type="bibr" target="#b11">[12]</ref>, GRID <ref type="bibr" target="#b20">[21]</ref>, 3DPES <ref type="bibr" target="#b1">[2]</ref> and Market-1501 <ref type="bibr" target="#b35">[36]</ref>, respectively. The tradeoff parameter λ in Eq. 10 is selected via cross validation. Evaluation protocol. Our experiments follow the evaluation protocol in <ref type="bibr" target="#b11">[12]</ref>. The dataset is separated into the training set and test set, where images of a same person can only appear in either set. The test set is further divided into probe set and gallery set, and the two sets contains the different images of a same person. We match each probe image with every image in gallery set, and rank the gallery images according to the similarity score. The results are evaluated by CMC curves <ref type="bibr" target="#b11">[12]</ref>, an estimate of the expectation of finding the correct match in the top n matches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Comparison to state-of-the-art Approaches</head><p>We term the proposed Spatially Constrained Similarity function on Polynomial feature map as SCSP. Besides, we also report the results of two variants G-All and L-All, where L-All corresponds to the integrated local similarity defined in Eq. 4, and G-All corresponds to the global similarity defined in Eq. 5. All the three methods are equipped with four visual cues. VIPeR <ref type="bibr" target="#b11">[12]</ref>. The VIPeR dataset is a challenging test bed for person re-identification. It contains 632 persons, and each person has 2 images taken from camera A and B with different viewpoints and illumination conditions. We randomly select 316 persons to form the training set, and select the remaining 316 persons to form the test set. The procedure is repeated 10 times to get an average performance.</p><p>We present the comparison results in <ref type="figure" target="#fig_1">Fig. 2a</ref> and Tab. 1. SCSP achieves the new state of art. Its rank-1 matching rate 53.54% outperforms the second best one ME by 7.65%. It also significantly improves Polymap, which is the original method employing polynomial feature map for person re-identification. By comparing L-All and G-All, we find that imposing spatial constraints improves the final performance (51.04% v.s. 48.10%). Such benefit is more significant when only using visual cue C 1 (43.70% v.s. 37.31%) as analyzed in <ref type="figure" target="#fig_3">Fig. 4a</ref>. GRID <ref type="bibr" target="#b20">[21]</ref>. The GRID dataset consists of 1275 person images. Among them, there are 250 pedestrian image pairs. Images in each pair belongs to a same person but are captured from different camera views. Besides, there are 775 additional person images that do not belong to any of the 250 persons. For the experiment, 10 partitions of the training and test samples have already been provided by the dataset. For each partition, 125 image pairs are used for training, and the remaining 125 image pairs and the 775 irrelevant images are used for testing. They form 125 probe images and 900 gallery images in one test.</p><p>Similar to the performance on VIPeR, SCSP significantly outperforms the previous state-of-the-arts, achieving 24.24% rank-1 matching rate. The CMC curves in <ref type="figure" target="#fig_1">Fig. 2b</ref>     show that L-All performs close to that of SCSP, and is much better than G-All, which indicates spatial constraints make a more important contribution on GRID.</p><p>3DPES <ref type="bibr" target="#b1">[2]</ref>. The 3DPES dataset includes 1011 images of 192 persons captured from 8 outdoor cameras with significantly different viewpoints. The image number of each person varies from 2 to 26. We utilize the same protocol with <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b24">25]</ref>, where the images of 96 persons are used for training and those of the remaining 96 persons are used for testing. As each person has more than two images, to reduce the computational burden of training, we take the mean descriptor of a person as the probe descriptor x n in Eq. 9.</p><p>The comparison results are shown in <ref type="figure" target="#fig_1">Fig. 2c</ref> and Tab. 3. Our method achieves the best on rank-1 and rank-5, but performs worse than ME <ref type="bibr" target="#b24">[25]</ref> and kLFDA <ref type="bibr" target="#b29">[30]</ref> on rank-20. The reason may be that both methods utilize non-linear kernels, which are effectiveness on this dataset, while our final similarity function (Eq. 6) is linear. Market-1501 <ref type="bibr" target="#b35">[36]</ref>. Market-1501 is a newly proposed largescale dataset containing the images of 1501 persons. It consists of three parts: the training set containing of 12936 images about 751 persons, the test set containing 19732 images about the remaining 750 persons, and the query set containing 3368 images about the same 750 persons with the test set. In testing, the query set is used as probe set and the test set is used as gallery set. The training process is same with that over 3DPES, but as the gallery set has multiple images of a person, the evaluation process is slightly different. Here, the top-n matching rate indicates the expectation of finding any one of the correct matched images. Besides, mAP <ref type="bibr" target="#b35">[36]</ref> is used to evaluate the performance.</p><p>Our approach again obtains superior results as shown in <ref type="figure" target="#fig_1">Fig. 2d</ref> and Tab. 3. However, we find G-All performs better than L-All, indicating imposing spatial constraint may have negative effects on Market-1501. We attribute the less effectiveness of spatial constraints to the severe misalignments of body parts. As shown in <ref type="figure" target="#fig_2">Fig. 3</ref>, the corresponding local regions may contain totally different parts, which violates our initial assumption about local matching. We introduce spatial constraints to handle local variation, but if the local regions are not even roughly associated, our method will hardly have any effects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Empirical Analysis of the Proposed Method</head><p>We perform empirical analysis of our approach on the VIPeR dataset with 316 gallery images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">Effect of Major Components</head><p>Effect of spatial constraints. We study the effect of spatial constraints by observing how the performance changes with the number of the stripes. In particular, we construct a series of variants by dividing the images into {1, 2, 4, 8, 16, 32} horizontal stripes, where the variant with only one stripe corresponds to the global similarity and other variants correspond to local similarities with different extents of spatial constraints. All the variants are trained with cue C 1 .   The results in <ref type="figure" target="#fig_3">Fig. 4</ref> show that the integrated local similarity is more effective than global similarity even by dividing the images into 2 stripes. Generally, more stripes tend to yield higher rank-1 matching rate. The results increase quickly with the strip number up to 8 and stay stable afterwards. The situation is slightly different for rank-10 matching rate, it shows that employing more than 4 stripes will decrease the performance. One possible reason is that small stripes are less robust when the persons in two images are misaligned along the vertical direction. The computational complexity is linear w.r.t. the number of stripes, we select the stripe number R = 4 in this work, which turns out to be a suitable mediation between effectiveness and efficiency.</p><p>To better understand the effectiveness of spatial constraints, we compare SCSP with other metric learning methods using a single visual cue C 1 , These methods include LADF <ref type="bibr" target="#b17">[18]</ref>, KISSME <ref type="bibr" target="#b14">[15]</ref>, MFA-χ 2 <ref type="bibr" target="#b29">[30]</ref> and XQDA <ref type="bibr" target="#b18">[19]</ref>. We also evaluate our global and local similarity using C 1 , denoted by G-C 1 and L-C 1 . In Tab. 4, G-C 1 performs close to XQDA, L-C 1 evidently improves G-C 1 by considering the spatial constraints. SCSP-C 1 takes advantages of the two, achieving 46.65% rank-1 matching rate. Effect of multi-cue integration. We investigate the effect of multi-cue integration for both integrated local similarity  (R=4) and global similarity. We compare the variants using 4 visual cues (L-All,G-All) with the variants using a single visual cue. In Figs. 5a and 5b, L-C 1 , L-C 2 , L-C 3 , L-C 4 denote the variants using corresponding visual cues independently for the integrated local similarity, and G-C 1 ,G-C 2 , G-C 3 , G-C 4 are the variants for the global similarity.</p><p>The two figures reflect that (1) multiple cue collaboration actually improves the variants using each visual cue individually and (2) the integrated local similarity outperforms global similarity using all kinds of visual cues. In the future, we will incorporate high-level feature descriptors such as CNN <ref type="bibr" target="#b7">[8]</ref> and fisher vector <ref type="bibr" target="#b22">[23]</ref>, which have different properties from low-level features, to further improve the performance.</p><p>Effect of global-local collaboration. To verify the benefits of global-local collaboration, we observe the performance by adjusting the hyper-parameter γ in Eq. 6. When γ = 0, SCSP degenerates to be L-All, the global similarity gradually takes a more important role as γ increases. The trend of rank-1 and rank-10 matching rates with respect to γ are demonstrated Figs. 6a and b. With γ = 1.1, the collaborative model can achieve 53.54% rank-1 matching rate, which outperforms the rate of local similarity 51.04% and the rate of global similarity 48.10%. The matching rate on rank-10 is not significantly influenced by the collaboration, but keeps consistency with the rank-1 matching rates. Effect of joint learning. Intuitively, integrating multiple distinguished similarity measurements will generally improve the performance, but how to effectively take their complementary strength still remains an open problem. The proposed joint learning goes beyond the sum fusion. It not only selects effective feature from each feature map but also makes consistencies between different feature maps. To verify this, we decompose SCSP into 20 similarity functions (5 regions, 4 cues), train them independently, and fuse them by sum. The comparison results are shown in Tab. 5, where SCSP consistently outperforms sum-SCSP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">Other Properties</head><p>Influence of training parameters. The trade-off parameter λ in Eq. 10 and penalty parameter ρ in Eq. 14 are  analyzed. As λ and ρ are mutually influenced, we show how the performance changes w.r.t. λ in <ref type="figure">Fig. 7a</ref> by fixing ρ = 10 −3 , and show the influence of ρ in <ref type="figure">Fig. 7b</ref> by fixing λ = 3 × 10 −4 . It can be seen that too large or too small λ will lead to inferior results. This is because large λ will impose over-sparse while small λ will cause over-fitting. The influence of ρ is a little complex, but the performance w.r.t. ρ is less sensitive than that w.r.t. λ. Sensitivity to occlusion. As each similarity measurement in our SCSP is associated with one local region. Once some region is occluded, the similarity measurements for other regions still work. Such mechanism implies that SCSP is potentially robust to occlusion. To verify this point, we design the following experiments to compare the performance of SCSP, L-All and G-All when occlusion happens.</p><p>In the experiment, we modify the probe images with various occlusion patterns at the test stage. In particular, 9 occlusion patterns given in <ref type="figure">Fig. 8a</ref> are randomly assigned to each probe image. The CMC curves show that all the three methods decrease heavily due to the occlusion (from solid line to dash line). The rank-1 matching rates of SCSP, L-All and G-All decrease 22.31%, 20.22% and 24.56%, respectively. In particular, L-All, which employs only local similarities, is the least influenced by occlusion. Effectiveness of different local regions. It is interesting to investigate which region is most effective in SCSP. At the testing stage, we only fire the similarities measurement for a single region and set the similarity scores of other regions to be 0. The CMC curves in <ref type="figure">Fig. 9b</ref> show that the similarity measurement of the whole region evidently outperforms the one of any local region. For local similarity measurements, the ones for upper body are more effective than those for lower body. In particular, the measurement of Region2 including the torso achieves the highest rank-1 matching rate 26.46%, while the measurement of Region1 gradually performs better when the rank increases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Runtime.</head><p>Our method was implemented in MAT-LAB/MEX with a 3.07Ghz, 2 Cores CPU. For 128×48 person images, it takes about 0.02 second(s) per image to extract the raw features. Taken VIPER for example, at training stage, it takes about 300s to learn 20 PCA projection matrices of 632 training images, and further takes about 105s to generate both positive and negative polynomial feature maps for 316 persons. ADMM spends about 6s. At test stage, it requires 0.016s to rank 316 gallery images for a probe image. Note that we don't need to explicitly generate polynomial feature map for testing, because SCSP can be decomposed into basic similarities related to Mahalanobis distance and bilinear similarity. The testing cost is linear w.r.t. (R+1) and C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>We have proposed a novel similarity learning approach by imposing spatial constraints. We grounded our similarity function upon the polynomial feature maps, formulated a convex objective function and provided its optimization algorithm. The effectiveness of our method stems from the spatial constraints, which reduces the risk of mismatching, increases robustness to occlusion and is more flexible to handle pose variation. Our method also benefits from the multiple cue integration that is complementary to the spatial constraints. The performance is further improved by localglobal similarity collaboration that measures the similarity in different scales. In the future, we will extend our framework by adopting other local region association strategies or by incorporating other types of features, which is expected to achieve better performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>System overview. (a) The process of matching representation for r-th local region with c-th visual cue, which includes patch feature extraction, feature concatenation, PCA based dimensionality reduction and feature map generation. (b) The flowchart of our similarity function. Our similarity combines one global similarity for the whole image region and multiple local similarities for associated local regions with multiple visual cues.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>CMC curves on VIPeR (316 gallery images)CMC curves on GRID (900 gallery images) CMC curves on 3DPES (96 gallery images) CMC curves on Market-1051 ( 750 gallery persons) CMC curves for method comparison on (a) the VIPeR dataset with 316 gallery images, (b) the GRID dataset with 900 gallery images, (c) the 3DPES dataset with 96 gallery images, (d) the Market-1501 dataset with 750 gallery persons. The rank-1 matching rates are shown after the method names.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Sample images of VIPeR, GRID, 3DPES and Market-1501. Images in the same column represent the same person.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>the VIPeR dataset with visual cue C1 Rank-10 on the VIPeR dataset with visual cue C1 Local similarity analysis: We report how the performance changes with the number of stripes on (a) rank-1 matching rate, (b) rank-5 matching rate. All the experiments are trained and tested with cue C 1 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>VIPeR with 316 gallery images CMC on VIPeR with 316 gallery images Multi-cue integration analysis: We compare the variant using 4 visual cues with the 4 variants using a single visual cue in (a),(b), where variants in (a) measure the local similarities, while the variants in (b) measures the global similarities.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :Figure 7 :</head><label>67</label><figDesc>Global-local collaboration analysis: (a) The rank-1 matching rate and (b) the rank-10 matching rate of SCSP with respect to γ. Parameter analysis: The rank-1 matching rate with respect to (a) parameter λ when ρ = 10 −3 ; (b) parameter ρ when λ = 3 × 10 −4 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :Figure 9 :</head><label>89</label><figDesc>Sensitivity to Occlusion. (a) 9 occlusion patterns for probe images when testing. (b) Influences of occlusion for SCSP, L-All, G-All. Effectiveness of each region. (a) SCSP are decomposed into 5 components associated with 5 regions. (b) Effectiveness of the 5 regions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>, 2 :</head><label>2</label><figDesc>Output: Coefficient U 3: for l = 0, ..., L−1 (until convergence) do by applying prox-operator in Eq. 17</figDesc><table>4: 

Update U l+1 

1 

by solving Eq. 16 

5: 

Update U l+1 

2 

6: 

Update U l+1 

3 

by projection in Eq. 18 

7: 

Update Λ l+1 

1 

and Λ l+1 

2 

8: end for 

9: 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 1 :</head><label>1</label><figDesc>Comparison of top-n matching rate(%) on the VIPeR dataset.</figDesc><table>Methods 
Top n matching rate (%) on VIPeR 
r = 1 
r = 5 
r = 10 r = 15 r = 20 
RPLM[13] 
27.34 
55.30 
69.02 
77.12 
82.69 
LADF[18] 
29.99 
64.71 
79.00 
86.71 
91.29 
kLFDA[30] 
32.38 
65.88 
79.82 
86.79 
90.83 
LOMO+XQDA[19] 
40.00 
68.13 
80.51 
87.37 
91.08 
Polymap [4] 
36.77 
70.35 
83.70 
88.73 
91.74 
Mirror-KMFA [5] 
42.97 
75.82 
87.28 
--
94.84 
LMF+LADF [35] 
43.39 
73.04 
84.87 
90.85 
93.70 
ME [25] 
45.89 
77.40 
88.87 
93.52 
95.84 
G-All 
48.10 
79.30 
89.78 
93.48 
95.76 
L-All 
51.04 
81.39 
90.35 
94.49 
96.30 
SCSP 
53.54 
82.59 
91.49 
95.09 
96.65 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 2 :</head><label>2</label><figDesc>Comparison of top-n matching rate(%) on the GRID dataset.</figDesc><table>Methods 
Top n matching rate (%) on GRID 
r = 1 
r = 5 
r = 10 r = 15 r = 20 
Mrank-PRDC[21] 
11.12 
26.08 
35.76 
41.76 
46.56 
Mrank-RankSVM[21] 
12.24 
27.84 
36.32 
42.24 
46.56 
Polymap[4] 
16.32 
35.84 
46.00 
52.80 
57.60 
XQDA+LOMO[19] 
16.56 
33.84 
41.84 
47.68 
52.40 
G-All 
19.20 
39.84 
49.44 
55.04 
59.36 
L-All 
22.72 
42.80 
52.40 
58.72 
63.92 
SCSP 
24.24 
44.56 
54.08 
59.68 
65.20 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="true"><head>Table 3 :</head><label>3</label><figDesc>Method comparison on the 3DPES and Market-1501 datasets.</figDesc><table>Methods 
3DPES 
Methods 
Market-1501 
r = 1 
r = 5 
r = 20 
r = 1 
mAP 
LF[26] 
45.50 69.18 86.06 
BoW[36] 
34.38 14.10 
ME[25] 
53.30 76.79 92.78 
+Mahalanobis 36.79 15.08 
kLFDA[30] 54.02 77.74 92.38 
+KISSME[15] 42.70 19.55 
G-All 
52.86 76.45 90.49 
G-All 
51.10 25.47 
L-All 
54.91 76.23 89.93 
L-All 
49.55 23.83 
SCSP 
57.29 78.97 91.51 
SCSP 
51.90 26.35 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4 :</head><label>4</label><figDesc>Comparison with other Metric learning method using cue C 1 .</figDesc><table>LADF KISSME MFA-χ 2 XQDA G-C 1 
L-C 1 
SCSP-C 1 
r=1 
26.96 
33.96 
35.57 
37.09 
37.31 
43.70 
46.65 
r=10 69.30 
78.99 
79.81 
79.68 
83.45 
87.47 
88.67 
r=20 81.80 
90.32 
90.19 
90.03 
94.40 
96.36 
95.73 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Comparison between joint learning and sum fusion of multiple 
similarity measurements. 

Methods 
r = 1 
r = 5 
r = 10 
r =15 
r= 20 
sum-SCSP 
49.49 
79.78 
90.09 
94.27 
96.20 
SCSP 
53.54 
82.59 
91.49 
95.09 
96.65 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Person re-identification using spatial covariance regions of human body parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Corvée</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Brémond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Thonnat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Advanced Video and Signal Based Surveillance</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Sarc3d: a new 3d body model for people tracking and re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Baltieri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vezzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cucchiara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Image Analysis and Processing</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Large scale online learning of image similarity through ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chechik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Shalit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1109" to="1135" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Similarity learning on an explicit polynomial kernel feature map for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Mirror representation for modeling view-specific transform in person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3402" to="3408" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Custom pictorial structures for re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cristani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stoppa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bazzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Murino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep feature learning with relative distance comparison for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">People re-identification and tracking from multiple cameras: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cicirelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Person re-identification by symmetry-driven accumulation of local features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Farenzena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bazzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Perina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Murino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cristani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Person Re-Identification. Advances in Computer Vision and Pattern Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cristani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Evaluating appearance models for recognition, reacquisition, and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Brennan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on PETS</title>
		<meeting><address><addrLine>Rio de Janeiro</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Relaxed pairwise learned metric for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hirzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kostinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Negative evidences and cooccurences in image retrieval: The benefit of PCA and whitening</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Large scale metric learning from equivalence constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Köstinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hirzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Sparse regression using mixed norms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kowalski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied and Computational Harmonic Analysis</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="303" to="324" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning locally-adaptive decision functions for person verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Person re-identification by local maximal occurrence representation and metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Modeling pixel process with scale invariant local patterns for background subtraction in complex scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kellokumpu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pietikäinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Person re-identification by manifold ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Bicov: a novel image representation for person re-identification and face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jurie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Local descriptors encoded by fisher vectors for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jurie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV Workshops and Demonstrations</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">PCCA: A new approach for distance learning from sparse pairwise constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mignon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jurie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning to rank in person re-identification with metric ensembles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Paisitkriangkrai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Boghossian. Local fisher discriminant analysis for pedestrian reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pedagadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Orwell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Velastin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Person reidentification by support vector ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Prosser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Tracklet association with online target-specific metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Luk Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Shape and appearance context modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Doretto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sebastian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rittscher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Person reidentification using kernel-based metric learning methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">I</forename><surname>Camps</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sznaier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Salient color names for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Model selection and estimation in regression with grouped variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series B (Statistical Methodology)</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="49" to="67" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Person re-identification by salience matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Unsupervised salience learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning mid-level filters for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Scalable person re-identification: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Reidentification by relative distance comparison</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Group association: Assisting re-identification by visual context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Person Re-Identification</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="183" to="201" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
