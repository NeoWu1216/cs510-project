<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CRAFT Objects from Images</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Yang</surname></persName>
							<email>bin.yang@nlpr.ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
							<email>yanjunjie@outlook.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
							<email>zlei@nlpr.ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
							<email>szli@nlpr.ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">CRAFT Objects from Images</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:38+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Object detection is a fundamental problem in image understanding. One popular solution is the R-CNN framework <ref type="bibr" target="#b14">[15]</ref> and its fast versions <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b26">27]</ref>. They decompose the object detection problem into two cascaded easier tasks: 1) generating object proposals from images, 2) classifying proposals into various object categories. Despite that we are handling with two relatively easier tasks, they are not solved perfectly and there's still room for improvement.</p><p>In this paper, we push the "divide and conquer" solution even further by dividing each task into two sub-tasks. We call the proposed method "CRAFT" (Cascade Regionproposal-network And FasT-rcnn), which tackles each task with a carefully designed network cascade. We show that the cascade structure helps in both tasks: in proposal generation, it provides more compact and better localized object proposals; in object classification, it reduces false positives (mainly between ambiguous categories) by capturing both inter-and intra-category variances. CRAFT achieves consistent and considerable improvement over the state-ofthe-art on object detection benchmarks like PASCAL VOC 07/12 and ILSVRC.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The problem definition of object detection is to determine where in the image the objects are and which category each object belongs to. The above definition gives us a clue of how to solve such a problem: by generating object proposals from an image (where they are), and then classifying each proposal into different object categories (which category it belongs to). This two-step solution matches to some extent with the attentional mechanism of humans seeing things, which is to first give a coarse scan of the whole scenario and then focus on regions of our interest.</p><p>As a matter of fact, the above intuitive solution is where * Corresponding author. the research community is moving forward for years. Recently, the two steps (proposal generation and object classification) have been solved quite satisfactorily by two advances in computer vision: first is the introduction of general object proposals, second is the revival of the Convolutional Neural Networks (CNN). The general object proposal algorithm (e.g., Selective Search <ref type="bibr" target="#b33">[34]</ref> and EdgeBox <ref type="bibr" target="#b37">[38]</ref>) can provide around 2000 proposals per image to cover most of the objects and make the employment of more complex classifier for each proposal possible. The prosperity of the Convolutional Neural Networks (CNN) comes from its rich representation capacity and powerful generalization ability in image recognition, which is proved in challenging Ima-geNet classification task <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b28">29]</ref>. With the off-the-shelf methods available, the seminal work R-CNN <ref type="bibr" target="#b14">[15]</ref> shows that Selective Search based region proposals plus the CNN based object classifier can achieve very promising performance in object detection. The R-CNN framework is further improved by Fast R-CNN <ref type="bibr" target="#b13">[14]</ref> and Faster R-CNN <ref type="bibr" target="#b26">[27]</ref>, while the former enables end-to-end learning of the whole pipeline, and the latter introduces the Region Proposal Network (RPN) to get object proposals of higher quality.</p><p>Although the R-CNN framework achieves superior performance on benchmarks like PASCAL VOC, we discover quite large room for improvement after a detailed analysis of the result on each task (proposal generation and classification). We claim that there exists an offset between current solution and the task requirement, which is the core problem of the popular two-step framework. Specifically, in proposal generation, the task demands for proposals for only objects, but the output of general object proposal algorithms still contains a large proportion of background regions. In object classification, the task requires classification among objects, while practically in R-CNN it becomes classification among object categories plus background. The existence of many background samples makes the feature representation capture less intra-category variance and more inter-category variance (ie, mostly between the object category and background), causing many false positives between ambiguous object categories (eg, classify tree as potted plant).</p><p>Inspired by the "divide and conquer" strategy, we propose to further divide each task via a network cascade to alleviate the above issues (see <ref type="figure" target="#fig_0">Figure 1</ref> for an illustration). Practically, in proposal generation task, we add another CNN based classifier to distinguish objects from background given the output of off-the-shelf proposal algorithm (eg, Region Proposal Network); and in object classification task, since the N+1 class (N object categories plus background) cross-entropy objective leads the feature representation to learn inter-category variance mainly, we add a binary classifier for each object category in order to focus more on intra-category variance. Through delicate design of the cascade structure in each task, we discover that it helps a lot: object proposals are more compact and better localized, while the detections are more accurate with fewer false positives between ambiguous object categories.</p><p>As a result, the object detection performance gets improved by a large margin. We show consistent and considerable gain over the Faster R-CNN baseline in object detection benchmark PASCAL VOC 07/12 as well as the more challenging ILSVRC benchmark.</p><p>The remainder of the paper is organized as follows. We review and analyze related works in Section 2. Our CRAFT approach is illustrated in Section 3 and validated in Section 4 respectively. Section 5 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>CRAFT can be seen as an incremental work built upon the state-of-the-art two-step object detection framework. In order to give readers a full understanding of our work and the underlying motivation, in this section we first review the development of the two-step framework from the "divide and conquer" perspective. We introduce in turn the significant advances in proposal generation and object classifica-tion respectively. After a summary of the building stones, we briefly introduce some related works that also try to improve upon the state-of-the-art two-step framework and also show our connection with them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Development of the two-step framework</head><p>Proposals are quite important for object detection and diverse methods for object proposal generation are proposed. In case of detecting one particular category of near rigid objects (like faces or pedestrians) with fixed aspect ratio, sliding window mechanism is often used <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b34">35]</ref>. The main disadvantage is that the number of candidate windows can be about O(10 6 ) for an image, therefore limiting the complexity of classifier due to efficiency issues. When it comes to generating proposals covering general objects of various categories and in various shapes, sliding window approach becomes more computationally expensive.</p><p>Many works are proposed to get more compact proposals, which can be divided into two types: the unsupervised grouping style and the supervised classification style. The most popular method in grouping style is the Selective Search <ref type="bibr" target="#b33">[34]</ref>, which hierarchically groups super-pixels generated through <ref type="bibr" target="#b9">[10]</ref> to form general object proposals. Other typical grouping style proposal methods include the Edge-Box <ref type="bibr" target="#b37">[38]</ref> which is faster and MCG <ref type="bibr" target="#b0">[1]</ref> which is more compact. With around 2000 proposals kept for each image, a recall rate of 98% on Pascal VOC and 92% on ImageNet can be achieved. Besides the smaller number of proposals, another advantage of grouping style over sliding window is that proposals at arbitrary scale and aspect ratio can be generated, which provides much more flexibility. Many works have been proposed for further improvement and an evaluation can be found in <ref type="bibr" target="#b15">[16]</ref>.</p><p>In the supervised camp, the proposal generation problem is defined as a classification and/or regression problem. Typical methods include the BING <ref type="bibr" target="#b3">[4]</ref> and Multi-box <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b7">8]</ref>. The BING uses the binary feature and SVM to efficiently classify objects from background. The Multi-box uses CNN to regress the object location in an end-to-end manner. A recently proposed promising solution is the Region Proposal Network (RPN) <ref type="bibr" target="#b26">[27]</ref>, where a multi-task fully convolutional network is used to jointly estimate proposal location and assign each proposal with a confidence score. The number of proposals is also reduced to be less than 300 with higher recall rate. We use the RPN as the baseline proposal algorithm in CRAFT.</p><p>Given object proposals, detection problem becomes an object classification task, which involves representation and classification. Browsing the history of computer vision, the feature representation is becoming more and more sophisticated, from hand-craft Haar <ref type="bibr" target="#b34">[35]</ref> and HOG <ref type="bibr" target="#b6">[7]</ref> to learning based CNN <ref type="bibr" target="#b14">[15]</ref>. Built on top of these feature representations, carefully designed models can be incorporated. The two popular models are the Deformable Part Model (DPM <ref type="bibr" target="#b8">[9]</ref>) and the Bag of Words (BOW <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b2">3]</ref>). Given the feature representation, classifiers such as Boosting <ref type="bibr" target="#b10">[11]</ref> and SVM <ref type="bibr" target="#b4">[5]</ref> are commonly used. Structural SVM <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b17">18]</ref> and its latent version <ref type="bibr" target="#b36">[37]</ref> are widely used when the problem has a structural loss.</p><p>In recent three years, with the revival of CNN <ref type="bibr" target="#b19">[20]</ref>, CNN based representation achieves excellent performance in various computer vision tasks, including object recognition and detection. Current state-of-the-art is the R-CNN approach. The Region-CNN (R-CNN) <ref type="bibr" target="#b14">[15]</ref> is the first to show that Selective Search region proposal and the CNN together can produce a large performance gain, where the CNN is pre-trained on large-scale datasets such as Ima-geNet to get robust feature representation and fine-tuned on target detection dataset. Fast R-CNN <ref type="bibr" target="#b13">[14]</ref> improves the speed by sharing convolutions among different proposals <ref type="bibr" target="#b18">[19]</ref> and boosts the performance by multi-task loss (region classification and box regression). <ref type="bibr" target="#b26">[27]</ref> uses Region Proposal Network to directly predict the proposals and makes the whole pipeline even faster by sharing full-image convolutional features with the detection network. We use the Fast R-CNN as the baseline object classification model in CRAFT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Improvements on the two-step framework</head><p>Based on the two-step object detection framework, many works have been proposed to improve it. Some of them focus on the proposal part. <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b35">36]</ref> find that using the CNN to shrink the proposals generated by grouping style proposals leads to performance gain. <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b20">21]</ref> use CNN cascade to rank sliding windows or re-rank object proposals. CRAFT shares both similarities and differences with these methods. The common part is that we both the "cascade" strategy to further shrink the number of proposals and improve the proposal quality. The discrepancy is that those methods are based on sliding window or grouping style proposals, while ours is based on RPN which already has proposals of much better quality. We also show that RPN proposals and grouping style proposals are somewhat complementary to each other and they can be combined through our cascade structure.</p><p>Some other works put the efforts in improving the detection network (R-CNN and Fast R-CNN are popular choices). <ref type="bibr" target="#b12">[13]</ref> proposes the multi-region pipeline to capture fine-grained object representation. <ref type="bibr" target="#b1">[2]</ref> introduces the Inside-Outside Net, which captures multi-scale representation by skip connections and incorporates image context via spatial recurrent units. These works can be regarded as learning better representation, while the learning objective is not changed. In CRAFT, we identify that current objective function in Fast R-CNN leads to flaws in the final detections, and address this by cascading another com-plementary objective function. In other words, works like <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b1">2]</ref> that aim to learn better representation are orthogonal to our work.</p><p>In a word, guided by the "divide and conquer" philosophy, we propose to further divide the two steps in current state-of-the-art object detection framework, and both tasks are improved considerably via a delicate design of network cascade. Our work is complementary to many other related works as well. Besides these improvements built on the two-step framework, there are also some works <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b25">26]</ref> on end-to-end detection framework that drops the proposal step. However, these methods work well under some constrained scenarios but the performance drops notably in general object detection in unconstrained environment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The CRAFT approach</head><p>In this section we explain why we propose CRAFT, how we design it and how it works. Following the proposal generation and classification framework, we elaborate in turn how we design the cascade structure based on the state-ofthe-art solutions to solve each task better. Implementation details are presented as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Cascade proposal generation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Baseline RPN</head><p>An ideal proposal generator should generate as few proposals as possible while covering almost all object instances. With the help of strong abstraction ability of CNN deep feature hierarchies, RPN is able to capture similarities among diverse objects. However, when classifying regions, it is actually learning the appearance pattern of an object that distinguishes it from non-object (such patterns may be colorful segments, sharp and closed edges). Therefore its outputs are actually object-like regions. The gap between objectlike regions and the demanded output -object instancesmakes room for improvement. In addition, due to the resolution loss caused by CNN pooling operation and the fixed aspect ratio of sliding window, RPN is weak at covering objects with extreme scales or shapes. On the contrast, the grouping style methods are complementary in this aspect.</p><p>To analyze the performance of the RPN method, we train a RPN model based on the VGG M model (defined in <ref type="bibr" target="#b28">[29]</ref>) using PASCAL VOC 2007 train+val and show its performance in <ref type="table">Table 1</ref> those usually immersed in object clutters, are also difficult to be distinguished from background by RPN, like plant, tv and chair.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Cascade structure</head><p>In order to make a bridge between the object-like regions provided by RPN and the object proposals demanded by the detection task, we introduce an additional classification network that comes after the RPN. According to definition, what we need here is to classify the object-like regions between real object instances and background/badly located proposals. Therefore we take the additional network as a 2class detection network (denoted as FRCN net in <ref type="figure" target="#fig_1">Figure 2</ref>) which uses the output of RPN as training data. In such a cascade structure, the RPN net takes universal image patches as input and is responsible to capture general patterns like texture, while the FRCN net takes input as object-like regions, and plays the role of learning patterns of finer details.</p><p>The advantages of the cascade structure are two-fold: First, the additional FRCN net further improves the quality of the object proposals and shrinks more background regions, making the proposals fit better with the task requirement. Second, proposals from multiple sources can be merged as the input of FRCN net so that complementary information can be used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Implementation</head><p>We train the RPN and FRCN nets consecutively. The RPN net is trained regularly in a sliding window manner to classify all regions at various scales and aspect ratios in the image, with the same parameters as in <ref type="bibr" target="#b26">[27]</ref>. After the RPN net is trained, we test it on the whole training set to produce 2000 primitive proposals of each training image. These proposals are used as training data to train the binary classifier FRCN net. Note that when training the second FRCN net, we use the same criterion of positive and negative sampling At testing phase, we first run the RPN net on the image to produce 2000 primitive proposals and then run FRCN net on the same image along with 2000 RPN proposals as the input to get the final proposals. After proper suppression or thresholding, we can get fewer than 300 proposals of higher quality.</p><p>We use the FRCN net rather than RPN net as the second binary classifier for that FRCN net has more parameters in its higher-level connections, making it more capable to handle with the more difficult classification problem. If we use the model definition of RPN net as the second classifier, the performance degrades. In our current implementation, we do not share full-image convolutional features between RPN net and FRCN net. If we share them, we expect little performance gain as in <ref type="bibr" target="#b26">[27]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Cascade object classification</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Baseline Fast R-CNN</head><p>A good object classifier is supposed to classify each object proposal correctly into certain number of categories. Due to the imperfection of the proposal generator, there exists quite a large number of background regions and badly located proposals in the proposals. Therefore when training the object classifier, an additional object category is often added as "background". In the successful solution Fast R-CNN, the classifier is learned with a multi-class crossentropy loss through softmax layer. Aided by the auxiliary loss of bounding box regression, the detection performance is superior to "softmax + SVM" paradigm in R-CNN approach. In order to get an end-to-end system, Fast R-CNN drops the one-vs-rest SVM in R-CNN, which creates the gap between the resulting solution and the task demand.</p><p>Given object proposals as input and final object detections as output, the task demands for not only further distinguishing objects of interested categories from non-objects, but also classifying objects into different classes, especially those with similar appearance and/or belong to semantically related genres (car and bus, plant and tree). This calls for a feature representation that captures both the inter-category and intra-category variances. In the case of Fast R-CNN, the multi-class cross-entropy loss is responsible for helping the learned feature hierarchies capture inter-category variance, while it is weak at capturing intra-category variance as the "background" class usually occupies a large proportion of training samples. Example detection results of Fast R-CNN are shown in <ref type="figure" target="#fig_2">Figure 3</ref>, where the mis-classification error is a major problem in the final detections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Cascade structure</head><p>To ameliorate the problem of too many false positives caused by mis-classification, we bring the one-vs-rest classifier back in the form of an additional two-class crossentropy loss for each object category (shown in <ref type="figure">Figure 4</ref>). In essence, the added classifier is playing the role of SVM in R-CNN framework. We find it important to train each one-vs-rest classifier using the detection output of that specific category (meaning the detection should have highest score on that specific category). In this way, each one-vsrest classifier sees proposals specific to one particular object category (also containing some false positives), making it focused at capturing intra-category variance.</p><p>For example, in PASCAL VOC dataset, the training samples for the additional classifier of class "potted plants" are usually trees, grass, potted plants and some other green things. After the training, it is able to capture the minute difference between various types of plants, so as to reduce false positives related to this class. This effect can hardly be achieved through a multi-class cross-entropy loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Implementation</head><p>During the training phase, a standard FRCN net (FRCN-1) is first trained using object proposals from the cascade <ref type="figure">Figure 4</ref>. The work flow of the cascade proposal classifier. We first train a standard Fast-RCNN network (FRCN-1) and use its output scores to assign each detection with a class label. Then detections belonging to "background" are discarded and the rest are used to train another Fast-RCNN network (FRCN-2) whose loss is the sum of N two-class softmax losses. Note that the auxiliary bounding box regression loss is also used in both FRCN nets but left out in the figure for better presentation. The two FRCN nets are optimized consecutively with shared convolution weights so that the image feature maps are computed only once during testing phase.</p><p>proposal generator. Thereafter, we train another FRCN net (FRCN-2) based on the output of FRCN-1 (which we call primitive detections). Since we are now dealing with classification task among objects, we discard the primitive detections which are classified as "background". The objective function of the FRCN-2 is the sum of N 2-class crossentropy losses (N equals the number of object categories), with each 2-class classifier depends only on primitive detections assigned with the corresponding class label. The criterion of positive and negative sampling for the one-vs-rest classifier is the same as RPN. Practically, there are roughly 20 primitive detections per image used for FRCN-2 training, which is quite limited.</p><p>To effectively train FRCN-2 and efficiently detect objects from proposals, we share the convolution weights of FRCN-1 and FRCN-2 so that the full-image feature maps need only be computed once. That is to say, the convolution weights of FRCN-2 are initialized from FRCN-1 and keep fixed during FRCN-2 training. The fully-connected layers of FRCN-2 are initialized from FRCN-1 as well, and new layers to produce 2N scores and 4N bounding box regression targets are initialized from a gaussian distribution.</p><p>At test time, with 300 object proposals as input, FRCN-1 outputs around 20 primitive detections, each with N primitive scores. Then each primitive detection is again classified by FRCN-2 and the output scores (N categories) is multiplied with the primitive scores (N categories) in a categoryby-category way to get the final N scores for this detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We first validate that the proposed cascade structure does improves the performance of each task in the two-step object detection framework through a delicate design, then we show the overall performance gain in object detection by evaluating CRAFT on benchmarks like PASCAL VOC 07/12 and ILSVRC. Note that we do not share full-image convolutional features between the proposal generation and classification tasks, therefore the proper baseline would be the unshared version of Faster R-CNN <ref type="bibr" target="#b26">[27]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Proposal generation</head><p>Firstly we justify the design choice of the cascade proposal generator. We answer two questions: 1) do we really need a more complex network in the second stage? 2) do we need the strict sampling criterion during training? We show evaluation of different parameterization in <ref type="table">Table 2</ref>. Since RPN already performs quite well on PASCAL VOC benchmarks, we show parameterization evaluation on the more challenging ILSVRC dataset, and then present a thorough evaluation of the final design of the cascade proposal generator on PASCAL VOC.</p><p>We show comparison of different choices of the sampling criterion and network definition of the cascade binary classifier in <ref type="table">Table 2</ref>. All models in the table are initialized from a pre-trained VGG19 model, trained on the ILSVRC DET train+val1 sets and tested on val2 set 1 , with a evaluation metric of 0.5IoU threshold. To handle with small objects in ILSVRC, we add two additional anchor scales (64 and 32) in RPN and change the batch-size to 2 images. However, the RPN's performance (89.94%) is still inferior to Selective Search (92.09%). When cascaded with an additional binary classifier ("+FRCN"), the recall rate increases by over 2%.</p><p>We show that the strict sampling criterion (0.7IoU threshold for positives, 0.3IoU threshold for negatives) leads to slightly better performance. When we replace the FRCN with a RPN-like network definition which uses a 512-d feature representation rather than 4096-d, the recall rate degrades. With the best design choice, we can achieve higher recall rate (92.37%) than Selective Search with only 300 proposals.</p><p>Next we throughly evaluate our cascade proposal generator on PASCAL VOC in <ref type="table">Table 3</ref>. Baselines are Selective Search ("SS"), RPN (VGG16 net with 512-d feature representation), and RPN L (VGG16 net with 4096-4096-d feature representation, meaning larger RPN). We evaluate with regard to not only recall rates at different IoU thresholds (from 0.5 to 0.9), but also the mAP of a Fast R-CNN detector trained on different proposal algorithms, which makes the comparison more meaningful because the proposals are eventually used for object detection. Note than all the methods in <ref type="table">Table 3</ref> are purely for proposal task without joint optimization with object detection. <ref type="bibr" target="#b0">1</ref> The splits of "val1" and "val2" are the same as <ref type="bibr" target="#b14">[15]</ref>   <ref type="table">Table 3</ref>. Proposal evaluation by recall rate (%) with regard to different IoUs and detection mAP (%) on PASCAL VOC 07 test set. All CNN based methods use VGG16 net as initialization and are trained on PASCAL VOC 07+12 trainval set. "Ours" keeps fixed number of proposals per image (same as RPN), while "Ours S" keeps proposals whose scores (output of the cascaded FRCN classifier) are above a fixed threshold.</p><p>From the table we can see that: (1) RPN proposals aren't so well localized compared with bottom-up methods (low recall rates at high IoU thresholds). (2) This cannot be ameliorated by using a larger network because it is caused by fixed anchors. (3) Our cascaded proposal generator not only further eliminates background proposals, but also brings better localization, both help in detection AP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Object classification</head><p>In this part we justify the use of the one-vs-rest classifier as well as explain how many layers to fine-tune for the onevs-rest classifier. We show the evaluation results in <ref type="table">Table  4</ref>.</p><p>The cascade object classifier can be regarded as a concatenation of two FRCN nets <ref type="figure" target="#fig_0">(FRCN-1 + FRCN-2)</ref>. In the top table in <ref type="table">Table 4</ref> we compare several strategies concerning training the FRCN-2 net, which are no fine-tuning ("the same"), fine-tuning the additional one-vs-rest classifier weights ("cls"), fine-tuning layers above the last convolution maps ("fc + clf"), and fine-tuning all layers except for conv1 ("conv + fc + clf"). In fact, "the same" is simply running the FRCN-1 net twice, and hopefully the iterative bounding box regression would help improve the result. Another three fine-tuning strategies are trying to improve the performance by introducing additional class-specific one-vs-rest classifiers to capture intra-category variance.</p><p>The difference between these three is different level of feature sharing with FRCN-1 net: "clf" uses exactly the same feature representation as FRCN-1, and "conv + fc + clf" trains totally new feature representation for itself. From the results in the table we can see that iteratively detecting twice improves the results a little, which mainly comes from iterative bounding box regression. As for finetuning the net with a binary softmax loss, different settings vary in performance. In a word, through sharing convolutional features but fine-tuning high-level connections we get best result. There are two possible reasons that account for it: 1) the training samples for the FRCN-2 are limited and biased; 2) we just want to learn another classifier rather than learn total different feature representation. What's more, the improvement gained by the cascade approach is more than that by iteratively detecting twice, proving that the one-vsrest softmax loss does play part of the role of hard negative mining and helps reduce the mis-classification error.</p><p>We additionally justify the cascade structure in the bottom table in <ref type="table">Table 4</ref>. One-vs-rest classifier alone performs poorly because each binary classifier has to handle with objects of various classes but binary label provides limited information, while in our case each binary classifier only handles with detections of one class (ie, detection output of the FRCN-1 net), making it more specialized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Object detection</head><p>After showing the superiority of CRAFT on both tasks in the two-step object detection framework, we now evaluate CRAFT on object detection benchmarks. We first evaluate CRAFT on PASCAL VOC 07&amp;12 in comparison with the state-of-the-art detectors Fast R-CNN and Faster R-CNN, and then show our results on the more challenging ILSVRC benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">PASCAL VOC 2007 &amp; 2012</head><p>We compare CRAFT with state-of-the-art detectors under the two-step detection framework, which are Fast R-CNN <ref type="bibr" target="#b13">[14]</ref> and Faster R-CNN <ref type="bibr" target="#b26">[27]</ref>. The comparative results on PASCAL VOC 2007 &amp; 2012 are shown in <ref type="table">Table 5</ref>. Qualitative results on PASCAL VOC 2007 test set are shown in <ref type="figure" target="#fig_3">Figure 5</ref>. All methods use VGG16 model, and "RPN un" represents the unshared version of Faster R-CNN. All baseline results are got from original papers or by running the original open source codes. On PASCAL VOC 2007, all methods use 07+12 trainval as training data. CRAFT outperforms the baseline "RPN un" by 4.1% absolute value in mAP (from 71.6% to 75.7%). On PASCAL VOC 2012, all methods use 12 trainval as training data, and this time CRAFT achieves an edge of 5.8% absolute value in mAP (from 65.5% to 71.3%).</p><p>We do not compare with many other detectors which also improve over the basic two-step detection framework like <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b1">2]</ref> because we believe that their contributions are orthogonal to ours. If we incorporate their methods in CRAFT, as well as using end-to-end multi-task network cascade training <ref type="bibr" target="#b5">[6]</ref>, we expect notable further improvement. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">ILSVRC object detection task</head><p>We validate that CRAFT generalizes well to large-scale problems like 200-class ILSVRC object detection task. As shown in <ref type="table">Table 2</ref>, RPN does not generalize very well to large-scale object detection tasks even if more scales are added to the anchors. However, with the help of our cascade structure, the recall rates boosts to be over Selective Search. However, the performance is still inferior to that on PASCAL VOC. Therefore, we add additional some additional modules to the cascade proposal generator to further improve its performance.</p><p>As shown in <ref type="table">Table 6</ref> top, using a stricter NMS policy (0.6 IoU threshold) increases the recall rate a bit because the localization accuracy of proposals has already been improved after the cascade structure. Re-scoring each proposal by considering both scores from two stages of cascade structure also helps. Finally, fusion of multiple proposal sources boosts the recall rate to be over 94%. We combine proposals output from "DeepBox" <ref type="bibr" target="#b20">[21]</ref> or "SS" (Selective Search) with the RPN proposals as the fusion input to the FRCN net in the cascade structure. Results show that "DeepBox" is better than "SS". Given high-quality object proposals, we train a regular Fast R-CNN detector and a cascade object classifier upon it. We use a GoogLeNet model with batch normalization <ref type="bibr" target="#b16">[17]</ref> (8.4% top-5 validation error on ILSVRC image classification task) as network initialization. We use ILSVRC 2013train + 2014train + val1 as training set, and evaluate it on val2 set. Since 2013train set does not align well with detection task, we adopt the following batch sampling strategy: each batch is made up of 12 images, with 8 from fully annotated sets (2014train + val1) and 4 from partially annotated sets (2013train). For each fully annotated image, we sample 32 proposals with 8 positives and 24 negatives, and the IoU threshold for distinguishing positives and negatives is 0.5. For each partially annotated image, we sample 8 proposals with 2 positives and 6 negatives, and the IoU range for positives is larger than 0.7 and for negatives it is smaller than 0.5. Due to large batch-size, we train the detector on a 4-GPU implementation.</p><p>In <ref type="table">Table 6</ref> bottom, a regular Fast R-CNN detector achieves 47.0% mAP on val2, which already surpasses the ensemble result of previous state-of-the-art systems like Superpixel Labeling <ref type="bibr" target="#b35">[36]</ref> and Deepid-Net <ref type="bibr" target="#b23">[24]</ref>. This edge is basically from better proposals. With cascade object classifier added, the mAP gets additional 1.5% absolute gain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we propose the CRAFT (Cascade Regionproposal-network And FasT-rcnn) for general object detection following the "divide and conquer" philosophy. It improves both the proposal generation and classification tasks through carefully designed convolutional neural network cascades. For the proposal task, CRAFT outputs more compact and better localized object proposals. For detection task, CRAFT helps the network learn both inter-and intracategory variances so that false positives among ambiguous categories are largely eliminated. CRAFT achieves consistent and considerable improvements over state-of-theart methods on PASCAL VOC and ILSVRC benchmarks, while being complementary to many other advances in object detection.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Overview of the widely used two-step framework in object detection, and the proposed CRAFT pipeline.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>The pipeline of the cascade proposal generator. We first train a standard Region Proposal Network (RPN net) and then use its output to train another two-class Fast-RCNN network (FRCN net). During testing phase, the RPN net and the FRCN net are concatenated together. The two nets do not share weights and are trained separately from the same pre-trained model. as in RPN (above 0.7 IoU for positives and below 0.3 IoU for negatives).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Example detections from a Fast-RCNN model. Different colors indicate different object categories. Specifically, orange color denotes "train", red denotes "boat" and blue denotes "potted plant".</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Example detections of CRAFT on PASCAL VOC 2007 test set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>. The recall rates in the table are calculated with 0.5 IoU (intersection of union) criterion and 300 proposals per image on the PASCAL VOC 2007 test set. The overall recall rate of all object categories is 94.87%, but the recall rate on each object category varies a lot. In accordance with our assumption, objects with extreme aspect ratio and scale are hard to be detected, such as boat and bottle. What's more, objects with less appearance complexity, orTable 1. Recall rates (%) of different classes of objects on VOC2007 test set, using 300 proposals from a Region Proposal Network for each image. The overall recall rate is 94.87%, and categories that get lower recall rates are highlighted. VGG M model is used as network initialization.</figDesc><table>aero 

bike 
bird 
boat 
bottle 
95.44 98.81 93.90 92.78 80.38 
bus 
car 
cat 
chair 
cow 
98.12 96.00 99.16 91.80 99.18 
table 
dog 
horse mbike persn 
95.15 99.59 97.70 96.31 95.49 
plant sheep 
sofa 
train 
tv 
86.87 98.76 98.74 97.52 90.58 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head></head><label></label><figDesc>Table 5. Object detection mAP (%) on PASCAL VOC 07+12. "voc07": 07+12 trainval for training, VGG16 net. "voc12": 12 trainval for training, VGG16 net. "FRCN" and "RPN" results are from original paper and report. "RPN un" are Faster R-CNN with unshared feature, whose results are got from open source codes (the proper baseline). Joint optimization is not used in "CRAFT", which would otherwise bring some gain. † : http://host.robots.ox.ac.uk:8080/ anonymous/AITNWY.html, ‡ : http://host.robots.ox.ac. uk:8080/anonymous/FFJGZH.html</figDesc><table>method 
proposal classifier voc07 voc12 
FRCN [14] 
SS 
FRCN 
70.0 
65.7 
RPN un [27] 
RPN 
FRCN 
71.6 
65.5  † 
RPN [27] 
RPN 
FRCN 
73.2 
67.0 
CRAFT 
cascade 
FRCN 
72.5 
-
CRAFT 
cascade 
cascade 
75.7 
71.3  ‡ 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head></head><label></label><figDesc>Basic 0.6 NMS re-score +DeepBox +SSTable 6. Top: Recall rate (%) of cascade proposal generator on ILSVRC detection val2 set with regard to 0.5 IoU evaluation metric. Bottom: Detection mAP (%) of CRAFT on ILSVRC detection val2 set in comparison with other state-of-the-art detectors.</figDesc><table>92.37 
93.61 
93.75 
94.13 
93.04 

method 
proposal classifier ilsvrc 
Ouyang et al. [24] 
SS+EB 
RCNN 
45.0 
Yan et al. [36] 
SS+EB 
RCNN 
45.4 
RPN un [27] 
RPN 
FRCN 
45.4 
CRAFT 
cascade 
FRCN 
47.0 
CRAFT 
cascade 
cascade 
48.5 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements The authors were supported by Chinese National Natural Science Foundation Projects #61375037, #61473291, #61572501, #61502491, #61572536, by National Science and Technology Support Program Project #2013BAK02B01, by Chinese Academy of Sciences Project No. KGZD-EW-102-2, and by Au-thenMetric R&amp;D Funds. We thank NVIDIA gratefully for GPU hardware donation and the reviewers for their many constructive comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Multiscale combinatorial grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.04143</idno>
		<title level="m">Insideoutside net: Detecting objects in context with skip pooling and recurrent neural networks</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The devil is in the details: an evaluation of recent feature encoding methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chatfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">BING: Binarized normed gradients for objectness estimation at 300fps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Support-vector networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Instance-aware semantic segmentation via multi-task network cascades</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.04412</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Scalable object detection using deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained partbased models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Efficient graphbased image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Huttenlocher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Additive logistic regression: a statistical view of boosting (with discussion and a rejoinder by the authors). The annals of statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deepboxes: Hunting objects by cascading deep convolutional layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ghodrati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pedersoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Diba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings ICCV 2015</title>
		<meeting>ICCV 2015</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Object detection via a multiregion and semantic segmentation-aware cnn model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">How good are detection proposals, really? In BMVC</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hosang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 32nd International Conference on Machine Learning</title>
		<meeting>The 32nd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Cutting-plane training of structural svms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Finley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-N</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kaiming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xiangyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shaoqing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deepbox: Learning objectness with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="2479" to="2487" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lenc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.06981</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">R-cnn minus r. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Pedestrian detection using wavelet templates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Papageorgiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Osuna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deepid-net: multistage and deformable deep convolutional neural networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="2403" to="2412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Improving the fisher kernel for large-scale image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.02640</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Neural network-based face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rowley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baluja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition. CoRR, abs/1409.1556</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">End-to-end people detection in crowded scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.04878</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep neural networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Large margin methods for structured and interdependent output variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Tsochantaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Altun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">JMLR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Selective search for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">E</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Robust real-time face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Object detection by labeling superpixels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="5107" to="5116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning structural svms with latent variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-N</forename><forename type="middle">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Edge boxes: Locating object proposals from edges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<editor>ECCV. Springer</editor>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
