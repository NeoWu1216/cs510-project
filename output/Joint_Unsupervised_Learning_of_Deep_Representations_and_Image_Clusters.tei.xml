<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Joint Unsupervised Learning of Deep Representations and Image Clusters</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
							<email>jw2yang@vt.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
							<email>parikh@vt.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
							<email>dbatra@vt.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Virginia</forename><surname>Tech</surname></persName>
						</author>
						<title level="a" type="main">Joint Unsupervised Learning of Deep Representations and Image Clusters</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:42+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we propose a recurrent framework for joint unsupervised learning of deep representations and image clusters. In our framework, successive operations in a clustering algorithm are expressed as steps in a recurrent process, stacked on top of representations output by a Convolutional Neural Network (CNN). During training, image clusters and representations are updated jointly: image clustering is conducted in the forward pass, while representation learning in the backward pass. Our key idea behind this framework is that good representations are beneficial to image clustering and clustering results provide supervisory signals to representation learning. By integrating two processes into a single model with a unified weighted triplet loss function and optimizing it endto-end, we can obtain not only more powerful representations, but also more precise image clusters. Extensive experiments show that our method outperforms the stateof-the-art on image clustering across a variety of image datasets. Moreover, the learned representations generalize well when transferred to other tasks. The source code can be downloaded from https://github.com/ jwyang/joint-unsupervised-learning.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>We are witnessing an explosion in visual content. Significant recent advances in machine learning and computer vision, especially via deep neural networks, have relied on supervised learning and availability of copious annotated data. However, manually labelling data is a time-consuming, laborious, and often expensive process. In order to make better use of available unlabeled images, clustering and/or unsupervised learning is a promising direction.</p><p>In this work, we aim to address image clustering and representation learning on unlabeled images in a unified framework. It is a natural idea to leverage cluster ids of images as supervisory signals to learn representations and in turn the representations would be beneficial to image clustering. At a high-level view, given a collection of n s unlabeled images </p><p>where L(·) is a loss function, y denotes the cluster ids for all images, and θ denotes the parameters for representations. If we hold one in {y, θ} to be fixed, the optimization can be decomposed into two alternating steps: </p><p>Intuitively, (2a) can be cast as a conventional clustering problem based on fixed representations, while (2b) is a standard supervised representation learning process.</p><p>In this paper, we propose an approach that alternates between the two steps -updating the cluster ids given the current representation parameters and updating the representation parameters given the current clustering result. Specifically, we cluster images using agglomerative clustering <ref type="bibr" target="#b15">[16]</ref> and represent images via activations of a Convolutional Neural Network (CNN).</p><p>The reason to choose agglomerative clustering is threefold: 1) it begins with an over-clustering, which is more reliable in the beginning when a good representation has not yet been learned. Intuitively, clustering with representations from a CNN initialized with random weights are not reliable, but nearest neighbors and over-clusterings are often acceptable; 2) These over-clusterings can be merged as better representations are learned; 3) Agglomerative clustering is a recurrent process and can naturally be interpreted in a recurrent framework.</p><p>Our final algorithm is farily intuitive. We start with an intial over-clustering, update CNN parameters (2b) using image cluster labels as supervisory signals, then merge clusters (2a) and iterate until we reach a stopping criterion. An outcome of the proposed framework is illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>. Initially, there are 1,762 clusters for MNIST test set (10k samples), and the representations (image intensities) are not that discriminative. After several iterations, we obtain 17 clusters and more discriminative representations. Finally, we obtain 10 clusters which are well-separated by the learned representations and interestingly correspond primarily to the groundtruth category labels in the dataset, even though the representation is learnt in an unsupervised manner. To summarize, the major contributions of our work are: <ref type="bibr" target="#b0">1</ref> We propose a simple but effective end-to-end learning framework to jointly learn deep representations and image clusters from an unlabeled image set; 2 We formulate the joint learning in a recurrent framework, where merging operations of agglomerative clustering are expressed as a forward pass, and representation learning of CNN as a backward pass; 3 We derive a single loss function to guide agglomerative clustering and deep representation learning, which makes optimization over the two tasks seamless; 4 Our experimental results show that the proposed framework outperforms previous methods on image clustering and learns deep representations that can be transferred to other tasks and datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Clustering Clustering algorithms can be broadly categorized into hierarchical and partitional approaches <ref type="bibr" target="#b23">[24]</ref>. Agglomerative clustering is a hierarchical clustering algorithm that begins with many small clusters, and then merges clusters gradually <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b29">30]</ref>. As for partitional clustering methods, the most well-known is K-means <ref type="bibr" target="#b35">[36]</ref>, which minimizes the sum of square errors between data points and their nearest cluster centers. Related ideas form the basis of a number of methods, such as expectation maximization (EM) <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b36">37]</ref>, spectral clustering <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b60">61]</ref>, and non-negative matrix factorization (NMF) based clustering <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b59">60]</ref>.</p><p>Deep Representation Learning Many works use raw image intensity or hand-crafted features <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b49">50]</ref> combined with conventional clustering methods. Recently, representations learned using deep neural networks have presented significant improvements over hand-designed features on many computer vision tasks, such as image classification <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b48">49]</ref>, object detection <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b42">43]</ref>, etc. However, these approaches rely on supervised learning with large amounts of labeled data to learn rich representations. A number of works have focused on learning representations from unlabled image data. One class of approaches cater to reconstruction tasks, such as autoencoders <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b52">53]</ref>, deep belief networks (DBN) <ref type="bibr" target="#b30">[31]</ref>, etc. Another group of techniques learn discriminative representations after fabricating supervisory signals for images, and then finetune them supervisedly for downstream applications <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b54">55]</ref>. Unlike our approach, the fabricated supervisory signal in these previous works is not updated during representation learning. Combination A number of works have explored combining image clustering with representation learning. In <ref type="bibr" target="#b50">[51]</ref>, the authors proposed to learn a non-linear embedding of the undirected affinity graph using stacked autoencoder, and then ran K-means in the embedding space to obtain clusters. In <ref type="bibr" target="#b51">[52]</ref>, a deep semi-NMF model was used to factorize the input into multiple stacking factors which are initialized and updated layer by layer. Using the representations on the top layer, K-means was implemented to get the final results. Unlike our work, they do not jointly optimize for the representation learning and clustering.</p><p>To connect image clustering and representation learning more closely, <ref type="bibr" target="#b57">[58]</ref> conducted image clustering and codebook learning iteratively. However, they learned codebook over SIFT feature <ref type="bibr" target="#b34">[35]</ref>, and did not learn deep representations. Instead of using hand-crafted features, Chen <ref type="bibr" target="#b1">[2]</ref> used DBN to learn representations, and then conducted a nonparametric maximum margin clustering upon the outputs of DBN. Afterwards, they fine-tuned the top layer of DBN based on clustering results. A more recent work on jointly optimizing two tasks is found in <ref type="bibr" target="#b55">[56]</ref>, where the authors trained a task-specific deep architecture for clustering. The deep architecture is composed of sparse coding modules which can be jointly trained through back propagation from a cluster-oriented loss. However, they used sparse coding to extract representations for images, while we use a CNN. Instead of fixing the number of clusters to be the number of categories and predicted labels based on softmax outputs, we predict the labels using agglomerative clustering based on the learned representations. In our experiments we show that our approach outperforms <ref type="bibr" target="#b55">[56]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Notation</head><p>We denote an image set with n s images by I = {I 1 , ..., I ns }. The cluster labels for this image set are y = {y 1 , ..., y ns }. θ are the CNN parameters, based on which we obtain deep representations X = {x 1 , ..., x ns } from I. Given the predicted image cluster labels, we or- Ci is the set of K c nearest neighbour clusters of C i . For convenience, we sort clusters in N Kc Ci in descending order of affinity with C i so that the nearest neighbour argmax C∈C t A(C i , C) is the first entry N Kc</p><formula xml:id="formula_2">y t y t+ 1 Clustering Clustering y T Clustering X t X t+1 X T h t h t+1 h T h t¡ 1 I CNN I I CNN CNN h T ¡1 (µ t+1 ) (µ t ) (µ T )</formula><p>Ci <ref type="bibr" target="#b0">[1]</ref>. Here, A is a function to measure the affinity (or similarity) between two clusters. We add a superscript t to {θ, X, y, C} to refer to their states at timestep t. We use Y to denote the sequence {y 1 , ..., y T } with T timesteps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Agglomerative Clustering</head><p>As background, we first briefly describe conventional agglomerative clustering <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b29">30]</ref>. The core idea in agglomerative clustering is to merge two clusters at each step until some stopping conditions. Mathematically, it tries to find two clusters C a and C b by</p><formula xml:id="formula_3">{Ca, C b } = argmax C i ,C j ∈C,i =j A(Ci, Cj)<label>(3)</label></formula><p>There are many methods to compute the affinity between two clusters <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b63">64]</ref>. More details can be found in <ref type="bibr" target="#b23">[24]</ref>. We now describe how the affinity is measured by A in our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Affinity Measure</head><p>First, we build a directed graph G =&lt; V, E &gt;, where V is the set of vertices corresponding to deep representations X for I, and E is the set of edges connecting vertices. We define an affinity matrix W ∈ R ns×ns corresponding to the edge set. The weight from vertex x i to x j is defined by</p><formula xml:id="formula_4">W (i, j) = exp(− ||xi−xj || 2 2 σ 2 ), if x j ∈ N Ks i 0, otherwise<label>(4)</label></formula><p>where</p><formula xml:id="formula_5">σ 2 = a nsKs xi∈X xj ∈N Ks i ||x i − x j || 2 2</formula><p>. This way to build up a directed graph can be found in many previous works such as <ref type="bibr" target="#b61">[62,</ref><ref type="bibr" target="#b63">64]</ref>. Here, a and K s are two predefined parameters (their values are listed in <ref type="table" target="#tab_1">Table 2</ref>). After constructing a directed graph for samples, we then adopt the graph degree linkage in <ref type="bibr" target="#b61">[62]</ref> to measure the affinity between cluster C i and C j , denoted by A(C i , C j ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">A Recurrent Framework</head><p>Our key insight is that agglomerative clustering can be interpreted as a recurrent process in the sense that it merges clusters over multiple timesteps. Based on this insight, we propose a recurrent framework to combine the image clustering and representation learning processes.</p><p>As shown in <ref type="figure" target="#fig_2">Fig. 2</ref>, at the timestep t, images I are first fed into the CNN to get representations X t and then used in conjunction with previous hidden state h t−1 to predict current hidden state h t , i.e, the image cluster labels at timestep t. In our context, the output at timestep t is y t = h t . Hence, at timestep t</p><formula xml:id="formula_6">X t = fr(I|θ t ) (5a) h t = fm(X t , h t−1 ) (5b) y t = fo(h t ) = h t (5c)</formula><p>where f r is a function to extract deep representations X t for input I using the CNN parameterized by θ t , and f m is a merging process for generating h t based on X t and h t−1 .</p><p>In a typical Recurrent Neural Network, one would unroll all timesteps at each training iteration. In our case, that would involve performing agglomerative clustering until we obtain the desired number of clusters, and then update the CNN parameters by back-propagation.</p><p>In this work, we introduce a partial unrolling strategy, i.e., we split the overall T timesteps into multiple periods, and unroll one period at a time. The intuitive reason we unroll partially is that the representation of the CNN at the beginning is not reliable. We need to update CNN parameters to obtain more discriminative representations for the following merging processes. In each period, we merge a number of clusters and update CNN parameters for a fixed number of iterations at the end of the period. An extreme case would be one timestep per period, but it involves updating the CNN parameters too frequently and is thus timeconsuming. Therefore, the number of timesteps per period (and thus the number of clusters merged per period) is determined by a parameter in our approach. We elaborate on this more in Sec. 3.6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Objective Function</head><p>In our recurrent framework, we accumulate the losses from all timesteps, which is formulated as</p><formula xml:id="formula_7">L({y 1 , ..., y T }, {θ 1 , ..., θ T }|I) = T t=1 L t (y t , θ t |y t−1 , I) (6)</formula><p>Here, y 0 takes each image as a cluster. At timestep t, we find two clusters to merge given y t−1 . In conventional agglomerative clustering, the two clusters are determined by finding the maximal affinity over all pairs of clusters. In this paper, we introduce a criterion that considers not only the affinity between two clusters but also the local structure surrounding the clusters. Assume from y t−1 to y t , we merged a cluster C t i and its nearest neighbour. Then the loss at timestep t is a combination of negative affinities, that is,</p><formula xml:id="formula_8">L t (y t , θ t |y t−1 , I) = −A(C t i , N Kc C t i [1]) (7a) − λ (K c − 1) Kc k=2 A(C t i , N Kc C t i [1]) − A(C t i , N Kc C t i [k])</formula><p>(7b) where λ weighs (7a) and (7b). Note that y t , y t−1 and θ t are not explicitly presented at the right side, but they determine the loss via the image cluster labels and affinities among clusters. On the right side of the above equation, there are two terms: 1) (7a) measures the affinity between cluster C i and its nearest neighbour, which follows conventional agglomerative clustering; 2) (7b) measures the difference between affinity of C i to its nearest neighbour cluster and affinities of C i to its other neighbour clusters. This term takes the local structure into account. See Sec. 3.5.1 for detailed explanation.</p><p>It is hard to simultaneously derive the optimal {y 1 , ..., y T } and {θ 1 , ..., θ T } that minimize the overall loss in Eq. <ref type="bibr" target="#b5">(6)</ref>. As aforementioned, we optimize iteratively in a recurrent process. We divide T timesteps into P partially unrolled periods. In each period, we fix θ and search optimal y in the forward pass, and then in the backward pass we derive optimal θ given the optimal y. Details will be explained in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.1">Forward Pass</head><p>In forward pass of the p-th (p ∈ {1, ..., P }) partially unrolled period, we update the cluster labels with θ fixed to θ p , and the overall loss in period p is</p><formula xml:id="formula_9">L p (Y p |θ p , I) = t e p t=t s p L t (y t |θ p , y t−1 , I)<label>(8)</label></formula><p>where Y p is the sequence of image labels in period p, and [t s p , t e p ] is the corresponding timesteps in period p. For optimization, we follow a greedy search similar to conventional agglomerative clustering. Starting from the time step t s p , it finds one cluster and its nearest neighbour to merge so that L t is minimized over all possible cluster pairs. In <ref type="figure">Fig. 3</ref>, we present a toy example to explain the reason why we employ the term (7b). As shown, it is often the case that the clusters are densely populated in some regions while sparse in some other regions. In conventional agglomerative clustering, it will choose two clusters with largest affinity (or smallest loss) at each time no mater  <ref type="figure">Figure 3</ref>: A toy illustration of (a) conventional agglomerative clustering strategy and (b) the proposed one. For simplification, we use a single circle to represent a cluster/sample. In conventional agglomerative clustering, node b and its nearest neighbour are chosen to merge because they are closest to each other; while node e is chosen in our proposed strategy considering the local structure.</p><p>where the clusters are located. In this specific case, it will choose cluster C b and its nearest neighbour to merge. In contrast, as shown in <ref type="figure">Fig. 3(b)</ref>, our algorithm by adding (7b) will find cluster C e , because it is not only close to it nearest neighbour, but also relatively far away from its other neighbours, i.e., the local structure is considered around one cluster. Another merit of introducing (7b) is that it will allow us to write the loss in terms of triplets as explained next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.2">Backward Pass</head><p>In forward pass of the p-th partially unrolled period, we have merged a number of clusters. Let the sequence of optimal image cluster labels be given by Y p * = {y t * }, and clusters merged in forward pass are denoted by {[C t * , N Kc</p><formula xml:id="formula_10">C t * [1]]}, t ∈ {t s p , ..., t e p }.</formula><p>In the backward pass, we aim to derive the optimal θ to minimize the losses generated in forward pass. Because the clustering in current period is conditioned on the clustering results of all previous periods, we accumulate the losses of all p periods, i.e.,</p><formula xml:id="formula_11">L(θ|{Y 1 * , ..., Y p * }, I) = p k=1 L k (θ|Y k * , I)<label>(9)</label></formula><p>Minimizing (9) w.r.t θ leads to representation learning on I supervised by {Y 1 * , ..., Y p * } or {y 1 * , ..., y t e p * }. Based on (7a) and (7b), the loss in Eq. 9 is reformulated to</p><formula xml:id="formula_12">− λ Kc − 1 t e p t=1 Kc k=2 λ ′ A(C t * , N Kc C t * [1]) − A(C t * , N Kc C t * [k])<label>(10)</label></formula><p>where λ ′ = (1 + 1/λ). (10) is a loss defined on clusters of points, which needs the entire dataset to estimate, making it difficult to use batch-based optimization. However, we show that this loss can be approximated by a samplebased loss, enabling us to compute unbiased estimators for the gradients using batch-statistics. Update y t to y t+1 by merging two clusters <ref type="bibr">5:</ref> if t = t e p then <ref type="bibr">6:</ref> Update θ p to θ p+1 by training CNN <ref type="bibr">7:</ref> p ← (p + 1) t ← t + 1 10: until Cluster number reaches n * c 11: y * ← y t ; θ * ← θ p</p><p>The intuition behind reformulation of the loss is that agglomerative clustering starts with each datapoint as a cluster, and clusters at a higher level in the hierarchy are formed by merging lower level clusters. Thus, affinities between clusters can be expressed in terms of affinities between datapoints. We show in the supplement that the loss in <ref type="formula" target="#formula_0">(10)</ref>  </p><p>where γ is a weight whose value depends on λ ′ and how clusters are merged during the forward pass. x i and x j are from the same cluster, while x k is from the neighbouring clusters, and their cluster labels are merely determined by the final clustering result y t e p * . To further simplify the optimization, we instead search x k in at most K c neighbour samples of x i from other clusters in a training batch. Hence, the batch-wise optimization can be performed using conventional stochastic gradient descent method. Note that such triplet losses have appeared in other works <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b53">54]</ref>. Because it is associated with a weight, we call (11) the weighted triplet loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Optimization</head><p>Given an image dataset with n s samples, we assume the number of desired clusters n * c is given to us as is standard in clustering. Then we can build up a recurrent process with T = n s − n * c timesteps, starting by regarding each sample as a cluster. However, such initialization makes the optimization time-consuming, especially when datasets contain a large number of samples. To address this problem, we can first run a fast clustering algorithm to get the initial clusters. Here, we adopt the initialization algorithm proposed in <ref type="bibr" target="#b62">[63]</ref> for fair comparison with their experiment results. Note that other kind of initializations can also be used, e.g. K-means. Based on the algorithm in <ref type="bibr" target="#b62">[63]</ref>, we obtain a number of clusters which contain a few samples for each (average is about 4 in our experiments). Given these initial clusters, our optimization algorithm learns deep representations and clusters. The algorithm is outlined in Alg. 1. In each partially unrolled period, we perform forward and backward passes to update y and θ, respectively. Specifically, in the forward pass, we merge two clusters at each timestep. In the backward pass, we run about 20 epochs to update θ, and the affinity matrix W is also updated based on the new representation. The duration of the p-th period is n p = ceil(η × n s c ) timesteps, where n s c is the number of clusters at the beginning of current period, and η is a parameter called unrolling rate to control the number of timesteps. The less η is, the more frequently we update θ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Image Clustering</head><p>We compare our approach with 12 clustering algorithms, including K-means <ref type="bibr" target="#b35">[36]</ref>, NJW spectral clustering (SC-NJW) <ref type="bibr" target="#b39">[40]</ref>, self-tuning spectral clustering (SC-ST) <ref type="bibr" target="#b60">[61]</ref>, large-scale spectral clustering (SC-LS) <ref type="bibr" target="#b2">[3]</ref>, agglomerative clustering with average linkage (AC-Link) <ref type="bibr" target="#b23">[24]</ref>, Zeta function based agglomerative clustering (AC-Zell) <ref type="bibr" target="#b63">[64]</ref>, graph degree linkage-based agglomerative clustering (AC-GDL) <ref type="bibr" target="#b61">[62]</ref>, agglomerative clustering via path integral (AC-PIC) <ref type="bibr" target="#b62">[63]</ref>, normalized cuts (N-Cuts) <ref type="bibr" target="#b46">[47]</ref>, locality preserving non-negative matrix factorization (NMF-LP) <ref type="bibr" target="#b0">[1]</ref>, NMF with deep model (NMF-D) <ref type="bibr" target="#b51">[52]</ref>, task-specific clustering with deep model (TSC-D) <ref type="bibr" target="#b55">[56]</ref>.</p><p>For evaluation, we use a commonly used metric: normalized mutual information (NMI) <ref type="bibr" target="#b58">[59]</ref>. It ranges in [0, 1]. Larger value indicates more precise clustering results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Datasets</head><p>We evaluate the clustering performance on two handwritten digit image datasets (MNIST <ref type="bibr" target="#b31">[32]</ref> and USPS 1 ), two multi-view object image datasets (COIL20 and COIL100 <ref type="bibr" target="#b38">[39]</ref>), and four face image datasets (UMist <ref type="bibr" target="#b16">[17]</ref>, FRGC-v2.0 2 , CMU-PIE <ref type="bibr" target="#b47">[48]</ref>, Youtube-Face (YTF)) <ref type="bibr" target="#b56">[57]</ref>. The number of samples and categories, and image size are listed in <ref type="table" target="#tab_0">Table 1</ref>. MNIST consists of training set (60,000) and testing set (10,000). To compare with different approaches, we experiment on the full set (MNIST-full) and testing set (MNIST-test), separately. For face image datasets such as UMist, CMU-PIE, we use the images provided as is without any changes. For FRGC-v2.0 and YTF datasets, we first  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Experimental Setup</head><p>All the hyper-parameters and their values for our approach are listed in <ref type="table" target="#tab_1">Table 2</ref>. In our experiments, K s is set to 20, the same value to <ref type="bibr" target="#b61">[62]</ref>. a and λ are simply set to 1.0. We search the values of K c and γ for best performance on MNIST-test set. The unrolling rate η for first four datasets is 0.9; and 0.2 for face datasets. The target cluster number n * c is set to be the number of categories in each dataset.</p><p>We use Caffe <ref type="bibr" target="#b25">[26]</ref> to implement our approach. We stacked multiple combinations of convolutional layer, batch normalization layer, ReLU layer and pooling layer. For all the convolutional layers, the number of channels is 50, and filter size is 5 × 5 with stride = 1 and padding = 0. For pooling layer, its kernel size is 2 and stride is 2. To deal with varying image sizes across datasets, the number of stacked convolutional layers for each dataset is chosen so that the size of the output feature map is about 10×10. On the top of all CNNs, we append an inner product (ip) layer whose dimension is 160. ip layer is followed by a L2-normalization layer before being fed to the weighted triplet loss layer or used for clustering. For each partially unrolled period, the base learning rate is set to 0.01, momentum 0.9, and weight decay 5 × 10 −5 . We use the inverse learning rate decay policy, with Gamma=0.0001 and Power=0.75. Stochastic gradient descent (SGD) is adopted for optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Quantitative Comparison</head><p>We report NMI for different methods on various datasets. Results are averaged from 3 runs. We report the results by re-running the code released by original papers. For those that did not release the code, the corresponding results are borrowed from the papers. We find the results we obtain are somewhat different from the one reported in original papers. We suspect that these differences may be caused by the different experimental settings or the released code is changed from the one used in the original paper. For all test algorithms, we conduct L2-normalization on the image intensities since it empirically improves the clustering performance. We report our own results in two cases: 1) the straight-forward clustering results obtained when the recurrent process finish, denoted by OURS-SF; 2) the clustering results obtained by re-running clustering algorithm after obtaining the final representation, denoted by OURS-RC. The quantitative results are shown in <ref type="table" target="#tab_2">Table 3</ref>. In the table cells, the value before '/' is obtained by re-running code while the value after '/' is that reported in previous papers.</p><p>As we can see from <ref type="table" target="#tab_2">Table 3</ref>, both OURS-SF and OURS-RC outperform previous methods on all datasets with noticeable margin. Interestingly, we achieved perfect results (NMI = 1) on COIL20 and CMU-PIE datasets, which means that all samples in the same category are clustered into the same group. The agglomerative clustering algorithms, such as AC-Zell, AC-GDL and AC-PIC perform better than other algorithms generally. However, on MNIST-full test, they all perform poorly. The possible reason is that MNIST-full has 70k samples, and these methods cannot cope with such large-scale dataset when using image intensity as representation. However, this problem is addressed by our learned representation. We show that we achieved analogous performance on MNIST-full to MNIST-test set. In most cases, we can find OURS-RC performs better on datasets that have room for improvement. We believe the reason is that OURS-RC uses the final learned representation over the entire clustering process, while OURS-SF starts with image intensity, which indicates that the learned representation is more discriminative than image intensity. <ref type="bibr" target="#b2">3</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.4">Generalization Across Clustering Algorithms</head><p>We now evaluate if the representations learned by our joint agglomerative clustering and representation learning approach generalize to other clustering techniques. We re-run all the clustering algorithms without any changes of parameters, but using our learned deep representations as features. The results are shown in <ref type="table" target="#tab_3">Table 4</ref>. It can be seen that all  clustering algorithms obtain more precise image clusters by using our learned representation. Some algorithms like Kmeans, AC-Link that performed very poorly with raw intensities perform much better with our learned representations, and the variance in performance across all clustering algorithms is much lower. These results clearly demonstrate that our learned representation is not over-fitting to a single clustering algorithm, but generalizes well across various algorithms. Interestingly, using our learned representation, some of the clustering algorithms perform even better than AC-GDL we build on in our approach.  In this section, we study whether our learned representations generalize across datasets. We train a CNN based on our approach on one dataset, and then cluster images from another (but related) dataset using the image features extracted via the CNN. Specifically, we experiment on two dataset pairs: 1) multi-view object datasets (COIL20 and COIL100); 2) hand-written digit datasets (USPS and MNIST-test). We use the representation learned from one dataset to represent another dataset, followed by agglomerative clustering. Note that because the image sizes or channels are different across datasets, we resize the input images and/or expand the channels before feeding them to CNN. The experimental results are shown in <ref type="table" target="#tab_4">Table 5</ref> and 6. We use the representations from top ip layer and also the convolutional or pooling layers (top-1, top-2) close to top layer for image clustering. In two tables, compared with directly using raw image from the data layer, the clustering performance based on learned representations from all layers improve, which indicates that the learned representations can be transferred across these datasets. As perhaps expected, the performance on target datasets is worse compared to learning on the target dataset directly. For COIL20 and COIL100, a possible reason is that they have different image categories. As for MNIST and USPS, the performance beats OURS-SF, but worse than OURS-RC. We find transferring representation learned on MNIST-test to USPS gets close performance to OURS-RC learned on USPS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Transferring Learned Representation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Cross-Dataset Clustering</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Face Verification</head><p>We now evaluate the performance of our approach by applying it to face verification. In particular, the representation is learned on Youtube-Face dataset and evaluated on LFW dataset <ref type="bibr" target="#b21">[22]</ref> under the restricted protocol. For training, we randomly choose about 10k, 20k, 30k, 50k, 100k samples from YTF dataset. All these subsets have 1446 categories. We implement our approach to train CNN model and cluster images on the training set. Then, we remove the L2-normalization layer and append a softmax layer to finetune our unsupervised CNN model based on the predicted image cluster labels. Using the same training samples and CNN architecture, we also train a CNN model with a softmax loss supervised by the groundtruth labels of the training set. According to the evaluation protocol in <ref type="bibr" target="#b21">[22]</ref>, we run 10fold cross-validation. The cosine similarity is used to compute the similarity between samples. In each of 10 crossvalidations, nine folds are used to find the optimal threshold, and the remaining one fold is used for evaluation. The average accuracy is reported in <ref type="table">Table.</ref> 7. As shown, though no groundtruth labels are used for representation learning in our approach, we obtain analogous performance to the supervised learning approach. Our approach even (slightly) beats the supervised learning method in one case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Image Classification</head><p>Recently, unsupervised representation learning methods are starting to achieve promising results for a variety of recognition tasks <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b33">34]</ref>. We are interested in knowing whether the proposed method can also learn useful representation for image classification. We experiment with CIFAR-10 <ref type="bibr" target="#b26">[27]</ref>. We follow the pipeline in <ref type="bibr" target="#b4">[5]</ref>, and base our experiments on their publicly available code. In this pipeline, codebook with 1600 codes is build upon 6 × 6 ZCA-whitened image patches, and then used to code the training and testing samples by extracting 1,600-d feature from each of 4 image quadrants. Afterwards, a linear SVM <ref type="bibr" target="#b5">[6]</ref> is applied for image classification on 6,400-d feature. In our approach, the only difference is that we learn a new representation from 6 × 6 patches, and then use these new representations to build the codebook with 1,600 codes. The CNN architecture we use contains two convolutional layers, each of which is combined with a ReLu and a pooling layer, followed by an inner product layer. Both convolutional layers have 50 3 × 3 filters with pad = 1. The kernel size of pooling layer is 2, and the stride is 2. To save on training time, 40k randomly extracted patches are extracted from 50k training set and used in all the experiments. Classification accuracies on test set with different settings are shown in <ref type="table" target="#tab_7">Table 8</ref>. We vary the number of training samples and evaluate the performance for representations from different layers. As we can see, the combination of representations from the first and second convolutional layer achieve the best performance. We also use the representation output by inner product layer to learn the codebook. However, it performs poorly. A possible reason is that it discards spatial information of image patches, which may be important for learning a codebook. When using 400k randomly extracted patches to learn the codebook, <ref type="bibr" target="#b4">[5]</ref> achieved 77.9%. However, it is still lower than what we achieved. This performance also beats several other methods listed in <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b33">34]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we have proposed an approach to jointly learn deep representations and image clusters. In our approach, we combined agglomerative clustering with CNNs and formulate them as a recurrent process. We used a partially unrolling strategy to divide the timesteps into multiple periods. In each period, we merged clusters step by step during the forward pass and learned representation in the backward pass, which are guided by a single weighted triplet-loss function. The extensive experiments on image clustering, deep representation transfer learning and image classification demonstrate that our approach can obtain more precise image clusters and discriminative representations that generalize well across many datasets and tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Clustering outputs for MNIST<ref type="bibr" target="#b31">[32]</ref> test set at different stages of the proposed method. We conduct PCA on the image representations and then choose the first three dimensions for visualization. Different colors correspond to different clusters. Samples are grouped together gradually and more discriminative representations are obtained. I = {I 1 , ..., I ns }, the global objective function for learning image representations and clusters can be written as:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Proposed recurrent framework for unsupervised learning of deep representations and image clusters. ganize them into n c clusters C = {C 1 , ..., C nc }, where C i = {x k |y k = i, ∀k ∈ 1, ..., n s }. N Ks i are the K s nearest neighbours of x i , and N Kc</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Algorithm 1</head><label>1</label><figDesc>Joint Optimization on y and θInput:I: = collection of image data; n * c : = target number of clusters; Output: y * , θ * : = final image labels and CNN parameters; 1: t ← 0; p ← 0 2: Initialize θ and y 3: repeat 4:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>(xi, xj) − A(xi, x k ))</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><head>Table 1 :</head><label>1</label><figDesc>Datasets used in our experiments.</figDesc><table>Dataset 
MNIST 
USPS 
COIL20 
COIL100 
UMist 
FRGC-v2.0 CMU-PIE 
YTF 

#Samples 
70000 
11000 
1440 
7200 
575 
2462 
2856 
10000 
#Categories 
10 
10 
20 
100 
20 
20 
68 
41 
Image Size 
28×28 16×16 128×128 128×128 112×92 
32×32 
32×32 
55×55 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 2 :</head><label>2</label><figDesc>Hyper-parameters in our approach.</figDesc><table>Hyper-parameter Ks 
a 
Kc 
λ 
γ 
η 

Value 
20 1.0 
5 
1.0 2.0 0.9 or 0.2 

crop faces and then resize them to a constant size. In FRGC-
v2.0 dataset, we randomly choose 20 subjects. As for YTF 
dataset, we choose the first 41 subjects which are sorted by 
their names in alphabet order. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 3 :</head><label>3</label><figDesc>Quantitative clustering performance (NMI) for different algorithms using image intensities as input.</figDesc><table>Dataset 
COIL20 
COIL100 
USPS 
MNIST-test MNIST-full UMist FRGC 
CMU-PIE 
YTF 

K-means [36] 
0.775 
0.822 
0.447 
0.528 
0.500 
0.609 
0.389 
0.549 
0.761 
SC-NJW [40] 0.860/0.889 0.872/0.854 0.409/0.690 0.528/0.755 
0.476 
0.727 
0.186 
0.543 
0.752 
SC-ST [61] 
0.673/0.895 0.706/0.858 0.342/0.726 0.445/0.756 
0.416 
0.611 
0.431 
0.581 
0.620 
SC-LS [49] 
0.877 
0.833 
0.681 
0.756 
0.706 
0.810 
0.550 
0.788 
0.759 
N-Cuts [47] 
0.768/0.884 0.861/0.823 0.382/0.675 0.386/0.753 
0.411 
0.782 
0.285 
0.411 
0.742 
AC-Link [24] 
0.512 
0.711 
0.579 
0.662 
0.686 
0.643 
0.168 
0.545 
0.738 
AC-Zell [64] 
0.954/0.911 0.963/0.913 0.774/0.799 0.810/0.768 
0.017 
0.755 
0.351 
0.910 
0.733 
AC-GDL [62] 0.945/0.937 0.954/0.929 0.854/0.824 0.864/0.844 
0.017 
0.755 
0.351 
0.934 
0.622 
AC-PIC [63] 
0.950 
0.964 
0.840 
0.853 
0.017 
0.750 
0.415 
0.902 
0.697 
NMF-LP [1] 
0.720 
0.783 
0.435 
0.467 
0.452 
0.560 
0.346 
0.491 
0.720 
NMF-D [52] 
0.692 
0.719 
0.286 
0.243 
0.148 
0.500 
0.258 
0.983/0.910 0.569 
TSC-D [56] 
-/0.928 
-
-
-
-/0.651 
-
-
-
-
OURS-SF 
1.000 
0.978 
0.858 
0.876 
0.906 
0.880 
0.566 
0.984 
0.848 
OURS-RC 
1.000 
0.985 
0.913 
0.915 
0.913 
0.877 
0.574 
1.00 
0.848 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="true"><head>Table 4 :</head><label>4</label><figDesc>Quantitative clustering performance (NMI) for different algorithms using our learned representations as inputs.</figDesc><table>Dataset 
COIL20 COIL100 USPS MNIST-test MNIST-full UMist FRGC CMU-PIE 
YTF 

K-means [36] 
0.926 
0.919 
0.758 
0.908 
0.927 
0.871 
0.636 
0.956 
0.835 
SC-NJW [40] 
0.915 
0.898 
0.753 
0.878 
0.931 
0.833 
0.625 
0.957 
0.789 
SC-ST [61] 
0.959 
0.922 
0.741 
0.911 
0.906 
0.847 
0.651 
0.938 
0.741 
SC-LS [49] 
0.950 
0.905 
0.780 
0.912 
0.932 
0.879 
0.639 
0.950 
0.802 
N-Cuts [47] 
0.963 
0.900 
0.705 
0.910 
0.930 
0.877 
0.640 
0.995 
0.823 
AC-Link [24] 
0.896 
0.884 
0.783 
0.901 
0.918 
0.872 
0.621 
0.990 
0.803 
AC-Zell [64] 
1.000 
0.989 
0.910 
0.893 
0.919 
0.870 
0.551 
1.000 
0.821 
AC-GDL [62] 
1.000 
0.985 
0.913 
0.915 
0.913 
0.870 
0.574 
1.000 
0.842 
AC-PIC [63] 
1.000 
0.990 
0.914 
0.909 
0.907 
0.870 
0.553 
1.000 
0.829 
NMF-LP [1] 
0.855 
0.834 
0.729 
0.905 
0.926 
0.854 
0.575 
0.690 
0.788 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 5 :</head><label>5</label><figDesc>NMI performance across COIL20 and COIL100.</figDesc><table>Layer 
data 
top(ip) top-1 top-2 

COIL20 → COIL100 0.924 
0.927 
0.939 0.934 
COIL100 → COIL20 0.944 
0.949 
0.957 0.951 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 6 :</head><label>6</label><figDesc>NMI performance across MNIST-test and USPS.</figDesc><table>Layer 
data 
top(ip) top-1 top-2 

MNIST-test → USPS 0.874 
0.892 
0.907 0.908 
USPS → MNIST-test 0.872 
0.873 
0.886 
-

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 7 :</head><label>7</label><figDesc>Face verification results on LFW.</figDesc><table>#Samples 
10k 
20k 
30k 
50k 
100k 

Supervised 0.737 0.746 0.748 0.764 0.770 
OURS 
0.728 0.743 0.750 0.762 0.767 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="true"><head>Table 8 :</head><label>8</label><figDesc>Image classification accuracy on CIFAR-10.</figDesc><table>#Samples 
K-means [5] 
conv1 
conv2 
conv1&amp;2 

5k 
62.81% 
63.05% 63.10% 
63.50% 
10k 
68.01% 
68.30% 68.46% 
69.11% 
25k 
74.01% 
72.83% 72.93% 
75.11% 
50k (full set) 
76.59% 
74.68% 74.68% 
78.55% 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">http://www.cs.nyu.edu/˜roweis/data.html 2 http://www3.nd.edu/˜cvrl/CVRL/Data_Sets.html</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">We experimented with hand-crafted features such as HOG, LBP, spatial pyramid on a subset of the datasets with some of the better clustering algorithms fromTable 3, and found that they performed worse than image intensity.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Locality preserving nonnegative matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1010" to="1015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1501.03084</idno>
		<title level="m">Deep learning with nonparametric clustering</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Large scale spectral clustering with landmark-based representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Selecting receptive fields in deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2528" to="2536" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">An analysis of single-layer networks in unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on artificial intelligence and statistics</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="215" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Support-vector networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="273" to="297" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Maximum likelihood from incomplete data via the em algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Dempster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Laird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the royal statistical society. Series B (methodological)</title>
		<imprint>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Convex and semi-nonnegative matrix factorizations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="45" to="55" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Mid-level visual element discovery as discriminative mode seeking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="494" to="502" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Unsupervised visual representation learning by context prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1422" to="1430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Discriminative unsupervised feature learning with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="766" to="774" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Selforganization in vision: stochastic clustering for image segmentation, perceptual grouping, and image database organization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gdalyahu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weinshall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Werman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1053" to="1074" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Spikeand-slab sparse coding for unsupervised feature discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1201.3382</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Agglomerative clustering using the concept of mutual nearest neighbourhood</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">C</forename><surname>Gowda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Krishna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern recognition</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="105" to="112" />
			<date type="published" when="1978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Characterising virtual eigensignatures for general purpose face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Allinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Face Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="446" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Unsupervised simultaneous orthogonal basis clustering feature selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5016" to="5023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Discriminative decorrelation for clustering and classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="459" to="472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="346" to="361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Reducing the dimensionality of data with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">313</biblScope>
			<biblScope unit="issue">5786</biblScope>
			<biblScope unit="page" from="504" to="507" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Labeled faces in the wild: A database for studying face recognition in unconstrained environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
		<idno>07-49</idno>
		<imprint>
			<date type="published" when="2007" />
			<pubPlace>University of Massachusetts, Amherst</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Affinity aggregation for spectral clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-Y</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-S</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="773" to="780" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Data clustering: a review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">N</forename><surname>Murty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Flynn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM computing surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="264" to="323" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Beyond spatial pyramids: Receptive field learning for pooled image features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3370" to="3377" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Conference on Multimedia</title>
		<meeting>the ACM International Conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="675" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Using very deep autoencoders for content-based image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ESANN. Citeseer</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">An efficient agglomerative clustering algorithm using a heap</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kurita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="205" to="209" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Building high-level features using large scale unsupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="8595" to="8598" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranganath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="609" to="616" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Stable and efficient representation learning with nonnegativity constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1323" to="1331" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Object recognition from local scaleinvariant features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1999" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1150" to="1157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Some methods for classification and analysis of multivariate observations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Macqueen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fifth Berkeley symposium on mathematical statistics and probability</title>
		<meeting>the fifth Berkeley symposium on mathematical statistics and probability<address><addrLine>Oakland, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1967" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="281" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Finite mixture models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mclachlan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Peel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A universal density profile from hierarchical clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Navarro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Frenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">D</forename><surname>White</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Astrophysical Journal</title>
		<imprint>
			<biblScope unit="volume">490</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">493</biblScope>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Columbia object image library (coil-20)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Nene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Nayar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Murase</surname></persName>
		</author>
		<idno>CUCS-005-96</idno>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">On spectral clustering: Analysis and an algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="849" to="856" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Unsupervised learning of invariant feature hierarchies with applications to object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-L</forename><surname>Boureau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Le-Cun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Dataset fingerprints: Exploring image collections through data mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rematas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Dellaert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4867" to="4875" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="page" from="1" to="42" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="815" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6229</idno>
		<title level="m">Overfeat: Integrated recognition, localization and detection using convolutional networks</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Normalized cuts and image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="888" to="905" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">The cmu pose, illumination, and expression (pie) database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bsat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FG</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="46" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Unsupervised discovery of mid-level discriminative patches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>ECCV</publisher>
			<biblScope unit="page" from="73" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Learning deep representations for graph clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1293" to="1299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">A deep semi-nmf model for learning hidden representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Trigeorgis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bousmalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1692" to="1700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Manzagol. Extracting and composing robust features with denoising autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1096" to="1103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Learning fine-grained image similarity with deep ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1386" to="1393" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations using videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2794" to="2802" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Learning a task-specific deep architecture for clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.00151</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Face recognition in unconstrained videos with matched background similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Maoz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="529" to="534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Integrating image clustering and codebook learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Document clustering based on non-negative matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th annual international ACM SI-GIR conference on Research and development in informaion retrieval</title>
		<meeting>the 26th annual international ACM SI-GIR conference on Research and development in informaion retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="267" to="273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Nonlinear nonnegative component analysis algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Petrou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1050" to="1066" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Self-tuning spectral clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zelnik-Manor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="1601" to="1608" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Graph degree linkage: Agglomerative clustering on a directed graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="428" to="441" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Agglomerative clustering via maximum incremental path integral</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3056" to="3065" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Cyclizing clusters via zeta function of a graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1953" to="1960" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
