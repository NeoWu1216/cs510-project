<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Minimizing the Maximal Rank</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Bylow</surname></persName>
							<email>erikb@maths.lth.se</email>
							<affiliation key="aff0">
								<orgName type="department">Centre for Mathematical Sciences</orgName>
								<orgName type="institution">Lund University</orgName>
								<address>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Olsson</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Centre for Mathematical Sciences</orgName>
								<orgName type="institution">Lund University</orgName>
								<address>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fredrik</forename><surname>Kahl</surname></persName>
							<email>fredrik.kahl@chalmers.se</email>
							<affiliation key="aff0">
								<orgName type="department">Centre for Mathematical Sciences</orgName>
								<orgName type="institution">Lund University</orgName>
								<address>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Signals and Systems</orgName>
								<orgName type="institution">Chalmers University of Technology</orgName>
								<address>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikael</forename><surname>Nilsson</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Centre for Mathematical Sciences</orgName>
								<orgName type="institution">Lund University</orgName>
								<address>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Minimizing the Maximal Rank</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:43+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In computer vision, many problems can be formulated as finding a low rank approximation of a given matrix. Ideally, if all elements of the measurement matrix are available, this is easily solved in the L 2 -norm using factorization. However, in practice this is rarely the case. Lately, this problem has been addressed using different approaches, one is to replace the rank term by the convex nuclear norm, another is to derive the convex envelope of the rank term plus a data term. In the latter case, matrices are divided into sub-matrices and the envelope is computed for each subblock individually. In this paper a new convex envelope is derived which takes all sub-matrices into account simultaneously. This leads to a simpler formulation, using only one parameter to control the trade-of between rank and data fit, for applications where one seeks low rank approximations of multiple matrices with the same rank. We show in this paper how our general framework can be used for manifold denoising of several images at once, as well as just denoising one image. Experimental comparisons show that our method achieves results similar to state-of-the-art approaches while being applicable for other problems such as linear shape model estimation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Low rank approximation and PCA type procedures are important in many disciplines, for example, statistics, bioinformatics, compression and prediction. In computer vision it has been proven useful for applications such as nonrigid and articulated structure from motion <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b11">12]</ref>, photometric stereo <ref type="bibr" target="#b2">[3]</ref>, optical flow <ref type="bibr" target="#b12">[13]</ref> and linear shape models <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b21">22]</ref>. The rank of the approximating matrix typically describes the complexity of the solution. Therefore one seeks to find a low rank factorization U V T ≈ M . If the measurement matrix M is complete and the rank of the appoximating matrix is known, then the best approximation, in a least squares sense, can be computed in closed form using the singular value decomposition (SVD) <ref type="bibr" target="#b9">[10]</ref>.</p><p>Alternatively the problem can be formulated as minimization of the objective function</p><formula xml:id="formula_0">f (X) = µ rank(X) + X − M 2 F .<label>(1)</label></formula><p>Here µ is a parameter that controls the trade-off between data fit and rank. While the solution is easy to compute using SVD the optimization problem itself is non-convex and non-differentiable. As a consequence it is difficult to modify the formulation without having to resort to heuristic optimization approaches. For example, in case there are missing entries and/or outliers the optimization problem is substantially more difficult. In structure from motion, recent approaches <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b19">20]</ref> attack these problems by optimizing jointly over fixed size U and V matrices. As a consequence the rank has to be predetermined and the quality of the result is dependent on initialization.</p><p>To achieve flexible formulations that are independent of initialization, researchers have instead started to consider convex surrogates of the rank function. Most commonly the convex nuclear norm, or sum-of-singular-values penalty is used <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b6">7]</ref>. One reason for its popularity is that it can be shown that if the locations of the missing entries are random the approach gives the best low rank approximation <ref type="bibr" target="#b5">[6]</ref>. In many computer vision applications missing entry locations are highly correlated which makes the approach break down.</p><p>An additional downside of using the nuclear norm is that it has a bias to small solutions. Due to its definition it penalizes both small and large singular values equally. Indeed its proximal operator corresponds to soft thresholding <ref type="bibr" target="#b5">[6]</ref>. In contrast, the desirable operation of hard thresholding, which is performed when solving (1) with SVD, leaves the larger singular values unchanged.</p><p>A convex formulation that only penalizes the small singular values was recently proposed in <ref type="bibr" target="#b15">[16]</ref>. It is shown that the convex envelope of (1) is given by where [·] + denotes truncation at 0 and σ i (X), i = 1, . . . , n are singular values of X. Since f * * is the convex envelope of f their minimum values coincide and f * * (X) is a lower bound f (X) for every X. Furthermore, singular values that are larger than √ µ get a constant penalty, which is similar to hard thresholding.</p><formula xml:id="formula_1">f * * (X) = n i=1 µ − [ √ µ − σ i (X)] 2 + + X − M 2 F ,<label>(2)</label></formula><p>In this paper we are interested in problems where multiple matrices of the same unknown rank need to be estimated. One example where this appears is in manifold estimation. All the tangent spaces of a connected manifold have the same dimension, equal to the dimension of the manifold. Locally a d-dimensional manifold can be thought of as a d-dimensional tangent space. Therefore approximating the data with a d-dimensional manifold can be thought of as locally approximating data with low rank matrices (all of rank d). Another problem that can be cast in the same framework is the missing data problem. In <ref type="bibr" target="#b15">[16]</ref> it was solved by applying the objective (1) on complete sub-blocks of the measurement matrix. To achieve the same rank on all subblocks, one µ-parameter for each block had to be selected. Optimal parameter selection is a major obstacle for this approach.</p><p>More specifically, in this paper we propose an approach where a trade-off between the maximal rank of a set of matrices and their fit to observed data is penalized. In contrast to the approach in <ref type="bibr" target="#b15">[16]</ref> we consider all matrices at the same time. The formulation, which has only one parameter, ensures that the estimated matrices are of the same (unknown) rank. Our main technical contribution is that we derive an expression for the convex envelope and show that its proximal operator is equivalent to a convex cone problem. This allows efficient optimization using an ADMM approach <ref type="bibr" target="#b3">[4]</ref>. We present several applications where this framework can be applied, including manifold denoising and missing data problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Regularization With the Maximal Rank</head><p>In applications like manifold estimation, one seeks to estimate a manifold by its tangent spaces, where the tangent spaces have a lower dimension than the ambient space, see <ref type="figure" target="#fig_0">Figure 1</ref>. In particular, all tangent spaces have the same dimension which is also equal to the dimension of the manifold. To achieve this, one can divide the data points in a measurement matrix M into different neighborhoods. These neighborhoods form blocks (or sub-matrices) M j of M , see left of <ref type="figure" target="#fig_1">Figure 2</ref>. Note that in this case, the blocks M j have the same number of rows as the matrix M , but they may vary in sizes and typically have no missing data. We will show how one can compute a low-rank approximation X where all blocks X j corresponding to M j have the same rank. These low-rank approximations correspond to the low dimensional (affine) tangent spaces. Further details of the specific formulation for this application will be given in Section 3.1. In this section, we will work with a more general formulation.</p><p>We letM = (M 1 , M 2 , ..., M b ) be a collection of measurement matrices that we wish to approximate withX = (X 1 , X 2 , ..., X b ), where X j , j = 1, . . . , b, are of the same (unknown) rank. Note that the matrices M j inM need not to have the same sizes, and thusM should be regarded as a collection of measurement matrices. Our objective function will be of the form</p><formula xml:id="formula_2">min X µr(X) + X −M 2 ,<label>(3)</label></formula><p>where the regularization term is</p><formula xml:id="formula_3">r(X) = max( rank(X 1 ), rank(X 2 ), ...., rank(X b )),<label>(4)</label></formula><p>the data fit is measured by</p><formula xml:id="formula_4">X −M 2 = b j=1 X j − M j 2 F ,<label>(5)</label></formula><p>and · F is the regular Frobenious norm. The parameter µ controls the trade-off between rank and data fit. In practice we are interested in solutions where the ranks of the X j matrices are the same. It can be seen that the regularizer (4) will achieve this under the assumption that the M j matrices are all of full rank. If for some j we have r(X) &gt; rank(X j ) then the data term X j − M j 2 F can be reduced by adding another singular value to X j without affecting any other term.</p><p>A common approach would be to simply replace the rank functions in (4) with nuclear norms. However in contrast to the rank function the nuclear norm is not scale invariant. Therefore this will result in a regularizer that penalizes the matrices unevenly. In particular if the matrices X j have varying sizes. Furthermore, the nuclear norm is only a lower bound on the rank function on the set {X; σ 1 (X) ≤ 1}, while in contrast our convex envelope will be valid on an unbounded domain. Recall that the convex envelope is by definition the tightest possible lower-bounding convex function, hence the ideal tool for our purposes.</p><p>In the following sections we will compute the convex envelope of our formulation via conjugate functions and derive its proximal operator <ref type="bibr" target="#b18">[19]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Conjugate Functions</head><p>To find the convex envelope of (3) we consider the conjugate function, which is by definition</p><formula xml:id="formula_5">f * (Ŷ ) = max X X ,Ŷ − µr(X) − X −M 2 , (6) where X ,Ŷ = b j=1 tr(X T j Y j ). By completing squares via ||X −(M +Ŷ 2 )|| 2 = ||X|| 2 −2 X ,M +Ŷ 2 +||M +Ŷ 2 || 2 ,<label>(7)</label></formula><p>the maximization in (6) can be written <ref type="bibr" target="#b7">(8)</ref> whereẐ =M +Ŷ 2 . For a fixed k the problem is separable in the matrices X j , j = 1, ..., b. That is, the optimal X j can be obtained from the SVD of Z j giving</p><formula xml:id="formula_6">max k max r(X)=k − X −Ẑ 2 + Ẑ 2 − M 2 − µk,</formula><formula xml:id="formula_7">X j = k i=1 σ i (Z j )u i v T i .<label>(9)</label></formula><p>Inserting into (8) we get</p><formula xml:id="formula_8">max k − n i=k+1 σ i (Ẑ) 2 2 + Ẑ 2 − M 2 − µk. (10)</formula><p>Here</p><formula xml:id="formula_9">σ i (Ẑ) is the vector (σ i (Z 1 ), σ i (Z 2 ), . . . , σ i (Z b ))</formula><p>and · 2 is the regular euclidean vector norm. To select the maximizing k we note that</p><formula xml:id="formula_10">µk + n i=k+1 σ i (Ẑ) 2 2 = k i=1 µ + n i=k+1 σ i (Ẑ) 2 2 .<label>(11)</label></formula><p>Since each entry in the vector σ i (Ẑ) is positive and decreasing in i, its norm σ i (Ẑ) 2 2 will also be decreasing with i. Therefore k should be selected such that</p><formula xml:id="formula_11">σ k+1 (Ẑ) 2 2 ≤ µ ≤ σ k (Ẑ) 2 2 .<label>(12)</label></formula><p>This gives the conjugate function</p><formula xml:id="formula_12">f * (Ŷ ) = − n i=1 min(µ, σ i (Ẑ) 2 2 ) + Ẑ 2 − M 2 . (13)</formula><p>Recall thatẐ depends onŶ throughẐ =M +Ŷ 2 . Next we consider the biconjugate, which is by definition</p><formula xml:id="formula_13">f * * (X) = max Y X ,Ŷ − f * (Ŷ ) (14) = max Z 2 X ,Ẑ −M − f * (2Ẑ − 2M ). (15)</formula><p>The objective function in <ref type="formula" target="#formula_0">(15)</ref> can be written</p><formula xml:id="formula_14">n i=1 min(µ, σ i (Ẑ) 2 2 ) − Ẑ −X 2 + X −M 2 . (16)</formula><p>Using von Neumann's trace theorem it can be seen that the optimal Z j has to have an SVD with the same U and V as X j . Therefore the optimization can be reduced to a search over the singular values of the Z j , j = 1, ..., b, giving the convex envelope</p><formula xml:id="formula_15">f * * (X) = R µ (X) + X −M 2 ,<label>(17)</label></formula><p>where</p><formula xml:id="formula_16">R µ (X) = max Z n i=1 min(µ, σ i (Ẑ) 2 2 )− σ i (Ẑ)−σ i (X) 2 2 .<label>(18)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">The Proximal Operator of f * *</head><p>The maximization over the singular values in <ref type="formula" target="#formula_0">(18)</ref> does not seem to have any closed form solution. Evaluation of f * * (X) therefore has to be done by numerically maximizing the (concave) objective function. At first glance it may therefore seem as though minimization of f * * would involve a search over numerical evaluations of f * * . Fortunately this can be avoided. In this section we show that the proximal operator</p><formula xml:id="formula_17">prox f * * (Ŷ ) = arg min X f * * (X) + ρ X −Ŷ 2 ,<label>(19)</label></formula><p>which is the basis for ADMM can be computed using a single cone program. The trick is to switch the order of minimization and maximization and thereby obtain a closed form solution forX. If ρ &gt; 0 the objective function is closed, proper convex-concave, continuous and the optimization can be restricted to a compact set. Switching optimization order is therefore justified by the existence of a saddle point, see <ref type="bibr" target="#b18">[19]</ref>. To find the optimalX we consider the terms of (19) that containX</p><formula xml:id="formula_18">− Ẑ −X 2 + X −M 2 + ρ X −Ŷ 2 .<label>(20)</label></formula><p>It can be seen (e.g., by taking derivatives of <ref type="bibr" target="#b19">(20)</ref>) that the optimalX is given bŷ</p><formula xml:id="formula_19">X =Ŷ +M −Ẑ ρ .<label>(21)</label></formula><p>Inserting into <ref type="bibr" target="#b19">(20)</ref> and completing squares gives</p><formula xml:id="formula_20">− ρ + 1 ρ Ẑ −Ŵ 2 + C,<label>(22)</label></formula><formula xml:id="formula_21">whereŴ = ρŶ +M ρ + 1<label>(23)</label></formula><p>and</p><formula xml:id="formula_22">C = 2ρ + 1 ρ M 2 + ρ Ŷ 2 − ρ Ŷ +M ρ 2 .<label>(24)</label></formula><p>Note that C is independent ofẐ. In practice we are only interested in finding the optimizersẐ andX and not the objective value itself. Hence we can ignore C. We therefore need to maximize</p><formula xml:id="formula_23">n i=1 min(µ, σ i (Ẑ) 2 2 ) − ρ + 1 ρ Ẑ −Ŵ 2 .<label>(25)</label></formula><p>The terms in the sum only depend on the singular values of the matrices Z j , j = 1, . . . , b. For the second term we have</p><formula xml:id="formula_24">Ẑ −Ŵ 2 = Ẑ 2 − 2 b j=1 Z j , W j + Ŵ 2 . (26)</formula><p>By von Neumann's trace theorem Z j , W j ≤ n i=1 σ i (Z j )σ i (W j ) one sees that the SVD of Z j has the same U and V as the SVD of W j . Therefore (25) simplifies to</p><formula xml:id="formula_25">n i=1 min(µ, σ i (Ẑ) 2 2 ) − ρ + 1 ρ σ i (Ẑ) − σ i (Ŵ ) 2 2 .<label>(27)</label></formula><p>The singular values can now be determined using a cone program. To see this we introduce the auxiliary variables s i , i = 1, ..., n and write</p><formula xml:id="formula_26">max n i=1 s i (28) s.t. s i ≤ µ − ρ + 1 ρ σ i (Ẑ) − σ i (Ŵ ) 2 2<label>(29)</label></formula><formula xml:id="formula_27">s i ≤ σ i (Ẑ) 2 2 − ρ + 1 ρ σ i (Ẑ) − σ i (Ŵ ) 2 2 .<label>(30)</label></formula><p>Note that as we are maximizing the sum of s i , (29) or (30) will always attain equality at the optimal solution. Thus the above program is equivalent to <ref type="bibr">(27)</ref>. For the singular values to be feasible they have to be decreasing for each block. To enforce this we add linear constraints on the entries of the vectors σ i (Ẑ) which results in the formulation</p><formula xml:id="formula_28">max n i=1 s i (31) s.t. σ i (Ẑ) − σ i (Ŵ ) 2 2 ≤ ρ ρ + 1 (µ − s i ) (32) ρ + 1 ρ σ i (Ẑ) − σ i (Ŵ ) 2 2 − σ i (Ẑ) 2 2 ≤ −s i (33) σ 1 (Ẑ) ≥ σ 2 (Ẑ) ≥ ... ≥ σ n (Ẑ) ≥ 0.<label>(34)</label></formula><p>Equation <ref type="formula" target="#formula_1">(32)</ref> is easily seen to be convex since the left side is a positive definite quadratic form and the right hand side is linear. To see that the same holds true for (33) we can rewrite this constraint as</p><formula xml:id="formula_29">σ i (Ẑ) − (ρ + 1)σ i (Ŵ ) 2 2 ≤ ρ (ρ + 1)σ i (Ŵ ) 2 2 − s i .</formula><p>(35) Constraints (32) and (35) can be realized using the cone</p><formula xml:id="formula_30">{(x 1 , x 2 , x 3 ); x 1 x 2 ≥ x 3 2 2 , x 1 + x 2 ≥ 0}.<label>(36)</label></formula><p>This type of cone (which is a rotation of the quadratic cone) is supported in SeDuMi <ref type="bibr" target="#b20">[21]</ref> and Mosek <ref type="bibr" target="#b0">[1]</ref> which we use to solve <ref type="bibr" target="#b18">(19)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Applications</head><p>In this section, we present two applications of our framework: (i) Manifold denoising and (ii) Linear shape basis models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Manifold Denoising</head><p>Manifold denoising can be formulated as seeking affine tangent spaces with the same dimension. If we have a set of images, possibly corrupted with noise, then the assumption is that the true uncorrupted images lie on a low-dimensional manifold. The images, represented by m i , i = 1, . . . , N , are assumed to be column-stacked so one image lies in R n , where n is the number of pixels. The assumption means that several points which are close to each other should be close to the tangent space of the manifold as illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>. To determine neighbourhoods, we find for each image point its K-closest neighbors in the euclidean distance and consider them to be one block.</p><p>Given  each sub-matrix M i will have high rank, and the task is to find a low-rank approximation X i for each M i . Note that the different X i share common varibles. Also, since we are interested in the affine tangent spaces, which do not necessarily go through the origin, we add the row-vise mean vectorx i of each X i . Assuming that X i have zero row means, that is X i ✶ = 0, the fitting terms in the objective function</p><formula xml:id="formula_31">can be written b i=1 X i +x i ✶ T − M i 2 = (37) b i=1 X i − (M i −m i ✶ T ) 2 F + k i x i −m i 2 2 ,<label>(38)</label></formula><p>wherem i is the row mean of M i and k i is equal to the number of columns in block M i . To ensure consistency between shared variables, we penalize the differences by adding to the objective</p><formula xml:id="formula_32">α b i=1 P i (X) − (X i +x i ✶ T ) 2 F ,</formula><p>where α is a weighting factor, X is the approximation of the measurement matrix M and P i (X) retrieves block i in X.</p><p>In summary, we have the following optimization problem min X,X,xi</p><formula xml:id="formula_33">r(X) + b i=1 X i − (M i −m i ✶ T ) 2 F + (39) k i x i −m i 2 2 + α P i (X) − (X i +x i ✶ T ) 2 F (40) s.t. X i ✶ = 0 i = 1, . . . , b.<label>(41)</label></formula><p>We have already derived the convex envelope for the terms on the first row (39) and the terms on the second row (40) are convex from the start. To minimize (39) we use the convex envelope and introduce auxiallary variables Z i , which results in</p><formula xml:id="formula_34">min X,X,xi,Ẑ f * * (X)+ b i=1 k i x i −m i 2 2 + α P i (X) − Z i −x i ✶ T 2 F (42) s.t. X i = Z i , Z i ✶ = 0, i = 1, . . . , b,<label>(43)</label></formula><p>and in turn, this leads to the ADMM formulation min X,X,xi,Ẑ</p><formula xml:id="formula_35">f * * (X) + ρ X −Ẑ +Λ 2 − ρ Λ 2 + b i=1 k i x i −m i 2 2 + α P i (X) − Z i −x i ✶ T 2 F (44) s.t. Z i ✶ = 0, i = 1, . . . , b.<label>(45)</label></formula><p>We get one part which depends onX,</p><formula xml:id="formula_36">min X f * * (X) + ρ X −Ẑ +Λ F ,<label>(46)</label></formula><p>which is precisely the proximal operator we have seen before. To minimize with respect to the other variablesẐ, x i and X is now straightforward. Keeping the other variables fixed and solving for one we get the following updates:</p><formula xml:id="formula_37">X t+1 = arg min X t α b i=1 P i (X t ) − Z t i −x t i ✶ T 2 F (47)</formula><p>which is a separable least squares problem,</p><formula xml:id="formula_38">Z t+1 i = arg min Z t i α P i (X t+1 ) − Z t i −x t i ✶ T 2 F + ρ X t i −Z t i + Λ t i 2 F i = 1, . . . , b,<label>(48)</label></formula><p>since all blocks are independent in the minimization,</p><formula xml:id="formula_39">x t+1 i = arg min x t i k i x t i −m i 2 2 + α P i (X t+1 ) − Z t+1 i −x t i ✶ T 2 F ,<label>(49)</label></formula><p>since we can minimize eachx i separately. To findX t+1 we use the proximal operator we have deduced:</p><formula xml:id="formula_40">X t+1 = prox f * * (Ẑ t+1 −Λ t+1 ).<label>(50)</label></formula><p>ForΛ we take a step in the ascent direction, that is,</p><formula xml:id="formula_41">Λ t+1 i = Λ t i + X t+1 i − Z t+1 i ,<label>(51)</label></formula><p>since again, each block is separable.</p><p>Experimental results: USPS.</p><p>To test the denoising method, we use the USPS dataset <ref type="bibr" target="#b14">[15]</ref> of handwritten digits. We choose 100 images of each digit and rescale the intensities to lie between [0, 1]. The images are perturbed by Gaussian noise with standard deviation σ = 0.3. We then stack all images into one measurement matrix M , and find the K = 30 closest neighbours for each image. With this data, we apply our optimization to obtain the new approximate   For comparison we use the well-known work called Manifold Denoising by <ref type="bibr" target="#b13">[14]</ref>. This work uses a different approach where a partial differential equation is solved on a graph created by the data points to obtain a manifold.</p><p>The results shown in <ref type="table">Table 1</ref> where obtained when adding noise with standard deviation σ = 0.3 to the USPS dataset. For Manifold Denoising we set the number of neighbors to 6 and re-weighting parameter λ = 1 and a symmetric graph, since that gave the best results in our experiment.</p><p>Experimental results: Single image denoising.</p><p>Our method for manifold denoising can also be used to denoise a single image. To apply our method, the image is first divided into several patches, and each patch is considered to be one point in R n . As above all points, or patches, are then stacked into one measurement matrix M . Thereafter,   the optimal X is found applying our optimization method and each column in X equals a denoised patch which can be used to rebuild the image.</p><p>This was tested on Lena, size 512 × 512 pixels and the Cameraman, size 256 × 256 pixels. On both images we added gaussian noise with a standard deviation σ = 0.1. As can be seen in <ref type="figure" target="#fig_4">Figure 4</ref>, much of the noise is reduced. To compare our method, we also provide results from state-ofthe-art method BM3D <ref type="bibr" target="#b8">[9]</ref>. In the closeup of <ref type="figure" target="#fig_6">Figure 6</ref>, it can be seen that our method keeps details better than BM3D, for example, the pupil is clearly distinguishable in our result but not in BM3D's result. The PSNR on the input data was 19.9914. Our method improved to 28.6064 and BM3D to 29.1560.</p><p>To get these results a patch size of 12 × 12 pixels was used together with an overlap of 2/3 between two consecutive patches, this results 15876 tangent spaces. The parameter α was set to 1.5 and µ to 75, 000 and the number of neighbors K = 20. The optimal blocks X i had rank 2.</p><p>The same approach was also tested on the Cameraman and as can be seen in <ref type="figure" target="#fig_5">Figure 5</ref>, our method performs well compared to BM3D. <ref type="figure" target="#fig_5">Figure 5</ref> shows that our method preserves more details compared to BM3D which smooths out some details. For example the camera is more detailed and the roof on the tower to the right is more preserved. For the Cameraman we used the same parameters as above except that µ = 22000 and the number of tangent spaces was 3844. The optimal blocks X i had rank 3. The denoising results are summarized in <ref type="table" target="#tab_2">Table 2</ref>  </p><formula xml:id="formula_42">b i=1 X i −M i 2 F</formula><p>for the method in <ref type="bibr" target="#b15">[16]</ref> and our method. Note that the method in <ref type="bibr" target="#b15">[16]</ref> outperforms the nuclear norm relaxation for the same error metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Linear Shape Basis Models</head><p>Another application we test our framework on is estimation of linear shape models. A common assumption is that a set of tracked image points moving non-rigidly can be described with a small number of basis elements in each frame. If we let M f = (m 1 f , m 2 f , . . . , m N f ) denote the N tracked 2D-points in frame f , we want to find shape basis models (S 1 , S 2 , . . . , S K ) -each of size 2 × N -and scalar coefficients C f 1 , C f 2 , . . . , C f K such that the points M f can be described by</p><formula xml:id="formula_43">M f = K k=1 C f k S k .<label>(52)</label></formula><p>Stacking the N points in F frames yields a 2F × N measurement matrix M . Since we want to use as few basis elements as possible, the matrix M should be of low rank. Due to occlusion and tracking failures, not all points will be seen in all frames. This gives a measurement matrix M with missing data. To handle this we create sub-blocks M i of M , where each M i has no missing entries, see right of <ref type="figure" target="#fig_1">Figure 2</ref>. Hence, we have turned the problem into finding low-rank approximations X i of M i , where the blocks in X i share common variables. The objective function we seek to minimize is</p><formula xml:id="formula_44">minX ,X f * * (X) (53) s.t. P i (X) = X i i = 1, . . . , b,</formula><p>whereX is the collection of blocks (X 1 , X 2 , . . . , X b ) and P i (X) retrieves block i from X. The constraint comes from the requirement that the blocks we optimize over shall coincide on the overlap. To minimize this, we use ADMM and our augmented Lagrangian becomes</p><formula xml:id="formula_45">f * * (X) + ρ X −P(X) +Λ 2 − ρ Λ 2 ,<label>(54)</label></formula><p>whereP(X) = (P 1 (X), ..., P b (X)).</p><p>In each iteration we perform the following updates:   When the low-rank approximation is found where only known data has been used, we complete the missing parts in X by applying the same method as in <ref type="bibr" target="#b15">[16]</ref>.</p><formula xml:id="formula_46">X t+1 = arg min X t f * * (X t ) + ρ X t −P(X t ) +Λ t || 2<label>(55)</label></formula><formula xml:id="formula_47">X t+1 = arg min X t ρ X t+1 −P(X t ) +Λ t 2<label>(56)</label></formula><formula xml:id="formula_48">Λ t+1 i = Λ t i + X t+1 i − P i (X t+1 ).<label>(57)</label></formula><p>Experimental results. Our framework has been applied to a number of image sequences obtained from the authors of <ref type="bibr" target="#b15">[16]</ref>. The results of the Hand-, Book-and Banner sequences are shown in <ref type="figure" target="#fig_7">Figures 7, 8 and 9</ref>.</p><p>One sees clearly in all sequences that the red and blue points, which are the reconstructed points, obey a motion which is reasonable compared to the input data, green points. The blue points are the reconstructed points which we could track and the red points are the reconstructed positions of points with no measurements available. The found rank for the solution in the Hand sequence is 5, in the Book sequence we get rank 3 and in the Banner sequence we get rank 9. The number of blocks in Hand, Book and Banner was 5,3 and 19.</p><p>To compare with <ref type="bibr" target="#b15">[16]</ref>, we test their method on the same datasets and measure the error b i=1 M i − X i 2 F and the results are shown in <ref type="table" target="#tab_4">Table 3</ref>. We choose to measure the error on the blocks since that will show if our method differs from <ref type="bibr" target="#b15">[16]</ref>. Note that this method was shown to perform better than the nuclear norm relaxation for this application.</p><p>As the results in <ref type="table" target="#tab_4">Table 3</ref> show, we do at least as good as they do on these datasets. We investigated the approximated sub-matrices from <ref type="bibr" target="#b15">[16]</ref> individually and saw that some submatrices had rank 8 and some rank 10. This shows that it is easier to get uniform rank on all sub-matrices using our formulation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusion</head><p>In this paper we have derived a novel and general convex framework to approximate low-rank matrices. Our method is suitable in situations where several matrices of the same rank need to be approximated. Our main contribution is the derivation of a strong convex formulation that can be optimized in general frameworks using the proximal operator in an ADMM fashion <ref type="bibr" target="#b3">[4]</ref>. One of the advantages of our formulation is that there is a single tuning parameter controlling the trade-of between rank and model fit which is important in manifold estimation where the number of sub-matrices may be well above 10, 000.</p><p>Experimental evaluations showed that our method achieves results similar to state-of-the-art approaches on manifold denoising problems and linear shape basis estimation. It should be mentioned that in the case of manifold denoising neighborhoods are computed using noisy patches. Therefore results could potentially be improved by reestimating neighborhoods using cleaned versions. However, to focus the evaluation on our convex formation we have refrained from such heuristics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Acknowledgements</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>A simple illustration of how to estimate tangent planes of the manifold.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Illustration of how measurement matrices can divided into blocks. Left: Blocks for tangent spaces. Right: Example of block division with missing data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>a set of images, stacked in a measurement matrix M = [m 1 , m 2 , ..., m N ], we determine a collection of blocks via the neighbourhoods,M = (M 1 , M 2 , . . . , M b ), see left of Figure 2. Since the images are corrupted by noise,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Some results from denoising the USPS digits. From left to right: Input images, our results and the results from Manifold Denoising.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Denoising results of the Lena image. Left: Noisy input image. Middle: Denoised image using our method. Right: Denoising results from BM3D.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Left: Input noisy Cameraman. Middle: Our result. Note the preserved details on the camera. Right: Result from BM3D. matrix X of M . Each column in X contains a denoised image corresponding to the noisy image in the same column in M .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Zooming in, one can see more details in our result (left) compared to BM3D's result (right). Note that one can see the pupil in the eye of the left image, but not in the right image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Frames 280 and 371 from the hand experiment. The solution has rank 5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Frames 297 and 337 in the Book sequence. The solution has rank 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 :</head><label>9</label><figDesc>Frames 160 and 250 of the Banner sequence. The solution has rank 9.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc>Denoising results from the Cameraman and Lena.</figDesc><table>BM3D gives a higher PSNR for Lena, but we do better on 
the Cameraman. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head></head><label></label><figDesc>.</figDesc><table>Dataset Loc. Rank Func. [16] Our method 
Hand 
0.474 
0.474 
Banner 
6.54 · 10 7 
4.73 · 10 7 
Book 
0.121 
0.121 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc>The error</figDesc><table></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">The MOSEK optimization toolbox for MATLAB manual</title>
		<ptr target="URLwww.mosek.com.4" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The generalized trace-norm and its application to structure-frommotion problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Angst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Photometric stereo with general, unknown lighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Basri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kemelmacher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="239" to="257" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Distributed optimization and statistical learning via the alternating direction method of multipliers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Peleato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Eckstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Found. Trends Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="122" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Recovering non-rigid 3d shape from image streams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hertzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Biermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A singular value thresholding algorithm for matrix completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Candès</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. on Optimization</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Robust principal component analysis?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Candès</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<idno>11:1- 11:37</idno>
	</analytic>
	<monogr>
		<title level="j">J. ACM</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Active appearance models. Trans. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">F</forename><surname>Cootes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><forename type="middle">C J</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="681" to="685" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">BM3D image denoising with shape-adaptive principal component analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dabov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Foi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Katkovnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Egiazarian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Signal Processing with Adaptive Sparse Structured Representations</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The approximation of one matrix by another of lower rank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Eckart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="218" />
			<date type="published" when="1936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Efficient computation of robust weighted low-rank matrix approximations using the L 1 norm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Eriksson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1681" to="1690" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Dense variational reconstruction of non-rigid surfaces from monocular video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roussos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>De Agapito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A variational approach to video registration with subspace constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roussos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>De Agapito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="286" to="314" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Manifold denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<meeting><address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A database for handwritten text recognition research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Hull</surname></persName>
		</author>
		<idno type="DOI">10.1109/34.291440</idno>
		<ptr target="http://dx.doi.org/10.1109/34.291440.5" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="550" to="554" />
			<date type="published" when="1994-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Rank minimization with structured data patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Larsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Olsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Bylow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Kahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Computer Vision</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A convex approach to low rank matrix approximation with missing data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Olsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oskarsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Scandinavian Conference on Image Analysis</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Guaranteed minimum-rank solutions of linear matrix equations via nuclear norm minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fazel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Parrilo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Rev</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="471" to="501" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Rockafellar</surname></persName>
		</author>
		<title level="m">Convex analysis. Princeton Mathematical Series</title>
		<meeting><address><addrLine>Princeton, N. J.</addrLine></address></meeting>
		<imprint>
			<publisher>Princeton University Press</publisher>
			<date type="published" when="1970" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">General and nested Wiberg minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Strelow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Using SeDuMi 1.02, a Matlab toolbox for optimization over symmetric cones</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Sturm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Optimization Methods and Software</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="625" to="653" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Robust face recognition via sparse representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ganesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="210" to="227" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">A factorization-based approach for articulated nonrigid shape, motion and kinematic chain recovery from video. Trans. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="865" to="877" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
