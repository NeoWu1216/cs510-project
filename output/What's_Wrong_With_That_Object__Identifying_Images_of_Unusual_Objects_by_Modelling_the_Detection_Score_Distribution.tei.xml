<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">What&apos;s Wrong with that Object? Identifying Images of Unusual Objects by Modelling the Detection Score Distribution *</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Queensland</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingqiao</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">The University of Adelaide</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">The University of Adelaide</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zi</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Queensland</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Van Den Hengel</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">The University of Adelaide</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Heng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Queensland</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">What&apos;s Wrong with that Object? Identifying Images of Unusual Objects by Modelling the Detection Score Distribution *</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:48+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper studies the challenging problem of identifying unusual instances of known objects in images within an "open world" setting. That is, we aim to find objects that are members of a known class, but which are not typical of that class. Thus the "unusual object" should be distinguished from both the "regular object" and the "other objects". Such unusual objects may be of interest in many applications such as surveillance or quality control. We propose to identify unusual objects by inspecting the distribution of object detection scores at multiple image regions. The key observation motivating our approach is that "regular object" images, "unusual object" images and "other objects" images exhibit different region-level scores in terms of both the score values and the spatial distributions. To model these distributions we propose to use Gaussian Processes (GP) to construct two separate generative models, one for the "regular object" and the other for the "other objects". More specifically, we design a new covariance function to simultaneously model the detection score at a single location and the score dependencies between multiple regions. We demonstrate that the proposed approach outperforms comparable methods on a new large dataset constructed for the purpose.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Humans have an innate ability to detect an unusual object, even when they have no experience of the particular manner in which it is unusual. Mimicking this ability in computer vision has a range of applications such as surveillance or quality control. Existing studies towards this goal are usually conducted on small datasets and controlled scenarios i.e., with relatively simple backgrounds <ref type="bibr" target="#b1">[2]</ref> or specific type of unusualness <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b10">11]</ref>. To address this issue, in this work we present a large dataset which captures more * The first two authors contributed to this work equally. P. Wang's contribution was made when visiting The University of Adelaide. C. Shen is the corresponding author (e-mail: chunhua.shen@adelaide.edu.au). This work was partially supported by the Data 2 Decsions CRC.  <ref type="figure">Figure 1</ref>. Illustration of the method applied to identifying the unusual "bicycle". By applying a detector trained on "regular bicycles" and "other objects", we are able to identify the "regular bicycle", the "unusual bicycle" and the "other object" (a bus in this case) through analysis of the distribution of the scores of multiple detectors. The discriminative information lies in both the values of the detection scores and the spatial dependencies between those scores, e.g. the score dependency between neighbouring proposals B and C.</p><p>general forms of unusualness, and has more complex backgrounds. Moreover, we adopt a more realistic "open world" evaluation protocol. That is, we need to distinguish the unusual version of an object-of-interest not only from typical examples from the same category but also from objects from other categories. Humans recognise unusual objects by identifying instances which share the key characteristics of the class, but not all of the typical incidental characteristics. Images of unusual objects are thus expected to be more similar to those of other instances of the same class of objects, than to those of other objects. Suppose we apply a detector trained using images of typical examples of a class of object as the positive data, and images of other objects as the negative data. The detection scores of the "unusual object" images are expected to be less than those of the typical objects of the same class, but greater than those of objects from other classes. Empirically, however, we have seen that the detection score is insufficient to distinguish unusual objects from regular ones and other objects. We thus propose here not only to exploit the detection score values, but also the spatial distributions of the detection scores.</p><p>As is illustrated in <ref type="figure">Fig. 1</ref>, positive detection scores should densely overlap in images of regular object instances, while in unusual-object images the score distribution will be altered by the existence of unusual parts. To model these two factors, we propose to use Gaussian Processes (GP) <ref type="bibr" target="#b12">[13]</ref> to construct two separate generative models for the detection scores of "regular object" image regions and "other objects" image regions. The mean function is defined to depict the prior information of the score values of either "regular object" images or "other objects" images. A new covariance function is designed to both non-parametrically model the detection score at a single region, and capture the inter-dependencies between scores over multiple regions. Note that unlike the conventional use of GP in computer vision, our model does not assume that the region scores of an image are i.i.d. This treatment allows our method to capture the spatial dependencies between detection scores, which turns out to be crucial for identifying unusual objects.</p><p>By comparing with several alternative solutions on the proposed dataset, we experimentally demonstrate the effectiveness of the proposed method. To summarize, the main contributions of this paper are:</p><p>• We propose a large dataset and present a more realistic "open world" evaluation protocol for the task of unusual-object identification from images.</p><p>• We propose a novel approach for unusual-object detection by looking into the detection score values as well as the spatial distributions of the detection scores of the image regions. We propose to use Gaussian Processes (GP) to simultaneously model the detection score at a single region and the score dependencies between multiple regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Related Work</head><p>Irregular Image/Video Detection. There exists a variety of work focusing on irregular image and/or video detection. While some approaches attempt to detect irregular image parts or video segments given a regular database <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b6">7]</ref>, other efforts are dedicated to addressing some specific types of irregularities <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b3">4]</ref> such as out-of-context via building some corresponding models. Standard approaches for irregularity detection are based on the idea of evaluating the dissimilarity from regular. The authors of <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b6">7]</ref> formulate the problem of unusual activity detection in video into a clustering problem where unusual activities are identified as the clusters with low inter-cluster similarity. The work <ref type="bibr" target="#b1">[2]</ref> detects the irregularities in image or video by checking whether the image regions or video segments can be composed using large continuous chunks of data from the regular database. Despite the good performance in irregularity detection, this method severely suffers from the scalability issue, because it requires to traverse the database given any new query data. Sparse coding <ref type="bibr" target="#b8">[9]</ref> is employed in <ref type="bibr" target="#b20">[21]</ref> for unusual events detection. This work is based on the assumption that unusual events cannot be well reconstructed by a set of bases learned from usual events.</p><p>Another stream of work focus on addressing specific types of irregularities. The work of <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref> focus on exploiting contextual information for object recognition or outof-context detection, like "car floating in the sky". In <ref type="bibr" target="#b2">[3]</ref>, they use a tree model to learn dependencies among object categories and in <ref type="bibr" target="#b3">[4]</ref> they extend it by integrating different sources of contextual information into a graph model. The work <ref type="bibr" target="#b10">[11]</ref> focuses on finding abnormal objects in given scenes. They consider wider range of irregular objects like those violate co-occurrence with surrounding objects or violate expected scale. However, the applications of these methods are very limited since they rely on pre-learned object detector to accurately localize the object-of-interest. Recently the work in <ref type="bibr" target="#b13">[14]</ref> delves into various types of atypicalities and makes a more comprehensive study. Gaussian Processes in Computer Vision. Due to the advantage in nonparametric data fitting, GP has widely been used in the fields like classification <ref type="bibr" target="#b0">[1]</ref>, tracking <ref type="bibr" target="#b16">[17]</ref>, motion analysis <ref type="bibr" target="#b7">[8]</ref> and object detection <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref>. The work <ref type="bibr" target="#b7">[8]</ref> uses GP regression to build spatio-temporal flow to model the motion trajectories for trajectory matching. In <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref>, object localization is done via using GP regression to predict the overlaps between image windows and the groundtruth objects from the window-level representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">A New Dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Dataset Description</head><p>Here we propose a new dataset for the task of irregular image detection. The data is collected from Google Images <ref type="table">Table 1</ref>. Comparison of the proposed dataset with existing datasets. The work of <ref type="bibr" target="#b3">[4]</ref> addresses the irregular type of out of context. The work of <ref type="bibr" target="#b10">[11]</ref> deals with violations of co-occurrence, positional relationship and scale.</p><p>dataset # images irregular category accurate detector <ref type="bibr" target="#b10">[11]</ref> 150 specific yes <ref type="bibr" target="#b3">[4]</ref> 218 specific yes ours 20,420 general no and Bing Images which is composed of 20,420 images belonging to 20 classes. We choose the 20 classes referring to the PASCAL VOC dataset <ref type="bibr" target="#b4">[5]</ref> but replace some classes that are not suitable for the task. The images of each class are composed of both regular images and irregular images. For regular images, we try different feasible queries to collect sufficient data. Taking "apple" for example, we try "fuji apple", "pink lady", "golden delicious", etc. To collect irregular images, we use keywords like "irregular", "unusual", "abnormal", "weird", "broken", "decayed", "rare", etc. After the images are returned, we manually remove the unrelated and low-quality data. Also, we perform near-duplicate detection to remove some duplicate images. In general, the number of irregular images per class is comparable to the sum of regular and "other class" images. <ref type="figure" target="#fig_1">Fig. 2</ref> shows some examples of irregular images. There exist some other datasets <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b10">11]</ref> for irregular image detection. A comparison between our dataset and the existing datasets is summarized in <ref type="table">Table 1</ref>. The main difference is twofold.</p><p>• Our dataset is large-scale comparing to the existing datasets, increasing the number of images from several hundred to more than twenty thousand.</p><p>• While the existing datasets are proposed for specific irregular category such as "out-of-context", "relative position violation" and "relative scale violation", our dataset is for general irregular cases.</p><p>Besides the above differences, we adopt a more practical evaluation protocol compared with <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b10">11]</ref>. That is, we evaluate the irregular object detection with the presence of irrelevant objects. This is different from <ref type="bibr" target="#b1">[2]</ref> where irregularity detection is performed in controlled environment with relatively simple background.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Problem Definition</head><p>For a given object category C, we divide it into two disjoint subcategories, a regular sub-class C r and an irregular sub-class C u , with C = C r ∪ C u and C r ∩ C u = ∅. We call an image I a regular image if I ∈ C r and an irregular image if I ∈ C u . If an image I does not contain the given object, we label it as belonging to the "other class" set C o . The task is to determine if a test image I ∈ C u . Note that for C, only the regular and "other class" images are available for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Key Motivation</head><p>Regular object images of the same class are alike; each irregular object image, however, is irregular in its own way. Thus, it is somehow impossible to collect a dataset to cover the space of the irregular images and one common idea to handle this difficulty is to build a "regular object" model to identify the "irregular objects" as outliers. While most traditional methods <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b1">2]</ref> build this model based on the visual features extracted from images, our approach takes an alternative methodology by firstly training a detector from the "regular object" images and "other objects" images and then discovering the irregularity based on the detection score patterns. The merit of using detection scores for irregularity detection are as follows. (1) It is more computationally efficient since the appearance information has been compressed to a single scalar of detection values. This enables us to explore complex interaction of multiple regions within an image while maintaining reasonable computational cost. (2) It naturally handles the background and "other class" distraction since our detector is trained by using the "regular object" and "other objects". More specifically, our method is inspired by two intuitive postulates of how humans recognize an "irregular object", which are elaborated as follows. Postulate I: discrimination in detection score values.</p><p>From the perspective of human vision, an irregular object is something "looks like an object-of-interest, but is still different from its common appearance". If we view the object detection score as a measure of the likelihood of an image containing the object, then the above postulate could correspond to a relationship in detection scores denote the detection score of the "other object", "irregular object" and "regular object" respectively. To verify this relationship, we train an image-level object classifier and plot the accumulated histograms of the scores of regular, irregular and other-class images of each class in <ref type="figure" target="#fig_2">Fig. 3</ref>. It can be seen from this figure that the distribution of the score values is generally consistent with our assumption. However, there are still overlaps especially between regular and irregular images, which means that using this criterion alone cannot perfectly distinguish the irregular images.</p><formula xml:id="formula_0">f (I o ) &lt; f (I u ) &lt; f (I r ), where f (I o ), f (I u ) and f (I r )</formula><p>Postulate II: discrimination in the spatial dependency of detection scores. When exposed to part of the regular object, human can predict what the neighbouring parts of the object should look like without any difficulty. But irregular object may break this smoothness. This suggests that if we apply an object detector to the object proposals of an image, the region-level detection scores of the three different types of images may exhibit different dependency patterns. <ref type="figure" target="#fig_3">Fig. 4</ref> shows the top 20 regions of some example images of car class according to the values of the detection scores. As seen, for regular car the positive bounding boxes are densely overlapped and images from other classes such as motorbike are supposed to have no positively scored proposals. Detection scores of irregular images may disobey both of these two distribution patterns. For example, two strongly overlapped regions may have opposite detection scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Proposed Approach</head><p>Motivated by the above analysis, we propose a two-step approach to the task of irregular image detection. We first apply a Multi-Instance Learning (MIL) approach to learn a region-level object detector and then design Gaussian Processes (GP) based generative models to model the detection score distributions of the "regular object" and the "other objects". Once the model parameters are learned, we can readily determine whether a test image is irregular by evaluating its fitting possibilities to these two generative models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Object Detector Learning</head><p>Taking the region proposals of images as instances, we represent each image as a bag of instances. Since we only have the image-level label indicating the presence or absence of the object, the learning of region-level detector is essentially a weakly supervised object localization problem. Considering both the localization accuracy and the scalability, we follow the MIL method in <ref type="bibr" target="#b9">[10]</ref> to learn an object detector for each class. For a class C, we have a set of regular images containing the object as positive training data and a set of images belonging to other classes where the object concerned do not appear as negative training data.</p><p>We use Selective Search <ref type="bibr" target="#b15">[16]</ref> to extract a set of object proposals for each image and from the perspective of MIL, each proposal is regarded as an instance. Then each image I i is represented by a N i × D matrix X i where N i denotes the number of proposals and D represents the dimensionality of the proposal representations. Inspired by <ref type="bibr" target="#b9">[10]</ref>, we optimize the following objective function to learn the detector,</p><formula xml:id="formula_1">J = i log(1 + e −y i maxj {w T x i j +b} ),<label>(1)</label></formula><p>where w ∈ R D×1 serves as an object detector, x i j indicates the jth instance of the ith image and w T x i j + b is its detection score. The single image-level score is aggregated via the max-pooling operator max{·} and it should be consistent with the image-level class label y i ∈ {1, −1}. The parameters w and b can be learned via back-propagation using stochastic gradient descent (SGD).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Gaussian Processes Based Generative Models</head><p>In this section, we elaborate how to use GP to model the distribution of the region-level detection scores. Unlike traditional GP based regression <ref type="bibr" target="#b19">[20]</ref> which takes a single feature vector as input, we treat multiple proposals within an image as the input and our model will return a probability to indicate the fitting likelihood of the proposal set.</p><p>GP assumes that any finite number of random variables drawn from the GP follow a joint Gaussian distribution and this distribution is fully characterized by a mean function m(x) and a covariance function k(x, x ′ ) <ref type="bibr" target="#b12">[13]</ref>. In our case, we treat the detection score of each proposal as a random variable. The mean function depicts the prior information of the score values, e.g. the value tends to be a positive scalar for the "regular object" images. The covariance function plays two roles. (1) As in standard GP regression, it serves as a non-parametric estimator of the score value. More specifically, if a proposal is similar (in terms of a defined proposal representation) to a proposal in the training set, it encourages them to share similar scores. (2) As one of our contributions, we also add a term in the covariance function to encourage the overlapped object proposals within the same test image to share similar detection scores. In the following subsections, we introduce the details of the design of the mean function and covariance function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">GP Construction</head><p>For each class C, we will construct two GP based generative models for regular images and "other objects" images separately. Without losing generality, we will focus on regular images in the following part.</p><p>Suppose that we have N C positive training images for class C. For each image I i (i ∈ {1, 2, · · · , N C }), we use the top-n scored proposals s i j (j ∈ {1, 2, · · · , n}) only in order to reduce the distraction impact of the background. Their associated detection scores can be obtained via the function f (s i j ). In our model we assume that f is distributed as a GP with a mean function m(·) and a covariance function k(·, ·) f ∼ GP(m, k).</p><p>(2)</p><p>Mean function: We define the mean function m(s) = µ, where µ is a scalar constant learned through parameter estimation. It can be intuitively understood as the bias of the detection score in the regular object or other object cases. For example, it tends to be a positive (negative) value for the "regular (other) object" case.</p><p>Covariance function: As aforementioned analysis, the covariance function is decomposed into two parts, an interimage part and an inner-image part. While the inter-image part is employed to regress the proposal-level detection score in the light of the proposals in the training set, the inner-image part is used to model the dependencies of the scores within one test image. To define the inter-image covariance function for a proposal pair belonging to different images, it needs to design a representation for each proposal so that their similarity can be readily measured. We leverage the spatial relationship between a proposal and the proposal with the maximum detection score within the same image as this representation. More specifically, assuming the maximum-scored proposal in an image I i is s i max , the representation of a proposal s in I i is defined as,</p><formula xml:id="formula_2">φ(s) = [IoU(s, s i max ), c(s, s i max )],<label>(3)</label></formula><p>where IoU(s, s i max ) denotes the intersection-over-union between s and s i max and c(s, s i max ) denotes the normalized distances between the centers of s and s i max . Note that these two measurements reflect a proposal's overlapping degree, distance to the maximum-scored proposal and indirectly the size of the proposal. Intuitively, these factors could be used to predict the detection score value of a proposal.</p><p>With this representation, we can define the inter-image covariance function k inter (s, s ′ ) of s and s ′ as,</p><formula xml:id="formula_3">exp − 1 2 φ(s) − φ(s ′ ) T diag(γ) φ(s) − φ(s ′ ) ,<label>(4)</label></formula><p>where diag(γ) is a diagonal weighting matrix to be learned. The inner-image covariance function serves as one of the key contributions of this work, which poses a smoothness constraint over the scores of the overlapped object proposals in an image. For a pair of inner-image proposals s and s ′ , we define the inner-image covariance function as follows (if two proposals s and s ′ are from different images, k inner (s, s ′ ) = 0),</p><formula xml:id="formula_4">k inner (s, s ′ ) = 2S(s ∩ s ′ ) S(s ∩ s ′ ) + S(s ∪ s ′ ) ,<label>(5)</label></formula><p>where S stands for the area. Note that the formula is variant to standard intersection-over-union <ref type="bibr" target="#b4">[5]</ref> commonly used as detection metric. The reason why we define it like this is because it is exactly χ 2 kernel and can guarantee the covariance matrix to be positive definite <ref type="bibr" target="#b17">[18]</ref>.</p><p>With both the inter-image and inner-image covariance function, we can obtain the overall covariance function of any proposal pair s and s ′ as,</p><formula xml:id="formula_5">k(s, s ′ ) = a · k inner (s, s ′ ) + b · k inter (s, s ′ ),<label>(6)</label></formula><p>where a, b are hyper-parameters regulating the weights of these two kernel functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Hyper-parameter Estimation</head><p>In this part, we introduce the hyper-parameter learning for the GPs. Still, we use regular images for description. In the definition of the mean and covariance functions of the GP, we introduce the hyper-parameters θ = {µ, γ, a, b}.</p><p>We estimate the hyper-parameters by minimizing the negative logarithm of the marginal likelihood of all the detection scores of the training proposals given the hyper-parameters,</p><formula xml:id="formula_6">−L = −log p(f (S)|S, θ),<label>(7)</label></formula><p>where S denotes the training proposals and f (S) denotes their detection scores. We use the toolbox introduced in <ref type="bibr" target="#b11">[12]</ref> for hyper-parameter optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Test Image Evaluation</head><p>For class C, let s r be a set of proposals of regular training images and f r be their detection scores. We can establish the covariance matrix K for the training data. Given a target set of proposals s t from a test image and their detection scores f t , the joint distribution of f r , f t can be written as,</p><formula xml:id="formula_7">f r f t ∼ N µ µ , K k(s r , s t ) k(s r , s t ) T k(s t , s t ) ,<label>(8)</label></formula><p>where µ is the mean vector, k(s r , s t ) calculates the interimage covariance matrix between training set and testing set and k(s t , s t ) calculates the inner-image covariance of the test data. The fitting likelihood of the testing set to the generative model of the regular images can be expressed as,</p><formula xml:id="formula_8">f t |f r ∼ N µ + k(s r , s t ) T K −1 (f r − µ), k(s t , s t ) − k(s r , s t ) T K −1 k(s r , s t ) .<label>(9)</label></formula><p>Similarly, we can obtain the likelihood of the testing set given the "other class" training set. After obtaining the likelihood of the testing set given both regular training data and "other class" training data, we can compute the logarithm of the overall fitting likelihood of f t as</p><formula xml:id="formula_9">max log p(f t |f r ), log p(f t |f o ) ,<label>(10)</label></formula><p>where f o represents the scores of "other class" training set.</p><p>For either regular or "other class" test images, they could fit one of the generative models better than the irregular images. In other words, irregular images are supposed to obtain lower values in Eq. <ref type="bibr" target="#b9">(10)</ref>. Since the score obtained from Eq. (10) is negative (logarithm of a probability), we use the negative value of the score as the irregularity measurement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Experimental Settings</head><p>In this paper, we use the pre-trained CNN model <ref type="bibr" target="#b14">[15]</ref> as feature extractors for object detector learning. Specifically, we use the activations of both the second fully-connected layer and the last convolutional layer as the representation of the object proposal or the whole image. Feeding an image into the CNN model, the activations of a convolutional layer are n × m × d (e.g., 14 × 14 × 512 for the last convolutional layer) with n, m corresponding to different spatial locations and d the number of feature maps. Given a proposal, we aggregate the convolutional features covered by it via max pooling to obtain the proposal-level convolutional features. We perform L2 normalization to these two types of features separately and concatenate them as the final representation. The dimensionality of the features is 4,608.</p><p>For each class, we construct GP based generative models for regular images and "other class" images separately. For regular images, we initialize the value of the mean function as 3 and for "other class" images we set the initial value to be −3. The hyper-parameters a, b in Eq. (6) are both initialized to be 0.5 and γ is initialized randomly. We use the top-20 scored proposals of each image for both generative model construction and test image evaluation. The test data of each class is divided into three parts including regular images, irregular images and images belonging to other classes. We label irregular images as 1 and label regular and "other class" images as −1. Mean Average Precision (mAP) is employed to evaluate the performances of the approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Alternative Solutions</head><p>We compare our method to the following methods. Positive-negative Ratio If we apply an object detector to the image regions, considerable portion of the regions of a regular image should be positively scored. While on the contrary, images of other classes are supposed to have negatively-scored proposals only. Based on this intuitive assumption, we use the ratio of positive proposal number to the number of negative proposals within one image as its representation to construct two Gaussian models for regular images and "other class" images separately. Given a test image, we determine whether it is irregular via evaluating its fitting degree to these two Gaussians. Global SVM According to the analysis in Postulate I in Section 3, the classification score of an image reflects the degree of containing the regular object-of-interest and the scores of the three types of images (regular, irregular, other class) should form the relationship of f (I o ) &lt; f (I u ) &lt; f (I r ). For this method, we train a classifier for each class based on the global features of the images using linear SVM <ref type="bibr" target="#b5">[6]</ref> where regular images are used as positive data and "other class" images are treated as negative data. Assuming the mean of the decision scores of irregular images is 0, we use negative absolute value of the decision score −|f (I t )| as the irregularity measurement for a test image I t . MIL + Max The global representation of an image is a mixture of the patterns of both the object-of-interest and the background. To avoid the distraction influence of the background, for the second solution we use the maximum proposal-level score f max (I t ) as the decision score of each image based on the object detector learned from MIL. Similarly we use −|f max (I t )| as the irregularity measurement. MIL + Max + Gaussian Different from above MIL + Max strategy, we take into consideration the uncertainty of the distribution of the maximum detection scores via modelling the maximum scores of regular images I r and "other class" images I o using two Gaussian distributions separately. We use maximum likelihood to estimate the parameters of these two Gaussians (means and variances). Given a test image I t , we can calculate the likelihood of the image belonging to regular images as p(I t |I r ) and similarly the possibility of belonging to other classes as p(I t |I u ). Since an irregular image is expected to be able to fit neither of these two models, we set the final score of a test image as −max(p(I t |I r ), p(I t |I u )). MIL + Top k Instead of using the maximum score only, for this method, we obtain the image-level score f topk (I t ) of a test image I t by averaging the top k scores of its proposals. And the final score for an image is −|f topk (I t )|. Sparse coding Similar to <ref type="bibr" target="#b20">[21]</ref>, we use sparse coding based reconstruction error as the criterion for irregular image detection. The assumption is that both regular images and "other class" images can be well reconstructed by their corresponding dictionaries. For each class, we learn dictionaries for regular images and "other class" images separately. We try dictionary size 200, 4,000 and 5,000. Given a test image I t , we infer the coding vectors of its proposals and calculate the reconstruction residues of the proposals. Let r t r be the mean residue for this image calculated based on the dictionary learned from regular images and r t o be the mean residue based on the dictionary learned from "other class" images. For an irregular image, the errors of both models will be large. Thus the irregularity measurement can be calculated as min(r t r , r t o ). <ref type="table" target="#tab_0">Table 2</ref> shows the quantitative results. As can be seen, our method outperforms other compared methods. Also we show the ROC performances of our method and two most competitive methods on some example categories in <ref type="figure">Fig. 5</ref>. Both these two measurements demonstrate the effectiveness of the proposed method. The proposal ratio based method performs worst among these methods which indicates that the irregularity detection cannot be achieved by simply counting the number of positive and/or negative proposals. There are two reasons. The first is that the number of proposals varies between different images and the second reason is that for some irregular object images e.g., images of severely damaged cars, there may be no positively scored proposals detected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Quantitative Results</head><p>The next four methods are classification-based methods. While the first three use single score per image from either the global image or the region with maximum de-tection score, MIL+Top k utilizes multiple region scores but treat them as i.i.d. Global SVM achieves a mAP of 77.3% (when using fully-connected features only, we obtain 75.4%) which to some extent justifies Postulate I. However, as illustrated in <ref type="figure" target="#fig_2">Fig. 3</ref>, this strategy fails to distinguish some irregular images that obtain extreme high or low decision scores. A drawback of using image-level representation is that the background can influence the decision score especially when the background dominates the image. Multi-instance learning is supposed to be a remedy because it makes it possible to focus on the object-ofinterest via considering the proposal with maximum detection score. But using maximum detection score alone may risk missing the irregular part of the object. From <ref type="table" target="#tab_0">Table 2</ref>, we can see MIL+Max obtains comparable results to Global SVM. To take into consideration the uncertainty of the detection scores, rather than directly using the maximum detection scores, we construct Guassian models for the maximum scores of regular images and "other class" images separately and determine whether an image is irregular via evaluating its fitting likelihood to these two Gaussian models. However, the performance degrades to 70.7%. The reason may be that the distribution of the maximum detection scores is not strictly Gaussian. Instead of using the maximum detection score of each image, in MIL+Top20, we aggregate the top 20 scores of each image via average pooling. Benefiting from this strategy, the performances on some classes like apple, boat are obviously boosted. However, on some other classes such as horse, table lamp it shows inferior performance to Global SVM and MIL+Max. As can be seen, our method significantly outperforms this strategy on all the classes. This big gap may to a large extent result from our capabilities of modelling the inter-dependencies of the proposal-level scores within one image.</p><p>For sparse coding, we first test the performance using  dictionaries of size 200 as <ref type="bibr" target="#b20">[21]</ref> and the result is unsatisfactory which means 200 bases are not sufficient to cover the feature spaces of regular images or "other class" images. When the dictionary size is increased to 4,000, the performance is significantly improved. But after that continuing to increase the dictionary size (we test 5,000) can lead to no improvement any more. Our method outperforms sparse coding by 5.3%. Apart from effectiveness, our method is also more efficient than sparse coding. Given a test image, while sparse coding needs to infer the coding vector for the high-dimensional appearance features our method works on quite low-dimensional space as defined in Eq. (3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">Qualitative Results</head><p>Fig <ref type="figure">. 6</ref> demonstrates the qualitative comparison between our method and two compared methods Global SVM (GC) and Sparse coding (SC) on three object categories that are boat, motorbike and shoes. Comparing to our method, GC suffers from two drawbacks: 1) it subjects to the distraction influence of the background, and 2) it may ignore the fine details of the objects. Due to the influence of the background, GC may mistakenly classify the regular object within com-plex background into irregular object like the "shoes" on the right side of <ref type="figure">Fig. 6</ref>. Also, only looking at the global appearance makes it hard for GC to identify some irregular objects with fine irregularities such as the "broken boat" and "broken shoes" in <ref type="figure">Fig. 6</ref>. SC has similar deficiency that is it can be distracted or even dominated by the background. For example, the "capsized boat" is identified as "regular boat" while "regular motorbike" within complex background is regarded as "irregular motorbike". Comparing to these two methods our method is more robust. While using detection scores enables us to getting rid of the distraction influence of the background, modelling the inter-dependencies of the detection scores at multiple regions can help us to effectively discover the finer irregularities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>We have proposed a novel approach for the task of irregular object identification in an "open world" setting via inspecting the detection score patterns of an image. We propose to use Gaussian Processes to model the values as well the spatial distribution of the detection scores. It shows superior performance to some compared methods on a large dataset presented in this work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Examples of irregular images. Left column: aeroplane, apple, bus. Right column: horse, dining table, road.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Histograms of decision scores for regular images, irregular images and "other class" images in the testing data. The decision scores are obtained by applying the classifiers learned from global images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Visualization of spatial distribution of detection scores for test images of car class. Top-20 scored bounding boxes of an image are visualized. Positive proposals are visualized in green box and negative are visualized in yellow. From left to right: regular car, irregular car and other object (motorbike).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .Figure 6 .</head><label>56</label><figDesc>ROC curve for Sparse coding, Global SVM and our method on three categories. From left to right: boat, motorbike, shoes. Qualitative performance comparison between our method (GP) and two alternative solutions, Global SVM (GC) and Sparse coding (SC). Left column displays the false negative examples when fixing the false positive rate to be 0.2 where cross mark indicates false negative and check mark indicates true positive. Right column displays the false positive examples when fixing the true positive rate to be 0.9 where cross mark denotes false positive and check mark denotes true negative. Three categories are boat, shoes and motorbike.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><head>Table 2 .</head><label>2</label><figDesc>Experimental results. Average precision for each class and mAP are reported.</figDesc><table>Methods 
aeroplane 
apple 
bicycle 
boat 
building 
bus 
car 
chair 
cow 
dinging table 

Positive-negative Ratio 
58.0 
26.6 
50.4 
52.4 
60.0 
37.8 
55.4 
48.7 
31.6 
28.8 
Global SVM 
88.8 
70.8 
81.3 
82.9 
85.5 
76.4 
87.6 
69.7 
61.7 
79.8 
MIL + Max 
86.9 
70.0 
85.0 
78.8 
81.7 
77.6 
87.8 
70.5 
63.9 
76.4 
MIL + Max + Gaussian 
86.0 
72.1 
83.1 
78.5 
74.5 
76.3 
83.2 
59.3 
56.7 
68.4 
MIL + Top 20 
86.7 
78.3 
86.6 
86.9 
79.6 
75.2 
86.5 
64.0 
63.8 
56.8 
Sparse coding (200) 
86.9 
48.6 
80.6 
81.0 
82.8 
57.4 
82.8 
71.7 
56.1 
72.2 
Sparse coding (4,000) 
93.6 
74.5 
89.8 
86.7 
94.5 
86.1 
92.8 
78.7 
76.8 
86.0 
Ours 
95.4 
82.2 
91.2 
93.0 
94.6 
92.8 
95.1 
92.8 
92.0 
74.8 
Methods 
horse 
house motorbike road 
shoes 
sofa 
street table lamp train 
tree 
mAP 

Positive-negative Ratio 
23.9 
47.4 
30.9 
48.2 
56.4 
39.7 
42.7 
16.9 
28.6 
44.7 
41.4 
Global SVM 
73.3 
82.0 
75.6 
81.3 
88.2 
77.7 
73.8 
66.5 
69.2 
73.9 
77.3 
MIL + Max 
70.3 
80.0 
74.8 
78.1 
87.7 
76.4 
69.1 
65.1 
67.3 
77.0 
76.3 
MIL + Max + Gaussian 
63.1 
74.6 
65.9 
66.1 
85.8 
69.7 
55.5 
60.5 
64.1 
69.8 
70.7 
MIL + Top 20 
63.7 
76.4 
76.9 
73.6 
90.3 
69.7 
63.7 
52.3 
67.2 
75.2 
73.7 
Sparse coding (200) 
61.5 
71.3 
61.0 
80.1 
82.3 
80.2 
84.1 
52.3 
65.5 
57.6 
70.8 
Sparse coding (4,000) 
80.0 
89.3 
75.5 
89.9 
87.2 
87.7 
91.1 
67.9 
81.9 
78.9 
84.4 
Ours 
85.4 
94.4 
85.0 
90.8 
95.3 
88.9 
94.8 
78.3 
91.3 
85.0 
89.7 

</table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Gaussian process classification for segmenting and annotating sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Altun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Detecting irregularities in images and in video. IJCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Boiman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Exploiting hierarchical context on a large database of object categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Willsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Context models and out-of-context objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Willsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">The pascal visual object classes (voc) challenge. IJCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Liblinear: A library for large linear classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R.-E</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Detection and explanation of anomalous activities: representing activities as bags of event n-grams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hamid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Batta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bobick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Isbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Coleman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Gaussian process regression flow for analysis of motion trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Essa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Efficient sparse coding algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Battle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Raina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Is object localization for free? -weakly-supervised learning with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Abnormal object detection by canonical scene-based contextual model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Gaussian processes for machine learning (gpml) toolbox. JMLR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">E</forename><surname>Rasmussen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nickisch</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">E</forename><surname>Rasmussen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<title level="m">Gaussian Processes for Machine Learning (Adaptive Computation and Machine Learning</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Toward a taxonomy and computational models of abnormalities in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Elgammal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Selective search for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smeulders</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>IJCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">3d people tracking with gaussian process dynamical models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Efficient additive kernels via explicit feature maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Associative embeddings for large-scale knowledge transfer with self-assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vezhnevets</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Object localization in imagenet by looking out of the window</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vezhnevets</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Online detection of unusual events in videos via dynamic sparse coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Detecting unusual activity in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Visontai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
