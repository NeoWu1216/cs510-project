<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /mnt/c/Users/pc/Desktop/cs510_proj/grobid-0.5.6/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Visual Word2Vec (vis-w2v): Learning Visually Grounded Word Embeddings Using Abstract Scenes</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satwik</forename><surname>Kottur</surname></persName>
							<email>1skottur@andrew.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakrishna</forename><surname>Vedantam</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Virginia Tech</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">José</forename><forename type="middle">M F</forename><surname>Moura</surname></persName>
							<email>moura@ece.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
							<email>parikh@vt.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Virginia Tech</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Visual Word2Vec (vis-w2v): Learning Visually Grounded Word Embeddings Using Abstract Scenes</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.5.6" ident="GROBID" when="2019-12-02T16:48+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a model to learn visually grounded word embeddings (vis-w2v)  to capture visual notions of semantic relatedness. While word embeddings trained using text have been extremely successful, they cannot uncover notions of semantic relatedness implicit in our visual world. For instance, although "eats" and "stares at" seem unrelated in text, they share semantics visually. When people are eating something, they also tend to stare at the food. Grounding diverse relations like "eats" and "stares at" into vision remains challenging, despite recent progress in vision. We note that the visual grounding of words depends on semantics, and not the literal pixels. We thus use abstract scenes created from clipart to provide the visual grounding. We find that the embeddings we learn capture fine-grained, visually grounded notions of semantic relatedness. We show improvements over text-only word embeddings (word2vec) on three tasks: common-sense assertion classification, visual paraphrasing and text-based image retrieval. Our code and datasets are available online.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Artificial intelligence (AI) is an inherently multi-modal problem: understanding and reasoning about multiple modalities (as humans do), seems crucial for achieving artificial intelligence (AI). Language and vision are two vital interaction modalities for humans. Thus, modeling the rich interplay between language and vision is one of fundamental problems in AI.</p><p>Language modeling is an important problem in natural language processing (NLP). A language model estimates the likelihood of a word conditioned on other (context) words in a sentence. There is a rich history of works on ngram based language modeling <ref type="bibr" target="#b4">[4,</ref><ref type="bibr" target="#b17">17]</ref>. It has been shown that simple, count-based models trained on millions of sentences can give good results. However, in recent years, neural language models <ref type="bibr" target="#b3">[3,</ref><ref type="bibr" target="#b32">32]</ref> have been explored. Neural language models learn mappings (W : words → R n ) from * Equal contribution We ground text-based word2vec (w2v) embeddings into vision to capture a complimentary notion of visual relatedness. Our method (vis-w2v) learns to predict the visual grounding as context for a given word. Although "eats" and "stares at" seem unrelated in text, they share semantics visually. Eating involves staring or looking at the food that is being eaten. As training proceeds, embeddings change from w2v (red) to vis-w2v (blue).</p><p>words (encoded using a dictionary) to a real-valued vector space (embedding), to maximize the log-likelihood of words given context. Embedding words into such a vector space helps deal with the curse of dimensionality, so that we can reason about similarities between words more effectively. One popular architecture for learning such an embedding is word2vec <ref type="bibr" target="#b31">[31,</ref><ref type="bibr" target="#b33">33]</ref>. This embedding captures rich notions of semantic relatedness and compositionality between words <ref type="bibr" target="#b33">[33]</ref>.</p><p>For tasks at the intersection of vision and language, it seems prudent to model semantics as dictated by both text and vision. It is especially challenging to model finegrained interactions between objects using only text. Consider the relations "eats" and "stares at" in <ref type="figure" target="#fig_0">Fig. 1</ref>. When reasoning using only text, it might prove difficult to realize that these relations are semantically similar. However, by grounding the concepts into vision, we can learn that these relations are more similar than indicated by text. Thus, visual grounding provides a complimentary notion of semantic relatedness. In this work, we learn word embeddings to capture this grounding.</p><p>Grounding fine-grained notions of semantic relatedness between words like "eats" and "stares at" into vision is a challenging problem. While recent years have seen tremendous progress in tasks like image classification <ref type="bibr" target="#b20">[20]</ref>, detection <ref type="bibr" target="#b13">[13]</ref>, semantic segmentation <ref type="bibr" target="#b25">[25]</ref>, action recognition <ref type="bibr" target="#b27">[27]</ref>, etc., modeling fine-grained semantics of interactions between objects is still a challenging task. However, we observe that it is the semantics of the visual scene that matter for inferring the visually grounded semantic relatedness, and not the literal pixels <ref type="figure" target="#fig_0">(Fig. 1)</ref>. We thus use abstract scenes made from clipart to provide the visual grounding. We show that the embeddings we learn using abstract scenes generalize to text describing real images (Sec. 6.1).</p><p>Our approach considers visual cues from abstract scenes as context for words. Given a set of words and associated abstract scenes, we first cluster the scenes in a rich semantic feature space capturing the presence and locations of objects, pose, expressions, gaze, age of people, etc. Note that these features can be trivially extracted from abstract scenes. Using these features helps us capture fine-grained notions of semantic relatedness <ref type="figure">(Fig. 4)</ref>. We then train to predict the cluster membership from pre-initialized word embeddings. The idea is to bring embeddings for words with similar visual instantiations closer, and push words with different visual instantiations farther <ref type="figure" target="#fig_0">(Fig. 1)</ref>. The word embeddings are initialized with word2vec <ref type="bibr" target="#b33">[33]</ref>. The clusters thus act as surrogate classes. Note that each surrogate class may have images belonging to concepts which are different in text, but are visually similar. Since we predict the visual clusters as context given a set of input words, our model can be viewed as a multi-modal extension of the continuous bag of words (CBOW) <ref type="bibr" target="#b33">[33]</ref> word2vec model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contributions:</head><p>We propose a novel model visual word2vec (vis-w2v) to learn visually grounded word embeddings. We use abstract scenes made from clipart to provide the grounding. We demonstrate the benefit of vis-w2v on three tasks which are ostensibly in text, but can benefit from visual grounding: common sense assertion classification <ref type="bibr" target="#b35">[35]</ref>, visual paraphrasing <ref type="bibr" target="#b24">[24]</ref>, and text-based image retrieval <ref type="bibr" target="#b15">[15]</ref>. Common sense assertion classification <ref type="bibr" target="#b35">[35]</ref> is the task of modeling the plausibility of common sense assertions of the form (boy, eats, cake). Visual paraphrasing <ref type="bibr" target="#b24">[24]</ref> is the task of determining whether two sentences describe the same underlying scene or not. Text-based image retrieval is the task of retrieving images by matching accompanying text with textual queries. We show consistent improvements over baseline word2vec (w2v) models on these tasks. Infact, on the common sense assertion classification task, our models surpass the state of the art.</p><p>The rest of the paper is organized as follows. Sec. 2 discusses related work on learning word embeddings, learning from visual abstraction, etc. Sec. 3 presents our approach. Sec. 4 describes the datasets we work with. We provide experimental details in Sec. 5 and results in Sec. 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Word Embeddings: Word embeddings learnt using neural networks <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b33">33]</ref> have gained a lot of popularity recently. These embeddings are learnt offline and then typically used to initialize a multi-layer neural network language model <ref type="bibr" target="#b3">[3,</ref><ref type="bibr" target="#b32">32]</ref>. Similar to those approaches, we learn word embeddings from text offline, and finetune them to predict visual context. Xu et al. <ref type="bibr" target="#b43">[43]</ref> and Lazaridou et al. <ref type="bibr" target="#b22">[22]</ref> use visual cues to improve the word2vec representation by predicting real image representations from word2vec and maximizing the dot product between image features and word2vec respectively. While their focus is on capturing appearance cues (separating cats and dogs based on different appearance), we instead focus on capturing fine-grained semantics using abstract scenes. Other works use visual and textual attributes (e.g. vegetable is an attribute for potato) to improve distributional models of word meaning <ref type="bibr" target="#b39">[39,</ref><ref type="bibr" target="#b40">40]</ref>. In contrast to these approaches, our set of visual concepts need not be explicitly specified, it is implicitly learnt in the clustering step. Many works use word embeddings as parts of larger models for tasks such as image retrieval <ref type="bibr" target="#b18">[18]</ref>, image captioning <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b42">42]</ref>, etc. These multi-modal embeddings capture regularities like compositional structure between images and words. For instance, in such a multi-modal embedding space, "image of blue car" -"blue" + "red" would give a vector close to "image of red car". In contrast, we want to learn unimodal (textual) embeddings which capture multi-modal semantics. For example, we want to learn that "eats" and "stares at" are (visually) similar.</p><p>Surrogate Classification: There has been a lot of recent work on learning with surrogate labels due to interest in unsupervised representation learning. Previous works have used surrogate labels to learn image features <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b9">9]</ref>. In contrast, we are interested in augmenting word embeddings with visual semantics. Also, while previous works have created surrogate labels using data transformations <ref type="bibr" target="#b9">[9]</ref> or sampling <ref type="bibr" target="#b7">[7]</ref>, we create surrogate labels by clustering abstract scenes in a semantically rich feature space.</p><p>Learning from Visual Abstraction: Visual abstractions have been used for a variety of high-level scene understanding tasks recently. Zitnick et al. <ref type="bibr" target="#b44">[44,</ref><ref type="bibr" target="#b45">45]</ref> learn the importance of various visual features (occurrence and cooccurrence of objects, expression, gaze, etc.) in determining the meaning or semantics of a scene. <ref type="bibr" target="#b46">[46]</ref> and <ref type="bibr" target="#b10">[10]</ref> learn the visual interpretation of sentences and the dynam-ics of objects in temporal abstract scenes respectively. Antol et al. <ref type="bibr" target="#b1">[2]</ref> learn models of fine-grained interactions between pairs of people using visual abstractions. Lin and Parikh <ref type="bibr" target="#b24">[24]</ref> "imagine" abstract scenes corresponding to text, and use the common sense depicted in these imagined scenes to solve textual tasks such as fill-in-the-blanks and paraphrasing. Vedantam et al. <ref type="bibr" target="#b35">[35]</ref> classify common sense assertions as plausible or not by using textual and visual cues. In this work, we experiment with the tasks of <ref type="bibr" target="#b24">[24]</ref> and <ref type="bibr" target="#b35">[35]</ref>, which are two tasks in text that could benefit from visual grounding. Interestingly, by learning vis-w2v, we eliminate the need for explicitly reasoning about abstract scenes at test time, i.e., the visual grounding captured in our word embeddings suffices.</p><p>Language, Vision and Common Sense: There has been a surge of interest in problems at the intersection of language and vision recently. Breakthroughs have been made in tasks like image captioning <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b8">8,</ref><ref type="bibr" target="#b14">14,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b42">42]</ref>, video description <ref type="bibr" target="#b8">[8,</ref><ref type="bibr" target="#b37">37]</ref>, visual question answering <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b11">11,</ref><ref type="bibr" target="#b12">12,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b36">36]</ref>, aligning text and vision <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b18">18]</ref>, etc. In contrast to these tasks (which are all multi-modal), our tasks themselves are unimodal (i.e., in text), but benefit from using visual cues. Recent work has also studied how vision can help common sense reasoning <ref type="bibr" target="#b35">[35,</ref><ref type="bibr" target="#b38">38]</ref>. In comparison to these works, our approach is generic, i.e., can be used for multiple tasks (not just common sense reasoning).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach</head><p>Recall that our vis-w2v model grounds word embeddings into vision by treating vision as context. We first detail our inputs. We then discuss our vis-w2v model. We then describe the clustering procedure to get surrogate semantic labels, which are used as visual context by our model. We then describe how word-embeddings are initialized. Finally, we draw connections to word2vec (w2v) models.</p><p>Input: We are given a set of pairs of visual scenes and as-</p><formula xml:id="formula_0">sociated text D = {(v, w)} d in order to train vis-w2v.</formula><p>Here v refers to the image features and w refers to the set of words associated with the image. At each step of training, we select a window S w ⊆ w to train the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model:</head><p>Our vis-w2v model <ref type="figure" target="#fig_1">(Fig. 2)</ref> is a neural network that accepts as input a set of words S w and a visual feature instance v. Each of the words w i ∈ S w is represented via a one-hot encoding. A one-hot encoding enumerates over the set of words in a vocabulary (of size N V ) and places a 1 at the index corresponding to the given word. This one-hot encoded input is transformed using a projection matrix W I of size N V × N H that connects the input layer to the hidden layer, where the hidden layer has a dimension of N H . Intuitively, N H decides the capacity of the representation. Consider an input one-hot encoded word w i whose j th in- dex is set to 1. Since w i is one-hot encoded, the hidden activation for this word (H wi ) is a row in the weight matrix W j I , i.e., H wi = W j I . The resultant hidden activation H would then be the average of individual hidden activations H wi as W I is shared among all the words S w , i.e.,:</p><formula xml:id="formula_1">H = 1 |S w | wi∈Sw⊆w H wi<label>(1)</label></formula><p>Given the hidden activation H, we multiply it with an output weight matrix W O of size N H × N K , where N K is the number of output classes. The output class (described next) is a discrete-valued function of the visual features G(v) (more details in next paragraph). We normalize the output activations O = H ×W O to form a distribution using the softmax function. Given the softmax outputs, we minimize the negative log-likelihood of the correct class conditioned on the input words:</p><formula xml:id="formula_2">min W I ,W O − log P (G(v)|S w , W I , W O )<label>(2)</label></formula><p>We optimize for this objective using stochastic gradient descent (SGD) with a learning rate of 0.01.</p><p>Output Classes: As mentioned in the previous section, the target classes for the neural network are a function G(·) of the visual features. What would be a good choice for G? Recall that our aim is to recover an embedding for words that respects similarities in visual instantiations of words <ref type="figure" target="#fig_0">(Fig. 1</ref>). To capture this visual similarity, we model G : v → {1, · · · ,N K } as a grouping function 1 . In practice, this function is learnt offline using clustering with Kmeans. That is, the outputs from clustering are the surro-gate class labels used in vis-w2v training. Since we want our embeddings to reason about fine-grained visual grounding (e.g. "stares at" and "eats"), we cluster in the abstract scenes feature space (Sec. 4). See <ref type="figure">Fig. 4</ref> for an illustration of what clustering captures. The parameter N K in K-means modulates the granularity at which we reason about visual grounding.</p><p>Initialization: We initialize the projection matrix parameters W I with those from training w2v on large text corpora.</p><p>The hidden-to-output layer parameters are initialized randomly. Using w2v is advantageous for us in two ways: i) w2v embeddings have been shown to capture rich semantics and generalize to a large number of tasks in text. Thus, they provide an excellent starting point to finetune the embeddings to account for visual similarity as well. ii) Training on a large corpus gives us good coverage in terms of the vocabulary. Further, since the gradients during backpropagation only affect parameters/embeddings for words seen during training, one can view vis-w2v as augmenting w2v with visual information when available. In other words, we retain the rich amount of non-visual information already present in it 2 . Indeed, we find that the random initialization does not perform as well as initialization with w2v when training vis-w2v.</p><p>Design Choices: Our model (Sec. 3) admits choices of w in a variety of forms such as full sentences or tuples of the form (Primary Object, Relation, Secondary Object). The exact choice of w is made depending upon on what is natural for the task of interest. For instance, for common sense assertion classification and text-based image retrieval, w is a phrase from a tuple, while for visual paraphrasing w is a sentence. Given w, the choice of S w is also a design parameter tweaked depending upon the task. It could include all of w (e.g., when learning from a phrase in the tuple) or a subset of the words (e.g., when learning from an n-gram context-window in a sentence). While the model itself is task agnostic, and only needs access to the words and visual context during training, the validation and test performances are calculated using the vis-w2v embeddings on a specific task of interest (Sec. 5). This is used to choose the hyperparameters N K and N H .</p><p>Connections to w2v: Our model can be seen as a multimodal extension of the continuous bag of words (CBOW) w2v models. The CBOW w2v objective maximizes the likelihood P (w|S w , W I , W O ) for a word w and its context S w . On the other hand, we maximize the likelihood of the visual context given a set of words S w (Eq. 2).  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Applications</head><p>We compare vis-w2v and w2v on the tasks of common sense assertion classification (Sec. 4.1), visual paraphrasing (Sec. 4.2), and text-based image retrieval (Sec. 4.3). We give details of each task and the associated datasets below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Common Sense Assertion Classification</head><p>We study the relevance of vis-w2v to the common sense (CS) assertion classification task introduced by Vedantam et al. <ref type="bibr" target="#b35">[35]</ref>. Given common sense tuples of the form (primary object or t P , relation or t R , secondary object or t S ) e.g. (boy, eats, cake), the task is to classify it as plausible or not. The CS dataset contains 14,332 TEST assertions (spanning 203 relations) out of which 37% are plausible, as indicated by human annotations. These TEST assertions are extracted from the MS COCO dataset <ref type="bibr" target="#b23">[23]</ref>, which contains real images and captions. Evaluating on this dataset allows us to demonstrate that visual grounding learnt from the abstract world generalizes to the real world. <ref type="bibr" target="#b35">[35]</ref> approaches the task by constructing a multimodal similarity function between TEST assertions whose plausibility is to be evaluated, and TRAIN assertions that are known to be plausible. The TRAIN dataset also contains 4260 abstract scenes made from clipart depicting 213 relations between various objects (20 scenes per relation). Each scene is annotated with one tuple that names the primary object, relation, and secondary object depicted in the scene. Abstract scene features (from <ref type="bibr" target="#b35">[35]</ref>) describing the interaction between objects such as relative location, pose, absolute location, etc. are used for learning vis-w2v. More details of the features can be found in <ref type="bibr" target="#b19">[19]</ref>. We use the VAL set from <ref type="bibr" target="#b35">[35]</ref> (14,548 assertions) to pick the hyperparameters. Since the dataset contains tuples of the form (t P , t R , t S ), we explore learning vis-w2v with separate models for each, and a shared model irrespective of the word being t P , t R , or t S .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Visual Paraphrasing</head><p>Visual paraphrasing (VP), introduced by Lin and Parikh <ref type="bibr" target="#b24">[24]</ref> is the task of determining if a pair of descriptions describes the same scene or two different scenes. The dataset introduced by <ref type="bibr" target="#b24">[24]</ref> contains 30,600 pairs of descriptions, of which a third are positive (describe the same scene) and the rest are negatives. The TRAIN dataset contains 24,000 VP pairs whereas the TEST dataset contains 6,060 VP pairs. Each description contains three sentences. We use scenes and descriptions from Zitnick et al. <ref type="bibr" target="#b46">[46]</ref> to train vis-w2v models, similar to Lin and Parikh. The abstract scene feature set from <ref type="bibr" target="#b46">[46]</ref> captures occurrence of objects, person attributes (expression, gaze, and pose), absolute spatial location and co-occurrence of objects, relative spatial location between pairs of objects, and depth ordering (3 discrete depths), relative depth and flip. We withhold a set of 1000 pairs (333 positive and 667 negative) from TRAIN to form a VAL set to pick hyperparameters. Thus, our VP TRAIN set has 23,000 pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Text-based Image Retrieval</head><p>In order to verify if our model has learnt the visual grounding of concepts, we study the task of text-based image retrieval. Given a query tuple, the task is to retrieve the image of interest by matching the query and ground truth tuples describing the images using word embeddings. For this task, we study the generalization of vis-w2v embeddings learnt for the common sense (CS) task, i.e., there is no training involved. We augment the common sense (CS) dataset <ref type="bibr" target="#b35">[35]</ref> (Sec. 4.1) to collect three query tuples for each of the original 4260 CS TRAIN scenes. Each scene in the CS TRAIN dataset has annotations for which objects in the scene are the primary and secondary objects in the ground truth tuples. We highlight the primary and secondary objects in the scene and ask workers on AMT to name the primary, secondary objects, and the relation depicted by the interaction between them. Some examples can be seen in <ref type="figure" target="#fig_2">Fig. 3</ref>. Interestingly, some scenes elicit diverse tuples whereas others tend to be more constrained. This is related to the notion of Image Specificity <ref type="bibr" target="#b15">[15]</ref>. Note that the workers do not see the original (ground truth) tuple written for the scene from the CS TRAIN dataset. More details of the interface are provided in <ref type="bibr" target="#b19">[19]</ref>. We use the collected tuples as queries for performing the retrieval task. Note that the queries used at test time were never used for training vis-w2v.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experimental Setup</head><p>We now explain our experimental setup. We first explain how we use our vis-w2v or baseline w2v (word2vec) model for the three tasks described above: common sense (CS), visual paraphrasing (VP), and text-based image retrieval. We also provide evaluation details. We then list the baselines we compare to for each task and discuss some design choices. For all the tasks, we preprocess raw text by tokenizing using the NLTK toolkit <ref type="bibr" target="#b26">[26]</ref>. We implement vis-w2v as an extension of the Google C implementation of word2vec 3 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Common Sense Assertion Classification</head><p>The task in common sense assertion classification (Sec. 4.1) is to compute the plausibility of a test assertion based on its similarity to a set of tuples (Ω = {t i } I i=1 ) known to be plausible. Given a tuple t ′ =(Primary Object t ′ P , Relation t ′ R , Secondary Object t ′ S ) and a training instance t i , the plausibility scores are computed as follows:</p><formula xml:id="formula_3">h(t ′ , t i ) = W P (t ′ P ) T W P (t i P ) + W R (t ′ R ) T W R (t i R ) + W S (t ′ S ) T W S (t i S ) (3)</formula><p>where W P , W R , W S represent the corresponding word embedding spaces. The final text score is given as follows:</p><formula xml:id="formula_4">f (t ′ ) = 1 |I| i∈I max(h(t ′ , t i ) − δ, 0)<label>(4)</label></formula><p>where i sums over the entire set of training tuples. We use the value of δ used by <ref type="bibr" target="#b35">[35]</ref> for our experiments. <ref type="bibr" target="#b35">[35]</ref> share embedding parameters across t P , t R , t S in their text based model. That is, W P = W R = W S . We call this the shared model. When W P , W R , W S are learnt independently for (t P , t R , t S ), we call it the separate model.</p><p>The approach in <ref type="bibr" target="#b35">[35]</ref> also has a visual similarity function that combines text and abstract scenes that is used along with this text-based similarity. We use the text-based approach for evaluating both vis-w2v and baseline w2v. However, we also report results including the visual similarity function along with text similarity from vis-w2v. In line with <ref type="bibr" target="#b35">[35]</ref>, we also evaluate our results using average precision (AP) as a performance metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Visual Paraphrasing</head><p>In the visual paraphrasing task (Sec. 4.2), we are given a pair of descriptions at test time. We need to assign a score to each pair indicating how likely they are to be paraphrases, i.e., describing the same scene. Following <ref type="bibr" target="#b24">[24]</ref> we average word embeddings (vis-w2v or w2v) for the sentences and plug them into their text-based scoring function. This scoring function combines term frequency, word co-occurrence statistics and averaged word embeddings to assess the final paraphrasing score. The results are evaluated using average precision (AP) as the metric. While training both vis-w2v and w2v for the task, we append the sentences from the train set of <ref type="bibr" target="#b24">[24]</ref> to the original word embedding training corpus to handle vocabulary overlap issues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Text-based Image Retrieval</head><p>We compare w2v and vis-w2v on the task of textbased image retrieval (Sec. 4.3). The task involves retrieving the target image from an image database, for a query tuple. Each image in the database has an associated ground truth tuple describing it. We use these to rank images by computing similarity with the query tuple. Given tuples of the form (t P , t R , t S ), we average the vector embeddings for all words in t P , t R , t S . We then explore separate and shared models just as we did for common sense assertion classification. In the separate model, we first compute the cosine similarity between the query and the ground truth for t P , t R , t S separately and average the three similarities. In the shared model, we average the word embeddings for t P , t R , t S for query and ground truth and then compute the cosine similarity between the averaged embeddings. The similarity scores are then used to rank the images in the database for the query. We use standard metrics for retrieval tasks to evaluate: Recall@1 (R@1), Recall@5 (R@5), Recall@10 (R@10) and median rank (med R) of target image in the returned result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Baselines</head><p>We describe some baselines in this subsection. In general, we consider two kinds of w2v models: those learnt from generic text, e.g., Wikipedia (w2v-wiki) and those learnt from visual text, e.g., MS COCO (w2v-coco), i.e., text describing images. Embeddings learnt from visual text typically contain more visual information <ref type="bibr" target="#b35">[35]</ref>. vis-w2v-wiki are vis-w2v embeddings learnt using w2v-wiki as an initialization to the projection matrix, while vis-w2v-coco are the vis-w2v embeddings learnt using w2v-coco as the initialization. In all settings, we are interested in studying the performance gains on using vis-w2v over w2v. Although our training procedure itself is task agnostic, we train separately on the common sense (CS) and the visual paraphrasing (VP) datasets. We study generalization of the embeddings learnt for the CS task on the text-based image retrieval task. Additional design choices pertaining to each task are discussed in Sec. 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Results</head><p>We present results on common sense (CS), visual paraphrasing (VP), and text-based image retrieval tasks. We compare our approach to various baselines as explained in Sec. 5 for each application. Finally, we train our model using real images instead of abstract scenes, and analyze differences. More details on the effect of hyperparameters on performance (for CS and VP) can be found in <ref type="bibr" target="#b19">[19]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Common Sense Assertion Classification</head><p>We first present our results on the common sense assertion classification task (Sec. 4.1). We report numbers with  <ref type="table">Table 1</ref>: Performance on the common sense task of <ref type="bibr" target="#b35">[35]</ref> a fixed hidden layer size, N H = 200 (to be comparable to <ref type="bibr" target="#b35">[35]</ref>) in <ref type="table">Table.</ref> 1. We use N K = 25, which gives the best performance on validation. We handle tuple elements, t P , t R or t S , with more than one word by placing each word in a separate window (i.e. |S w | = 1). For instance, the element "lay next to" is trained by predicting the associated visual context thrice with "lay", "next" and "to" as inputs.</p><p>Overall, we find an increase of 2.6% with vis-w2v-coco (separate) model over the w2v-coco model used in <ref type="bibr" target="#b35">[35]</ref>. We achieve larger gains (5.8%) with vis-w2v-wiki over w2v-wiki. Interestingly, the tuples in the common sense task are extracted from the MS COCO <ref type="bibr" target="#b23">[23]</ref> dataset. Thus, this is an instance where vis-w2v (learnt from abstract scenes) generalizes to text describing real images.</p><p>Our vis-w2v-coco (both shared and separate) embeddings outperform the joint w2v-coco + vision model from <ref type="bibr" target="#b35">[35]</ref> that reasons about visual features for a given test tuple, which we do not. Note that both models use the same training and validation data, which suggests that our vis-w2v model captures the grounding better than their multi-modal text + visual similarity model. Finally, we sweep for the best value of N H for the validation set and find that vis-w2v-coco (separate) gets the best AP of 75.4% on TEST with N H = 50. This is our best performance on this task.</p><p>Separate vs. Shared: We next compare the performance when using the separate and shared vis-w2v models. We find that vis-w2v-coco (separate) does better than vis-w2v-coco (shared) (74.8% vs. 74.5%), presumably because the embeddings can specialize to the semantic roles words play when participating in t P , t R or t S . In terms of shared models alone, vis-w2v-coco (shared) achieves a gain in performance of 2.3% over the w2v-coco model of <ref type="bibr" target="#b35">[35]</ref>, whose textual models are all shared.</p><p>What Does Clustering Capture? We next visualize the semantic relatedness captured by clustering in the abstract scenes feature space <ref type="figure">(Fig. 4)</ref>. Recall that clustering gives us surrogate labels to train vis-w2v. For the visualization, we pick a relation and display other relations that co-occur the most with it in the same cluster. Interestingly, words like "prepare to cut", "hold", "give" occur often with "stare at". Thus, we discover the fact that when we "prepare to cut" lay next to stand near stare at enjoy <ref type="figure">Figure 4</ref>: Visualization of the clustering used to supervise vis-w2v training. Relations that co-occur more often in the same cluster appear bigger than others. Observe how semantically close relations co-occur the most, e.g., eat, drink, chew on for the relation enjoy. something, we also tend to "stare at" it. Reasoning about such notions of semantic relatedness using purely textual cues would be prohibitively difficult. We provide more examples in <ref type="bibr" target="#b19">[19]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Visual Paraphrasing</head><p>We next describe our results on the Visual Paraphrasing (VP) task (Sec. 4.2). The task is to determine if a pair of descriptions are describing the same scene. Each description has three sentences. <ref type="table">Table.</ref> 2 summarizes our results and compares performance to w2v. We vary the size of the context window S w and check performance on the VAL set. We obtain best results with the entire description as the context window S w , N H = 200, and N K = 100. Our vis-w2v models give an improvement of 0.7% on both w2v-wiki and w2v-coco respectively. In comparison to w2v-wiki approach from <ref type="bibr" target="#b24">[24]</ref>, we get a larger gain of 1.2% with our vis-w2v-coco embeddings 4 . Lin and Parikh <ref type="bibr" target="#b24">[24]</ref> imagine the visual scene corresponding to text to solve the task. Their combined text + imagination model performs 0.2% better (95.5%) than our model. Note that our approach does not have the additional expensive step of generating an imagined visual scene for each instance at test time. Qualitative examples of success and failure cases are shown in <ref type="figure">Fig. 5</ref>.</p><p>Window Size: Since the VP task is on multi-sentence descriptions, it gives us an opportunity to study how size of the window (S w ) used in training affects performance. We evaluate the gains obtained by using window sizes of entire description, single sentence, 5 words, and single word respectively. We find that description level windows and sentence level windows give equal gains. However, performance tapers off as we reduce the context to 5 words (0.6% gain) and a single word (0.1% gain). This is intuitive, since VP requires us to reason about entire descriptions to deter-Jenny is kicking Mike.</p><p>Mike dropped the soccer ball on the duck. There is a sandbox nearby.</p><p>Mike and Jenny are surprised. Mike and Jenny are playing soccer. The duck is beside the soccer ball.</p><p>Mike is in the sandbox.</p><p>Jenny is waving at Mike. It is a sunny day at the park.</p><p>Jenny is very happy. Mike is sitting in the sand box. Jenny has on the color pink.</p><p>Mike and Jenny say hello to the dog. Mike's dog followed him to the park. Mike and Jenny are camping in the park.</p><p>The cat is next to Mike. The dog is looking at the cat. Jenny is waving at the dog.  <ref type="table">Table 2</ref>: Performance on visual paraphrasing task of <ref type="bibr" target="#b24">[24]</ref>. mine paraphrases. Further, since the visual features in this dataset are scene level (and not about isolated interactions between objects), the signal in the hidden layer is stronger when an entire sentence is used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Text-based Image Retrieval</head><p>We next present results on the text-based image retrieval task (Sec. 4.3). This task requires visual grounding as the query and the ground truth tuple can often be different by textual similarity, but could refer to the same scene <ref type="figure" target="#fig_2">(Fig. 3)</ref>. As explained in Sec. 4.3, we study generalization of the embeddings learnt during the commonsense experiments to this task. <ref type="table">Table.</ref> 3 presents our results. Note that vis-w2v here refers to the embeddings learnt using the CS dataset. We find that the best performing models are vis-w2v-wiki (shared) (as per R@1, R@5, medR) and vis-w2v-coco (separate) (as per R@10, medR). These get Recall@10 scores of ≈49.5% whereas the baseline w2v-wiki and w2v-coco embeddings give scores of 45.4% and 47.6%, respectively.  <ref type="table">Table 3</ref>: Performance on text-based image retrieval. R@x: higher is better, medR: lower is better</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.">Real Image Experiment</head><p>Finally, we test our vis-w2v approach with real images on the CS task, to evaluate the need to learn fine-grained visual grounding via abstract scenes. Thus, instead of semantic features from abstract scenes, we obtain surrogate labels by clustering real images from the MS COCO dataset using fc7 features from the VGG-16 <ref type="bibr" target="#b41">[41]</ref> CNN. We cross validate to find the best number of clusters and hidden units. We perform real image experiments in two settings: 1) We use all of the MS COCO dataset after removing the images whose tuples are in the CS TEST set of <ref type="bibr" target="#b35">[35]</ref>. This gives us a collection of ≈ 76K images to learn vis-w2v. MS COCO dataset has a collection of 5 captions for each image. We use all these five captions with sentence level context 5 windows to learn vis-w2v80K. 2) We create a real image dataset by collecting 20 real images from MS COCO and their corresponding tuples, randomly selected for each of 213 relations from the VAL set (Sec. 5.1). Analogous to the CS TRAIN set containing abstract scenes, this gives us a dataset of 4260 real images along with an associate tuple, depicting the 213 CS VAL relations. We refer to this model as vis-w2v4K.</p><p>We report the gains in performance over w2v baselines in both scenario 1) and 2) for the common sense task. We find that using real images gives a best-case performance of 73.7% starting from w2v-coco for vis-w2v80K (as compared to 74.8% using CS TRAIN abstract scenes). For vis-w2v4K-coco, the performance on the validation actually goes down during training. If we train vis-w2v4K starting with generic text based w2v-wiki, we get a performance of 70.8% (as compared to 74.2% using CS TRAIN abstract scenes). This shows that abstract scenes are better at visual grounding as compared to real images, due to their rich semantic features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Discussion</head><p>Antol et al. <ref type="bibr" target="#b1">[2]</ref> have studied generalization of classification models learnt on abstract scenes to real images. The idea is to transfer fine-grained concepts that are easier to learn in the fully-annotated abstract domain to tasks in the real domain. Our work can also be seen as a method of studying generalization. One can view vis-w2v as a way <ref type="bibr" target="#b5">5</ref> We experimented with other choices but found this works best.</p><p>to transfer knowledge learnt in the abstract domain to the real domain, via text embeddings (which are shared across the abstract and real domains). Our results on commonsense assertion classification show encouraging preliminary evidence of this.</p><p>We next discuss some considerations in the design of the model. A possible design choice when learning embeddings could have been to construct a triplet loss function, where the similarity between a tuple and a pair of visual instances can be specified. That is, given a textual instance A, and two images B and C (where A describes B, and not C), one could construct a loss that enforces sim(A, B) &gt; sim(A, C), and learn joint embeddings for words and images. However, since we want to learn hidden semantic relatedness (e.g."eats", "stares at"), there is no explicit supervision available at train time on which images and words should be related. Although the visual scenes and associated text inherently provide information about related words, they do not capture the unrelatedness between words, i.e., we do not have negatives to help us learn the semantics.</p><p>We can also understand vis-w2v in terms of data augmentation. With infinite text data describing scenes, distributional statistics captured by w2v would reflect all possible visual patterns as well. In this sense, there is nothing special about the visual grounding. The additional modality helps to learn complimentary concepts while making efficient use of data. Thus, the visual grounding can be seen as augmenting the amount of textual data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Conclusion</head><p>We learn visually grounded word embeddings (vis-w2v) from abstract scenes and associated text. Abstract scenes, being trivially fully annotated, give us access to a rich semantic feature space. We leverage this to uncover visually grounded notions of semantic relatedness between words that would be difficult to capture using text alone or using real images. We demonstrate the visual grounding captured by our embeddings on three applications that are in text, but benefit from visual cues: 1) common sense assertion classification, 2) visual paraphrasing, and 3) text-based image retrieval. Our method outperforms word2vec (w2v) baselines on all three tasks. Further, our method can be viewed as a modality to transfer knowledge from the abstract scenes domain to the real domain via text. Our datasets, code, and vis-w2v embeddings are available for public use.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: We ground text-based word2vec (w2v) embeddings into vision to capture a complimentary notion of visual relatedness. Our method (vis-w2v) learns to predict the visual grounding as context for a given word. Although "eats" and "stares at" seem unrelated in text, they share semantics visually. Eating involves staring or looking at the food that is being eaten. As training proceeds, embeddings change from w2v (red) to vis-w2v (blue).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Proposed vis-w2v model. The input layer (red) has multiple one-hot word encodings. These are connected to the hidden layer with the projection matrix W I , i.e., all the inputs share the same weights. It is finally connected to the output layer via W O . Model predicts the visual context O given the text input S w = {w l }.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Examples tuples collected for the text-based image retrieval task. Notice that multiple relations can have the same visual instantiation (left).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>Query Tuple: Query Tuple: baby lays with woman baby on top of woman baby is held by woman</figDesc><table>baby sleep next to lady 
woman hold onto cat 

woman holds cat 
woman holds cat 
woman holds cat 

Original Tuple: 
Original Tuple: 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head></head><label></label><figDesc>Note that the red instance is tough as the textual descriptions do not intuitively seem to be talking about the same scene, even for a human reader.</figDesc><table>Figure 5: The visual paraphrasing task is to identify if two 
textual descriptions are paraphrases of each other. Shown 
above are three positive instances, i.e., the descriptions (left, 
right) actually talk about the same scene (center, shown for 
illustration, not avaliable as input). Green boxes show two 
cases where vis-w2v correctly predicts and w2v does not, 
while red box shows the case where both vis-w2v and 
w2v predict incorrectly. Approach 
Visual Paraphrasing AP (%) 

w2v-wiki (from [24]) 
94.1 
w2v-wiki 
94.4 
w2v-coco 
94.6 
vis-w2v-wiki 
95.1 
vis-w2v-coco 
95.3 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head></head><label></label><figDesc>Approach R@1 (%) R@5 (%) R@10 (%) med R</figDesc><table>w2v-wiki 
14.6 
34.4 
45.4 
13 
w2v-coco 
15.3 
35.2 
47.6 
11 
vis-w2v-wiki (shared) 
15.5 
37.2 
49.3 
10 
vis-w2v-coco (shared) 
15.7 
37.7 
47.6 
10 
vis-w2v-wiki (separate) 
14.0 
32.7 
43.5 
15 
vis-w2v-coco (separate) 
15.4 
37.6 
49.5 
10 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Alternatively, one could regress directly to the feature values v. However, we found that the regression objective hurts performance.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We verified empirically that this does not cause calibration issues. Specifically, given a pair of words where one word was refined using visual information but the other was not (unseen during training), using vis-w2v for the former and w2v for the latter when computing similarities between the two outperforms using w2v for both.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://code.google.com/p/word2vec/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Our implementation of<ref type="bibr" target="#b24">[24]</ref> performs 0.3% higher than that reported in<ref type="bibr" target="#b24">[24]</ref>.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">VQA: Visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Zero-shot learning via visual abstraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<title level="m">Proceedings, Part IV</title>
		<meeting>Part IV<address><addrLine>Zurich, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="401" to="416" />
		</imprint>
	</monogr>
	<note>European Conference</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jauvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">An empirical study of smoothing techniques for language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Goodman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Learning a recurrent visual representation for image caption generation. CoRR, abs/1411</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">5654</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing: Deep neural networks with multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning, ICML</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Unsupervised visual representation learning by context prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno>abs/1411.4389</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Discriminative unsupervised feature learning with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 27 (NIPS)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Predicting object dynamics in scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">F</forename><surname>Fouhey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Are you talking to a machine? dataset and methods for multilingual image question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Visual turing test for computer vision systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Geman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Geman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Hallonquist</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Younes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3618" to="3623" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Framing image description as a ranking task: Data, models and evaluation metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hodosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hockenmaier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Artif. Intell. Res. (JAIR)</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="853" to="899" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Image Specificity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep visual-semantic alignments for generating image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3128" to="3137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Estimation of probabilities from sparse data for the language model component of a speech recognizer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Katz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="1987" />
			<biblScope unit="page" from="400" to="401" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Unifying visual-semantic embeddings with multimodal neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Visual word2vec (vis-w2v): Learning visually grounded word embeddings using abstract scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M F</forename><surname>Moura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<idno>abs/1511.07067</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Baby talk: Understanding and generating image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Premraj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th CVPR</title>
		<meeting>the 24th CVPR</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Combining language and vision with a multimodal skip-gram model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">T</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Baroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Denver, Colorado</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-06" />
			<biblScope unit="page" from="153" to="163" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Don&apos;t just listen, use your imagination: Leveraging visual common sense for non-visual tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2015-11" />
		</imprint>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Nltk: The natural language toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bird</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL Workshop on Effective Tools and Methodologies for Teaching Natural Language Processing and Computational Linguistics. Philadelphia: Association for Computational Linguistics</title>
		<meeting>the ACL Workshop on Effective Tools and Methodologies for Teaching Natural Language Processing and Computational Linguistics. Philadelphia: Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Action recognition from a distributed representation of pose and appearance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">A multi-world approach to question answering about real-world scenes based on uncertain input</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
		<idno>abs/1410.0210</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Ask your neurons: A neural-based approach to answering questions about images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
		<idno>abs/1505.01121</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Explain images with multimodal recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno>abs/1410.1090</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Efficient Estimation of Word Representations in Vector Space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Neural network based language models for highly inflective languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kopecky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Glembek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cernocky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="4725" to="4728" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Distributed Representations of Words and Phrases and their Compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Midge: Generating descriptions of images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hayes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh International Natural Language Generation Conference, INLG &apos;12</title>
		<meeting>the Seventh International Natural Language Generation Conference, INLG &apos;12<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="131" to="133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning common sense through visual abstraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">B C L Z D P</forename><surname>Ramakrishna Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Image question answering: A visual semantic embedding model and a new dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<idno>abs/1505.02074</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Translating video content to natural language descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pinkal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2013-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Viske: Visual knowledge extraction and question answering by visual verification of relation phrases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sadeghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1456" to="1464" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Models of semantic representation with visual attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Silberer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Association for Computer Linguistics</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="572" to="582" />
		</imprint>
	</monogr>
	<note>ACL (1)</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning grounded meaning representations with autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Silberer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014-06" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="721" to="732" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>abs/1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Show and tell: A neural image caption generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3156" to="3164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Improving word representations via global visual context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Adopting abstract images for semantic scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Bringing semantics into focus using visual abstraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Learning the visual interpretation of sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Vanderwende</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
